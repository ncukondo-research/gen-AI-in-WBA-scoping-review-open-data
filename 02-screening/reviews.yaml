# yaml-language-server: $schema=./review.schema.json
sessionId: 20260216_genaiwbav8_0134a3
articles:
  - title: "Using AI to Train Future Clinicians in Depression Assessment: Feasibility Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments; students interacted with a GPT-4-powered chatbot simulating patients, not in a real clinical workplace."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Uses AI-simulated virtual patients for practice in a simulated setting rather than workplace-based assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.2196/87102
    pmid: "41678789"
    authors: Holderried F, Sonanini A, Philipps A, Stegemann-Philipps C, Herschbach L, Festl-Wietek T, Zipfel S, Erschens R, Herrmann-Werner A
    year: "2026"
    abstract: |-
      BACKGROUND: Depression is a major global health care challenge, causing significant individual distress but also contributing to a substantial global burden. Timely and accurate diagnosis is crucial. To help future clinicians develop these essential skills, we trained a generative pretrained transformer (GPT)-powered chatbot to simulate patients with varying degrees of depression and suicidality.

      OBJECTIVE: This study aims to evaluate the applicability and transferability of our GPT-4-powered chatbot for psychosomatic cases. Specifically, we aim to investigate how accurately the chatbot can simulate patients exhibiting various stages of depression and phases of suicidal ideation, while adhering to a predefined role script and maintaining a sufficient level of authenticity. Additionally, we want to analyze to what level the chatbot is suitable for practicing correctly diagnosing depressive disorders in patients, as well as assessing suicidality stages.

      METHODS: We developed 3 virtual patient role scripts depicting complex, realistic cases of depression and varying degrees of suicidality collaboratively with field experts and aligned with mental health assessment guidelines. These cases were integrated into a GPT-4-powered chatbot for practicing clinical history-taking. A total of 148 medical students, with an average age of 22.71 years and mostly in their sixth semester, interacted individually with one of the randomly assigned virtual patients through chat. Following this, they completed a questionnaire assessing their demographics and user experience. Chats were analyzed descriptively to assess diagnostic accuracy and suicidality assessments, as well as the role script adherence and authenticity of the artificial intelligence (AI). This was done to gain further insight into the chatbot's behavior and the students' diagnostic accuracy.

      RESULTS: In over 90% (725/778) of the answers, the chatbot maintained its assigned role. On average, students correctly identified the severity of depression in 60% (81/135) and the phase of suicidality in 67% (91/135) of the cases. Notably, the majority either failed to address or insufficiently explored the topic of suicidality despite explicit instructions beforehand.

      CONCLUSIONS: This study demonstrates that a GPT-powered chatbot can simulate patients with depression fairly accurately. More than two-thirds of participants perceived the AI-simulated patients with depression as authentic, and nearly 80% (106/135) indicated they would like to use the application for further practice, highlighting its potential as a training tool. While a small proportion of students expressed reservations, and the overall diagnostic accuracy varied depending on the severity of the case, the findings overall support the feasibility and educational value of AI-based role-playing in clinical training. AI-supported virtual patients provide a highly flexible, standardized, and readily available training tool, independent of real-life constraints.
    mergedFrom:
      - source: pubmed
        pmid: "41678789"
        doi: 10.2196/87102
    finalDecision: exclude
  - title: "Lost in the Edit: Reclaiming the Clinical Narrative in the Age of Synthetic Records."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Editorial/opinion piece, not an original research article"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Editorial/opinion article, not original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.7759/cureus.100866
    pmid: "41658700"
    authors: Lakhan S
    year: "2026"
    abstract: 'We are entering the era of the synthetic record, where ambient artificial intelligence (AI) and large language models have begun to automate the clinical encounter. While this technology promises to alleviate administrative burden, it brings a more profound risk: the erosion of explicit clinical reasoning and the human clinical narrative, with downstream consequences for patient safety, care transitions, reimbursement, auditing, and medicolegal accountability. This editorial examines the crisis of the clinical note through a cinematic lens, arguing that the modern record has evolved into a disjointed CGI blockbuster that prioritizes data-rich special effects over a coherent script. It identifies 10 distinct audiences, from patients and forensic auditors to data-mining algorithms, and demonstrates how attempts to satisfy these conflicting "studios" have fractured clinical reasoning. By introducing a taxonomy of narrative failures, including fluent but intent-poor AI-generated prose and unclosed diagnostic loops, this piece advocates for a shift toward omni-channel clinical production. In this paradigm, the clinician serves as the content originator of a single narrative synthesis that explains what was thought and why, while AI renders this core clinical intent into formats tailored for diverse stakeholders. Reclaiming the clinical note as a persuasive argument rather than a transactional receipt is not merely a professional courtesy; it is a safety imperative for the digital age.'
    mergedFrom:
      - source: pubmed
        pmid: "41658700"
        doi: 10.7759/cureus.100866
    finalDecision: exclude
  - title: "Benchmarking Open-Source Vision Language Models in Orthopedic In-Training Examination: A Comparison with Residents, Domain-Specific Evaluation, and Parameter Scaling."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks vision-language models on OITE exam questions compared to residents; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.4055/cios25183
    pmid: "41647505"
    authors: Ko S, Lee J, Ko K, Kim J
    year: "2026"
    abstract: |-
      BACKGROUD: Advancing orthopedic care through large language models requires both multimodal processing capabilities for medical images and open-source deployment options for secure in-house operations, yet these remain underexplored in current literature. This study aims to benchmark open-source vision-language models (VLMs) against orthopedic residents using the Orthopedic In-Training Examination (OITE), assess domain-specific performance across orthopedic subspecialties, and investigate the relationship between model parameter size and performance.

      METHODS: Six open-source VLMs of varying sizes (Alibaba Qwen2.5-VL-72B-Instruct, Alibaba Qwen2.5-VL-32B-Instruct, Alibaba Qwen2.5-VL-7B-Instruct, Alibaba Qwen2.5-VL-3B-Instruct, Meta Llama-3.2-90B-Vision-Instruct, Meta Llama-3.2-11B-Vision-Instruct) were evaluated using the 2023 OITE (210 questions; 111 with images). Model performance was compared to resident scores from the 2023 OITE technical report. Pearson correlation coefficient was used to assess the association between model size and performance.

      RESULTS: The 2 largest open-source models, Qwen2.5-VL-72B and Llama-3.2-90B, demonstrated performance levels comparable to those of second-year orthopedic residents on the OITE examination. A mid-sized model, Qwen-32B, slightly outscored first-year residents. In contrast, small-sized models (under 11 billion parameters) performed worse than first-year residents. Qwen2.5-VL-72B performed best in foot & ankle and sports medicine topics, while Llama-3.2-90B was strongest in basic science and hand & wrist. All models had the most difficulty with spine and pediatric questions. Overall, model accuracy increased steadily with model size up to 72 billion parameters, but larger sizes showed little additional improvement.

      CONCLUSIONS: Smaller models offer reduced accuracy in exchange for lower hardware requirements. Spine and pediatric domains remain consistently areas of underperformance across all models. Model selection should be based on domain-specific benchmark results to balance clinical needs with hardware limitations. While promising, open-source VLMs currently require further refinement and validation before they can be reliably applied in clinical or educational settings.
    mergedFrom:
      - source: pubmed
        pmid: "41647505"
        doi: 10.4055/cios25183
    finalDecision: exclude
  - title: "Effectiveness of Informed AI Use on Clinical Competence of General Practitioners and Internists: Pre-Post Intervention Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates AI-assisted performance on test-based clinical competence assessments, not workplace-based assessment"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Test-based AI-assisted competence assessments are not workplace-based assessment or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.2196/75534
    pmid: "41643188"
    authors: Qunaibi E, Al-Qaaneh A, Ismail B, Muhidat H, Rageh F, Musallam N, Fawzy A
    year: "2026"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) shows promise in clinical diagnosis, treatment support, and health care efficiency. However, its adoption in real-world practice remains limited due to insufficient clinical validation and an unclear impact on practitioners' competence. Addressing these gaps is essential for effective, confident, and ethical integration of AI into modern health care settings.

      OBJECTIVE: This study aimed to evaluate the effectiveness of informed AI use, following a tailored AI training course, on the performance of general practitioners (GPs) and internists in test-based clinical competence assessments and their attitudes toward clinical AI applications.

      METHODS: A pre-post intervention study was conducted with 326 physicians from 39 countries. Participants completed a baseline test of clinical decision-making skills, covering diagnosis, treatment planning, and patient counseling; attended a 1.5-hour online training on effective AI use; and then took a similar postcourse test with AI assistance permitted (GPT-4.0). Test performance and time per question were compared before and after the training. Participants also rated AI accuracy, efficiency, perceived need for structured AI training, and their willingness to use AI in clinical practice before and after the course.

      RESULTS: The average test scores improved from 56.9% (SD 15.7%) to 77.6% (SD 12.7%; P<.001), and the pass rate increased from 6.4% (21/326) to 58.6% (191/326), with larger gains observed among GPs and younger physicians. All skill domains (diagnosis, treatment planning, and patient counseling) improved significantly (all P<.001), while time taken to complete the test increased slightly from before to after the course (mean 40.25, SD 16.14 min vs 42.29, SD 14.02 min; P=.03). By the end of the intervention, physicians viewed AI more favorably, reporting increased confidence in its accuracy and time efficiency, greater appreciation for the need for structured AI training, and increased confidence and willingness to integrate AI into patient care.

      CONCLUSIONS: Informed use of AI, based on tailored training, was associated with higher performance in test-based clinical decision-making assessments and greater confidence in using AI among GPs and internists. Building on previous research that often lacked structured training, focused primarily on model performance, or was limited in clinical scope, this study provides empirical evidence of both competence and perceptual improvement following informed AI use in a large, multinational cohort, enhancing the generalizability. These findings support the integration of structured AI training into medical education and continuing professional development to improve clinical performance and promote competent use of AI in clinical practice.
    mergedFrom:
      - source: pubmed
        pmid: "41643188"
        doi: 10.2196/75534
    finalDecision: exclude
  - title: "Towards accurate and interpretable competency-based assessment: enhancing clinical competency assessment through multimodal AI and anomaly detection."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Multimodal AI anomaly detection framework for competency assessment of anesthesia residents explicitly in simulation-based education; assessment entirely in simulated environment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Competency assessment is explicitly simulation-based rather than workplace-based clinical assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1038/s41746-025-02299-2
    pmid: "41634143"
    authors: Gershov S, Mahameed F, Raz A, Laufer S
    year: "2026"
    abstract: Artificial Intelligence (AI) is reshaping medical education, particularly in the domain of competency-based assessment, where current methods remain subjective and resource-intensive. We introduce a multimodal AI framework that integrates video, audio, and patient monitor data to provide objective and interpretable competency assessments. Using 90 anesthesia residents, we established "ideal" performance benchmarks and trained an anomaly detection model (MEMTO) to quantify deviations from these benchmarks. Competency scores derived from these deviations showed strong alignment with expert ratings (Spearman's ρ = 0.78; ICC = 0.75) and demonstrated high ranking precision (Relative L2-distance = 0.12). SHAP analysis revealed that communication and eye contact with the patient monitor are key drivers of variability. By linking AI-assisted anomaly detection with interpretable feedback, our framework addresses critical challenges of fairness, reliability, and transparency in simulation-based education. This work provides actionable evidence for integrating AI into medical training and advancing scalable, equitable evaluation of competence.
    mergedFrom:
      - source: pubmed
        pmid: "41634143"
        doi: 10.1038/s41746-025-02299-2
    finalDecision: exclude
  - title: "Artificial Intelligence Augmented Competence Committees: A Collaborative Path Forward in Competency-Based Medical Education."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Appears to be a perspective/commentary article rather than eligible study design"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: This is a viewpoint article (opinion/commentary), not an original research article, systematic review, meta-analysis, or preprint. No original data were collected or analyzed; the Data Availability Statement confirms no datasets were generated."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:53.576000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Viewpoint article (not original research, systematic review, meta-analysis, or preprint)."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:01.509000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Viewpoint article, not original research/SR/MA."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1111/tct.70358
    pmid: "41631742"
    authors: Yasin N, Bilgic E, Zubairi M
    year: "2026"
    mergedFrom:
      - source: pubmed
        pmid: "41631742"
        doi: 10.1111/tct.70358
      - source: scopus
        doi: 10.1111/tct.70358
        scopusId: SCOPUS_ID:105029278405
    finalDecision: exclude
  - title: "Mapping the landscape of AI-assisted formative feedback in medical education: A bibliometric analysis."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Bibliometric analysis is not original research, systematic review, or meta-analysis"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Bibliometric analysis, not original research, systematic review, or meta-analysis."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on broad bibliometric trends in AI-assisted formative feedback, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.1097/MD.0000000000047489
    pmid: "41630316"
    authors: Yu S, Liu J
    year: "2026"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) is transforming medical education, particularly in formative feedback. This study conducted a comprehensive bibliometric analysis to map the intellectual landscape, research trends, and future directions of AI-assisted formative feedback in medical education.

      METHODS: A systematic search was performed in the Web of Science Core Collection database for English-language articles published between January 1, 2021, and October 12, 2025. Bibliometric analysis was conducted using VOSviewer, CiteSpace, and the R-bibliometrix package to analyze publication trends, geographic distribution, institutional collaboration, journal impact, author contributions, co-citation patterns, and keyword occurrences.

      RESULTS: The analysis included 116 publications, revealing exponential growth in AI-assisted formative feedback research from 2021 to 2025. The United States dominated the research landscape, followed by China and other European nations. Institutional collaboration centered on the University of Michigan, connecting North American and Asian research clusters. BMC Medical Education emerged as the leading journal, while interdisciplinary knowledge flow originated from clinical medicine and drew on health sciences and educational psychology. Keyword bursts identified "feedback," "large language model," and "medical education" as the most prominent research hotspots in 2024 to 2025.

      CONCLUSION: AI-assisted formative feedback in medical education is a rapidly evolving field driven by advancements in large language models, immersive technologies, and personalized assessments. Future research should prioritize theory-informed, ethically grounded, and patient-oriented AI integration to augment human instruction and demonstrably improve learner competence and patient care. Increased international collaboration and interdisciplinary knowledge exchange are crucial for the responsible adoption of AI in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "41630316"
        doi: 10.1097/MD.0000000000047489
    finalDecision: exclude
  - title: "Innovative Application of Chatbots in Clinical Nutrition Education: The E+DIEting_Lab Experience in University Students."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting (university classroom context)."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments; virtual patient chatbot platform for clinical nutrition training, not real workplace-based assessment"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Chatbot-supported training study not centered on WBA assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.3390/nu18020257
    pmid: "41599870"
    authors: El&#xed;o I, Tutusaus K, Eguren-Garc&#xed;a I, Lasarte-Garc&#xed;a &, Ortega-Mansilla A, Prola T, Sumalla-Cano S
    year: "2026"
    abstract: "Background/Objectives: The growing integration of Artificial Intelligence (AI) and chatbots in health professional education offers innovative methods to enhance learning and clinical preparedness. This study aimed to evaluate the educational impact and perceptions in university students of Human Nutrition and Dietetics, regarding the utility, usability, and design of the E+DIEting_Lab chatbot platform when implemented in clinical nutrition training. Methods: The platform was piloted from December 2023 to April 2025 involving 475 students from multiple European universities. While all 475 students completed the initial survey, 305 finished the follow-up evaluation, representing a 36% attrition rate. Participants completed surveys before and after interacting with the chatbots, assessing prior experience, knowledge, skills, and attitudes. Data were analyzed using descriptive statistics and independent samples t-tests to compare pre- and post-intervention perceptions. Results: A total of 475 university students completed the initial survey and 305 the final evaluation. Most university students were females (75.4%), with representation from six languages and diverse institutions. Students reported clear perceived learning gains: 79.7% reported updated practical skills in clinical dietetics and communication were improved, 90% felt that new digital tools improved classroom practice, and 73.9% reported enhanced interpersonal skills. Self-rated competence in using chatbots as learning tools increased significantly, with mean knowledge scores rising from 2.32 to 2.66 and skills from 2.39 to 2.79 on a 0-5 Likert scale (p < 0.001 for both). Perceived effectiveness and usefulness of chatbots as self-learning tools remained positive but showed a small decline after use (effectiveness from 3.63 to 3.42; usefulness from 3.63 to 3.45), suggesting that hands-on experience refined, but did not diminish, students' overall favorable views of the platform. Conclusions: The implementation and pilot evaluation of the E+DIEting_Lab self-learning virtual patient chatbot platform demonstrate that structured digital simulation tools can significantly improve perceived clinical nutrition competences. These findings support chatbot adoption in dietetics curricula and inform future digital education innovations."
    mergedFrom:
      - source: pubmed
        pmid: "41599870"
        doi: 10.3390/nu18020257
    finalDecision: exclude
  - title: Artificial intelligence, extended reality, and emerging AI-XR integrations in medical education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Broad systematic review of AI/XR in medical education covering procedural training, clinical decision-making, and communication skills; not specifically about assessment, feedback, or analysis of observation records in WBA"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Reviews broad AI/XR interventions and is not specifically about generative AI/LLMs for WBA-related assessment tasks."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.3389/fdgth.2025.1740557
    pmid: "41586206"
    authors: Tene T, Vique L&#xf3;pez D, Garc&#xed;a Veloz M, Rojas Oviedo B, Tene-Fernandez R
    year: "2025"
    abstract: |-
      INTRODUCTION: Artificial intelligence (AI) and extended reality (XR)-including virtual, augmented, and mixed reality-are increasingly adopted in health-professions education. However, the educational impact of AI, XR, and especially their combined use within integrated AI-XR ecosystems remains incompletely characterized.

      OBJECTIVE: To synthesize empirical evidence on educational outcomes and implementation considerations for AI-, XR-, and combined AI-XR-based interventions in medical and health-professions education.

      METHODS: Following PRISMA and PICO guidance, we searched three databases (Scopus, PubMed, IEEE Xplore) and screened records using predefined eligibility criteria targeting empirical evaluations in health-professions education. After deduplication (336 records removed) and two-stage screening, 13 studies published between 2019 and 2024 were included. Data were extracted on learner population, clinical domain, AI/XR modality, comparators, outcomes, and implementation factors, and narratively synthesized due to heterogeneity in designs and measures.

      RESULTS: The 13 included studies involved undergraduate and postgraduate learners in areas such as procedural training, clinical decision-making, and communication skills. Only a minority explicitly integrated AI with XR within the same intervention; most evaluated AI-based or XR-based approaches in isolation. Across this mixed body of work, studies more often than not reported gains in at least one outcome-knowledge or skills performance, task accuracy, procedural time, or learner engagement-relative to conventional instruction, alongside generally high acceptability. Recurrent constraints included costs, technical reliability, usability, faculty readiness, digital literacy, and data privacy and ethics concerns.

      CONCLUSIONS: Current evidence on AI, XR, and emerging AI-XR integrations suggests promising but preliminary benefits for learning and performance. The small number of fully integrated AI-XR interventions and the methodological limitations of many primary studies substantially limit the certainty and generalizability of these findings. Future research should use more rigorous and standardized designs, explicitly compare AI-only, XR-only, and AI-XR hybrid approaches, and be coupled with faculty development, robust technical support, and alignment with competency-based assessment.
    mergedFrom:
      - source: pubmed
        pmid: "41586206"
        doi: 10.3389/fdgth.2025.1740557
      - source: scopus
        doi: 10.3389/fdgth.2025.1740557
        scopusId: SCOPUS_ID:105028558413
    finalDecision: exclude
  - title: "Re-evaluating the role of personal statements in pediatric residency admissions in the era of artificial intelligence: comparing faculty ratings of human and AI-generated statements."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About residency admissions personal statements, not assessment/feedback in WBA"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Admissions personal statement evaluation is not a workplace-based or clinical education assessment context."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    doi: 10.36834/cmej.81345
    pmid: "41584952"
    authors: Curry B, Kirpalani A, Remington M, Van Hooren T, Shen Y, Peebles E
    year: "2025"
    abstract: |-
      BACKGROUND: Personal statements play a large role in pediatric residency applications, providing insights into candidates' motivations, experiences, and fit for the program. With large language models (LLMs) such as Chat Generative Pre-trained Transformer (ChatGPT), concerns have arisen regarding how this may influence the authenticity of statements in evaluating candidates. This study investigates the efficacy and perceived authenticity of LLM-generated personal statements compared to human-generated statements in residency applications.

      METHODS: We conducted a blinded study comparing 30 ChatGPT-generated personal statements with 30 human-written statements. Four pediatric faculty raters assessed each statement using a standardized 10-point rubric. We analyzed the data using linear mixed-effects models, a chi-square sensitivity analysis, an evaluation of rater accuracy in identifying statement origin as well as consistency of scores amongst raters using intraclass correlation coefficients (ICC).

      RESULTS: There was no significant difference in mean scores between AI and human-written statements. Raters could only identify the source of a letter (AI or human) with 59% accuracy. There was considerable disagreement in scores between raters as indicated by negative ICCs.

      CONCLUSIONS: AI-generated statements were rated similarly to human-authored statements and were indistinguishable by reviewers, highlighting the sophistication of these LLM models and the challenge in detecting their use. Furthermore, scores varied substantially between reviewers. As AI becomes increasingly used in application processes, it is imperative to examine its implications in the overall evaluation of applicants.
    mergedFrom:
      - source: pubmed
        pmid: "41584952"
        doi: 10.36834/cmej.81345
    finalDecision: exclude
  - title: "OSCEai dermatology: augmenting dermatologic medical education with Large Language Model GPT-4."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: OSCE-based assessment is a simulated environment"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: LLM-driven OSCE-like dermatology role-play tool; assessment conducted in simulated environment, not workplace-based."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: OSCE-like role-play training/feedback is simulation-based and lacks a workplace-based assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.36834/cmej.80056
    pmid: "41584944"
    authors: Park Y, Guo E, Sachdeva M, Ma B, Mirali S, Rankin B, Nathanielsz N, Abduelmula A, Lapa T, Gupta M, Champagne T
    year: "2025"
    abstract: OSCEai Dermatology demonstrates how large language models (LLMs) like GPT-4 can be integrated into medical education to enhance trainees' history taking and management skills in an OSCE-like format, including in visual-based specialties like dermatology. By generating diverse, realistic skin cancer role-play scenarios across different skin tones alongside the integration of pre-existing, evidence-based images, the app provides learners with valuable, personalized feedback. This innovation offers a novel, interactive learning tool that supplements traditional teaching methods and can be applied across various specialties. Institutions can adopt or adapt similar LLM-driven educational tools to introduce trainees to a wider range of clinical cases, fostering improved diagnostic skills and patient-centred, culturally sensitive care.
    mergedFrom:
      - source: pubmed
        pmid: "41584944"
        doi: 10.36834/cmej.80056
    finalDecision: exclude
  - title: "Investigating the threat of AI to undergraduate medical school admissions: a study of its potential impact on the rating of applicant essays."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; focuses on medical school admissions essays"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    doi: 10.36834/cmej.79690
    pmid: "41584942"
    authors: Choi J, Zhao J, Ngo T, Grierson L
    year: "2025"
    abstract: |-
      BACKGROUND: Medical school applications often require short written essays or personal statements, which are purportedly used to assess professional qualities related to the practice of medicine. With generative artificial intelligence (AI) tools capable of supplementing or replacing inputs by human applicants, concerns about how these tools impact written assessments are growing. This study explores how AI influences the ratings of essays used for medical school admissions.

      METHODS: A within-subject experimental design was employed. Eight participants (academic clinicians, faculty researchers, medical students, and a community member) rated essays written by 24 undergraduate students and recent graduates from McMaster University. The students were divided into four groups: medical school aspirants with AI assistance (ASP-AI), aspirants without AI assistance (ASP), non-aspirants with AI assistance (NASP-AI), and essays generated solely by ChatGPT 3.5 (AI-ONLY). Participants were provided training in the application of single Likert scale tool before rating. Differences in ratings by writer group were determined via one-way between group ANOVA.

      RESULTS: Analyses revealed no statistically significant differences in ratings across the four writer groups (p = .358). The intraclass correlation coefficient was .147.

      CONCLUSION: The proliferation of AI adds to prevailing questions about the value personal statements and essays have in supporting applicant selection. We speculate that these assessments hold less value than ever in providing authentic insight into applicant attributes. In this context, we suggest that medical schools move away from the use of essays in their admissions processes.
    mergedFrom:
      - source: pubmed
        pmid: "41584942"
        doi: 10.36834/cmej.79690
    finalDecision: exclude
  - title: "OSCEai: personalized interactive learning for undergraduate medical education."
    reviews:
      - decision: uncertain
        comment: AI for OSCE learning; abstract needed to determine if involves WBA or assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: LLM-based platform simulates clinical encounters for OSCE preparation; assessment occurs entirely in a simulated environment, not workplace-based."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:51.586000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment/training is conducted in simulated OSCE-style encounters only."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:58.822000+00:00
    doi: 10.36834/cmej.79220
    pmid: "41584941"
    authors: Guo E, Ramchandani R, Park Y, Gupta M
    year: "2025"
    abstract: |-
      BACKGROUND: This study aims to evaluate the effectiveness of the OSCEai, a large language model-based platform that simulates clinical encounters, in enhancing undergraduate medical education.

      METHODS: A web-based application, OSCEai, was developed to bridge theoretical and practical learning. Following use, medical students from the University of Calgary Class of 2026 completed an anonymized survey on the usability, utility, and overall experience of OSCEai.

      RESULTS: A total of 37 respondents answered the anonymized survey. The OSCEai platform was highly valued for its ability to provide data on demand (33/37), support self-paced learning (30/37), and offer realistic patient interactions (29/37). The ease of use and medical content quality were rated at 4.73 (95% CI: 4.58 to 4.88) and 4.70 (95% CI: 4.55 to 4.86) out of 5, respectively. Some participants (8/37) commented that few cases were not representative and needed clarification about app functionality. Despite these limitations, OSCEai was favorably compared to lecture-based teaching methods, with an overall reception rating of 4.62 (95% CI: 4.46 to 4.79) out of 5.

      INTERPRETATION: The OSCEai platform fills a gap in medical training through its scalable, interactive, and personalized design. The findings suggest that integrating technologies, like OSCEai, into medical curricula can enhance the quality and efficacy of medical education.
    mergedFrom:
      - source: pubmed
        pmid: "41584941"
        doi: 10.36834/cmej.79220
    finalDecision: exclude
  - title: "Performance of DeepSeek and ChatGPT on the Chinese Health Professional and Technical Examination: A comparative study."
    reviews:
      - decision: uncertain
        comment: LLMs on health professional exams; abstract may reveal WBA-relevant elements
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates LLM performance on a Chinese nursing licensing examination"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Nursing licensing-exam performance comparison, not workplace-based assessment/feedback or clinical observation record analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.1371/journal.pone.0338328
    pmid: "41569998"
    authors: Li X, Hu X, Xu H, Sun Z, Yu P, Ju H
    year: "2026"
    abstract: |-
      BACKGROUND: Large language models (LLMs) are increasingly applied in medical education, yet their reliability in specialized, high-stakes assessments such as the Chinese Health Professional and Technical Examination remains unclear. DeepSeek-R1, a recently released reasoning-enhanced LLM, has shown promising performance, but empirical evidence within nursing examination contexts is limited.

      OBJECTIVE: To compare the performance of DeepSeek-R1 and the GPT-4o API on the Chinese Health Professional and Technical Examination (Intermediate Nursing), focusing on accuracy, response consistency, and consistent accuracy.

      METHODS: Four hundred official practice examination multiple-choice questions were categorized into four competency units and two question types (A/B). Both models were evaluated using overall accuracy, consistency (agreement across repeated responses), and consistent accuracy (proportion of responses that were both consistent and correct). Stratified analyses were performed across units, question types, and disciplines. Chi-square tests were used for statistical comparison, and Holm-Bonferroni correction was applied for multiple comparisons.

      RESULTS: DeepSeek-R1 demonstrated significantly higher overall accuracy than the GPT-4o API (88.5% vs. 67.9%, P < 0.001). GPT-4o API showed higher response consistency (96.5% vs. 88.5%) but lower consistent accuracy (66.7% vs. 84.0%). After multiple-comparison correction, significant differences in consistent accuracy remained in basic knowledge, professional knowledge, professional practice ability and Type A questions, as well as in surgical and gynecological nursing disciplines, while other domains showed no statistically significant differences.

      CONCLUSION: DeepSeek-R1 outperformed the GPT-4o API across multiple dimensions of nursing competency assessment, particularly in overall accuracy and consistent accuracy. GPT-4o API exhibited high response stability but a tendency toward systematic errors, underscoring the need for careful interpretation of model outputs. Further research is needed to evaluate LLM performance using open-ended clinical reasoning tasks and real-world assessment data to support safe and effective educational integration.
    mergedFrom:
      - source: pubmed
        pmid: "41569998"
        doi: 10.1371/journal.pone.0338328
    finalDecision: exclude
  - title: "The impact of artificial intelligence-driven simulation on the development of non-technical skills in medical education: a systematic review."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted entirely in simulated environments"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments. Systematic review of AI-driven simulation for non-technical skills development, not workplace-based assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted in AI-driven simulation settings without a workplace-based component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.3352/jeehp.2025.22.37
    pmid: "41566844"
    authors: Loubbairi S, Moussaoui Y, Lahlou L, Chakri I, Nassik H
    year: "2025"
    abstract: |-
      PURPOSE: Artificial intelligence (AI)-driven simulation is an emerging approach in healthcare education that enhances learning effectiveness. This review examined its impact on the development of non-technical skills among medical learners.

      METHODS: Following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines, a systematic review was conducted using the following databases: Web of Science, ScienceDirect, Scopus, and PubMed. The quality of the included studies was assessed using the Mixed.

      METHODS: Appraisal Tool. The protocol was previously registered in PROSPERO (CRD420251038024).

      RESULTS: Of the 1,442 studies identified in the initial search, 20 met the inclusion criteria, involving 2,535 participants. The simulators varied considerably, ranging from platforms built on symbolic AI methods to social robots powered by computational AI. Among the 15 AI-driven simulators, 10 used ChatGPT or its variants as virtual patients. Several studies evaluated multiple non-technical skills simultaneously. Communication and clinical reasoning were the most frequently assessed skills, appearing in 12 and 6 studies, respectively, which generally reported positive outcomes. Improvements were also noted in decision-making, empathy, self-confidence, critical thinking, and problem-solving. In contrast, emotional regulation, assessed in a single study, showed no significant difference. Notably, none of the studies examined reflection, reflective practice, teamwork, or leadership.

      CONCLUSION: AI-driven simulation shows substantial potential for enhancing non-technical skills in medical education, particularly communication and clinical reasoning. However, its effects on several other non-technical skills remain unclear. Given heterogeneity in study designs and outcome measures, these findings should be interpreted cautiously. These considerations highlight the need for further research to support integrating this innovative approach into medical curricula.
    mergedFrom:
      - source: pubmed
        pmid: "41566844"
        doi: 10.3352/jeehp.2025.22.37
    finalDecision: exclude
  - title: "AI-Driven Objective Structured Clinical Examination Generation in Digital Health Education: Comparative Analysis of Three GPT-4o Configurations."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: About OSCE generation (simulated assessment environment), not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Focuses on OSCE generation in simulated assessment context."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.2196/82116
    pmid: "41539673"
    authors: Zouakia Z, Logak E, Szymczak A, Jais J, Burgun A, Tsopra R
    year: "2026"
    abstract: |-
      BACKGROUND: Objective Structured Clinical Examinations (OSCEs) are used as an evaluation method in medical education, but require significant pedagogical expertise and investment, especially in emerging fields like digital health. Large language models (LLMs), such as ChatGPT (OpenAI), have shown potential in automating educational content generation. However, OSCE generation using LLMs remains underexplored.

      OBJECTIVE: This study aims to evaluate 3 GPT-4o configurations for generating OSCE stations in digital health: (1) standard GPT with a simple prompt and OSCE guidelines; (2) personalized GPT with a simple prompt, OSCE guidelines, and a reference book in digital health; and (3) simulated-agents GPT with a structured prompt simulating specialized OSCE agents and the digital health reference book.

      METHODS: Overall, 24 OSCE stations were generated across 8 digital health topics with each GPT-4o configuration. Format compliance was evaluated by one expert, while educational content was assessed independently by 2 digital health experts, blind to GPT-4o configurations, using a comprehensive assessment grid. Statistical analyses were performed using Kruskal-Wallis tests.

      RESULTS: Simulated-agents GPT performed best in format compliance and most content quality criteria, including accuracy (mean 4.47/5, SD 0.28; P=.01) and clarity (mean 4.46/5, SD 0.52; P=.004). It also had 88% (14/16) for usability without major revisions and first-place preference ranking, outperforming the other configurations. Personalized GPT showed the lowest format compliance, while standard GPT scored lowest for clarity and educational value.

      CONCLUSIONS: Structured prompting strategies, particularly agents' simulation, enhance the reliability and usability of LLM-generated OSCE content. These results support the use of artificial intelligence in medical education, while confirming the need for expert validation.
    mergedFrom:
      - source: pubmed
        pmid: "41539673"
        doi: 10.2196/82116
    finalDecision: exclude
  - title: "Evaluation of DeepSeek-R1 and ChatGPT-4o on the Chinese national medical licensing examination: a multi-year comparative study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: National medical licensing examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks DeepSeek-R1 and ChatGPT-4o on Chinese medical licensing exam; not about using AI for assessment or feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Medical licensing exam benchmark study, not workplace-based assessment feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.1038/s41598-025-31874-6
    pmid: "41526606"
    authors: Wang X, Long Z, Zhu B, Cao Y, Tang H, He K, Zhang S
    year: "2026"
    abstract: Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and reasoning. However, their real-world applicability in high-stakes medical assessments remains underexplored, particularly in non-English contexts. This study aims to evaluate the performance of DeepSeek-R1 and ChatGPT-4o on the Chinese National Medical Licensing Examination (NMLE), a comprehensive benchmark of medical knowledge and clinical reasoning. We evaluated the performance of ChatGPT-4o and DeepSeek-R1 on the Chinese National Medical Licensing Examination (2019-2021) using question-level binary accuracy (correct = 1, incorrect = 0) as the outcome. A generalized linear mixed model (GLMM) with a binomial distribution and logit link was used to examine fixed effects of model type, year, and subject unit, including their interactions, while accounting for random intercepts across questions. Post hoc pairwise comparisons were conducted to assess differences across model-year interactions. DeepSeek-R1 significantly outperformed ChatGPT-4o overall (β = - 1.829, p < 0.001). Temporal analysis revealed a significant decline in ChatGPT-4o's accuracy from 2019 to 2021 (p < 0.05), whereas DeepSeek-R1 appeared to maintain a more stable performance. Subject-wise, Unit 3 showed the highest accuracy (β = 0.344, p = 0.001) compared to Unit 1. A significant interaction in 2020 (β = - 0.567, p = 0.009) indicated an amplified performance gap between the two models. These results highlight the importance of model selection and domain adaptation. Further investigation is needed to account for potential confounding factors, such as variations in question difficulty or language biases over time, which could also influence these trends. This longitudinal evaluation highlights the potential and limitations of LLMs in medical licensing contexts. While current models demonstrate promising results, further fine-tuning is necessary for clinical applicability. The NMLE offers a robust benchmark for future development of trustworthy AI-assisted medical decision support tools in non-English settings.
    mergedFrom:
      - source: pubmed
        pmid: "41526606"
        doi: 10.1038/s41598-025-31874-6
    finalDecision: exclude
  - title: "GPT-4o and OpenAI o1 Performance on the 2024 Spanish Competitive Medical Specialty Access Examination: Cross-Sectional Quantitative Evaluation Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates LLM performance on the Spanish MIR national medical licensing exam, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: National specialty access exam benchmarking study, not workplace-based assessment in clinical settings."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.2196/75452
    pmid: "41525685"
    authors: Benito P, Isla-Jover M, Gonz&#xe1;lez-Castro P, Fern&#xe1;ndez Esparcia P, Carpio M, Blay-Sim&#xf3;n I, Guti&#xe9;rrez-Bedia P, Lapastora M, Carratal&#xe1; B, Carazo-Casas C
    year: "2026"
    abstract: |-
      BACKGROUND: In recent years, generative artificial intelligence and large language models (LLMs) have rapidly advanced, offering significant potential to transform medical education. Several studies have evaluated the performance of chatbots on multiple-choice medical examinations.

      OBJECTIVE: The study aims to assess the performance of two LLMs-GPT-4o and OpenAI o1-on the Médico Interno Residente (MIR) 2024 examination, the Spanish national medical test that determines eligibility for competitive medical specialist training positions.

      METHODS: A total of 176 questions from the MIR 2024 examination were analyzed. Each question was presented individually to the chatbots to ensure independence and prevent memory retention bias. No additional prompts were introduced to minimize potential bias. For each LLM, response consistency under verification prompting was assessed by systematically asking, "Are you sure?" after each response. Accuracy was defined as the percentage of correct responses compared to the official answers provided by the Spanish Ministry of Health. It was assessed for GPT-4o, OpenAI o1, and, as a benchmark, for a consensus of medical specialists and for the average MIR candidate. Subanalyses included performance across different medical subjects, question difficulty (quintiles based on the percentage of examinees correctly answering each question), and question types (clinical cases vs theoretical questions; positive vs negative questions).

      RESULTS: Overall accuracy was 89.8% (158/176) for GPT-4o and 90% (160/176) after verification prompting, 92.6% (163/176) for OpenAI o1 and 93.2% (164/176) after verification prompting, 94.3% (166/176) for the consensus of medical specialists, and 56.6% (100/176) for the average MIR candidate. Both LLMs and the consensus of medical specialists outperformed the average MIR candidate across all 20 medical subjects analyzed, with ≥80% LLMs' accuracy in most domains. A performance gradient was observed: LLMs' accuracy gradually declined as question difficulty increased. Slightly higher accuracy was observed for clinical cases compared to theoretical questions, as well as for positive questions compared to negative ones. Both models demonstrated high response consistency, with near-perfect agreement between initial responses and those after the verification prompting.

      CONCLUSIONS: These findings highlight the excellent performance of GPT-4o and OpenAI o1 on the MIR 2024 examination, demonstrating consistent accuracy across medical subjects and question types. The integration of LLMs into medical education presents promising opportunities and is likely to reshape how students prepare for licensing examinations and change our understanding of medical education. Further research should explore how the wording, language, prompting techniques, and image-based questions can influence LLMs' accuracy, as well as evaluate the performance of emerging artificial intelligence models in similar assessments.
    mergedFrom:
      - source: pubmed
        pmid: "41525685"
        doi: 10.2196/75452
    finalDecision: exclude
  - title: "From Lecture Slides to Personalized Assessments: ChatGPT-Driven Digestive Pathology Questions for Targeted Learning."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT generating pathology questions from lecture slides, classroom-based not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Title suggests classroom/course assessment context rather than workplace-based clinical setting."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.1111/pin.70083
    pmid: "41508759"
    authors: Apornvirat S, Pisutpunya A, Atiroj N, Laohawetwanit T
    year: "2026"
    abstract: "This study evaluated ChatGPT's ability to generate clinically integrated multiple-choice questions (MCQs) directly from digestive pathology lecture slides and assessed the educational quality of the output using expert review, psychometric analysis, and student feedback. Lecture slides covering 30 digestive diseases were uploaded to ChatGPT-4o using a structured prompt to produce 30 MCQs, each with a clinical vignette, five answer choices, and explanations. Three pathologists and a gastrointestinal and hepatobiliary pathologist evaluated question quality. Psychometric analysis was conducted using difficulty index, discrimination index, and distractor quality based on responses from 31 s-year medical students. Reviewer and student feedback were also analyzed. Of the 30 MCQs, 28 (93.3%) were rated as excellent by all pathologists. Minor issues such as factual inaccuracies or overly leading clues were found in 11 questions (36.7%) and corrected. Most questions showed acceptable psychometric properties: 23 items (76.7%) met difficulty index thresholds, and 28 (93.3%) met discrimination index thresholds. Of 120 distractors, 54 (45%) were functional. Students reported improved clarity, relevance, and clinical integration. ChatGPT can generate accurate, clinically relevant MCQs aligned with lecture material. With expert review, this approach provides a practical and scalable tool for pathology education in resource- and time-limited settings."
    mergedFrom:
      - source: pubmed
        pmid: "41508759"
        doi: 10.1111/pin.70083
    finalDecision: exclude
  - title: "Assessment of ChatGPT-5 as an Artificial Intelligence Tool for Exploring Emerging Dimensions of Clinical Simulation: A Proof-of-concept Study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: Explicitly about clinical simulation, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1007/s10916-025-02334-5
    pmid: "41507587"
    authors: Rios-Garcia W, Silva-Jim&#xe9;nez S, G&#xe1;lvez-Rodr&#xed;guez E, Alberca-Naira Y, Via-Y-Rada-Torres A, Rios-Garcia A
    year: "2026"
    abstract: Artificial intelligence (AI) and large language models (LLMs) such as ChatGPT-5 are increasingly applied in medical education. However, their potential role in clinical simulation remains largely unexplored. This descriptive proof-of-concept study aimed to examine ChatGPT-5's ability to synthesize and generate educational content related to clinical simulation, focusing on the coherence, factual accuracy, and understandability of its outputs. Seven exploratory questions covering conceptual, historical, and technological aspects of clinical simulation were submitted to ChatGPT-5. Each query was regenerated three times to assess consistency. Responses were independently evaluated by multiple reviewers using a five-point Likert scale for content quality and accuracy, and the Patient Education Materials Assessment Tool (PEMAT) for understandability. Authenticity of AI-generated references was verified through PubMed and Google Scholar. ChatGPT-5 produced coherent and organized responses reflecting major milestones and trends in clinical simulation. Approximately 80% of cited references were verifiable, while some inconsistencies indicated residual fabrication. The average agreement score for accuracy and coherence was 4 ("agree"), suggesting generally acceptable quality. PEMAT analysis showed that content was structured and clear but occasionally used complex terminology, limiting accessibility. Within the exploratory scope of this proof-of-concept study, ChatGPT-5 demonstrated potential as a supportive tool for synthesizing information about clinical simulation. Nonetheless, interpretive depth, citation reliability, and pedagogical adaptation require further refinement. Future research should assess the integration of LLMs into immersive simulation environments under robust ethical and educational frameworks.
    mergedFrom:
      - source: pubmed
        pmid: "41507587"
        doi: 10.1007/s10916-025-02334-5
      - source: scopus
        doi: 10.1007/s10916-025-02334-5
        scopusId: SCOPUS_ID:105027038684
    finalDecision: exclude
  - title: Can artificial intelligence chatbots think like dentists? A comparative analysis based on dental specialty examination questions in restorative dentistry.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Dental specialty examination questions are standardized exams, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:20.264000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Specialty examination question analysis is not clearly workplace-based or clinical education context."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:21.767000+00:00
    doi: 10.1186/s12903-025-07612-9
    pmid: "41495725"
    authors: Haberal M, Han&#xe7;erlio&#x11f;ullar&#x131; D
    year: "2026"
    abstract: |-
      BACKGROUND: The integration of artificial intelligence (AI) in healthcare and medical education has advanced rapidly, with conversational AI systems gaining attention for their potential in academic assessment and clinical reasoning. This study aimed to evaluate AI chatbots' performance on restorative dentistry questions from the Turkish Dental Specialty Examination (DUS), a high-stakes national exam assessing theoretical and clinical knowledge.

      METHODS: An in silico, cross-sectional, comparative design was employed. A total of 190 multiple-choice questions (MCQs) from 19 DUS sessions between 2012 and 2025 were obtained from the Assessment, Selection, and Placement Center (ÖSYM) website. After excluding annulled items, 188 questions were analyzed. Eight AI chatbots (ChatGPT-3.5, ChatGPT-4o Free, ChatGPT-4o Plus, Claude Sonnet 4, Microsoft Copilot, DeepSeek, Gemini 1.5, and Gemini Advanced) were tested using a standardized single-attempt protocol in Turkish. Performance measures included accuracy, response length, and response time. Questions were categorized by year, content domain, and length for subgroup analyses. Statistical analyses were conducted in Python using standard libraries. Descriptive statistics and Pearson's correlation were calculated, while comparisons involved the Shapiro-Wilk test, Levene's test, Kruskal-Wallis test, and Dunn's post hoc test, with significance set at p < 0.05.

      RESULTS: No significant difference was found in overall accuracy (p = 0.18). However, response time and word count differed significantly (p < 0.001). Gemini Advanced showed the highest accuracy (96.28%), followed by ChatGPT-4o Plus (93.62%). Gemini 1.5 produced the longest yet fastest responses, while DeepSeek had the lowest accuracy and slowest responses. Accuracy remained stable across years but varied by topic, with lower performance in complex areas such as cavity preparation. In case-based questions, Gemini Advanced, Gemini 1.5, and ChatGPT-4o Plus achieved 100% accuracy. Performance in image-based questions was inconsistent, underscoring limitations in visual reasoning.

      CONCLUSIONS: AI chatbots demonstrated high accuracy in answering restorative dentistry exam questions, with Gemini Advanced, ChatGPT-4o Plus, and Gemini 1.5 showing superior performance. Despite differences in response time and content length, their potential as supplementary tools in dental education is evident, warranting further validation across specialties and contexts.

      TRIAL REGISTRATION: Not applicable.
    mergedFrom:
      - source: pubmed
        pmid: "41495725"
        doi: 10.1186/s12903-025-07612-9
    finalDecision: exclude
  - title: "Comparing AI-Assisted Problem-Solving Ability With Internet Search Engine and e-Books in Medical Students With Variable Prior Subject Knowledge: Cross-Sectional Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Compares student MCQ performance using AI vs other tools in a classroom/exam setting; not in workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares tools for solving MCQs, not assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.2196/81264
    pmid: "41493542"
    authors: Xavier A, Naeem S, Rizwi W, Rabha H
    year: "2026"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI), particularly large language models (LLMs) such as ChatGPT (OpenAI), is rapidly influencing medical education. Its effectiveness for students with varying levels of prior knowledge remains underexplored.

      OBJECTIVE: This study aimed to evaluate the performance of medical students with and without formal pharmacology knowledge when using AI-LLM GPTs, internet search engines, e-books, or self-knowledge to solve multiple-choice questions (MCQs).

      METHODS: A cross-sectional study was conducted at a tertiary care teaching hospital with 100 medical students, divided into a "naive" group (n=50; no pharmacology training) and a "learned" group (n=50; completed pharmacology training). The study was started after approval from the Institutional Ethics Committee of Jawaharlal Nehru Medical College Hospital, Aligarh Muslim University (1018/IEC/23/8/23). Each participant answered 4 sets of 20 MCQs using self-knowledge, e-books, Google, or ChatGPT-4o. Scores were compared using analysis of covariance with self-knowledge scores as a covariate.

      RESULTS: Learned students significantly outperformed naive students across all methods (P<.001), with the largest effect size in the AI-LLM GPT set (partial η²=0.328). For both groups, the performance hierarchy was AI-LLM GPT > internet search engine > self-knowledge ≈ e-books. Notably, the naive students who used AI scored higher (mean 13.24, SD 3.31) than the learned students who used Google (mean 12.14, SD 2.01; P=.01) or e-books (mean 10.22, SD 3.12; P<.001).

      CONCLUSIONS: AI-LLM GPTs can significantly enhance problem-solving performance in MCQ-based assessments, particularly for students with limited prior knowledge, even allowing them to outperform knowledgeable peers using traditional digital resources. This underscores the potential of AI to transform learning support in medical education, although its impact on deep learning and critical thinking requires further investigation.
    mergedFrom:
      - source: pubmed
        pmid: "41493542"
        doi: 10.2196/81264
    finalDecision: exclude
  - title: "The use of large language models in generating multiple choice questions for health professions education: A systematic review and network meta-analysis."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: SR/NMA of LLM-generated MCQ quality for health professions education; focuses on exam question generation, not workplace-based assessment or clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Reviews MCQ generation quality, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1371/journal.pone.0340277
    pmid: "41481658"
    authors: Riehm L, Nanji K, Lakhani M, Pankiv E, Hasanee D, Pfeifer W
    year: "2026"
    abstract: |-
      PURPOSE: Large language models (LLMs) have the potential to change medical education. Whether LLMs can generate multiple-choice questions (MCQs) that are of similar quality to those created by humans is unclear. This investigation assessed the quality of MCQs generated by LLMs compared to humans.

      METHODS: This review was registered with PROSPERO (CRD42025608775). A systematic review and frequentist random-effects network meta-analysis (NMA) or pairwise meta-analysis was performed. Ovid MEDLINE, Ovid EMBASE, and Scopus were searched from inception to November 1, 2024. The quality of MCQs was assessed with seven pre-defined outcomes: question relevance, clarity, accuracy/correctness; distractor quality; item difficulty analysis; and item discrimination analysis (point biserial correlation and item discrimination index). Continuous data were transformed to a 10-point scale to facilitate statistical analysis and reported as mean differences (MD). The MERSQI and the Grade of Recommendations, Assessment, Development and Evaluation (GRADE) NMA guidelines were used to assess risk of bias and certainty of evidence assessments.

      RESULTS: Five LLMs were included. NMA demonstrated that ChatGPT 4 generated similar quality MCQs to humans with regards to question relevance (MD -0.13; 95% CI: -0.44,0.18; GRADE: VERY LOW), question clarity (MD -0.03; 95% CI: -0.15,0.10; GRADE: VERY LOW), and distractor quality (MD -0.10; 95% CI: -0.24,0.04; GRADE: VERY LOW); however, MCQs generated by Llama 2 performed worse than humans with regards to question clarity (MD -1.21; 95% CI: -1.60,-0.82; GRADE: VERY LOW) and distractor quality (MD -1.50; 95% CI: -2.03,-0.97; GRADE: VERY LOW). Exploratory post-hoc t-tests demonstrated that ChatGPT 3.5 performed worse than Llama 2 and ChatGPT 4 with regards to question clarity and distractor quality (p < 0.001).

      CONCLUSION: ChatGPT 4 may create similar quality MCQs to humans, whereas ChatGPT 3.5 and Llama 2 may be of worse quality. Further studies that directly compare these LLMs to human-generated questions and administer MCQs to students are required.
    mergedFrom:
      - source: pubmed
        pmid: "41481658"
        doi: 10.1371/journal.pone.0340277
    finalDecision: exclude
  - title: "Generative artificial intelligence for teaching and assessment in health professions education: A scoping review."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. The scoping review covers GenAI in general health professions education broadly without specific focus on workplace-based or clinical education settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Broad review of teaching and assessment applications without focus on assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1016/j.nepr.2025.104697
    pmid: "41477967"
    authors: Astbury H, Fortune E, Ponte C, Lyons K, Shaw L
    year: "2026"
    abstract: |-
      AIM: To review the literature on generative artificial intelligence for teaching and assessment in health professions education BACKGROUND: Advancements in generative artificial intelligence (GenAI), such as ChatGPT, offer new possibilities for health professions education. These technologies offer potential benefits in teaching and assessment, including personalised learning and automated resource generation. Despite its potential, concerns about accuracy, ethics and reliability remain. This scoping review examines GenAI's implementation, benefits and challenges in teaching and assessment across health professions education.

      DESIGN: Scoping review.

      METHODS: Following Arksey and O'Malley's five-stage framework, with refinements based on the Joanna Briggs Institute (JBI) methodology, Medline, CINAHL and Web of Science Core Collection were searched for peer-reviewed studies published between January 2019 and June 2024. Studies were screened independently by two reviewers and data extraction performed systematically to ensure consistency.

      RESULTS: Studies (n = 5826) were assessed for eligibility, with 23 meeting the inclusion criteria. All included studies were published in 2023 and 2024. The primary applications of GenAI were in learning resource development and assessment, with reported benefits such as time savings, personalised learning and reduced resource use. Challenges included accuracy concerns, inconsistent outputs, technical limitations, algorithmic bias and risks to academic integrity.

      CONCLUSIONS: This scoping review provides an overview of how GenAI is being integrated into health professions education. While the technology offers opportunities to enhance teaching and assessment, its implementation requires consideration of reliability, ethical concerns and educator preparedness. This review is the first to examine GenAI implementation across multiple AHPRA-regulated health professions and proposes a practical framework (AI HPE checklist) to guide responsible use.
    mergedFrom:
      - source: pubmed
        pmid: "41477967"
        doi: 10.1016/j.nepr.2025.104697
    finalDecision: exclude
  - title: "AI in Psychiatric Education and Training From 2016 to 2024: Scoping Review of Trends."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses on general AI trends, not clearly generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Broad AI scoping review using a deliberately broad definition including conventional ML and NLP, not specifically about generative AI; also covers topics well beyond assessment/feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Scoping review (not an original research article, systematic review, meta-analysis, or preprint per protocol)."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.2196/81517
    pmid: "41474738"
    authors: Weightman M, Chur-Hansen A, Clark S
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) is rapidly changing both clinical psychiatry and the education of medical professionals. However, little is currently known about how AI is being discussed in the education and training of psychiatry for medical students and doctors around the world.

      OBJECTIVE: This paper aims to provide a snapshot of the available data on this subject as of 2024. A deliberately broad definition of AI was adopted to capture the widest range of relevant literature and applications, including machine learning, natural language processing, and generative AI tools.

      METHODS: A scoping review was conducted using both peer-reviewed publications from PubMed, Embase, PsycINFO, and Scopus databases, and gray literature sources. The criterion for inclusion was a description of how AI could be applied to education or training in psychiatry.

      RESULTS: A total of 26 records published between 2016 and 2024 were included. The key themes identified were (1) the imperative for an AI curriculum for students or doctors training in psychiatry, (2) uses of AI to develop educational resources, (3) uses of AI to develop clinical skills, (4) uses of AI for assessments, (5) academic integrity or ethical considerations surrounding the use of AI, and (6) tensions relating to competing priorities and directions.

      CONCLUSIONS: Although a nascent field, it is clear that AI will increasingly impact assessment, clinical skills training, and the development of teaching resources in psychiatry. Training curricula will need to reflect the new knowledge and skills required for future clinical practice. Educators will need to be mindful of academic integrity risks and to emphasize development of critical thinking skills. Attitudes of psychiatrists toward the rise of AI in training remain underexplored.
    mergedFrom:
      - source: pubmed
        pmid: "41474738"
        doi: 10.2196/81517
    finalDecision: exclude
  - title: "Evaluation of Few-Shot AI-Generated Feedback on Case Reports in Physical Therapy Education: Mixed Methods Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: uncertain
        comment: AI-generated feedback (Gemini 2.5 Pro) on physical therapy case reports with data on perceived quality and acceptability. Unclear from abstract whether case reports are from clinical/workplace placements or classroom exercises. If workplace-based, this would qualify as AI feedback on clinical documentation. Forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Conducted in physical therapy educational coursework without a clear workplace-based clinical education context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. The study evaluates AI-generated (Gemini 2.5 Pro) formative feedback on case reports written by fourth-year physical therapy college students in a classroom educational setting, not in a workplace-based clinical context."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:50.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Conducted in a non-workplace educational setting without a clear workplace-based clinical education component."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:25:58.854000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Classroom educational setting, not workplace-based."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.2196/85614
    pmid: "41468580"
    authors: Sudo H, Noborimoto Y, Takahashi J
    year: "2025"
    abstract: |-
      BACKGROUND: While artificial intelligence (AI)-generated feedback offers significant potential to overcome constraints on faculty time and resources associated with providing personalized feedback, its perceived usefulness can be undermined by algorithm aversion. In-context learning, particularly the few-shot approach, has emerged as a promising paradigm for enhancing AI performance. However, there is limited research investigating its usefulness, especially in health profession education.

      OBJECTIVE: This study aimed to compare the quality of AI-generated formative feedback from 2 settings, feedback generated in a zero-shot setting (hereafter, "zero-shot feedback") and feedback generated in a few-shot setting (hereafter, "few-shot feedback"), using a mixed methods approach in Japanese physical therapy education. Additionally, we examined the effect of algorithm aversion on these 2 feedback types.

      METHODS: A mixed methods study was conducted with 35 fourth-year physical therapy students (mean age 21.4, SD 0.7 years). Zero-shot feedback was created using Gemini 2.5 Pro with default settings, whereas few-shot feedback was generated by providing the same model with 9 teacher-created examples. The participants compared the quality of both feedback types using 3 methods: a direct preference question, the Feedback Perceptions Questionnaire (FPQ), and focus group interviews. Quantitative comparisons of FPQ scores were performed using the Wilcoxon signed rank test. To investigate algorithm aversion, the study examined how student perceptions changed before and after disclosure of the feedback's identity.

      RESULTS: Most students (26/35, 74%) preferred few-shot feedback over zero-shot feedback in terms of overall usefulness, although no significant difference was found between the 2 feedback types for the total FPQ score (P=.22). On the specific FPQ scales, few-shot feedback scored significantly higher than zero-shot feedback on fairness across all 3 items: "satisfied" (P=.02; r=0.407), "fair" (P=.04; r=0.341), and "justified" (P=.02; r=0.392). It also scored significantly higher on 1 item of the usefulness scale ("useful"; P=.02; r=0.401) and 1 item of the willingness scale ("invest a lot of effort"; P=.02; r=0.394). In contrast, zero-shot feedback scored significantly higher on the affect scale across 2 items: "successful" (P=.03; r=0.365) and "angry" (P=.008; r=0.443). Regarding algorithm aversion, evaluations for zero-shot feedback became more negative for 83% (15/18) of the items after identity disclosure, whereas positive perceptions of few-shot feedback were maintained or increased. Qualitative analysis revealed that students valued zero-shot feedback for its encouraging tone, whereas few-shot feedback was appreciated for its contextual understanding and concrete guidance for improvement.

      CONCLUSIONS: Japanese physical therapy students perceived few-shot feedback more favorably than zero-shot feedback on case reports. This few-shot AI model shows potential to resist algorithm aversion and serves as an effective educational tool to support autonomous writing, facilitate reflection on clinical reasoning, and cultivate advanced thinking skills.
    mergedFrom:
      - source: pubmed
        pmid: "41468580"
        doi: 10.2196/85614
    finalDecision: exclude
  - title: Leveraging Large Language Models to Evaluate the Quality of Narrative Feedback for Surgery Residents in Competency-Based Medical Education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: include
        comment: "Generative AI (GPT-3.5, GPT-4) used to evaluate narrative feedback quality in EPA assessments for surgery residents in CBME. Directly relevant: LLMs analyzing WBA (EPA) narrative feedback with validity data (comparison to human QuAL scores)."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
      - decision: include
        comment: "Uses GPT-3.5 and GPT-4 to evaluate narrative feedback quality in EPA assessments for surgery residents using QuAL score. Workplace-based setting (Surgical Foundations program EPA assessments at Queen's University, 2017-2022). Presents validity data: F1 scores for Evidence (0.827), Suggestion (0.949), Connection (0.933) with fine-tuned GPT-3.5. Meets all inclusion criteria."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:57.506000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:05.615000+00:00
    doi: 10.1097/AS9.0000000000000608
    pmid: "41451174"
    authors: Kwan B, Zhou Z, Rogoza N, Aghaei N, de Vries I, Hanmore T, Zevin B
    year: "2025"
    abstract: |-
      OBJECTIVE: This study aimed to investigate large language model (LLM) performance in evaluating narrative feedback quality in the entrustable professional activities (EPAs) assessments within a Surgical Foundations program.

      BACKGROUND: Transitioning to competency-based medical education (CBME) has increased the volume of narrative feedback for surgery residents. However, evaluating narrative feedback quality is time-consuming, requiring manual review by humans. LLMs show potential for automating this process.

      METHODS: An existing dataset of 2229 deidentified comments from EPA assessments for surgery residents in an academic program (2017-2022) was analyzed using generative pre-trained transformer (GPT)-3.5-turbo-1106 and GPT-4-1106-preview. LLM-generated scores were compared to Quality of Assessment for Learning (QuAL) scores assigned by human raters. F1 score was the primary metric for model accuracy. Performance improvements were measured for each LLM by comparing F1 scores across different prompting techniques and fine-tuning strategies against baseline performance.

      RESULTS: GPT-3.5 and GPT-4 performance varied significantly across prompting techniques due to differences in model architecture. GPT-4 achieved the highest F1 scores for Suggestion (0.901) and Connection (0.882) but underperformed in the Evidence dimension (0.554) of the QuAL score. Fine-tuning was not available for GPT-4 during the study, although fine-tuned GPT-3.5 showed improved LLM performance with high F1 scores for Evidence (0.827), Suggestion (0.949), and Connection (0.933).

      CONCLUSIONS: Fine-tuned GPT-3.5 demonstrated strong potential for automating the evaluation of narrative feedback quality for surgery residents. However, LLM performance depends on the task and how well task structure aligns with the LLM architecture. LLM use in CBME may facilitate continuous quality improvement, providing faculty with automated feedback on their feedback.
    mergedFrom:
      - source: pubmed
        pmid: "41451174"
        doi: 10.1097/AS9.0000000000000608
    finalDecision: include
  - title: "Asynchronous tele-mentoring in proficiency-based training for the fundamentals of laparoscopic surgery (FLS): a use of smartphones and deliberate practice."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on tele-mentoring and deliberate practice in laparoscopic surgery"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    doi: 10.1007/s00464-025-12488-5
    pmid: "41436616"
    authors: Abdalgadir M, Soliman D, Shaher Y, Nelson A, Worrell S, Ditillo M, Ghaderi I
    year: "2026"
    abstract: |-
      INTRODUCTION: The Fundamentals of Laparoscopic Surgery (FLS) curriculum is a validated simulation program for surgical training, yet time constraints in clinical settings limit faculty-guided skill acquisition. This study aimed to implement an asynchronous tele-mentoring model using smartphones and deliberate practice to train residents to FLS proficiency.

      METHODS AND PROCEDURES: Surgical residents (PGY1-5) engaged in self-directed practice of FLS psychomotor tasks without restrictions on session frequency or duration. Smartphones mounted inside FLS boxes recorded performances, mimicking the onboard camera angle. Residents submitted videos post-practice to an expert instructor, who provided written feedback within 24-48 h. Feedback themes were analyzed qualitatively using artificial intelligence (ChatGPT).

      RESULTS: Forty-one residents (2017-2024) participated. Thematic analysis revealed early feedback emphasized technical skill development (62% of comments, e.g., instrument handling), shifting to efficiency optimization (22%) and error reduction (10%) as proficiency increased. All residents surpassed the FLS passing score (356), achieving a mean score of 598 (± 72.9 SD), with a range of 421-721.

      CONCLUSION: This asynchronous tele-mentoring model demonstrates feasibility in proficiency-based FLS training, combining self-directed practice with timely expert feedback. Smartphone integration and AI-aided feedback analysis enhance accessibility and objectivity, addressing barriers to faculty availability. The evolution of feedback themes aligns with skill acquisition theory, underscoring the value of structured, phase-specific guidance. AI's role in parsing feedback patterns offers scalability without compromising expert-led mentorship. This approach provides a framework for flexible, technology-enhanced surgical education.
    mergedFrom:
      - source: pubmed
        pmid: "41436616"
        doi: 10.1007/s00464-025-12488-5
    finalDecision: exclude
  - title: AI-generated biochemistry test item parameters in MST test conditions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Uses ChatGPT to generate and calibrate standardized biochemistry exam items with simulated examinees."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment and validation are conducted entirely in simulated testing conditions."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.1186/s12909-025-08292-3
    pmid: "41430216"
    authors: Polat M, Karadag E
    year: "2025"
    abstract: |-
      BACKGROUND: This study investigated whether ChatGPT 4o could accurately estimate the difficulty of medical assessment items by comparing its predictions with empirically-derived parameters from multistage testing simulations.

      METHODS: Using a hybrid simulation-validation design, the researchers had ChatGPT 4o generate 80 multiple-choice biochemistry questions with difficulty estimates (b-parameters), which were then administered via simulated multistage testing to 5,000 virtual examinees.

      RESULTS: The analysis revealed moderate agreement between AI-generated and simulation-derived difficulty parameters (r = 0.612, 95% CI [0.472, 0.725]), though ChatGPT systematically overestimated item difficulty with a mean bias of 0.240 (SD = 0.503). While the mean absolute error was relatively modest at 0.447, with 91% of items showing errors below 1.0 logits, the AI's estimates were particularly inaccurate for very easy items, where 83% exhibited absolute errors exceeding 0.5 logits compared to only 29% for medium difficulty items. These findings suggest that while ChatGPT 4o demonstrates promise as a tool for preliminary item generation in medical education assessment, it requires empirical calibration and expert oversight before operational implementation, as the systematic bias indicates the AI lacks access to real-world performance feedback.

      CONCLUSIONS: The study's conclusions are tempered by important limitations, including its reliance on simulation-based validation rather than actual student performance data and its single-institution sample, underscoring the need for rigorous psychometric validation when integrating artificial intelligence into medical education assessment.
    mergedFrom:
      - source: pubmed
        pmid: "41430216"
        doi: 10.1186/s12909-025-08292-3
    finalDecision: exclude
  - title: "Comparing ChatGPT and DeepSeek for Assessment of Multiple-Choice Questions in Orthopedic Medical Education: Cross-Sectional Study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Multiple-choice question assessment, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Comparing LLMs answering orthopedic MCQs; benchmarking AI performance on exam questions, not about using AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares LLM performance on orthopedic MCQs, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.2196/75607
    pmid: "41418321"
    authors: Anusitviwat C, Suwannaphisit S, Bvonpanttarananon J, Tangtrakulwanich B
    year: "2025"
    abstract: |-
      BACKGROUND: Multiple-choice questions (MCQs) are essential in medical education for assessing knowledge and clinical reasoning. Traditional MCQ development involves expert reviews and revisions, which can be time-consuming and subject to bias. Large language models (LLMs) have emerged as potential tools for evaluating MCQ accuracy and efficiency. However, direct comparisons of these models in orthopedic MCQ assessments are limited.

      OBJECTIVE: This study compared the performance of ChatGPT and DeepSeek in terms of correctness, response time, and reliability when answering MCQs from an orthopedic examination for medical students.

      METHODS: This cross-sectional study included 209 orthopedic MCQs from summative assessments during the 2023-2024 academic year. ChatGPT (including the "Reason" function) and DeepSeek (including the "DeepThink" function) were used to identify the correct answers. Correctness and response times were recorded and compared using a χ2 test and Mann-Whitney U test where appropriate. The two LLMs' reliability was assessed using the Cohen κ coefficient. The MCQs incorrectly answered by both models were reviewed by orthopedic faculty to identify ambiguities or content issues.

      RESULTS: ChatGPT achieved a correctness rate of 80.38% (168/209), while DeepSeek achieved 74.2% (155/209; P=.04). ChatGPT's Reason function also outperformed DeepSeek's DeepThink function (177/209, 84.7% vs 168/209, 80.4%; P=.12). The average response time for ChatGPT was 10.40 (SD 13.29) seconds, significantly shorter than DeepSeek's 34.42 (SD 25.48) seconds (P<.001). Regarding reliability, ChatGPT demonstrated an almost perfect agreement (κ=0.81), whereas DeepSeek showed substantial agreement (κ=0.78). A completely false response was recorded in 7.7% (16/209) of responses for both models.

      CONCLUSIONS: ChatGPT outperformed DeepSeek in correctness and response time, demonstrating its efficiency in evaluating orthopedic MCQs. This high reliability suggests its potential for integration into medical assessments. However, our results indicate that some MCQs will require revisions by instructors to improve their clarity. Further studies are needed to evaluate the role of artificial intelligence in other disciplines and to validate other LLMs.
    mergedFrom:
      - source: pubmed
        pmid: "41418321"
        doi: 10.2196/75607
    finalDecision: exclude
  - title: "Generative Artificial Intelligence in Urology: Navigating the Frontier of Ethical, Legal, and Clinical Challenges."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Title suggests a narrative review or commentary on ethical/legal challenges, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Narrative review of generative AI in urology covering ethical, legal, and clinical challenges; not original research, systematic review, or meta-analysis."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Narrative review/opinion-style article, not an original research study, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.7759/cureus.97047
    pmid: "41416297"
    authors: Khalil W, Sheikh M, Islam J
    year: "2025"
    abstract: The emergence of generative artificial intelligence (AI), particularly large language models and image-generation tools, is poised to transform the field of urology. These technologies enable innovative applications in medical education, clinical decision support, patient communication, and surgical planning, extending beyond traditional analytical AI by creating new content, from synthetic clinical notes to simulated surgical environments. In urology, these capabilities translate into automated summarization of complex patient histories, generation of patient-specific three-dimensional anatomical reconstructions, and support for differential diagnosis in conditions such as prostate cancer or renal masses. However, the rapid adoption of generative AI also introduces significant ethical, legal, and clinical challenges. Risks are amplified in urological practice, where sensitive imaging data, biomarker profiles, and diagnostic decision pathways may be vulnerable to privacy breaches, algorithmic bias, or erroneous AI-generated recommendations. Hallucinated outputs, such as incorrect treatment summaries or misinterpreted radiologic features, can directly compromise patient safety if not rigorously validated. This review synthesizes the current landscape of generative AI in urology, critically examines these discipline-specific risks, and proposes a structured framework for responsible integration into clinical workflows. We highlight the need for transparent governance, bias mitigation, prospective validation, and interdisciplinary collaboration to ensure that generative AI enhances, rather than undermines, the quality, equity, and safety of urologic care.
    mergedFrom:
      - source: pubmed
        pmid: "41416297"
        doi: 10.7759/cureus.97047
    finalDecision: exclude
  - title: "Diagnosis of shoulder dislocation on AP radiographs: A comparative analysis of diagnostic performance between orthopedic surgeons, emergency physicians, and ChatGPT models."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About comparing diagnostic performance of AI vs clinicians on radiographs, not about assessment or feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares clinical diagnosis performance, not educational assessment/feedback of learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    doi: 10.1016/j.injury.2025.112957
    pmid: "41406757"
    authors: Kirilmaz A, Erdem T, Yaka H, Yildirim A, Ozer M
    year: "2026"
    abstract: |-
      OBJECTIVE: This study aimed to evaluate the diagnostic performance of ChatGPT in identifying acute shoulder dislocations and to compare its accuracy with that of orthopedic specialists and emergency medicine residents.

      METHODS: A total of 250 anteroposterior (AP) shoulder radiographs were included. All images were evaluated for the presence or absence of dislocation and for dislocation subtype (anterior, posterior, inferior) by four groups: orthopedic specialists (n = 10), orthopedic residents (n = 10), emergency medicine residents (n = 10), and ChatGPT. ChatGPT-4o (OpenAI, May 2024) and ChatGPT-5.1 (OpenAI, July 2025) were accessed through the web interface using a standardized single image + text-based prompt. The models had no prior training with radiological images. Diagnostic performance was assessed using sensitivity, specificity, positive and negative predictive values, overall accuracy, area under the ROC curve (AUC), F1 score, and Cohen's kappa for inter-reader agreement.

      RESULTS: In the detection of shoulder dislocation (yes/no), orthopedic specialists demonstrated the highest accuracy (95.0 %), whereas ChatGPT-4o showed the lowest (72.4 %). Orthopedic residents achieved 90.1 % accuracy, emergency medicine residents 89.0 %, and ChatGPT-5.1 78.0 %. When subtype classification (anterior, posterior, inferior) was included, orthopedic specialists again performed best (89.7 %), while ChatGPT-4o had the lowest accuracy (68.0 %). Orthopedic residents (84.7 %) outperformed emergency medicine residents (76.7 %), while ChatGPT-5.1 achieved 69.6 % accuracy. Internal-rotation AP images of nondislocated shoulders were frequently misinterpreted as posterior dislocations.

      CONCLUSION: This study demonstrates that the diagnostic accuracy for acute shoulder dislocation varies according to the clinicians' level of experience. The use of a single AP shoulder radiograph alone is not sufficient for diagnosing shoulder dislocation. Clinicians most frequently misinterpreted internally rotated AP shoulder radiographs as posterior dislocations. ChatGPT models showed moderate performance and are not yet suitable as standalone diagnostic tools in clinical decision-making. However, with further development of artificial intelligence-based systems, these models may serve as rapid preliminary screening aids in emergency settings.
    mergedFrom:
      - source: pubmed
        pmid: "41406757"
        doi: 10.1016/j.injury.2025.112957
    finalDecision: exclude
  - title: "Reasoning-optimised large language models reach near-expert accuracy on board-style orthopaedic exams: A multi-model comparison on 702 multiple-choice questions."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Benchmarks LLMs on board-style orthopaedic MCQs; not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Board-style MCQ benchmarking is outside workplace-based or clinical education settings."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.1002/ksa.70222
    pmid: "41404998"
    authors: Diniz P, Yokoe T, &#xd6;ttl F, Pereira H, Henriques R, Samuelsson K
    year: "2026"
    abstract: |-
      PURPOSE: The purpose of this study was to compare the accuracy, calibration, reproducibility and operating cost of seven large language models (LLMs)-including four newer models capable of using advanced reasoning techniques to analyse complex medical information and generate accurate responses-on text-only orthopaedic multiple-choice questions (MCQs) and to quantify gains over GPT-4.

      METHODS: From Orthobullets, 702 unique, non-image MCQs (drawn from AAOS Self-Assessment Examinations, Self-Assessment-Based Questions and Orthopaedic In Training Examination-Based Questions banks) were extracted. Each question was submitted to the following LLMs: OpenAI o3, Anthropic Claude Sonnet 4, Claude Opus 4 (with/without 'Extended Thinking') and Google Gemini 2.5 Pro. Additionally, OpenAI's GPT-4, GPT-4o and the open-weight Gemma 3 27B served as comparators. The primary outcome was overall accuracy. The secondary outcomes were topic and difficulty-stratified accuracy, calibration (expected calibration error [ECE] and Brier score), reproducibility (flip rate on a retest question subset), latency, token use and cost. Statistical tests included paired McNemar, Cochran Q, ordinal logistic regression and Fleiss κ (Bonferroni-adjusted α = 0.05).

      RESULTS: GPT-4 achieved 69.7% accuracy (95% CI = 66.2-72.9). All four reasoning-optimised models scored ≥14 percentage points higher (p < 3.3 × 10-15); OpenAI o3 led with 93.6% (95% CI = 91.5-95.2), which represents a 34% relative error reduction. Accuracy tended to decline with question difficulty, yet the reasoning advantage persisted in every difficulty stratum. Claude Opus 4 showed the best calibration (ECE = 0.023), while GPT-4 exhibited overconfidence (ECE = 0.215). All models except Gemma 3 27B exhibited non-zero flip rates. Median query time: 0.9 s (Gemma) to 15.9 s (Gemini 2.5 Pro). Cost: 0 to 29.9 USD per 1000 queries.

      CONCLUSIONS: Reasoning-optimised LLMs now answer text-based orthopaedic exam questions with high accuracy and substantially better confidence calibration than earlier models. However, persistent stochasticity and large latency-cost disparities may limit clinical deployment.

      LEVEL OF EVIDENCE: N/A.
    mergedFrom:
      - source: pubmed
        pmid: "41404998"
        doi: 10.1002/ksa.70222
    finalDecision: exclude
  - title: "Multiple Large Language Models' Performance on the Chinese Medical Licensing Examination: Quantitative Comparative Study."
    reviews:
      - decision: uncertain
        comment: LLMs on licensing exam; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study benchmarks multiple LLMs on Chinese NMLE questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: National licensing exam performance study is outside workplace-based clinical education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.2196/77978
    pmid: "41401211"
    authors: Diao Y, Wu M, Xu J, Pan Y
    year: "2025"
    abstract: |-
      BACKGROUND: ChatGPT excels in natural language tasks, but its performance in the Chinese National Medical Licensing Examination (NMLE) and Chinese medical education remains underexplored. Meanwhile, Chinese corpus-based large language models (LLMs) such as ERNIE Bot, Tongyi Qianwen, Doubao, and DeepSeek have emerged, yet their effectiveness in the NMLE awaits systematic evaluation.

      OBJECTIVE: This study aimed to quantitatively compare the performance of 6 LLMs (GPT-3.5, GPT-4, ERNIE Bot, Tongyi Qianwen, Doubao, and DeepSeek) in answering NMLE questions from 2018 to 2024 and analyze their feasibility as supplementary tools in Chinese medical education.

      METHODS: We selected questions from the 4 content units of the NMLE's General Written test (2018-2024), preprocessed image- and table-based content into standardized text, and input the questions into each model. We evaluated the accuracy, comprehensiveness, and logical coherence of the responses, with quantitative comparison centered on scores and accuracy rates against the official answer keys (passing score: 360/600).

      RESULTS: GPT-4 outperformed GPT-3.5 across all units, achieving average accuracies of 66.57% (SD 3.21%; unit 1), 69.05% (SD 2.87%; unit 2), 71.71% (SD 2.53%; unit 3), and 80.67% (SD 2.19%; unit 4), with consistent scores above the passing threshold. Among the Chinese models, DeepSeek demonstrated the highest overall performance, with an average score of 454.8 (SD 17.3) and average accuracies of 73.2% (unit 1, SD 2.89%) and 71.5% (unit 3, SD 2.64%), as well as average accuracies of 70.3% (unit 2, SD 3.02%) and 78.2% (unit 4, SD 2.47%). ERNIE Bot (mean score 442.3, SD 19.6; unit 1 accuracy =70.8%, SD 3.01%; unit 2 accuracy =68.7%, SD 3.15%; unit 3 accuracy =69.1%, SD 2.93%; unit 4 accuracy =68.3%, SD 2.76%), Tongyi Qianwen (mean score 426.5, SD 21.4; unit 1 accuracy =67.4%, SD 3.22%; unit 2 accuracy =65.9%, SD 3.31%; unit 3 accuracy =66.2%, SD 3.08%; unit 4 accuracy =67.2%, SD 2.89%), and Doubao (mean score 413.7, SD 23.1; unit 1 accuracy =65.2%, SD 3.45%; unit 2 accuracy =63.8%, SD 3.52%; unit 3 accuracy =64.1%, SD 3.27%; unit 4 accuracy =62.8%, SD 3.11%) all exceeded the passing score. DeepSeek's overall average accuracy (75.8%, SD 2.73%) was significantly higher than those of the other Chinese models (χ²₁=11.4, P=.001 vs ERNIE Bot; χ²₁=28.7, P<.001 vs Tongyi Qianwen; χ²₁=45.3, P<.001 vs Doubao). GPT-4's overall average accuracy (77.0%, SD 2.58%) was slightly higher than that of DeepSeek but not statistically significant (χ²₁=2.2, P=.14), while both outperformed GPT-3.5 (overall accuracy =68.5%, SD 3.67%; χ²₁=89.8, P<.001 for GPT-4 vs GPT-3.5; χ²₁=76.3, P<.001 for DeepSeek vs GPT-3.5).

      CONCLUSIONS: GPT-4 and Chinese-developed LLMs such as DeepSeek show potential as supplementary tools in Chinese medical education given their solid performance on the NMLE. However, further optimization is required for complex reasoning, multimodal processing, and dynamic knowledge updates, with human medical expertise remaining central to clinical practice and education.
    mergedFrom:
      - source: pubmed
        pmid: "41401211"
        doi: 10.2196/77978
    finalDecision: exclude
  - title: "Assessment of large language models in musculoskeletal radiological anatomy: A comparative study with radiologists."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Assesses LLM performance vs radiologists, not about assessment or feedback of human learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; compares LLM diagnostic accuracy with radiologists on anatomy MCQs from an open-access database."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares LLMs and radiologists on anatomy MCQs, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.52312/jdrs.2026.2436
    pmid: "41391131"
    authors: Salbas A, Baysan E
    year: "2026"
    abstract: |-
      OBJECTIVES: This study aims to evaluate the diagnostic performance of large language models (LLMs) in musculoskeletal radiological anatomy and to compare their accuracy with radiologists of varying experience levels.

      PATIENTS AND METHODS: Between May 16, 2025 and June 12, 2025, a total of 175 multiple-choice questions (82 image-based, 93 text-only) were retrieved from Radiopaedia's open-access database. Questions were classified by anatomical region and imaging modality. Three LLMs, ChatGPT-4o (OpenAI), Claude 3.7 Sonnet (Anthropic), and Grok 3 (×AI), were assessed in a zero-shot setting. Their responses were compared to those of an attending musculoskeletal radiologist and two residents (senior and junior). Accuracy rates were calculated and statistically compared.

      RESULTS: The attending radiologist achieved the highest overall accuracy (79.4%), followed by the senior (72.6%) and junior resident (66.9%). Among LLMs, ChatGPT-4o performed best overall (69.7%), particularly in text-based questions (88.2%). All LLMs outperformed radiologists in text-based questions but underperformed in image-based ones. The attending radiologist significantly outperformed all LLMs in image interpretation (p<0.001). Variations in performance were also noted across anatomical regions and imaging modalities, with some LLMs exceeding radiologists in specific domains such as spinal or shoulder anatomy.

      CONCLUSION: While LLMs, particularly ChatGPT-4o, show strong performance in text-based anatomical questions, their accuracy in image-based musculoskeletal radiology remains limited compared to human radiologists. These findings suggest that LLMs can serve as supplementary tools in education but require further optimization, particularly for visual interpretation tasks, before clinical implementation.
    mergedFrom:
      - source: pubmed
        pmid: "41391131"
        doi: 10.52312/jdrs.2026.2436
    finalDecision: exclude
  - title: "Modernizing pathology and oncology education: integrating genomics, artificial intelligence, and clinical relevance into medical training."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Appears to be a narrative review or commentary on modernizing education, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article; opinion/perspective piece on curriculum reform in pathology and oncology education"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/reform proposal, not original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.1093/jnci/djaf358
    pmid: "41390932"
    authors: de Jesus Paniza A, Ynoe Moraes F
    year: "2025"
    abstract: Pathology and oncology education are at an inflection point. Beyond abbreviated preclinical blocks, the central problem is pedagogical misalignment with learners who expect relevance, interactivity, and clinical application. We advocate a shift from content delivery to concept integration anchored in clinical reasoning and data literacy. In oncology, trainees must learn to interpret next‑generation sequencing and biomarker profiles, participate in molecular tumor boards, sequence precision therapies, manage toxicities, and incorporate patient‑reported outcomes-competencies rarely taught in a structured way. The digitization of histopathology and the integration of artificial intelligence demand exposure to digital pathology and critical appraisal of algorithmic outputs, including AI‑supported IHC quantification, variant classification, and methylation‑based classifiers. Large language models may enhance self‑directed learning but require faculty oversight, instruction in appraisal and ethics, and safeguards against inaccuracy and overconfidence. Operationalizing these reforms requires institutional commitment, curriculum redesign that integrates pathology, oncology, genomics, and decision‑making, and expanded residency time to acquire competencies in informatics and AI (machine learning, deep learning, supervised and unsupervised methods, and validation). Faculty development, adoption of digital platforms and virtual microscopy, competency‑based assessment, and collaboration with computer scientists, bioinformaticians, and ethicists are essential. Implementation barriers-including limited faculty time, resource constraints, and accreditation requirements-can be mitigated by pilot programs, strategic partnerships, phased integration, and attention to transparency, equity, and accountability. Absent deliberate reform within LCME and ACGME frameworks that currently do not mandate genomics or AI literacy, future physicians will enter practice unprepared for precision medicine. Modernizing curricula to meet the genomics and AI era is therefore urgent.
    mergedFrom:
      - source: pubmed
        pmid: "41390932"
        doi: 10.1093/jnci/djaf358
    finalDecision: exclude
  - title: "Token-splitting improves GPT-4.1 performance on plastic surgery exams: implications for AI-Assisted medical education."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: GPT performance on plastic surgery exams, standardized exam-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Investigates a token-splitting strategy to improve LLM performance on plastic surgery board exam questions; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1080/10872981.2025.2602788
    pmid: "41385283"
    authors: Lei Y, Chen C, Shen C
    year: "2025"
    abstract: Large language models (LLMs), such as ChatGPT, have demonstrated impressive performance on general medical examinations; however, their effectiveness significantly declines in specialized board examinations due to limited domain-specific training data and computational constraints inherent to their self-attention mechanisms. This study investigates a novel token-splitting strategy informed by Cognitive Load Theory (CLT), aimed at overcoming these limitations by optimizing cognitive processing and enhancing knowledge retention in specialized educational contexts. We implemented a token-splitting approach by segmenting Taiwan plastic surgery board examination materials and associated textbook content into cognitively manageable segments ranging from 4,000 to 20,000 tokens. These segmented inputs were provided to GPT-4.1 via its standard ChatGPT web interface. Model performance was rigorously evaluated, comparing accuracy and efficiency across various token lengths and question complexities classified according to Bloom's taxonomy.The GPT-4.1 model utilizing the token-splitting strategy significantly outperformed the baseline (unmodified) model, achieving notably higher accuracy. The optimal segmentation length was determined to be 6,000 tokens, effectively balancing cognitive coherence with information retention and model attention. Errors observed at this optimal length primarily resulted from content absent from textual materials or requiring multimodal interpretation (e.g., image-based reasoning). Provided relevant textual content was adequately segmented, GPT-4.1 consistently demonstrated high accuracy (From 75.88% to 92.93%). The findings highlight that a token-splitting approach, grounded in Cognitive Load Theory, significantly enhances LLM performance on specialized medical board examinations. This accessible, user-friendly strategy provides educators and clinicians with a practical means to improve AI-assisted education outcomes without requiring complex technical skills or infrastructure. Future research and development integrating multimodal capabilities and adaptive segmentation strategies promise to further optimize educational applications and clinical decision-making support.
    mergedFrom:
      - source: pubmed
        pmid: "41385283"
        doi: 10.1080/10872981.2025.2602788
    finalDecision: exclude
  - title: "Performance of Artificial Intelligence Chatbot on the Canadian Otolaryngology in-Training Exam: Unlocking Insights on the Intersection of Technology and Education."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Benchmarks ChatGPT-4 performance on in-training exam MCQs, not about using AI for learner assessment or WBA feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Study assesses ChatGPT on in-training exam items, not feedback generation or analysis of workplace-based clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1177/19160216251387618
    pmid: "41369028"
    authors: Ajit-Roger E, Kazerooni K, Savage J, Nguyen L
    year: "2025"
    abstract: "ImportanceThe performance of large language models has been compared to that of physicians.ObjectiveTo evaluate the performance of ChatGPT-4 in the field of otolaryngology and head and neck surgery (OTOHNS) residency training.DesignObservational.SettingVirtual.ParticipantsChatGPT-4.InterventionsAll questions from the OTOHNS National In-Training Exam (NITE) for 2022 and 2023 were submitted to ChatGPT-4. Answers were graded by 2 reviewers using the official grading rubric, and the average score was used. Mean exam results from residents who have taken this exam were obtained from the lead faculty.Main Outcome MeasuresZ-tests were used to compare ChatGPT-4's performance to that of residents. The questions were categorized by type (image or text), task, subspecialty, taxonomic level and prompt length.ResultsChatGPT-4 scored 66% (350/529) and 65% (243/374) on the 2022 and 2023 exams, respectively. ChatGPT-4 outperformed the residents on both exams, among all training levels and within all sub-specialties except for the general/pediatrics section of the 2023 exam (Z-test -2.54). For the 2022 exam, ChatGPT-4 would rank in the 99th percentile among post-graduate year (PGY)-2 and 73rd percentile among PGY-4 classmates. For the 2023 exam, it would rank in the 99th percentile among PGY-2 and 71st percentile among PGY-4 classmates. ChatGPT-4 performed best on text-based questions (74%, P < .001) with an effect size of 1.27 (confidence interval (CI): 0.99-1.55), level 1 taxonomic questions (75%, P < .001) with an effect size of 0.084 (CI: 0.03-0.14) and guideline-based questions (70%, P = .048) with an effect size of 0.11 (CI: 0-0.23). It had no significant difference in performance based on subspecialty (P = .36) or prompt length (P = .39).ConclusionsChatGPT-4 not only achieved passing grades on 2 versions of the Canadian OTOHNS NITE, but it also significantly outperformed residents.RelevanceThis study underscores a critical need to redesign residency assessment methods."
    mergedFrom:
      - source: pubmed
        pmid: "41369028"
        doi: 10.1177/19160216251387618
    finalDecision: exclude
  - title: "Performance of large language models in non-English medical ethics-related multiple choice questions: comparison of ChatGPT performance across versions and languages."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT accuracy on Korean medical ethics MCQs across versions and languages; not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on LLM performance on ethics MCQs, not workplace-based assessment or feedback records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1186/s12910-025-01316-z
    pmid: "41366422"
    authors: Kim Y, Shin S, Yoo S
    year: "2025"
    abstract: |-
      BACKGROUND: As large language models (LLMs) evolve, assessing their competence in ethically sensitive domains such as medical ethics has become increasingly important. Since medical ethics is a universal component of medical education, disparities in AI performance across languages may result in unequal benefits for learners. Therefore, it is essential to examine performances in non-English contexts. While previous studies have evaluated performance of Chat Generative Pre-trained Transformer(ChatGPT) on English-language multiple-choice questions (MCQs) in medical ethics, none have examined version-based improvements across non-English contexts. This study therefore evaluated ChatGPT versions 3.5, 4.0, and 4.5 for MCQs on Korean medical ethics and their English translations, with a focus on performance trends across versions and languages.

      METHODS: We selected 36 MCQs from the Korean National Medical Licensing Examination and the Comprehensive Clinical Medicine Evaluation databases. Each question was entered ten times per ChatGPT version (3.5, 4.0, 4.5) and language (Korean, English) for a total of 60 trials. Additionally, to assess the model's capacity to identify the ethical core without relying on the options provided, 31 of the 36 questions were modified by masking the correct choice. Accuracy was analyzed using independent sample t-tests and Mann Whitney U test, and consistency was assessed using Krippendorff's alpha.

      RESULTS: Overall, the accuracy and consistency of ChatGPT improved with each version. Version 4.5 achieved near-perfect scores and high reliability in both languages, while version 3.5 showed limited performance, particularly in the Korean test. Performance gaps between languages decreased with model upgrades but remained statistically significant in version 4.5 for some questions. In the masked-answer condition, all versions showed notable drops in accuracy and consistency, with version 4.5 still outperforming earlier versions. However, the performance remained below 50%, indicating limitations in the model's autonomous ethical reasoning.

      CONCLUSIONS: ChatGPT demonstrated substantial improvements in medical ethics MCQ performance across versions, particularly in terms of consistency and accuracy. However, performance disparities between languages and reduced accuracy under masked answer conditions highlight the ongoing limitations of non-English ethical reasoning and context recognition. These findings emphasize the need for further research on language-sensitive fine-tuning and the evaluation of LLMs in specialized ethical domains. The findings suggest that advanced LLMs may serve as valuable supplementary tools in medical education and clinical ethics training. At the same time, the observed language disparities call for context-sensitive adaptations to prevent inequities in practice.
    mergedFrom:
      - source: pubmed
        pmid: "41366422"
        doi: 10.1186/s12910-025-01316-z
    finalDecision: exclude
  - title: Comparative study of the performance of ChatGPT-4, Claude, Gemini, Mistral, and perplexity on multiple-choice questions in cardiology.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks five LLMs on cardiology MCQs; does not use generative AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares LLM accuracy on MCQs, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1186/s12872-025-05431-y
    pmid: "41366313"
    authors: Nacanabo M, Bayala Y, Seghda A, Tall/Thiam A, Yam&#xe9;ogo A, Yam&#xe9;ogo N, Samadoulougou A, Zabsonr&#xe9; P
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence, particularly Large Language Models (LLMs), has revolutionized the field of medicine. Their ability to understand and answer medical questions is generating growing interest, especially in cardiology, where diagnostic and therapeutic accuracy is essential.

      OBJECTIVE: The objective of our study was to assess and compare the performance of five LLMs on multiple-choice questions (MCQs) in cardiology.

      MATERIALS AND METHODS: This was a comparative study conducted in the cardiology department of the Bogodogo University Hospital, Ouagadougou, involving 83 MCQs derived from the 2020 French national cardiology curriculum. The questions were submitted to ChatGPT-4, Claude, Gemini, Mistral, and Perplexity. Performance was evaluated based on overall and thematic accuracy, as well as the number of discordances. Agreement between the LLMs was assessed using the Kruskal-Wallis test.

      RESULTS: Claude achieved the highest overall accuracy (78.31%), followed by ChatGPT-4 and Gemini (75.90%), then Mistral (72.29%) and Perplexity (68.67%). Each LLM demonstrated a distinct performance profile by topic, with Claude excelling in heart failure (100%) and arrhythmias (90.9%), and ChatGPT-4 in diagnostic investigations (87.5%). The analysis of discordances showed a slightly higher precision for ChatGPT-4. The Kruskal-Wallis test with effect size revealed statistically significant differences in performance between the LLMs, whether globally, by topic (p < 0.05) and with generally large effect sizes.

      CONCLUSION: Despite variations in their performance profiles, these five LLMs studied have relatively similar capabilities for answering well-structured cardiology multiple-choice questions. They could therefore be valuable tools in medical education in our resource-limited context.
    mergedFrom:
      - source: pubmed
        pmid: "41366313"
        doi: 10.1186/s12872-025-05431-y
    finalDecision: exclude
  - title: "Quantifying Emergency Medicine Residency Learning Curves Using Natural Language Processing: Retrospective Cohort Study."
    reviews:
      - decision: uncertain
        comment: NLP for residency learning curves; NLP may involve LLMs, residency assessment is WBA
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP; title does not indicate generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Uses NLP/RAG pipeline to quantify clinical topic exposure patterns over residency training, not to generate assessment or feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
      - decision: include
        comment: Uses generative AI (Gemini 1.5 Flash LLM via retrieval-augmented generation pipeline) to analyze clinical documentation from real ED encounters at Stanford Hospital (workplace-based setting). Quantifies EM resident clinical exposure and learning progression by mapping 244,255 clinical notes to standardized medical topics. Presents validity evidence (89.76% agreement with board-certified physician review, kappa=0.71) and educational impact data (topic exposure curves, case complexity progression across PGY years).
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 03:36:32.323000+00:00
      - decision: include
        comment: Analyzes residents' real clinical documentation using an LLM-based NLP pipeline (Gemini) to quantify workplace learning exposure, with validation and educational-impact evidence.
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 03:36:33.873000+00:00
    doi: 10.2196/82326
    pmid: "41364786"
    authors: Preiksaitis C, Hughes J, Kabeer R, Dixon W, Rose C
    year: "2025"
    abstract: |-
      BACKGROUND: The optimal duration of emergency medicine (EM) residency training remains a subject of national debate, with the Accreditation Council for Graduate Medical Education considering standardizing all programs to 4 years. However, empirical data on how residents accumulate clinical exposure over time are limited. Traditional measures, such as case logs and diagnostic codes, often fail to capture the breadth and depth of diagnostic reasoning. Natural language processing (NLP) of clinical documentation offers a novel approach to quantifying clinical experiences more comprehensively.

      OBJECTIVE: This study aimed to (1) quantify how EM residents acquire clinical topic exposure over the course of training, (2) evaluate variation in exposure patterns across residents and classes, and (3) assess changes in workload and case complexity over time to inform the discussion on optimal program length.

      METHODS: We conducted a retrospective cohort study of EM residents at Stanford Hospital, analyzing 244,255 emergency department encounters from July 1, 2016, to November 30, 2023. The sample included 62 residents across 4 graduating classes (2020-2023), representing all primary training site encounters where residents served as primary or supervisory providers. Using a retrieval-augmented generation NLP pipeline, we mapped resident clinical documentation to the 895 subcategories of the 2022 Model for Clinical Practice of Emergency Medicine (MCPEM) via intermediate mapping to the Systematized Nomenclature of Medicine, Clinical Terms, Clinical Observations, Recordings, and Encoding problem list subset. We generated cumulative topic exposure curves, quantified the diversity of topic coverage, assessed variability between residents, and analyzed the progression in clinical complexity using Emergency Severity Index (ESI) scores and admission rates.

      RESULTS: Residents encountered the largest increase in new topics during postgraduate year 1 (PGY1), averaging 376.7 (42.1%) unique topics among a total of 895 MCPEM subcategories. By PGY4, they averaged 565.9 (63.2%) topics, representing a 9.9% (51/515) increase over PGY3. Exposure plateaus generally occurred at 39 to 41 months, although substantial individual variation was observed, with some residents continuing to acquire new topics until graduation. Annual case volume more than tripled from PGY1 (mean 445.7, SD 112.7 encounters) to PGY4 (mean 1528.4, SD 112.7 encounters). Case complexity increased, as evidenced by a decrease in mean ESI score from 2.94 to 2.79, and a rise in high-acuity (ESI 1-2) cases from 16% (4374/27,340) to 30.9% (9418/30,466).

      CONCLUSIONS: NLP analysis of clinical documentation provides a scalable, detailed method for tracking EM residents' clinical exposure and progression. Many residents continue to gain new experiences into their fourth year, particularly in higher-acuity cases. These findings suggest that a 4-year training model may offer meaningful additional educational value, while also highlighting the importance of individualized assessment given the variability in learning trajectories.
    mergedFrom:
      - source: pubmed
        pmid: "41364786"
        doi: 10.2196/82326
    finalDecision: include
  - title: "The Validity of Generative Artificial Intelligence in Evaluating Medical Students in Objective Structured Clinical Examination: Experimental Study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: Generative AI evaluating students in OSCE, which is a simulated assessment environment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    doi: 10.2196/79465
    pmid: "41343812"
    authors: Yokose M, Hirosawa T, Sakamoto T, Kawamura R, Suzuki Y, Harada Y, Shimizu T
    year: "2025"
    abstract: |-
      BACKGROUND: The Objective Structured Clinical Examination (OSCE) has been widely used to evaluate students in medical education. However, it is resource-intensive, presenting challenges in implementation. We hypothesized that generative artificial intelligence (AI) such as ChatGPT-4 could serve as a complementary assessor and alleviate the burden of physicians in evaluating OSCE.

      OBJECTIVE: By comparing the evaluation scores between generative AI and physicians, this study aims to evaluate the validity of generative AI as a complementary assessor for OSCE.

      METHODS: This experimental study was conducted at a medical university in Japan. We recruited 11 fifth-year medical students during the general internal medicine clerkship from April 2023 to December 2023. Participants conducted a mock medical interview with a patient experiencing abdominal pain and wrote patient notes. Four physicians independently evaluated the participants by reviewing medical interview videos and patient notes, while ChatGPT-4 was provided with interview transcripts and notes. Evaluations were conducted using the 6-domain rubric (patient care and communication, history taking, physical examination, patient notes, clinical reasoning, and management). Each domain was scored using a 6-point Likert scale, ranging from 1 (very poor) to 6 (excellent). Median scores were compared using the Wilcoxon signed-rank test, and the agreement between ChatGPT-4 and physicians was assessed using intraclass correlation coefficients (ICCs). All P values <.05 were considered statistically significant.

      RESULTS: Although ChatGPT-4 assigned higher scores than physicians in terms of physical examination (median 4.0, IQR 4.0-5.0 vs median 4.0, IQR 3.0-4.0; P=.02), patient notes (median 6.0, IQR 5.0-6.0 vs median 4.0, IQR 4.0-4.0; P=.002), clinical reasoning (median 5.0, IQR 5.0-5.0 vs median 4.0, IQR 3.0-4.0; P<.001), and management (median 6.0, IQR 5.0-6.0 vs median 4.0, IQR 2.5-4.5; P=.002), there were no significant differences in the scores of patient care and communication (median 5.0, IQR 5.0-5.0 vs median 5.0, IQR 4.0-5.0; P=.06) and history taking (median 5.0, IQR 4.0-5.0 vs median 5.0, IQR 4.0-5.0; P>.99), respectively. ICC values were low in all domains, except for history taking, where the agreement was still poor (ICC=0.36, 95% CI -0.32 to 0.78).

      CONCLUSIONS: ChatGPT-4 produced higher evaluation scores than physicians in several OSCE domains, though the agreement between them was poor. Although these preliminary results suggest that generative AI may be able to support assessment in some domains of OSCE, further research is needed to establish its reproducibility and validity. Generative AI like ChatGPT-4 shows potential as a complementary assessor for OSCE.

      TRIAL REGISTRATION: University Hospital Medical Information Network Clinical Trials Registry UMIN000050489; https://center6.umin.ac.jp/cgi-open-bin/ctr/ctr_his_list.cgi?recptno=R000057513.
    mergedFrom:
      - source: pubmed
        pmid: "41343812"
        doi: 10.2196/79465
    finalDecision: exclude
  - title: Artificial intelligence based personalized student feedback system -Sisu Athwala' to enhance exam performance of medical undergraduates.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; AI feedback tool addresses exam performance, study techniques, and stress management, not clinical/workplace assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Intervention targets exam feedback for undergraduates in a non-workplace educational setting rather than workplace-based clinical education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1371/journal.pone.0336154
    pmid: "41343424"
    authors: Seneviratne T, Manathunga S, Idirisingha W, Somaratne K, Marambe K, Dangahadeniya U
    year: "2025"
    abstract: |-
      BACKGROUND: In medical education, mentoring and feedback play crucial roles. Providing feedback on exam performance is a vital component as it allows students to improve. Feedback has to be tailor made and specific to the individual student. This needs lot of time and human resources, which are always not in abundance. Use of artificial intelligence (AI) is a promising proposition yet it comes with the integral problem of generating inaccurate responses by the Large language models (LLM). To alleviate and minimize this, we have developed our unique model 'Sisu Athwala' using retrieval augment generation (RAG) with custom LLM's.

      OBJECTIVE: To design and implement an AI-based tool using RAG to provide customized feedback to medical students to enhance their exam performance, minimizing the risk of generating inaccurate responses by the LLM's. To evaluate the AI tool by expert student mentors and by the end users.

      METHODS: The study was conducted at the Faculty of Medicine, University of Peradeniya, Sri Lanka. An AI based feedback tool was developed powered by Generative Pre-trained Transformers-4 (GPT-4) LLM using a RAG pipeline. Expert instruction sets were used to develop the data base through embedding model to minimize potential inaccuracies and biases. To generate user queries, students were provided with a self-evaluation form which was processed using Representative Vector Summarization (RVS). Hence most critical concerns of each student are distilled and captured accurately, minimizing noise or irrelevant details. The role of the AI tool was defined as a counsellor during Pre-processional alignment allowing professional manner throughout the interaction. User queries were processed using Open AI Application Programming Interface (API), utilizing GPT-4-turbo LLM. Students were invited to engage in conversations with the newly developed feedback tool. The AI tool was evaluated by the expert student mentors, as per its ability to give personalized feedback, use varied language expressions, and to introduce novel perspectives to students. End user perception on the use of AI tool was assessed using a questionnaire.

      RESULTS: Post implementation end user survey of the Sisu Athwala AI tool was largely positive. 92% mentioned the advices given by the tool on stress management were helpful. 60% believed that the study techniques suggested were useful. While further 60% thought they are comfortable using the tool. 52% find the advices on exam performances were helpful. In their open comments some suggested to have the tool as a mobile APP. 15 expert student mentors took part in evaluating the tool. 100% agreed that it effectively addressed key points of student strengths and identifies areas for improvements going by the Pendleton model. 90% agreed that Sisu- Athwala gives clear actionable plans.

      CONCLUSION: Sisu Athwala AI tool provided comprehensive tailor made feedback and guidance to medical students which was well received by the end users. Expert student mentors evaluation of the material generated by the AI tool were quite positive. Though this is not a replacement for human mentors it supports mentoring to be delivered circumventing the human resource constraints.
    mergedFrom:
      - source: pubmed
        pmid: "41343424"
        doi: 10.1371/journal.pone.0336154
    finalDecision: exclude
  - title: Comparative performance of ChatGPT-4o, ChatGPT-5, and gemini 2.5 flash on Persian internal medicine subspecialty board exams.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Tests LLM performance on board exams, not about assessment or feedback of human learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting (board examination context)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1038/s41598-025-31251-3
    pmid: "41339707"
    authors: Sheikhalishahi S, Haddadi A, Sadeghipour S, Rafiei F, Soltani H
    year: "2025"
    abstract: This study compared the performance of ChatGPT-4o, ChatGPT-5, and Gemini 2.5 Flash on the 2025 Iranian internal medicine subspecialty board examinations. A total of 650 multiple-choice questions from six subspecialties were tested, excluding image-based items. Each question was presented in Persian, and responses were evaluated against the official answer key. Accuracy rates were 68.9% for ChatGPT-4o, 74.5% for ChatGPT-5, and 79.9% for Gemini 2.5 Flash, with Gemini performing significantly better than both ChatGPT versions. ChatGPT-5 also showed a significant improvement over ChatGPT-4o, confirming rapid progress in model development. Subspecialty analysis revealed stronger results in rheumatology and respiratory medicine compared to nephrology, while question type and length had no significant impact on outcomes. An artificial neural network that combined the outputs of all three models reached 81.6% accuracy, slightly exceeding Gemini alone. These findings highlight Gemini-2.5 as the most reliable model for this high-stakes internal medicine exam. The results support the growing role of advanced AI systems as assistants in medical education and clinical practice. However, further research is needed to assess their use in multimodal and real-world clinical tasks.
    mergedFrom:
      - source: pubmed
        pmid: "41339707"
        doi: 10.1038/s41598-025-31251-3
    finalDecision: exclude
  - title: "Feasibility of a Specialized Large Language Model for Postgraduate Medical Examination Preparation: Single-Center Proof-Of-Concept Study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on examination preparation, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates specialized LLM for grading mock short-answer examination questions in postgraduate exam preparation"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Examines LLM-assisted postgraduate exam preparation and grading, not workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.2196/77580
    pmid: "41337739"
    authors: Leong Y, Nambiar L, Tay V, Lie S, Yuhe K
    year: "2025"
    abstract: |-
      BACKGROUND: Large language models (LLMs) are increasingly used in medical education for feedback and grading; yet their role in postgraduate examination preparation remains uncertain due to inconsistent grading, hallucinations, and user acceptance.

      OBJECTIVE: This study evaluates the Personalized Anesthesia Study Support (PASS), a specialized GPT-4 model developed to assist candidates preparing for Singapore's postgraduate specialist anesthesiology examination. We assessed user acceptance, grading interrater reliability, and hallucination detection rates to determine the feasibility of integrating specialized LLMs into high-stakes examination preparation.

      METHODS: PASS was built on OpenAI's GPT-4 and adapted with domain-specific prompts and references. Twenty-one senior anesthesiology residents completed a mock short answer question examination, which was independently graded by 3 human examiners and 3 PASS iterations. Participants reviewed feedback from both PASS and standard GPT-4 and completed a technology acceptance model (TAM) survey. Grading reliability was evaluated using Cohen and Fleiss κ. Hallucination rates were assessed by participants and examiners.

      RESULTS: Of the 21 participants, 17 (81%) completed the TAM survey, generating 136 responses. PASS scored significantly higher than standard GPT-4 in usefulness (mean 4.25, SD 0.50 vs mean 3.44, SD 0.82; P<.001), efficiency (mean 4.12, SD 0.61 vs mean 3.41, SD 0.74; P<.001), and likelihood of future use (mean 4.13, SD 0.75 vs mean 3.59, SD 0.90; P<.001), with no significant difference in ease of use (mean 4.56, SD 0.63 vs mean 4.50, SD 0.61; P=.35). Internal grading reliability was moderate for PASS (κ=0.522) and fair for human examiners (κ=0.275). Across 316 PASS-generated responses, 67 hallucinations and 189 deviations were labeled. Hallucination labeling rates were comparable between candidates (10/67, 15%) and examiners (57/249, 22.9%; P=.21), while examiners labeled significantly more deviations (168/249, 67.5% vs 21/67, 31%; P<.001).

      CONCLUSIONS: PASS demonstrated strong user acceptance and grading reliability, suggesting feasibility in high-stakes examination preparation. Experienced learners could identify major hallucinations at comparable rates to examiners, suggesting potential in self-directed learning but with continued need for caution. Further research should refine grading accuracy and explore multicenter evaluation of specialized LLMs for postgraduate medical education.
    mergedFrom:
      - source: pubmed
        pmid: "41337739"
        doi: 10.2196/77580
    finalDecision: exclude
  - title: "When AI models take the exam: large language models vs medical students on multiple-choice course exams."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on multiple-choice course exams, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares LLM performance with medical students on course MCQ exams, not about AI-based assessment, feedback, or analysis of clinical observation records in workplace settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:51.586000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Multiple-choice course exam comparison does not evaluate workplace-based clinical education assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:58.822000+00:00
    doi: 10.1080/10872981.2025.2592430
    pmid: "41316903"
    authors: Ros-Arlanz&#xf3;n P, Gutarra-&#xc1;vila R, Arrarte-Esteban V, Bertomeu-Gonz&#xe1;lez V, Hern&#xe1;ndez-Blasco L, Masi&#xe1; M, Navarro-Canto L, Nieto-Navarro J, Abarca J, Sempere A
    year: "2025"
    abstract: Large language models (LLMs) are increasingly used in healthcare and medical education, but their performance on institution-authored multiple-choice questions (MCQs), particularly with negative marking, remains unclear. To compare the examination performance of five contemporary LLMs with enrolled medical students on final multiple-choice (MCQ-style) course exams across four clinical courses. We conducted a comparative cross-sectional study at Miguel Hernández University (Spain) in 2025. Final exams in Infectious Diseases, Neurology, Respiratory Medicine, and Cardiovascular Medicine were administered under routine conditions in Spanish. Five LLMs (OpenAI o1, GPT-4o, DeepSeek R1, Microsoft Copilot, and Google Gemini 1.5 Flash) completed all MCQs in two independent runs. Scores were averaged and test-retest was estimated with Gwet's AC1. Student scores (n = 442) were summarized as mean ± SD or median (IQR). Pairwise differences between models were explored with McNemar's test; student-LLM contrasts were descriptive. Across courses, LLMs consistently exceeded the student median and, in several instances, the highest student score. Mean LLM courses scores ranged 7.46-9.88, versus student means 4.28-7.32. OpenAI o1 achieved the highest mean in three courses; Copilot led in Cardiovascular Medicine (text-only subset due to image limitations). All LLMs answered every MCQ and short term test-retest agreement was high (AC1 0.79-1.00). Aggregated across courses, LLMs averaged 8.75 compared with 5.76 for students. On department-set Spanish MCQ exams with negative marking, LLMs outperformed enrolled medical students, answered every item, and showed high short-term reproducibility. These findings support cautious, faculty-supervised use of LLMs as adjuncts to MCQ assessment (e.g. automated pretesting, feedback). Confirmation across institutions, languages, and image-rich formats, and evaluation of educational impact beyond accuracy are needed.
    mergedFrom:
      - source: pubmed
        pmid: "41316903"
        doi: 10.1080/10872981.2025.2592430
    finalDecision: exclude
  - title: "Towards A Fair Duel: Reflections on the Evaluation of DeepSeek-R1 and ChatGPT-4o in Chinese Medical Education."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article. This is a commentary/letter to the editor reflecting on methodological issues in another study."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: This is a reflections/commentary piece rather than original research, a systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1007/s10916-025-02316-7
    pmid: "41315131"
    authors: Li S
    year: "2025"
    abstract: The recent study by Wu et al. (2025) comparing DeepSeek-R1 and ChatGPT-4o on the Chinese National Medical Licensing Examination (CNMLE) provides an important contribution to understanding large language model (LLM) performance in non-English medical contexts. While their findings highlight the potential of LLMs in medical knowledge assessment, several methodological issues merit further discussion. First, the exclusive use of Chinese-language items without bilingual comparison may favor DeepSeek-R1, which demonstrates strong performance in Chinese, over ChatGPT-4o, whose training corpus is predominantly English-based. Second, the evaluation was conducted before the release of GPT-5, leading to potential disparities in reasoning capabilities between models. Third, the restriction to multiple-choice questions limits the assessment to factual recall rather than higher-order reasoning or clinical judgment. We commend the authors for initiating this valuable cross-linguistic analysis and suggest that future studies incorporate bilingual testing, ensure model functional parity, and include open-ended clinical items to more comprehensively evaluate LLMs' reasoning and interpretive competence in real-world medical education contexts.
    mergedFrom:
      - source: pubmed
        pmid: "41315131"
        doi: 10.1007/s10916-025-02316-7
    finalDecision: exclude
  - title: "Comparison of ChatGPT and DeepSeek on a Standardized Audiologist Qualification Examination in Chinese: Observational Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT and DeepSeek on an audiologist qualification exam; does not use generative AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Qualification-exam performance study; not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.2196/79534
    pmid: "41313805"
    authors: Qi B, Zheng Y, Wang Y, Xu L
    year: "2025"
    abstract: |-
      BACKGROUND: Generative artificial intelligence (GenAI), exemplified by ChatGPT and DeepSeek, is rapidly advancing and reshaping human-computer interaction with its growing reasoning capabilities and broad applications across fields such as medicine and education.

      OBJECTIVE: This study aimed to evaluate the performance of 2 GenAI models (ie, GPT-4-turbo and DeepSeek-R1) on a standardized audiologist qualification examination in Chinese and to explore their potential applicability in audiology education and clinical training.

      METHODS: The 2024 Taiwan Audiologist Qualification Examination, comprising 300 multiple-choice questions across 6 subject areas (ie, basic hearing science, behavioral audiology, electrophysiological audiology, principles and practice of hearing devices, health and rehabilitation of the auditory and balance systems, and hearing and speech communication disorders [including professional ethics]), was used to assess the performance of the 2 GenAI models. The complete answering process and reasoning paths of the models were recorded, and performance was analyzed by overall accuracy, subject-specific scores, and question-type scores. Statistical comparisons were performed at the item level using the McNemar test.

      RESULTS: ChatGPT and DeepSeek achieved overall accuracies of 80.3% (241/300) and 79.3% (238/300), respectively, which are higher than the passing criterion of the Taiwan Audiologist Qualification Examination (ie, 60% correct answers). The accuracies for the 6 subject areas were 88% (44/50), 70% (35/50), 86% (43/50), 76% (38/50), 82% (41/50), and 80% (40/50) for ChatGPT and 82% (41/50), 72% (36/50), 78% (39/50), 80% (40/50), 80% (40/50), and 84% (41/50) for DeepSeek. No significant differences were found between the two models at the item level (overall P=.79), with a small effect size (accuracy difference=+1%, Cohen h=0.02, odds ratio 0.90, 95% CI 0.53-1.52) and substantial agreement (κ=0.71). ChatGPT scored highest in basic hearing science (88%), whereas DeepSeek performed the best in hearing and speech communication disorders (84%). Both models scored lowest in behavioral audiology (ChatGPT: 70% and DeepSeek: 72%). Question-type analysis revealed that both models performed well on reverse logic questions (ChatGPT: 79/95, 83%; DeepSeek: 80/95, 84%) but performed moderately on complex multiple-choice questions (ChatGPT: 9/17, 53%; DeepSeek: 11/17, 65%). However, both models performed poorly on graph-based questions (ChatGPT: 2/11, 18%; DeepSeek: 4/11, 36%).

      CONCLUSIONS: Both GenAI models demonstrated strong professional knowledge and stable reasoning ability, meeting the basic requirements of clinical audiologists and suggesting their potential as supportive tools in audiology education. However, the presence of errors underscores the need for cautious use under educator supervision. Future research should explore their performance in open-ended, real-world clinical scenarios to assess practical applicability and limitations.
    mergedFrom:
      - source: pubmed
        pmid: "41313805"
        doi: 10.2196/79534
    finalDecision: exclude
  - title: "Evaluation of AI models for radiology exam preparation: DeepSeek vs. ChatGPT-3.5."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks DeepSeek and ChatGPT accuracy on radiology board-style questions; not about assessment or feedback of learner performance."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Radiology exam-preparation benchmark is not workplace-based assessment or feedback tied to clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1080/10872981.2025.2589679
    pmid: "41311245"
    authors: Hu N, Luo Y, Lei P
    year: "2025"
    abstract: "The rapid advancement of artificial intelligence (AI) chatbots has generated significant interest regarding their potential applications within medical education. This study sought to assess the performance of the open-source large language model DeepSeek-V3 in answering radiology board-style questions and to compare its accuracy with that of ChatGPT-3.5.A total of 161 questions (comprising 207 items) were randomly selected from the Exercise Book for the National Senior Health Professional Qualification Examination: Radiology. The question set included single-choice, multiple-choice, shared-stem, and case analysis questions. Both DeepSeek-V3 and ChatGPT-3.5 were evaluated using the same question set over a seven-day testing period. Response accuracy was systematically assessed, and statistical analyses were performed using Pearson's chi-square test and Fisher's exact test.DeepSeek-V3 achieved an overall accuracy of 72%, which was significantly higher than the 55.6% accuracy achieved by ChatGPT-3.5 (P < 0.001). Performance analysis by question type revealed DeepSeek's superior accuracy in single-choice questions (87.1%), though with comparatively lower performance in multiple-choice (55.7%) and case analysis questions (68.0%). Across clinical subspecialties, DeepSeek consistently outperformed ChatGPT, particularly in peripheral nervous system (P = 0.003), respiratory system (P = 0.008), circulatory system (P = 0.012), and musculoskeletal system (P = 0.021) domains.In conclusion, DeepSeek demonstrates considerable potential as an educational tool in radiology, particularly for knowledge recall and foundational learning applications. However, its relatively weaker performance on higher-order cognitive tasks and complex question formats suggests the need for further model refinement. Future research should investigate DeepSeek's capability in processing image-based questions and perform comparative analyses with more advanced models (e.g., GPT-5) to better evaluate its potential for medical education."
    mergedFrom:
      - source: pubmed
        pmid: "41311245"
        doi: 10.1080/10872981.2025.2589679
    finalDecision: exclude
  - title: "Evaluating the performance of large language models on the ASPS In-Service Examination: A comparative analysis with resident norms."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks LLM accuracy on plastic surgery in-service exam questions; not about assessment, feedback, or analysis of learner performance."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam benchmarking study does not address workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1016/j.bjps.2025.11.031
    pmid: "41308375"
    authors: Shekouhi R, Holohan M, Mirzalieva O, Byrd B, Guidry M, Palines P, Chim H
    year: "2026"
    abstract: The emergence of large language models (LLMs) has raised critical questions about their potential roles in surgical education. This study aims to evaluate the accuracy and comparative performance of three leading LLMs including ChatGPT 4.0, DeepSeek V3, and Gemini 2.5, on the American Board of Plastic Surgery Plastic Surgery In-Service Training Examination (PSITE) across a 20-year period. Our results showed that ChatGPT achieved the highest overall accuracy (75.0%), followed closely by DeepSeek (74.8%) and Gemini (74.5%), with no significant differences between models (p>0.05). When benchmarked against normative data, DeepSeek reached the highest percentile ranks (81st among residents, 89th among practitioners), followed by ChatGPT (78th and 84th), and Gemini (72nd and 90th), without significant differences in rankings across LLMs (p > 0.05). In conclusion, Modern LLMs demonstrate consistent and high-level performance on the PSITE, frequently exceeding the median performance of plastic surgery residents and practitioners.
    mergedFrom:
      - source: pubmed
        pmid: "41308375"
        doi: 10.1016/j.bjps.2025.11.031
    finalDecision: exclude
  - title: "Knowledge-level comparison in pulpal and periapical diseases: dental students versus artificial intelligence models (Gemini, Microsoft Copilot, ChatGPT-3.5, ChatGPT-4o): cross-sectional study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Compares AI chatbot accuracy to dental students on MCQs; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is based on multiple-choice test questions in a simulated testing context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.1186/s12909-025-08263-8
    pmid: "41299627"
    authors: Kurt &, &#x15e;imsek E
    year: "2025"
    abstract: |-
      BACKGROUND: This study explored the diagnostic accuracy of artificial intelligence (AI) chatbots and dental students when responding to questions related to pulpal and periapical diseases. Rapid advancements in AI have led to increased interest in their applicability to clinical education and decision-making in dentistry.

      OBJECTIVE: To compare the accuracy rates of responses given by dental students and various AI-based chatbots (ChatGPT-3.5, ChatGPT-4o, Gemini, and Microsoft Copilot) to multiple-choice questions designed to assess knowledge related to pulpal and periapical diseases.

      METHODS: The study included third- and fifth-year dental students representing different levels of clinical training, along with four distinct AI-based chatbots. A total of 327 responses were collected from students, while each chatbot generated 450 responses. The evaluation was based on 15 multiple-choice questions developed in accordance with the 2020 version of the American Association of Endodontists (AAE) clinical guidelines. The accuracy rates of the groups were compared using descriptive statistics, one-way ANOVA, Bonferroni post hoc tests for significant differences, and Chi-square tests for correct versus incorrect response ratios.

      RESULTS: The highest accuracy rate was observed among fifth-year dental students (85.1%), followed by ChatGPT-4o (79.6%), ChatGPT-3.5 (75.1%), Gemini (71.6%), third-year students (64.9%), and Microsoft Copilot (61.3%). A statistically significant difference was found among the groups (p < 0.05). ChatGPT-4o demonstrated a comparable accuracy rate to fifth-year students with more clinical experience (p > 0.05), whereas other chatbots and third-year students showed lower performance.

      CONCLUSION: Chatbots exhibited varying levels of accuracy in diagnosing pulpal and periapical diseases. ChatGPT-4o performed at a level similar to that of more clinically experienced students, suggesting its potential as a supportive tool in dental education and clinical decision support systems. However, the relatively lower accuracy rates of models such as Gemini and Microsoft Copilot underscore the continued importance of human expertise. These findings suggest that while AI systems may serve as complementary tools in education, they cannot fully replace clinical judgment grounded in human experience.
    mergedFrom:
      - source: pubmed
        pmid: "41299627"
        doi: 10.1186/s12909-025-08263-8
    finalDecision: exclude
  - title: The pitfalls of multiple-choice questions in generative AI and medical education.
    reviews:
      - decision: uncertain
        comment: Mentions generative AI and MCQs in medical education; may discuss assessment validity
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; evaluates validity of MCQ benchmarks for measuring LLM medical capabilities."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmark study on MCQ versus free-response performance, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1038/s41598-025-26036-7
    pmid: "41298584"
    authors: Singh S, Alyakin A, Alber D, Stryker J, Tong A, Sangwon K, Goff N, De La Paz M, Hernandez-Rovira M, Park K, Leuthardt E, Oermann E
    year: "2025"
    abstract: The performance of Large Language Models (LLMs) on multiple-choice question (MCQ) benchmarks is frequently cited as proof of their medical capabilities. We hypothesized that LLM performance on medical MCQs may in part be illusory and driven by factors beyond medical content knowledge and reasoning capabilities. To assess this, we created a novel benchmark of free-response questions with paired MCQs (FreeMedQA). Using this benchmark, we evaluated three state-of-the-art LLMs (GPT-4o, GPT-3.5, and LLama-3-70B-instruct) and found an average absolute deterioration of 39.43% in performance on free-response questions relative to multiple-choice (p = 1.3 * 10-5) which was greater than the human performance decline of 22.29%. To isolate the role of the MCQ format on performance, we performed a masking study, iteratively masking out parts of the question stem. At 100% masking, the average LLM multiple-choice performance was 6.70% greater than random chance (p = 0.002) with one LLM (GPT-4o) obtaining an accuracy of 37.34%. Notably, for all LLMs the free-response performance was near zero. Our results highlight the shortcomings in medical MCQ benchmarks for overestimating the capabilities of LLMs in medicine, and, broadly, the potential for improving both human and machine assessments using LLM-evaluated free-response questions.
    mergedFrom:
      - source: pubmed
        pmid: "41298584"
        doi: 10.1038/s41598-025-26036-7
    finalDecision: exclude
  - title: "Comparing AI chatbot simulation and peer role-play for OSCE preparation: a pilot randomized controlled trial."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: OSCE preparation is a simulated environment, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment/preparation is in simulated OSCE environment only."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    doi: 10.1186/s12909-025-08308-y
    pmid: "41286823"
    authors: Lee H, Kim J, Choi H, Bae H, Jeong A, Choi S, Kim J, Kim C
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) is increasingly applied in medical education, but its role in fostering interactive clinical competencies remains underexplored. This pilot study aimed to compare the feasibility and educational impact of an AI chatbot-based simulation with traditional peer role-play (PRP) for Objective Structured Clinical Examination (OSCE) preparation, and to share practical lessons from implementing a novel AI tool in a trial setting.

      METHODS: Nineteen final-year Korean medicine students were randomly assigned to either an AI chatbot group (n = 9) or a PRP group (n = 10) after a baseline knowledge test. Both groups underwent a 30-minute physical examination practice session, followed by a one-hour clinical interview training session specific to their group. The AI chatbot group practiced with a GPT-4o/Claude 3.5-based chatbot providing scenario-driven responses and automated feedback, while the PRP group practiced in pairs under tutor supervision. All participants then completed two OSCE stations (dizziness and shoulder pain). Performance was assessed using a structured checklist covering four domains: history taking, physical examination, patient education, and physician-patient interaction. Post-study questionnaires evaluated the learning experience.

      RESULTS: Although the differences in OSCE scores between the groups did not reach statistical significance, several interesting and complementary trends were observed. For example, the PRP group tended to score higher in history taking (mean 74.4 vs. 66.2 in dizziness scenario; Hedges' g = -0.68, mean 58.6 vs. 54.5 in shoulder pain scenario; Hedges' g = -0.21), while the AI chatbot group showed a tendency towards higher scores in patient education (32.5 vs. 22.2 in dizziness scenario Hedges' g = 0.44, 85.0 vs. 66.7 in shoulder pain scenario; Hedges' g = 0.99). Survey results reflected these following trends. The PRP group valued the authenticity of the interaction and the exam-like environment. In contrast, the AI chatbot group reported higher satisfaction with the autonomy, opportunity for repetitive practice, and structured feedback.

      CONCLUSION: In this pilot study, AI chatbot-based training and PRP demonstrated complementary strengths for OSCE preparation. While PRP appears effective for developing performance-based procedural and communication skills in a realistic setting, AI chatbots show potential for fostering clinical reasoning in a self-paced, reflective learning environment. These complementary strengths suggest a blended learning model, combining both methods, may be optimal for holistic clinical skills development. Further research is needed to validate these preliminary findings.
    mergedFrom:
      - source: pubmed
        pmid: "41286823"
        doi: 10.1186/s12909-025-08308-y
    finalDecision: exclude
  - title: Large Language Models Can Generate High-Quality Pathology Multiple-Choice Questions Comparable With Questions Written by a Human Expert.
    reviews:
      - decision: uncertain
        comment: LLMs generating pathology MCQs; abstract needed to determine assessment context
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records. Study focuses on LLM generation of pathology MCQs for examinations."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Centers on generation/quality of multiple-choice exam items, not assessment or feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.1016/j.modpat.2025.100940
    pmid: "41276086"
    authors: Borowitz M, Blackford A, Nagelia S, Hruban R
    year: "2026"
    abstract: Multiple-choice questions can be effective tools to assess student and trainee performance, but the creation of these questions can be time consuming and requires expertise. To test the quality of pathology test questions created by large language models (LLMs), 100 questions on pancreas pathology were written by a human expert, and 50 questions were generated by each of 2 LLMs (ChatGPT-4.0 and Gemini 2.5 Flash). After an initial review, 16% of the multiple-choice questions generated by the 2 LLMs had to be revised through additional interactive prompting. The final set of questions was then evaluated by 190 volunteers with a variety of backgrounds and levels of expertise. We found that ChatGPT-generated-but not Gemini-generated-questions were rated as easier than human-authored questions; there were slightly more poor/unacceptable questions compared with adequate/good/excellent questions written by the LLMs than those written by the human expert (11.7% vs 10.1%; odds ratio, 1.64; 95% CI, 1.13-2.37; P = .009), but there was no difference in the proportion of questions rated good or excellent. Qualitatively, human-authored questions were thought to be most clinically realistic but felt to be more inconsistent and sometimes thought to be testing trivial points. There was no difference in the mean point biserial between human-authored and LLM-generated questions (0.31 vs 0.29; P = .56). As LLMs improve, they will form a useful tool for the efficient generation of large numbers of high-quality pathology test questions.
    mergedFrom:
      - source: pubmed
        pmid: "41276086"
        doi: 10.1016/j.modpat.2025.100940
    finalDecision: exclude
  - title: "Teaching Clinical Reasoning in Health Care Professions Learners Using AI-Generated Script Concordance Tests: Mixed Methods Formative Evaluation."
    reviews:
      - decision: uncertain
        comment: AI-generated assessments for clinical reasoning; may involve WBA
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; SCTs are classroom-based written assessments administered to medical students, not conducted in a clinical workplace."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Conducted as classroom/test-based clinical reasoning education, not in a workplace-based or clinical practice assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.2196/76618
    pmid: "41264864"
    authors: Hudon A, Phan V, Charlin B, Wittmer R
    year: "2025"
    abstract: |-
      BACKGROUND: The integration of artificial intelligence (AI) in medical education is evolving, offering new tools to enhance teaching and assessment. Among these, script concordance tests (SCTs) are well-suited to evaluate clinical reasoning in contexts of uncertainty. Traditionally, SCTs require expert panels for scoring and feedback, which can be resource-intensive. Recent advances in generative AI, particularly large language models (LLMs), suggest the possibility of replacing human experts with simulated ones, though this potential remains underexplored.

      OBJECTIVE: This study aimed to evaluate whether LLMs can effectively simulate expert judgment in SCTs by using generative AI to author, score, and provide feedback for SCTs in cardiology and pneumology. A secondary objective was to assess students' perceptions of the test's difficulty and the pedagogical value of AI-generated feedback.

      METHODS: A cross-sectional, mixed methods study was conducted with 25 second-year medical students who completed a 32-item SCT authored by ChatGPT-4o (OpenAI). Six LLMs (3 trained on the course material and 3 untrained) served as simulated experts to generate scoring keys and feedback. Students answered SCT questions, rated perceived difficulty, and selected the most helpful feedback explanation for each item. Quantitative analysis included scoring, difficulty ratings, and correlations between student and AI responses. Qualitative comments were thematically analyzed.

      RESULTS: The average student score was 22.8 out of 32 (SD 1.6), with scores ranging from 19.75 to 26.75. Trained AI systems showed significantly higher concordance with student responses (ρ=0.64) than untrained models (ρ=0.41). AI-generated feedback was rated as most helpful in 62.5% of cases, especially when provided by trained models. The SCT demonstrated good internal consistency (Cronbach α=0.76), and students reported moderate perceived difficulty (mean 3.7, SD 1.1). Qualitative feedback highlighted appreciation for SCTs as reflective tools, while recommending clearer guidance on Likert-scale use and more contextual detail in vignettes.

      CONCLUSIONS: This is among the first studies to demonstrate that trained generative AI models can reliably simulate expert clinical reasoning within a script-concordance framework. The findings suggest that AI can both streamline SCT design and offer educationally valuable feedback without compromising authenticity. Future studies should explore longitudinal effects on learning and assess how hybrid models (human and AI) can optimize reasoning instruction in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "41264864"
        doi: 10.2196/76618
    finalDecision: exclude
  - title: Medical Students' Perception of Automated Note Feedback After Simulated Encounters.
    reviews:
      - decision: uncertain
        comment: Automated note feedback for medical students; may involve generative AI for assessment/feedback, though mentions simulated encounters
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments (simulated encounters)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments; automated NLP grading of patient notes written after standardized patient encounters"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses non-generative NLP-based automated grading rather than generative AI/LLMs."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1111/tct.70273
    pmid: "41251038"
    authors: Bansal S, Yadav M, Zhou J, Ebert-Allen R, Klute R, Bond W, Bhat S
    year: "2025"
    abstract: |-
      BACKGROUND: Grading medical student patient notes (PNs) is resource-intensive. Natural language processing (NLP) offers a promising solution to automatically grade PNs. We deployed an automated grading system that uses NLP and explored the perceived value of PN feedback.

      APPROACH: The automated system graded written notes after two standardized patient encounters by third-year medical students. The system generated an individualized report on 'items found' and 'items not found' in the history, physical examination, and diagnosis sections, which was shared with students for feedback via a web-based interface. By rotation, block students received either the automated case feedback first or the faculty-written model note feedback first (the pre-intervention baseline).

      EVALUATION: After reviewing feedback, students completed surveys for both automated feedback and model note feedback and participated in follow-up focus groups. In total, 44 students received feedback, 37 completed surveys, and 28 participated in focus groups. Qualitative themes that emerged suggested the automated feedback was visually appealing and allowed for easy comparison of items found vs. missing, which would help improve students' documentation skills. Model note appeared trustworthy.

      IMPLICATIONS: We found automated systems can be a potential tool for formative feedback on note writing activity although in terms of quality it does not surpass the pre-existing feedback methods, such as model note feedback used in our study. Order effects may have influenced these perceptions and the small sample size limits generalizability. Tested software had occasional errors in recognizing a phrase or showing a false positive.
    mergedFrom:
      - source: pubmed
        pmid: "41251038"
        doi: 10.1111/tct.70273
    finalDecision: exclude
  - title: "Performance of large language models in medical licensing examinations: a systematic review and meta-analysis."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on medical licensing examinations, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Meta-analysis of LLM accuracy on medical licensing exams; benchmarking AI on exams, not using AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Meta-analysis of medical licensing exam accuracy, not assessment/feedback or analysis of workplace clinical observation documentation."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.3352/jeehp.2025.22.36
    pmid: "41248547"
    authors: Nouri H, Mahdavi A, Abedi A, Mohammadnia A, Hamedan M, Amanzadeh M
    year: "2025"
    abstract: |-
      PURPOSE: This study systematically evaluates and compares the performance of large language models (LLMs) in answering medical licensing examination questions. By conducting subgroup analyses based on language, question format, and model type, this meta-analysis aims to provide a comprehensive overview of LLM capabilities in medical education and clinical decision-making.

      METHODS: This systematic review, registered in PROSPERO and following PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines, searched MEDLINE (PubMed), Scopus, and Web of Science for relevant articles published up to February 1, 2025. The search strategy included Medical Subject Headings (MeSH) terms and keywords related to ("ChatGPT" OR "GPT" OR "LLM variants") AND ("medical licensing exam*" OR "medical exam*" OR "medical education" OR "radiology exam*"). Eligible studies evaluated LLM accuracy on medical licensing examination questions. Pooled accuracy was estimated using a random-effects model, with subgroup analyses by LLM type, language, and question format. Publication bias was assessed using Egger's regression test.

      RESULTS: This systematic review identified 2,404 studies. After removing duplicates and excluding irrelevant articles through title and abstract screening, 36 studies were included after full-text review. The pooled accuracy was 72% (95% confidence interval, 70.0% to 75.0%) with high heterogeneity (I2=99%, P<0.001). Among LLMs, GPT-4 achieved the highest accuracy (81%), followed by Bing (79%), Claude (74%), Gemini/Bard (70%), and GPT-3.5 (60%) (P=0.001). Performance differences across languages (range, 62% in Polish to 77% in German) were not statistically significant (P=0.170).

      CONCLUSION: LLMs, particularly GPT-4, can match or exceed medical students' examination performance and may serve as supportive educational tools. However, due to variability and the risk of errors, they should be used cautiously as complements rather than replacements for traditional learning methods.
    mergedFrom:
      - source: pubmed
        pmid: "41248547"
        doi: 10.3352/jeehp.2025.22.36
    finalDecision: exclude
  - title: "Evaluating the Performance of DeepSeek-R1 and DeepSeek-V3 Versus OpenAI Models in the Chinese National Medical Licensing Examination: Cross-Sectional Comparative Study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on national medical licensing examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Benchmarks LLM performance on the Chinese NMLE, not about using AI for learner assessment or WBA feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM performance on national licensing exam questions rather than WBA-related assessment feedback or clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.2196/73469
    pmid: "41237388"
    authors: Wang W, Zhou Y, Fu J, Hu K
    year: "2025"
    abstract: |-
      BACKGROUND: Deepseek-R1, an open-source large language model (LLM), has generated significant global interest in the past months.

      OBJECTIVE: This study aimed to compare the performance of DeepSeek and OpenAI LLMs on the Chinese National Medical Licensing Examination (NMLE) and evaluate their potential in medical education.

      METHODS: This cross-sectional study assessed 2 DeepSeek models (DeepSeek-R1 and DeepSeek-V3), 3 OpenAI models (ChatGPT-o1 pro, ChatGPT-o3 mini, and GPT-4o), and 2 additional Chinese LLMs (ERNIE 4.5 Turbo and Qwen 3) using the 2021 NMLE. Model performance was evaluated based on overall accuracy, accuracy across question types (A1, A2, A3 and A4, and B1), case analysis and non-case analysis questions, medical specialties, and accuracy consensus between different model combinations.

      RESULTS: All LLMs successfully passed the NMLE. DeepSeek-R1 achieved the highest accuracy (573/597, 96%), followed by DeepSeek-V3 (558/600, 93%), both of which significantly outperformed ChatGPT-o1 pro (450/600, 75%), ChatGPT-o3 mini (455/600, 75.8%), and GPT-4o (452/600, 75.3%; P<.001 for all comparisons). Performance disparities were consistent across various question types (A1, A2, A3 and A4, and B1), case analysis and non-case analysis questions, different types of case analyses, and medical specialties. The accuracy consensus between DeepSeek-R1 and DeepSeek-V3 reached 97.7% (544/557), significantly outperforming DeepSeek-R1 alone (P=.04). Two additional Chinese LLMs, ERNIE 4.5 Turbo (572/600, 95.3%) and Qwen 3 (555/600, 92.5%), also exhibited significantly better performance compared to the 3 OpenAI models (all P<.001).

      CONCLUSIONS: This study demonstrates that DeepSeek-R1 and DeepSeek-V3 significantly outperform OpenAI models on the NMLE. DeepSeek models show promise as tools for medical education and exam preparation in the Chinese language.
    mergedFrom:
      - source: pubmed
        pmid: "41237388"
        doi: 10.2196/73469
    finalDecision: exclude
  - title: "AI performance in emergency medicine fellowship examination: comparative analysis of ChatGPT-4o, Gemini 2.0, Claude 3.5, and DeepSeek R1 models."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM accuracy on a standardized fellowship certification exam (MCQs), not about AI-based assessment, feedback, or analysis of clinical observation records in workplace settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:51.586000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Exam-based benchmarking study, not in a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:58.822000+00:00
    doi: 10.55730/1300-0144.6083
    pmid: "41234459"
    authors: "&#x15e;an &, Akkan &#xd6;z M, Yortanli M, Gen&#xe7; M, Bulut B, G&#xfc;r A, Mutlu H, Yazici R, G&#xf6;nen M"
    year: "2025"
    abstract: |-
      BACKGROUND/AIM: This study evaluated the accuracy rates and response consistency of four different large language models (ChatGPT-4o, Gemini 2.0, Claude 3.5, and DeepSeek R1) in answering questions from the Emergency Medicine Fellowship Examination (YDUS), which was administered for the first time in Türkiye.

      MATERIALS AND METHODS: In this observational study, 60 multiple-choice questions from the Emergency Medicine YDUS administered on 15 December 2024, were classified as knowledge-based (n = 26), visual content (n = 2), and case-based (n = 32). Each question was presented three times to the four large language models. The models' accuracy rates were evaluated according to overall accuracy, strict accuracy, and ideal accuracy criteria. Response consistency was measured using Fleiss' Kappa test.

      RESULTS: The ChatGPT-4o model was the most successful in terms of overall accuracy (90.0%), while DeepSeek R1 showed the lowest performance (76.7%). Claude 3.5 (83.3%) and Gemini 2.0 (80.0%) demonstrated moderate success. When analyzed by category, ChatGPT-4o achieved the highest success with 92.3% accuracy in knowledge-based questions and 90.6% in case-based questions. In terms of response consistency, the Claude 3.5 model (Fleiss' Kappa = 0.68) showed the highest consistency, while Gemini 2.0 (Fleiss' Kappa = 0.49) showed the lowest. Inconsistent hallucinations were more frequent in the Gemini 2.0 and DeepSeek R1 models, whereas persistent hallucinations were less common in the ChatGPT-4o and Claude 3.5 models.

      CONCLUSION: Large language models can achieve high accuracy rates for knowledge and clinical reasoning questions in emergency medicine but show differences in terms of response consistency and hallucination tendency. While these models have significant potential for use in medical education and as clinical decision support systems (CDSS), they need further development to provide reliable, up-to-date, and accurate information.
    mergedFrom:
      - source: pubmed
        pmid: "41234459"
        doi: 10.55730/1300-0144.6083
    finalDecision: exclude
  - title: "Large Language Models for the National Radiological Technologist Licensure Examination in Japan: Cross-Sectional Comparative Benchmarking and Evaluation of Model-Generated Items Study."
    reviews:
      - decision: uncertain
        comment: LLMs on licensure exam and item generation; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM accuracy on licensure exam and quality of LLM-generated exam items; not about assessment or feedback of learners in workplace-based settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on licensure exam performance and item generation, not workplace-based assessment or clinical observation-record feedback/analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.2196/81807
    pmid: "41232030"
    authors: Ito T, Ishibashi T, Hayashi T, Kojima S, Sogabe K
    year: "2025"
    abstract: |-
      BACKGROUND: Mock examinations are widely used in health professional education to assess learning and prepare candidates for national licensure. However, instructor-written multiple-choice items can vary in difficulty, coverage, and clarity. Recently, large language models (LLMs) have achieved high accuracy in medical examinations, highlighting their potential for assisting item-bank development; however, their educational quality remains insufficiently characterized.

      OBJECTIVE: This study aimed to (1) identify the most accurate LLM for the Japanese National Examination for Radiological Technologists and (2) use the top model to generate blueprint-aligned multiple-choice questions and evaluate their educational quality.

      METHODS: Four LLMs-OpenAI o3, o4-mini, o4-mini-high (OpenAI), and Gemini 2.5 Flash (Google)-were evaluated on all 200 items of the 77th Japanese National Examination for Radiological Technologists in 2025. Accuracy was analyzed for overall items and for 173 nonimage items. The best-performing model (o3) then generated 192 original items across 14 subjects by matching the official blueprint (image-based items were excluded). Subject-matter experts (≥5 y as coordinators and routine mock examination authors) independently rated each generated item on five criteria using a 5-point scale (1=unacceptable, 5=adoptable): item difficulty, factual accuracy, accuracy of content coverage, appropriateness of wording, and instructional usefulness. Cochran Q with Bonferroni-adjusted McNemar tests compared model accuracies, and one-sided Wilcoxon signed-rank tests assessed whether the median ratings exceeded 4.

      RESULTS: OpenAI o3 achieved the highest accuracy overall (90.0%; 95% CI 85.1%-93.4%) and on nonimage items (92.5%; 95% CI 87.6%-95.6%), significantly outperforming o4-mini on the full set (P=.02). Across models, accuracy differences on the non-image subset were not significant (Cochran Q, P=.10). Using o3, the 192 generated items received high expert ratings for item difficulty (mean, 4.29; 95% CI 4.11-4.46), factual accuracy (4.18; 95% CI 3.98-4.38), and content coverage (4.73; 95% CI 4.60-4.86). Ratings were comparatively lower for appropriateness of wording (3.92; 95% CI 3.73-4.11) and instructional usefulness (3.60; 95% CI 3.41-3.80). For these two criteria, the tests did not support a median rating >4 (one-sided Wilcoxon, P=.45 and P≥.99, respectively). Representative low-rated examples (ratings 1-2) and the rationale for those scores-such as ambiguous phrasing or generic explanations without linkage to stem cues-are provided in the supplementary materials.

      CONCLUSIONS: OpenAI o3 can generate radiological licensure items that align with national standards in terms of difficulty, factual correctness, and blueprint coverage. However, wording clarity and the pedagogical specificity of explanations were weaker and did not meet an adoptable threshold without further editorial refinement. These findings support a practical workflow in which LLMs draft syllabus-aligned items at scale, while faculty perform targeted edits to ensure clarity and formative feedback. Future studies should evaluate image-inclusive generation, use Application Programming Interface (API)-pinned model snapshots to increase reproducibility, and develop guidance to improve explanation quality for learner remediation.
    mergedFrom:
      - source: pubmed
        pmid: "41232030"
        doi: 10.2196/81807
    finalDecision: exclude
  - title: Performance of Foundation Models vs Physicians in Textual and Multimodal Ophthalmological Questions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; benchmarks foundation model performance against physicians on ophthalmology examination questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates foundation models on ophthalmology exam-style questions, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1001/jamaophthalmol.2025.4255
    pmid: "41231508"
    authors: Rocha H, Chong Y, Thirunavukarasu A, Wong Y, Wong S, Chang Y, Azzopardi M, Tan B, Song A, Malem A, Jain N, Zhou S, Tan T, Rauz S, Ang M, Mehta J, Ting D, Ting D
    year: "2026"
    abstract: |-
      IMPORTANCE: There is an increasing amount of literature evaluating the clinical knowledge and reasoning performance of large language models (LLMs) in ophthalmology, but to date, investigations into its multimodal abilities clinically-such as interpreting images and tables-have been limited.

      OBJECTIVE: To evaluate the multimodal performance of the following 7 foundation models (FMs): GPT-4o (OpenAI), Gemini 1.5 Pro (Google), Claude 3.5 Sonnet (Anthropic), Llama-3.2-11B (Meta), DeepSeek V3 (High-Flyer), Qwen2.5-Max (Alibaba Cloud), and Qwen2.5-VL-72B (Alibaba Cloud) in answering offline Fellowship of the Royal College of Ophthalmologists part 2 written multiple-choice textual and multimodal questions, with head-to-head comparisons with physicians.

      DESIGN, SETTING, AND PARTICIPANTS: This cross-sectional study was conducted between September 2024 and March 2025 using questions sourced from a textbook used as an examination preparation resource for the Fellowship of the Royal College of Ophthalmologists part 2 written examination.

      EXPOSURE: FM performance.

      MAIN OUTCOMES AND MEASURES: The primary outcome measure was FM accuracy, defined as the proportion of answers generated by the model matching the textbook's labeled letter answer.

      RESULTS: For textual questions, Claude 3.5 Sonnet (accuracy, 77.7%) outperformed all other FMs (followed by GPT-4o [accuracy, 69.9%], Qwen2.5-Max [accuracy, 69.3%], DeepSeek V3 [accuracy, 63.2%], Gemini Advanced [accuracy, 62.6%], Qwen2.5-VL-72B [accuracy, 58.3%], and Llama-3.2-11B [accuracy, 50.7%]), ophthalmology trainees (difference, 9.0%; 95% CI, 2.4%-15.6%; P = .01) and junior physicians (difference, 35.2%; 95% CI, 28.3%-41.9%; P < .001), with comparable performance with expert ophthalmologists (difference, 1.3%; 95% CI, -5.1% to 7.4%; P = .72). GPT-4o (accuracy, 69.9%) outperformed GPT-4 (OpenAI; difference, 8.5%; 95% CI, 1.1%-15.8%; P = .02) and GPT-3.5 (OpenAI; difference, 21.8%; 95% CI, 14.3%-29.2%; P < .001). For multimodal questions, GPT-4o (accuracy, 57.5%) outperformed all other FMs (Claude 3.5 Sonnet [accuracy, 47.5%], Qwen2.5-VL-72B [accuracy, 45%], Gemini Advanced [accuracy, 35%], and Llama-3.2-11B [accuracy, 25%]) and the junior physician (difference, 15%; 95% CI, -6.7% to 36.7%; P = .18) but was weaker than expert ophthalmologists (accuracy range, 70.0%-85.0%; P = .16) and trainees (accuracy range, 62.5%-80%; P = .35).

      CONCLUSIONS AND RELEVANCE: Results of this cross-sectional study suggest that for textual questions, current FMs exhibited notable improvements in ophthalmological knowledge reasoning when compared with older LLMs and ophthalmology trainees, with performance comparable with that of expert ophthalmologists. These models demonstrated potential for medical assistance for answering ophthalmological textual queries, but their multimodal abilities remain limited. Further research or fine-tuning models with diverse ophthalmic multimodal data may lead to more capable applications with multimodal functionalities.
    mergedFrom:
      - source: pubmed
        pmid: "41231508"
        doi: 10.1001/jamaophthalmol.2025.4255
    finalDecision: exclude
  - title: Performance of Large Language Models on Radiology Residency In-Training Examination Questions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks LLM accuracy on radiology residency in-training exam questions; not about assessment/feedback of learners' clinical performance in workplace settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Residency in-training exam question study, not workplace-based clinical assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1016/j.acra.2025.10.043
    pmid: "41224539"
    authors: Salbas A, Yogurtcu M
    year: "2026"
    abstract: |-
      RATIONALE AND OBJECTIVES: Large language models (LLMs) are increasingly investigated in radiology education. This study evaluated the performance of several advanced LLMs on radiology residency in-training examination questions, with a focus on whether recently released versions show improved accuracy compared with earlier models.

      MATERIALS AND METHODS: We analyzed 282 multiple-choice questions (191 text-only, 91 image-based) from institutional radiology residency examinations conducted between 2023 and 2025. Five LLMs were tested: ChatGPT-4o, ChatGPT-5, Claude 4 Opus, Claude 4.1 Opus, and Gemini 2.5 Pro. Radiology resident performance on the same set of questions was also analyzed for comparison. Accuracy rates were calculated for overall, text-only, and image-based questions, and results were compared using Cochran's Q and Bonferroni-adjusted McNemar tests. Outputs were also assessed for hallucinations.

      RESULTS: Gemini 2.5 Pro achieved the highest overall accuracy (83.0%), followed by ChatGPT-5 (82.3%). By comparison, radiology residents achieved an overall accuracy of 78.2%. ChatGPT-5 showed significantly higher accuracy compared with ChatGPT-4o (p = 0.021), and Gemini 2.5 Pro showed significantly higher accuracy compared with Claude 4 Opus (p = 0.026). For text-only questions, the highest accuracy was obtained with Gemini 2.5 Pro (88.0%). For image-based questions, radiology residents achieved the highest accuracy (80.4%), followed by ChatGPT-5 (73.6%). The highest accuracies by subspecialty were observed in interventional radiology and physics, whereas breast imaging yielded the lowest accuracy across the models. No instances of hallucination were observed.

      CONCLUSION: LLMs demonstrated generally good performance on radiology residency assessments, with newer versions showing measurable improvements. However, limitations persist in image-based interpretation and certain subspecialties. LLMs should therefore be regarded as supportive resources in radiology education, with careful validation and continued refinement of medical training data.
    mergedFrom:
      - source: pubmed
        pmid: "41224539"
        doi: 10.1016/j.acra.2025.10.043
    finalDecision: exclude
  - title: "DeepSeek-R1 for automated scoring in radiology residency examinations: an agreement and test-retest reliability study."
    reviews:
      - decision: uncertain
        comment: LLM for automated scoring of residency examinations with reliability data; may involve workplace-based assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: uncertain
        comment: LLM-based automated scoring of diagnostic radiology reports written by residents with inter-rater reliability data (ICC). Needs full-text review to confirm whether reports are from clinical workplace or exam-specific tasks
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. The study evaluates DeepSeek-R1 for scoring radiology residents' diagnostic reports produced in a formal residency training completion examination using curated X-ray cases from a database presented via PowerPoint. This is a standardized examination setting, not authentic workplace-based assessment of clinical performance."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:52.271000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment was conducted as a structured residency examination using case sets, not workplace-based clinical assessments."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:00.184000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Standardized examination setting using curated cases via PowerPoint, not authentic WBA. Criterion 3 (simulation) is not appropriate here as this is a written exam, not a clinical simulation (OSCE)."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1186/s12909-025-08184-6
    pmid: "41219771"
    authors: Niu S, Liu X, Huang L, Li Y, Wang G
    year: "2025"
    abstract: |-
      OBJECTIVE: This study evaluates the feasibility of employing DeepSeek-R1 for automated scoring in examinations for radiology residents, comparing its performance with that of radiologists.

      METHODS: A cross-sectional study was undertaken to assess 504 diagnostic radiology reports produced by eighteen third-year radiology residents. The evaluations were independently conducted by Radiologist A, Radiologist B, and DeepSeek-R1 (as of June 15, 2025), utilizing standardized scoring rubrics and predefined evaluation criteria. One month after the initial evaluation, a re-assessment was performed by DeepSeek-R1 and Radiologist A. The inter-rater reliability among Radiologist A, Radiologist B, and DeepSeek-R1, in addition to the test-retest reliability, was analyzed using intraclass correlation coefficients (ICC).

      RESULTS: The ICC values between DeepSeek-R1 and Radiologist A, DeepSeek-R1 and Radiologist B, and Radiologist A and Radiologist B were found to be 0.879, 0.820, and 0.862, respectively. The test-retest ICC for DeepSeek-R1 was determined to be 0.922, whereas for Radiologist A, it was 0.952. The ICC between DeepSeek-R1 (re-test) and Radiologist A (re-test) was 0.885.

      CONCLUSION: The performance of DeepSeek-R1 was comparable to that of radiologists in the evaluation of radiology residents' reports. The integration of DeepSeek-R1 into medical education could effectively assist in assessment tasks, potentially alleviating faculty workload while preserving the quality of evaluations.
    mergedFrom:
      - source: pubmed
        pmid: "41219771"
        doi: 10.1186/s12909-025-08184-6
    finalDecision: exclude
  - title: "Stigmatizing Mothers: Qualitative Analysis of Language in Prenatal Records."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; qualitative analysis of language in prenatal records"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Not focused on medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    doi: 10.1353/nib.2025.a974066
    pmid: "41208679"
    authors: Gross M, Mendoza-Cervantes D, Zabec J, Dewan A, Beach M
    year: "2025"
    abstract: 'Pregnant people experience moral judgment in healthcare settings that may be coded into clinical documentation. Stigmatizing language in medical records transmits bias between clinicians, potentially exacerbating disparities in maternal morbidity and mortality. We examined obstetrical records from 100 randomly selected patients who received prenatal and delivery care in an academic hospital system. Qualitative analysis sought to identify linguistic features conveying negative attitudes or moral judgment, revealing themes of epistemic injustice: (1) discrediting patient testimony as incompetent, unreliable, and hysterical; (2) unnecessary details that are objectifying, stigmatizing, or unprofessional; and (3) judgments of maternal fitness, where women are labeled "bad mothers" by emphasizing neglectful, selfish, and debauched characteristics. We conclude by advocating for further validation of our findings, revisiting medical education paradigms, and supporting the development of natural language processing (NLP) technologies to detect and intercept stigma.'
    mergedFrom:
      - source: pubmed
        pmid: "41208679"
        doi: 10.1353/nib.2025.a974066
    finalDecision: exclude
  - title: "Artificial intelligence-based chatbots improve the efficiency of course orientation among medical students: a cross-sectional study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About AI chatbots for course orientation, not about assessment or feedback"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1186/s12909-025-08146-y
    pmid: "41194059"
    authors: Fodor G, Tolnai J, R&#xe1;rosi F, Nagy A, Pet&#xe1;k F
    year: "2025"
    abstract: |-
      BACKGROUND: Large language models (LLMs) like ChatGPT offer new ways to improve academic and administrative workflows in medical education, particularly for students studying in a language that is not their native tongue. We set out to examine whether a custom-trained chatbot could better support course orientation compared to traditional text-based resources for an ESL student population.

      METHODS: Seventy-five first-year international medical students at the University of Szeged, Hungary participated during the first session of an introductory medical informatics course to use either a custom-trained ChatGPT-based chatbot (AI group, n = 35) or the university's standard e-learning platform (Traditional group, n = 40) to locate key course information. A six-item test based on the syllabus assessed information retrieval within a 10-min window. Test scores, time per question, and question revisits were compared using independent t-tests and two-way ANOVA.

      RESULTS: No differences were found in previous use of AI-based tools between the two groups (Traditional: 90.0% vs AI group: 91.4%, P = 1). The AI group scored significantly higher (5.0 [4.8 - 6.0], median[IQR]) than the Traditional group (3.5 [2.8 - 4.3], P < 0.001). The majority of AI users (24/35) achieved 5 or more correct answers, compared to just 6/40 in the Traditional group. The lowest scores (0 points) were found exclusively in the Traditional group. No significant difference was observed in total test completion time (Traditional: 380.3 ± 158.6; AI: 433.7 ± 121.4; P = 0.10), time spent per question (P = 0.90), or question revisits (P = 0.99). Notably, the AI group outperformed on specific questions related to course logistics, such as attendance (62.9% vs 17.5% correct) and grading criteria (82.9% vs 55.0% correct) (both P < 0.001).

      CONCLUSIONS: Artificial intelligence-assisted information retrieval significantly improved students' ability to locate and understand key course content. Despite comparable time-on-task and navigation behaviours between groups, students using the ChatGPT-based chatbot achieved markedly higher test scores, likely due to the simplified and focused responses it provided. The absence of zero scores in the AI group and enhanced performance on logistical questions suggest that conversational interfaces may improve comprehension and retention of procedural information. These results highlight the potential of LLM-based tools to support more efficient onboarding in medical education, with a potential for broader applications across educational settings.
    mergedFrom:
      - source: pubmed
        pmid: "41194059"
        doi: 10.1186/s12909-025-08146-y
    finalDecision: exclude
  - title: "Position statement from the society of University surgeons, surgical education committee: Artificial intelligence in surgical training for medical students, residents, and fellows."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Explicitly a position statement, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:20.264000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Position statement is not an original research article, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:21.767000+00:00
    doi: 10.1016/j.surg.2025.109849
    pmid: "41187419"
    authors: Kewalramani D, Jawa R, Martin C, Gosain A, Wan D, Varghese T, Boland G, Dream S, Sifri Z, Mollen K, Chu D, Narayan M
    year: "2026"
    abstract: |-
      OBJECTIVE: To provide guidance from the Society of University of Surgeons' Surgical Education Committee on the responsible integration of artificial intelligence into surgical training.

      METHODS: Members of the Education Committee reviewed current applications of artificial intelligence in surgical training and convened expert discussions to frame opportunities, risks, and implementation strategies. The Committee emphasized narrow artificial intelligence (eg, computer vision, predictive analytics) and generative artificial intelligence (eg, large language models), clarifying their definitions and describing their applications to surgical education. Stakeholder-specific recommendations were developed for accrediting bodies, professional societies, institutions, and individual educators.

      RESULTS: Although artificial intelligence offers the potential to enhance learner engagement, reduce training variability, and improve global access to high-quality education, its integration raises critical challenges related to academic integrity, redefining faculty roles, adapting assessment methods, and ensuring patient safety. This position statement from the Society of University of Surgeons provides a framework for responsibly incorporating artificial intelligence from policy-level decisions and national standard-setting, institutional infrastructure and ethical oversight, down to practical day-to-day mentorship by surgeon educators. We emphasize foundational competencies such as prompt engineering and artificial intelligence literacy, clinical and educational applications, ethical considerations in research, robust skill evaluation methods, institutional accountability, and global equity, particularly in low-resource settings.

      CONCLUSION: Artificial intelligence has the potential to be a powerful adjunct to surgical education if deployed thoughtfully and ethically. This position statement outlines a phased, stakeholder-specific framework to ensure artificial intelligence complements, rather than replaces the vital educator-learner relationship, while fostering safer, more reflective, and globally connected surgeons.
    mergedFrom:
      - source: pubmed
        pmid: "41187419"
        doi: 10.1016/j.surg.2025.109849
    finalDecision: exclude
  - title: Performance of AI Chatbots on Head and Neck Pathology Board-Style Exam Questions and Guidelines for Responsible Use.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: AI chatbot performance on board-style exam questions, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; evaluates chatbot accuracy on board-style head and neck pathology MCQs and generates study revision questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Pilot study of chatbot performance on board-style pathology MCQs, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1007/s12105-025-01850-7
    pmid: "41182557"
    authors: Khoury Z, Sultan A
    year: "2025"
    abstract: The promising integration of artificial intelligence (AI), particularly large language models (LLMs) or AI chatbots, into medical education and practice necessitates rigorous evaluation of their capabilities. While chatbot performance has been assessed against standardized exams like the bar, medical and dental boards, and in diagnostic specialties such as radiology and ophthalmology, a critical gap exists within the complex discipline of pathology, specifically concerning subspecialty areas like head and neck pathology. Conducting a pilot study evaluating chatbot responses to head and neck pathology board-style questions is therefore essential and novel. Crucially for education, this pilot study evaluated six AI chatbots on their performance in answering multiple-choice questions (MCQs) retrieved from authorities in the field of head and neck pathology. Twenty MCQs relevant to head and neck pathology were identified from the public domain. A total of 120 responses from six chatbots were evaluated for response and citation accuracy. Although AI chatbots answered head and neck pathology board-style questions with 85-100% accuracy, citation accuracy and performance on image-based questions were poor. Considering the pedagogical principle that actively challenging exam questions enhances memory retention and learning outcomes compared to passive text review, AI-generated multiple-choice questions specifically designed for head and neck pathology study revision purposes were generated. The generated MCQs were determined to be at a more fundamental level and suitable for predoctoral dental students or first-year pathology residents. This pilot study probed the limitations of these tools in accurately addressing diagnostic and interpretive challenges. Furthermore, the recommendations and guidelines informed from the results of this pilot study represent a vital first step in the addressing the responsible use of AI for head and neck pathology education and practice.
    mergedFrom:
      - source: pubmed
        pmid: "41182557"
        doi: 10.1007/s12105-025-01850-7
    finalDecision: exclude
  - title: "Evaluating ChatGPT-4o's accuracy in answering American Board of Dermatology practice questions: an analysis of AI in dermatology residency education."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; evaluates LLM accuracy on dermatology board exam practice questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Assesses ChatGPT-4o on dermatology board practice questions, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.25251/bvjddn46
    pmid: "41171802"
    authors: McGrath L, Schedler N, Martin S, Hrin M, Mariencheck M, Feldman S, Akkurt Z
    year: "2025"
    abstract: Artificial intelligence may enhance medical education. This study evaluates ChatGPT-4o's accuracy in answering sample questions from the American Board of Dermatology BASIC, CORE, and APPLIED examinations. Fifty publicly available questions, with and without images, were analyzed for accuracy and performance across difficulty levels and categories. Its performance varied significantly between text-only and image-based questions, with lower accuracy on image-based questions (47%). Improvements in artificial intelligence for the use in dermatology residency education are necessary, as limitations in visual diagnostic skills were evident.
    mergedFrom:
      - source: pubmed
        pmid: "41171802"
        doi: 10.25251/bvjddn46
    finalDecision: exclude
  - title: Assessment of the Ability of the ChatGPT-5 Model to Pass the Endocrinology Specialization Exam.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on specialization exam, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates ChatGPT-5 accuracy on Polish endocrinology specialty examination questions"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT performance on a specialty exam rather than workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.7759/cureus.93479
    pmid: "41170234"
    authors: Latkowska A, Sawina P, Dolata T, Boczkowski D, Wielochowska A, Kowalczyk A, Loson-Kawalec M, Radej D, Jaworski W, Majchrowicz W, Olender M, Adamiak J, Sroczynska J, Suleiman R, Glinska J, Szczerbanowicz P, Dadynska P
    year: "2025"
    abstract: "Background In recent years, AI has undergone rapid development, particularly with the advancement of the ChatGPT model developed by OpenAI, which has found broad applications across scientific disciplines, including medicine. This study aims to evaluate the performance of the most recent ChatGPT-5 model in completing the Polish specialty examination in endocrinology. Specifically, we assessed the model's accuracy in answering clinical and theoretical questions, as well as the confidence and consistency of its responses. Materials and methods This study utilized the Polish spring 2025 endocrinology specialty examination, which comprised 120 multiple-choice questions, each with five options and a single correct answer. The ChatGPT-5 model was provided with the official examination regulations as well as the complete set of questions and answer options in Polish. Model-generated responses were compared against the official answer key published by the Medical Examination Center (CEM), and the declared confidence level (1-5 scale) was recorded. Questions were classified into two categories: theoretical/other and clinical. Statistical analyses were performed using Microsoft Excel and GraphPad Prism 10, employing the chi-square test and the Mann-Whitney U test. Results The latest ChatGPT-5 Plus model achieved a score of 76.47%, corresponding to 91 correct and 28 incorrect answers (23.53%). One question was excluded due to inconsistency with current medical knowledge. The passing threshold of at least 60% was exceeded. Conclusions The ChatGPT-5 model successfully completed the Polish endocrinology specialty examination, surpassing the official pass mark of 60%. Its result, an accuracy of 76.47%, suggests that large language models may serve as a meaningful source of support in postgraduate medical education and assessment preparation. However, the reliability and usefulness of the ChatGPT-5 model in clinical decision-making remain uncertain. There is no evidence that this tool can effectively handle ambiguous endocrinological cases, nor that its safe integration with electronic medical records is feasible. Therefore, further research is required to assess whether ChatGPT-5 may be applied not only in medical training but also as a complementary aid in clinical practice."
    mergedFrom:
      - source: pubmed
        pmid: "41170234"
        doi: 10.7759/cureus.93479
    finalDecision: exclude
  - title: "Large language models as educational collaborators: developing non-conventional teaching aids in pharmacology & therapeutics."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares LLMs for generating educational content (SLOs, reading materials, MCQs, CBL scenarios) for pharmacology curriculum; not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Addresses curriculum-content generation rather than workplace-based assessment or feedback on observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1186/s12909-025-08134-2
    pmid: "41168789"
    authors: Sridharan K, Sivaramakrishnan G
    year: "2025"
    abstract: |-
      BACKGROUND: With the growing integration of artificial intelligence in medical education, this study compares the quality and educational robustness of content generated by two large language models (LLMs), DeepSeek-V3 and ChatGPT 4.0, on the emerging, non-conventional topic (and not present in textbooks) of gender-affirming hormone therapy (GAHT) across three educational phases: preclerkship and clerkship phases in undergraduate medical curriculum, and master's level in pharmacology.

      METHODS: A total of 23 prompts were designed to generate Specific Learning Objectives (SLOs), reading materials, assessment items (MCQs, SAQs, and OSPEs), and case-based learning (CBL) scenarios across the three learner stages. The outputs from both LLMs were evaluated independently using rubric-based frameworks assessing content appropriateness, pedagogical structure, assessment alignment, and inclusivity.

      RESULTS: Both LLMs produced pedagogically sound outputs; however, DeepSeek consistently demonstrated superior adherence to rubric criteria. For SLOs, DeepSeek maintained a clear hierarchical progression across phases and showed greater precision, contextual alignment, and time-bound formulation. Its objectives were more assessable and reflective of increasing cognitive complexity. ChatGPT's SLOs were inclusive and coherent but occasionally lacked time-specificity and structural clarity. In reading materials, DeepSeek outperformed by integrating clinical relevance, scaffolded structure, and interactive learning tools across all phases. It included visual aids, case vignettes, and phase-specific assessments, while ChatGPT's content was accurate and readable but leaned toward text-heavy exposition with fewer embedded learning activities. MCQs from both models adhered to core psychometric principles. DeepSeek avoided testwiseness cues more consistently and offered better stratification of difficulty and realism, especially at the master's level. ChatGPT demonstrated strong pharmacological accuracy but occasionally showed testwiseness cues and illogical distractor sequencing. In CBL and OSPE outputs, DeepSeek showed stronger alignment with instructional and assessment criteria through modular formatting, diverse patient representation, and integration of formative tools. ChatGPT's cases and OSPEs were realistic and engaging but more narrative and occasionally less standardized.

      CONCLUSION: While both LLMs demonstrated educational utility, DeepSeek produced more rubric-aligned, contextually rich, and assessment-ready content across all learner stages. This study supports the integration of advanced LLMs like DeepSeek and ChatGPT in curriculum design, provided there is oversight to ensure alignment with pedagogical goals and learner needs.
    mergedFrom:
      - source: pubmed
        pmid: "41168789"
        doi: 10.1186/s12909-025-08134-2
    finalDecision: exclude
  - title: Large language models for generating key-feature questions in medical education.
    reviews:
      - decision: uncertain
        comment: LLMs generating assessment items; abstract needed to determine if WBA-related
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Uses LLMs for generating standardized key-feature exam questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on generating exam questions, not feedback/assessment of workplace clinical observations or records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.1080/10872981.2025.2574647
    pmid: "41159362"
    authors: K&#x131;yak Y, G&#xf3;rski S, Tokarek T, Pers M, Kononowicz A
    year: "2025"
    abstract: In this study, we conducted a descriptive study to evaluate the quality of KFQs generated by OpenAI's o3 model. We developed a reusable generic prompt for KFQ generation, designed in alignment with the Medical Council of Canada's KFQ development guidelines. We also created an evaluation metric to systematically assess the quality of the KFQs based on the KFQ development guideline. Twenty unique cardiology-focused KFQs were created using recent European Society of Cardiology guidelines as reference. Each KFQ was independently assessed by two cardiology experts using the quality checklist, with disagreements resolved by a third reviewer. Descriptive statistics were used to summarize checklist compliance and final acceptability ratings. Of the 20 KFQs, 3 (15%) were rated 'Accept as is' and 17 (85%) 'Accept with minor revisions'; none required major revisions or were rejected. The overall compliance rate across checklist criteria was 93.7%, with perfect scores in domains such as key feature definition, scenario plausibility, and alignment between questions and scenarios. Lower performance was observed for inclusion of genuinely harmful 'killer' responses (50%), plausibility of distractors (77.8%), and active language use in phrasing the question (80%). The findings showed that an LLM, guided by a structured prompt, can generate KFQs that closely adhere to established quality standards, with most requiring only minor refinements. While expert review remains essential to ensure clinical accuracy and patient safety, AI-assisted workflows have strong potential to streamline KFQ development and enhance the scalability of CDM assessment in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "41159362"
        doi: 10.1080/10872981.2025.2574647
    finalDecision: exclude
  - title: Comparison of accuracy and consistency of AI Language models when answering standardised dental MCQs.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Tests AI models on standardized dental textbook MCQs, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Standardized dental MCQ testing is not a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.1186/s12909-025-07624-7
    pmid: "41152856"
    authors: Alshammari A, Madfa A, Anazi B, Alenezi Y, Alkurdi K
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) models have been increasingly integrated into dental education for assessment and learning support. However, their accuracy and reliability in assessment of dental knowledge requires further evaluation.

      OBJECTIVE: This study aimed to assess and compare the accuracy and response consistency of five AI language models- ChatGPT-4, Grok XI, Gemini, Qwen 2.5 and DeepSeek-V3- using standardised dental multiple-choice questions (MCQs).

      METHODS: A set of 150 MCQs from two textbooks was used. Each AI model was tested twice, 10 days apart, using identical questions. Accuracy was determined by comparing responses to reference answers, and consistency was measured using Cohen's kappa and McNemar's test. The inter-model agreement was also analysed.

      RESULTS: ChatGPT-4 showed the highest accuracy (91.3%) in both assessments, followed by Grok XI (90.7-92.7%) and Qwen 2.5 (89.3%). Gemini and DeepSeek performed slightly lower (86.7-88.7%). ChatGPT, Grok XI and Gemini demonstrated strong consistency, whereas Qwen 2.5 and DeepSeek exhibited more variation between test administrations. No significant differences were found in inter-model agreement (p > 0.05).

      CONCLUSION: All five AI models showed high levels of accuracy in answering dental MCQs, and three of the models, ChatGPT-4, Grok XI and Gemini had strong test-retest reliability. These AI models show promise as educational tools, though continued evaluation and refinement are needed for broader clinical or academic applications.
    mergedFrom:
      - source: pubmed
        pmid: "41152856"
        doi: 10.1186/s12909-025-07624-7
    finalDecision: exclude
  - title: Evolution of AI in anatomy education study based on comparison of current large language models against historical ChatGPT performance.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on anatomy education (preclinical), not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Anatomy education context is not clearly workplace-based clinical education"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    doi: 10.1038/s41598-025-22437-w
    pmid: "41152541"
    authors: Bolgova O, Mavrych V
    year: "2025"
    abstract: The integration of Large Language Models (LLMs) in medical education has gained significant attention, particularly in their ability to handle complex medical knowledge assessments. However, a comprehensive evaluation of their performance in anatomical education remains limited. To evaluate the performance accuracy of current LLMs compared to previous versions in answering anatomical multiple-choice questions and assessing their reliability across different anatomical topics. We analyzed the performance of four LLMs (GPT-4o, Claude, Copilot, and Gemini) on 325 USMLE-style MCQs covering seven anatomical topics. Each model attempted the questions three times. Results were compared with the previous year's GPT-3.5 performance and random guessing. Statistical analysis included chi-square tests for performance differences. Current LLMs achieved an average accuracy of 76.8 ± 12.2%, significantly higher than GPT-3.5 (44.4 ± 8.5%) and random responses (19.4 ± 5.9%). GPT-4o demonstrated the highest accuracy (92.9 ± 2.5%), followed by Claude (76.7 ± 5.7%), Copilot (73.9 ± 11.9%), and Gemini (63.7 ± 6.5%). Performance varied significantly across anatomical topics, with Head & Neck (79.5%) and Abdomen (78.7%) showing the highest accuracy rates, while Upper Limb questions showed the lowest performance (72.9%). Only 29.5% of questions were answered correctly by all LLMs, and 2.5% were never answered correctly. Statistical analysis confirmed significant differences between models and across topics (χ2 = 182.11-518.32, p < 0.001). Current LLMs show markedly improved performance in anatomical knowledge assessment compared to previous versions, with GPT-4o demonstrating superior accuracy and consistency. However, performance variations across anatomical topics and between models suggest the need for careful consideration in educational applications. These tools show promise as supplementary resources in medical education while highlighting the continued necessity for human expertise.
    mergedFrom:
      - source: pubmed
        pmid: "41152541"
        doi: 10.1038/s41598-025-22437-w
    finalDecision: exclude
  - title: Diagnostic Performance of GPT-4o Compared to Radiology Residents in Emergency Abdominal Tomography Cases.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; evaluates AI diagnostic performance on radiology images compared to residents, not assessment/feedback of learners"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates diagnostic performance of GPT-4o on CT cases, not learner assessment/feedback or analysis of clinical observation records in medical education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.3390/tomography11100108
    pmid: "41150165"
    authors: Tanyeri A, Akbulut R, G&#xfc;ndo&#x11f;du C, &#xd6;zt&#xfc;rk T, Ceylan B, Yal&#xe7;&#x131;n N, Dural &, Kasap S, &#xc7;ilda&#x11f; M, &#xdc;nsal A, &#xd6;zsunar Y
    year: "2025"
    abstract: "Purpose: This study aimed to evaluate the diagnostic performance of GPT-4 Omni (GPT-4o) in emergency abdominal computed tomography (CT) cases compared to radiology residents with varying levels of experience, under conditions that closely mimic real clinical scenarios. Material and Methods: A total of 45 emergency cases were categorized into three levels of difficulty (easy, moderate, and difficult) and evaluated by six radiology residents with varying levels of experience (limited: R1-R2; intermediate: R3-R4; advanced: R5-R6) and GPT-4o. Cases were presented sequentially to both groups with consistent clinical data and CT images. Each case included 4 to 7 CT slice images, resulting in a total of 243 images. The participants were asked to provide the single most likely diagnosis for each case. GPT-4o's CT image interpretation performance without clinical data and hallucination rate were evaluated. Results: Overall diagnostic accuracy rates were 76% for R1-R2, 89% for R3, 82% for R4-R5, 84% for R6, and 82% for GPT-4o. Case difficulty significantly affected the diagnostic accuracy for both the residents and GPT-4o, with accuracy decreasing as case complexity increased (p < 0.001). No statistically significant differences in diagnostic accuracy were found between GPT-4o and the residents, regardless of the experience level or case difficulty (p > 0.05). GPT-4o demonstrated a hallucination rate of 75%. Conclusions: GPT-4o demonstrated a diagnostic accuracy comparable to that of radiology residents in emergency abdominal CT cases. However, its dependence on structured prompts and high hallucination rate indicates the need for further optimization before clinical integration."
    mergedFrom:
      - source: pubmed
        pmid: "41150165"
        doi: 10.3390/tomography11100108
    finalDecision: exclude
  - title: "Generative AI in medical education: feasibility and educational value of LLM-generated clinical cases with MCQs."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: LLM-generated clinical cases and MCQs for undergraduate ophthalmology review classes; not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on AI-generated teaching cases and MCQs, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1186/s12909-025-08085-8
    pmid: "41146115"
    authors: Zhang Q, Huang Z, Huang Y, Wang G, Zhang R, Yang J, Cheng Y, Chen B, Wang H, Qiu K, Chen H
    year: "2025"
    abstract: |-
      OBJECTIVE: To evaluate the feasibility and educational value of employing large language models (LLMs) to generate clinical case scenario with multiple-choice questions (MCQs) for undergraduate medical education.

      METHODS: Twelve ophthalmology clinical case scenarios with MCQs generated by ChatGPT 4.0 were assessed for quality by eight teachers. High-scoring cases with MCQs were selected for review classes to test students' learning. Student perceptions were collected via in-class and after-class questionnaires using a 5-point Likert scale.

      RESULTS: The average quality score of the 12 cases with MCQs was 52.33 ± 5.44 (range: 48-54.25; max = 60). There were statistical differences in the teachers' scores for identical clinical cases (F = 16.050, P < 0.001). Among 20 students, 95% agreed AI-generated cases enriched learning resources, 80% reported improved interdisciplinary integration and learning efficiency, while 85% used LLMs for post-class practice but raised concerns about content accuracy and difficulty calibration.

      CONCLUSION: LLMs like ChatGPT can rapidly generate clinically relevant case scenarios and MCQs under precise prompts, offering a novel tool for educators and learners. However, expert review remains critical to mitigate risks of AI hallucinations (observed in 16.67% of cases, 2/12) and ensure alignment with curricular standards. Key issues included contradictions in imaging descriptions (e.g., inappropriate use of high-frequency ultrasound for chalazion) and diagnostic logic (e.g., inconsistent gonioscopy findings), underscoring the necessity of human oversight to refine content accuracy and educational utility.
    mergedFrom:
      - source: pubmed
        pmid: "41146115"
        doi: 10.1186/s12909-025-08085-8
    finalDecision: exclude
  - title: "Virtual case reasoning and AI-assisted diagnostic instruction: an empirical study based on body interact and large language models."
    reviews:
      - decision: uncertain
        comment: LLMs in diagnostic instruction; may involve clinical assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment/instruction appears based on virtual cases (simulated environment only)."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments (Body Interact virtual patient platform with simulated cases)"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment/instruction is conducted in simulated virtual cases only."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1186/s12909-025-07872-7
    pmid: "41137029"
    authors: Chen G, Lin C, Zhang L, Luo Z, Shin Y, Li X
    year: "2025"
    abstract: |-
      BACKGROUND: Integrating large language models (LLMs) with virtual patient platforms offers a novel approach to teaching clinical reasoning. This study evaluated the performance and educational value of combining Body Interact with two AI models, ChatGPT-4 and DeepSeek-R1, across acute care scenarios.

      METHODS: Three standardized cases (coma, stroke, trauma) were simulated by two medical researchers. Structured case summaries were input into both models using identical prompts. Outputs were assessed for diagnostic and treatment consistency, alignment with clinical reasoning stages, and educational quality using expert scoring, AI self-assessment, text readability indices, and Grammarly analysis.

      RESULTS: ChatGPT-4 performed best in stroke scenarios but was less consistent in coma and trauma cases. DeepSeek-R1 showed more stable diagnostic and therapeutic output across all cases. While both models received high expert and self-assessment scores, ChatGPT-4 produced more readable outputs, and DeepSeek-R1 demonstrated greater grammatical precision.

      CONCLUSIONS: Our findings suggest that ChatGPT-4 and DeepSeek-R1 each offer unique strengths for AI-assisted instruction. ChatGPT-4's accessible language may better support early learners, whereas DeepSeek-R1 may be more aligned with formal clinical reasoning. Selecting models based on specific teaching goals can enhance the effectiveness of AI-driven medical education.
    mergedFrom:
      - source: pubmed
        pmid: "41137029"
        doi: 10.1186/s12909-025-07872-7
    finalDecision: exclude
  - title: Evaluating the Quality of AI-Written Scenarios for Virtual Oral Surgical Board Preparatory Examination.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Virtual board preparatory examination indicates simulated assessment context."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in a simulated examination environment (virtual mock oral board examination for surgical residents), not workplace-based."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted in virtual mock oral board examination scenarios (simulated environment only)."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.1016/j.jsurg.2025.103736
    pmid: "41125020"
    authors: Panni U, Donald C, Blatnik J, Williams M, Yu J, Wise P
    year: "2025"
    abstract: |-
      OBJECTIVE: The objective of this study was to evaluate whether ChatGPT can generate level-appropriate clinical scenarios that are suitable for use in oral board preparatory examination (mock oral exam) for senior surgical residents.

      DESIGN: This was a prospective, blinded study in which AI-written and faculty-written scenarios were reviewed, randomized and used for testing in virtual mock oral exam. Both faculty examiners and test-taking residents were blinded to the true authorship of the scenarios. After the examination, participants completed a survey evaluating the complexity of each scenario and their perceptions of its authorship.

      SETTING: The study was conducted at Washington University in St. Louis (WashU), an academic medical center located in St. Louis, Missouri. The participating institutions also included Saint Louis University (SLU).

      PARTICIPANTS: Study participants included twenty-five senior general surgery residents (PGY4 and PGY5) and twenty faculty examiners from WashU and SLU, who took part in virtual mock oral examination. Post-exam surveys were completed both residents and faculty.

      RESULTS: Faculty rated most AI-written and faculty written scenarios as "level-appropriate" in terms of both the quality of the text and the degree of complexity. Similarly, when residents were asked to identify the most difficult scenarios, they selected both AI- and faculty-written scenarios at comparable rates. Notably, both faculty and residents struggled to correctly distinguish the origin of the scenarios, with frequent misidentification across both groups.

      CONCLUSION: AI-written clinical scenarios were comparable to faculty-written scenarios in terms of complexity and appropriateness for senior surgical residents when used in a virtual mock oral board examination, highlighting the potential utility of AI-based tools in oral board preparation and surgical education.
    mergedFrom:
      - source: pubmed
        pmid: "41125020"
        doi: 10.1016/j.jsurg.2025.103736
    finalDecision: exclude
  - title: "Automated Evaluation of Reflection and Feedback Quality in Workplace-Based Assessments by Using Natural Language Processing: Cross-Sectional Competency-Based Medical Education Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses NLP methods without clear generative AI/LLM focus in the title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: uncertain
        comment: "Borderline on criterion 1: Uses BERT (a pretrained language model) for classifying EPA narrative quality, which is NLP classification rather than generative AI. However, BERT is a language model and the study directly addresses WBA (EPA assessments) with validity evidence. Forwarded to full-text screening to determine whether the approach qualifies as LLM-based."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP/BERT classification rather than generative AI or LLMs."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs. The study uses BERT (an encoder-only transformer for classification) along with logistic regression and SVM to classify narrative quality. The authors explicitly chose BERT over generative LLMs like ChatGPT, noting that ChatGPT lacked discriminative precision. BERT is a conventional NLP/ML model, not a generative AI model."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:52.271000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP/ML classifiers (LR, SVM, BERT) rather than generative AI/LLMs for the assessment task."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:00.184000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses BERT/SVM/LR for classification, not generative AI/LLMs."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.2196/81718
    pmid: "41124694"
    authors: Chen J, Tu H, Chang C, Hsu W, Wang P, Liao C, Chen M
    year: "2025"
    abstract: |-
      BACKGROUND: Competency-based medical education relies heavily on high-quality narrative reflections and feedback within workplace-based assessments. However, evaluating these narratives at scale remains a significant challenge.

      OBJECTIVE: This study aims to develop and apply natural language processing (NLP) models to evaluate the quality of resident reflections and faculty feedback documented in Entrustable Professional Activities (EPAs) on Taiwan's nationwide Emyway platform for otolaryngology residency training.

      METHODS: This 4-year cross-sectional study analyzes 300 randomly sampled EPA assessments from 2021 to 2025, covering a pilot year and 3 full implementation years. Two medical education experts independently rated the narratives based on relevance, specificity, and the presence of reflective or improvement-focused language. Narratives were categorized into 4 quality levels-effective, moderate, ineffective, or irrelevant-and then dichotomized into high quality and low quality. We compared the performance of logistic regression, support vector machine, and bidirectional encoder representations from transformers (BERT) models in classifying narrative quality. The best performing model was then applied to track quality trends over time.

      RESULTS: The BERT model, a multilingual pretrained language model, outperformed other approaches, achieving 85% and 92% accuracy in binary classification for resident reflections and faculty feedback, respectively. The accuracy for the 4-level classification was 67% for both. Longitudinal analysis revealed significant increases in high-quality reflections (from 70.3% to 99.5%) and feedback (from 50.6% to 88.9%) over the study period.

      CONCLUSIONS: BERT-based NLP demonstrated moderate-to-high accuracy in evaluating the narrative quality in EPA assessments, especially in the binary classification. While not a replacement for expert review, NLP models offer a valuable tool for monitoring narrative trends and enhancing formative feedback in competency-based medical education.
    mergedFrom:
      - source: pubmed
        pmid: "41124694"
        doi: 10.2196/81718
    finalDecision: exclude
  - title: "Contraception-related topics in chat dialogues between healthcare students and generative AI patients: a natural language processing analysis."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted entirely in simulated environments"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environment. Students interact with AI-simulated patients in a chat simulation, not in a workplace-based clinical setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Conducted in AI-patient simulated chat dialogues without a workplace-based clinical assessment component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1186/s12909-025-08032-7
    pmid: "41121071"
    authors: Vilanti T, Luiro K, Dahlqvist I, Piipponen J, Hemminki-Reijonen U, Tkalcan S, Ketamo H, Koivisto J
    year: "2025"
    abstract: |-
      BACKGROUND: Rapidly growing technology has enabled real-time digital services in healthcare, and new opportunities for sexual and reproductive health care. As assessing patient needs and communication effectively on digital platforms can be challenging, healthcare students must practise written communication, such as chat dialogues. Despite its increasing use in education, the way in which generative AI can enhance chat interactions between healthcare providers and patients remains poorly understood. The aim of this cross-sectional study was to explore contraception topics in chat dialogues between healthcare students and AI patients during AI simulations.

      METHODS: The AI application simulated a written chat dialogue between student and AI patient, using the CurreChat interface, to enable students to practise clinical skills and communication in digital health service chat dialogues. Purposive sampling was used to collect the data from fifth-year medical students (n = 24) and graduating midwifery students (n = 20) in higher education institutions of medicine and midwifery. Data were collected in August and October 2024. The data consisted of chat dialogues between healthcare students and generative AI patients. Natural language processing (NLP) and automated text analysis examined the contraception topics in the dialogues. The analysis software was based on pre-taught (self-supervised learning), industry-specific language models that detect meanings and their semantics in given texts.

      RESULTS: The most significant result was that the students discussed essential aspects of contraception in the dialogues with the AI patients. Several topics in the students' part of the dialogues were similar to those in MeSH terminology and to work-related topics. The students' dialogues covered essential topics such as contraindications (114 times), contraceptive methods (93 times), and smoking (80 times), aligning with the Current Care Guideline.

      CONCLUSIONS: Generative AI chat simulations can enhance the education of healthcare professionals globally in contraception issues by improving educational outcomes. To fully utilise the advantages of AI chat interactions, effective prompting is essential. NLP was an appropriate method for analysing the conversations and could be utilised more in future research across diverse healthcare settings.
    mergedFrom:
      - source: pubmed
        pmid: "41121071"
        doi: 10.1186/s12909-025-08032-7
    finalDecision: exclude
  - title: "Certainty-based marking in multiple-choice assessments in physiology: a web-based implementation using an AI assistant."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Physiology MCQ assessment context is not workplace-based clinical education"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: AI (Claude 3.5) used to build a certainty-based MCQ tool for classroom physiology teaching; not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Uses AI only to help build a classroom MCQ certainty-based test, not generative AI for WBA feedback/assessment of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1152/advan.00087.2025
    pmid: "41106847"
    authors: Suryavanshi C, Nayak K
    year: "2025"
    abstract: Certainty-based marking (CBM) requires students to indicate their certainty levels alongside their answers. CBM has been shown to enhance self-assessment and metacognitive awareness. This study aimed to explore the implementation of CBM in multiple-choice assessments in physiology. The CBM assessment tool was developed with an artificial intelligence (AI) assistant, Claude 3.5, with prompts focused on functional rather than technical requirements. The assessment consisted of 15 multiple-choice questions (MCQs), which were administered as a pretest and posttest during a small group teaching session to first-year medical students. Following the assessment, students completed a survey to evaluate their perceptions regarding the format, knowledge-gap identification, and overall acceptability. Answers from 195 students were analyzed, and significant improvements were observed in performance measures and certainty indices from the pretest to the posttest. Most students (80.9%) found the certainty scale beneficial, and 78.3% changed their answers after reflecting on their certainty. CBM demonstrated metacognitive benefits, with 86.4% of students better recognizing their knowledge gaps and 85.8% feeling more aware of their learning progress. About 73% of students preferred the CBM format and expressed greater engagement (82.8%) than traditional MCQs. CBM implemented through a web-based platform functioned as an assessment tool and an instructional intervention that enhanced students' metacognitive awareness and self-monitoring skills in physiology education. Our study focused on a single physiology topic and showed improvements in knowledge retention and certainty calibration. However, further longitudinal studies across multiple topics are needed to determine whether students maintain these self-assessment skills over time.NEW & NOTEWORTHY To introduce certainty-based marking (CBM) to novice students, a custom web-based multiple-choice question (MCQ) test was developed with assistance from an artificial intelligence (AI) tool. This enhanced accessibility and allowed for data collection to evaluate and analyze student performance. The integration of AI in creating this assessment tool highlights the potential of technology to improve educational practices, especially in designing various assessment strategies.
    mergedFrom:
      - source: pubmed
        pmid: "41106847"
        doi: 10.1152/advan.00087.2025
    finalDecision: exclude
  - title: Leveraging Large Language Models to Generate Multiple-Choice Questions for Ophthalmology Education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not workplace-based assessment; GPT-4 used to generate board-style MCQs for ophthalmology residency training evaluation/certification exams"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM-generated MCQs rather than WBA assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1001/jamaophthalmol.2025.3622
    pmid: "41100119"
    authors: Gholami S, Mummert D, Wilson B, Page S, Dodhia R, Lavista Ferres J, Weeks W, Fajardo D, Bojikian K
    year: "2025"
    abstract: |-
      IMPORTANCE: Multiple choice questions (MCQs) are an important and integral component of ophthalmology residency training evaluation and board certification; however, high-quality questions are difficult and time-consuming to draft.

      OBJECTIVE: To evaluate whether general-domain large language models (LLMs), particularly OpenAI's Generative Pre-trained Transformer 4 (GPT-4), can reliably generate high-quality, novel, and readable MCQs comparable to those of a committee of experienced examination writers.

      DESIGN, SETTING, AND PARTICIPANTS: This survey study, conducted from September 2024 to April 2025, assesses LLM performance in generating MCQs based on the American Academy of Ophthalmology (AAO) Basic and Clinical Science Course (BCSC) compared with a committee of human experts. Ten expert ophthalmologists, who were masked to the generation source, independently evaluated MCQs using a 10-point Likert scale (1 = extremely poor; 10 = criterion standard quality) across 5 criteria: appropriateness, clarity and specificity, relevance, discriminative power, and suitability for trainees.

      INTERVENTION: Relevant BCSC content and AAO question-writing guidelines were input into GPT-4o via Microsoft's Azure OpenAI Service, and structured prompts were used to generate MCQs.

      MAIN OUTCOMES AND MEASURES: The primary outcomes were median scores and statistical comparisons using the bootstrapping method; string similarity scores based on Levenshtein distance (0-100, with 100 indicating identical content) between LLM-MCQs and the entire BCSC question bank; Flesch Reading Ease metric for readability; and intraclass correlation coefficient (ICC) for inter-rater agreement are reported.

      RESULTS: The 10 graders had between 1 and 28 years of clinical experience in ophthalmology (median [IQR] experience, 6 years [3-15 years]). Questions generated by GPT-4 and a committee of experts received median scores of 9 and 9 in combined scores, appropriateness, clarity and specificity, and relevance (difference, 0; 95% CI, 0-0; P > .99); 8 and 9 in discriminative power (difference, 1; 95% CI, -1 to 1; P = .52); and 8 and 8 in suitability for trainees (difference, 0; 95% CI, -1 to 0; P > .99), respectively. Nearly 95% of LLM-MCQs had similarity scores less than 60, indicating most LLM-MCQs had limited or no resemblance to existing content. Interrater reliability was moderate (ICC, 0.63; P < .001), and mean (SD) readability scores were similar across sources (37.14 [22.54] vs 42.60 [22.84]; P > .99).

      CONCLUSIONS AND RELEVANCE: In this survey study, results indicate that an LLM could be used to develop ophthalmology board-style MCQs and expand examination banks to further support ophthalmology residency training. Despite most questions having a low similarity score, the quality, novelty, and readability of the LLM-generated questions need to be further assessed.
    mergedFrom:
      - source: pubmed
        pmid: "41100119"
        doi: 10.1001/jamaophthalmol.2025.3622
    finalDecision: exclude
  - title: "Performance evaluation of large language models on Korean medical licensing examination: a three-year comparative analysis."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Benchmarks LLM performance on the Korean Medical Licensing Examination, not about using AI for learner assessment or WBA feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam-based LLM benchmarking (KMLE questions) without workplace-based assessment feedback or observation-record analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1038/s41598-025-20066-x
    pmid: "41094133"
    authors: Kim H, Jung K, Shin S, Lee W, Lee J, Park H, Choi Q
    year: "2025"
    abstract: Performance evaluation of large language models (LLMs) in non-English medical contexts remains limited, particularly for medical licensing examinations including both text- and image-based questions. Therefore, the performance and reliability of three LLMs-GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro was evaluated using the Korean Medical Licensing Examination (KMLE) questions from 2022 to 2024. We analyzed 942 KMLE questions encompassing text-only and image-based formats across various medical specialties. Reproducibility was evaluated through repeated testing, and inter-model agreement was analyzed using pairwise comparisons. GPT-4o achieved the highest accuracy (83.2%), followed by Claude 3.5 Sonnet (79.5%) and Gemini 1.5 Pro (76.6%). While GPT-4o and Claude 3.5 Sonnet outperformed text-only questions, Gemini 1.5 Pro consistently performed across both question formats. LLMs demonstrated the strongest performance in internal medicine, pediatrics, and psychiatry, with relatively weak results in medical law. Reproducibility was outstanding, with Claude 3.5 Sonnet, Gemini 1.5 Pro, and GPT-4o showing 99.9%, 99.5%, and 97.7%, respectively. Strong inter-model agreement was observed, particularly between GPT-4o and Claude 3.5 Sonnet. LLMs demonstrate competent performance in medical knowledge assessments, even in non-English contexts, although challenges persist in processing image-based questions and specialized domains. This study provides valuable insights that may inform the future development and application of LLMs in medical education and assessment, although further validation in real-world educational settings is necessary to establish their practical utility.
    mergedFrom:
      - source: pubmed
        pmid: "41094133"
        doi: 10.1038/s41598-025-20066-x
    finalDecision: exclude
  - title: "From Hype to Implementation: Embedding GPT-4o in Medical Education."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Viewpoint/opinion piece arguing for GPT-4o integration in medical education; not original research, systematic review, or meta-analysis."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Viewpoint article; not original research, systematic review, or meta-analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.2196/79309
    pmid: "41092421"
    authors: Sabouni S, Moufti M, Taha M
    year: "2025"
    abstract: The release of GPT-4 Omni (GPT-4o), an advanced multimodal generative artificial intelligence (AI) model, generated substantial enthusiasm in the field of higher education. However, one year later, medical education continues to face significant challenges, demonstrating the need to move from initial experimentation with the integration of multimodal AIs in medical education toward meaningful integration. In this Viewpoint, we argue that GPT-4o's true value lies not in novelty, but in its potential to enhance training in communication skills, clinical reasoning, and procedural skills by offering real-time simulations and adaptive learning experiences using text, audio, and visual inputs in a safe, immersive, and cost-effective environment. We explore how this innovation has made it possible to address key medical educational challenges by simulating realistic patient interactions, offering personalized feedback, and reducing educator workloads and costs, where traditional teaching methods struggle to replicate the complexity and dynamism of real-world clinical scenarios. However, we also address the critical challenges of this approach, including data accuracy, bias, and ethical decision-making. Rather than seeing GPT-4o as a replacement, we propose its use as a strategic supplement, scaffolded into curriculum frameworks and evaluated through ongoing research. As the focus shifts from AI novelty to sustainable implementation, we call on educators, policymakers, and curriculum designers to establish governance mechanisms, pilot evaluation strategies, and develop faculty training. The future of AI in medical education depends not on the next breakthrough, but on how we integrate today's tools with intention and rigor.
    mergedFrom:
      - source: pubmed
        pmid: "41092421"
        doi: 10.2196/79309
    finalDecision: exclude
  - title: "AI's Accuracy in Extracting Learning Experiences From Clinical Practice Logs: Observational Study."
    reviews:
      - decision: uncertain
        comment: AI extracting from clinical practice logs; directly relevant to analysis of clinical observation records
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: include
        comment: LLM (GPT-4-turbo) used to extract and analyze clinical learning experiences from medical students' practice logs during clinical clerkship; presents validity data (sensitivity, specificity, Jaccard index, Cohen kappa); generative AI applied to analysis of clinical observation records in workplace-based education
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
      - decision: include
        comment: Uses GPT-4-turbo to extract clinical experiences from medical students learning logs during clinical clerkship. Workplace-based setting (clinical participation-based clerkship). Presents validity data including sensitivity, specificity, Jaccard index, and Cohen kappa. Directly applies generative AI to analysis of clinical observation records for assessment purposes.
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:54.870000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:03.048000+00:00
    doi: 10.2196/68697
    pmid: "41092407"
    authors: Kondo T, Nishigori H
    year: "2025"
    abstract: |-
      BACKGROUND: Improving the quality of education in clinical settings requires an understanding of learners' experiences and learning processes. However, this is a significant burden on learners and educators. If learners' learning records could be automatically analyzed and their experiences could be visualized, this would enable real-time tracking of their progress. Large language models (LLMs) may be useful for this purpose, although their accuracy has not been sufficiently studied.

      OBJECTIVE: This study aimed to explore the accuracy of predicting the actual clinical experiences of medical students from their learning log data during clinical clerkship using LLMs.

      METHODS: This study was conducted at the Nagoya University School of Medicine. Learning log data from medical students participating in a clinical clerkship from April 22, 2024, to May 24, 2024, were used. The Model Core Curriculum for Medical Education was used as a template to extract experiences. OpenAI's ChatGPT was selected for this task after a comparison with other LLMs. Prompts were created using the learning log data and provided to ChatGPT to extract experiences, which were then listed. A web application using GPT-4-turbo was developed to automate this process. The accuracy of the extracted experiences was evaluated by comparing them with the corrected lists provided by the students.

      RESULTS: A total of 20 sixth-year medical students participated in this study, resulting in 40 datasets. The overall Jaccard index was 0.59 (95% CI 0.46-0.71), and the Cohen κ was 0.65 (95% CI 0.53-0.76). Overall sensitivity was 62.39% (95% CI 49.96%-74.81%), and specificity was 99.34% (95% CI 98.77%-99.92%). Category-specific performance varied: symptoms showed a sensitivity of 45.43% (95% CI 25.12%-65.75%) and specificity of 98.75% (95% CI 97.31%-100%), examinations showed a sensitivity of 46.76% (95% CI 25.67%-67.86%) and specificity of 98.84% (95% CI 97.81%-99.87%), and procedures achieved a sensitivity of 56.36% (95% CI 37.64%-75.08%) and specificity of 98.92% (95% CI 96.67%-100%). The results suggest that GPT-4-turbo accurately identified many of the actual experiences but missed some because of insufficient detail or a lack of student records.

      CONCLUSIONS: This study demonstrated that LLMs such as GPT-4-turbo can predict clinical experiences from learning logs with high specificity but moderate sensitivity. Future improvements in AI models, providing feedback to medical students' learning logs and combining them with other data sources such as electronic medical records, may enhance the accuracy. Using artificial intelligence to analyze learning logs for assessment could reduce the burden on learners and educators while improving the quality of educational assessments in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "41092407"
        doi: 10.2196/68697
      - source: scopus
        doi: 10.2196/68697
        scopusId: SCOPUS_ID:105026764765
    finalDecision: include
  - title: "\"It's like having that supervisor in the room\": Examining AI as a reflective partner."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are practicing mental health clinicians, not medical/health-professional learners; study focuses on professional reflective practice rather than learner assessment"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are practicing clinicians rather than medical/health-professional learners."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.1080/10503307.2025.2569047
    pmid: "41084310"
    authors: Matthews E, Lerman D, Beach N, Wiczyk D, Goldkind L
    year: "2025"
    abstract: "Objective: Reflective practice is vital to high-quality therapeutic work, enabling clinicians to critically examine their professional practices. Increasing opportunities to engage in reflective processes stands to increase clinician wellbeing and thereby improve quality of care. Generative Artificial Intelligence (GAI), specifically large language models, provide new ways for clinicians to gain insight into their work. This study explores a unique unintended consequence of GAI clinical notetaking platforms, the support of reflective practice among mental health clinicians.Method: Semi structured interviews were conducted with clinicians (n = 17) to learn about their experience using GAI clinical note-taking platforms. Interviews were recorded and analyzed using a grounded theory approach to identify themes.Results: Respondents described how AI platforms functioned as a reflective partner by offering alternative clinical interpretations, lending clarity to their existing practice approach, and creating space to be more present during sessions.Conclusions: AI note-taking platforms are a feasible and scalable way to support reflective practice. Implications for clinical training and ongoing professional development are discussed, and opportunities for future research on the use of AI tools to enhance reflective practice are identified."
    mergedFrom:
      - source: pubmed
        pmid: "41084310"
        doi: 10.1080/10503307.2025.2569047
      - source: scopus
        doi: 10.1080/10503307.2025.2569047
        scopusId: SCOPUS_ID:105019188733
    finalDecision: exclude
  - title: "Diagnosis education: a decade of progress, with more needed."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About diagnosis education broadly, no mention of generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.1515/dx-2025-0132
    pmid: "41074690"
    authors: Olson A, Rencic J, Wijesekera T
    year: "2025"
    abstract: In the decade since the National Academies of Sciences, Engineering, and Medicine (NASEM) report Improving Diagnosis in Health Care, substantial progress has been made in understanding and teaching diagnostic reasoning. This manuscript reviews key advancements in the science and theory of clinical reasoning, including the shift from exclusive focus on individual cognitive models to those that embrace context and team-based approaches. Recent innovations in diagnosis education, such as development of formal competencies, use of structured reflection, and approaches to assessment are discussed. Despite these gains, challenges remain in translating theory into practice, particularly in curricular innovation and implementation, faculty development, and assessment. The emergence of generative artificial intelligence presents both opportunities and imperatives for reimagining diagnosis education. The authors call for sustained efforts to embed diagnostic excellence across health professions education, emphasizing interprofessional collaboration, patient engagement, and system-level reform to reduce diagnostic error and improve outcomes.
    mergedFrom:
      - source: pubmed
        pmid: "41074690"
        doi: 10.1515/dx-2025-0132
    finalDecision: exclude
  - title: "Effects of artificial intelligence based physiotherapy educational approach in developing clinical reasoning skills: a randomized controlled trial."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records. AI (ChatGPT) is used as a learning intervention in problem-based learning; Mini-CEX is only used as an outcome measure by human evaluators, not by the AI."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: AI-assisted PBL intervention is an educational/classroom approach without a clear workplace-based clinical assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.1186/s12909-025-07926-w
    pmid: "41068907"
    authors: Ergezen Sahin G, Aras Bayram G, Sanchez Sierra A, Akdemir S, Kurc D, Tarakci D, Tunali A
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) tools such as ChatGPT are increasingly being integrated into health professions education, but evidence regarding their application in physiotherapy remains limited. This study aims to investigate the impact of AI-assisted problem-based learning (AI-PBL) on theoretical knowledge, clinical competence, AI self-efficacy, internet addiction, and reading motivation compared with traditional PBL.

      METHODS: A randomized controlled trial was conducted with undergraduate physiotherapy students assigned to AI-PBL or PBL groups. Participants completed assessments before, immediately after, and two weeks after the group intervention. Outcome measures included a theoretical knowledge test, the Mini Clinical Evaluation Exercise (Mini-CEX), the AI ​​Self-Efficacy Scale (AI-SES), the Internet Addiction Test (IAT), and the Adult Reading Motivation Scale (ARMS).

      RESULTS: Forty students were randomized equally into two groups: AI-PBL (n = 20) and traditional PBL (n = 20). Both groups showed significant improvements in knowledge and reading motivation. The AI-PBL group showed significantly greater improvement in knowledge retention at 2 weeks (Cohen's d = 3.14) and greater gains in AI self-efficacy. Although Mini-CEX scores were higher in the AI-PBL group, the differences between groups were not statistically significant. No significant increase in internet addiction was observed in the AI-PBL group.

      CONCLUSION: These findings emphasize that supervised, structured use of generative AI in education can enhance sustained learning and digital self-efficacy without posing behavioral risks. The AI-PBL approach appears to foster active reflection, self-directed learning, and deeper academic engagement offering a promising direction for digital innovation in physiotherapy education. Future studies should explore long-term outcomes, track behavioral engagement, and further validate the benefits of AI-enhanced instructional strategies.

      TRIAL REGISTRATION: Prior to the initiation of the study, the protocol was registered on https//www.

      CLINICALTRIALS: gov/, and registration status was made publicly available (Identifier NCT07010991 Date 08.06.2025). ( https://clinicaltrials.gov/study/NCT07010991?term=NCT07010991&rank=1 ).
    mergedFrom:
      - source: pubmed
        pmid: "41068907"
        doi: 10.1186/s12909-025-07926-w
      - source: scopus
        doi: 10.1186/s12909-025-07926-w
        scopusId: SCOPUS_ID:105018287675
    finalDecision: exclude
  - title: Performance of multimodal large language models in the Japanese surgical specialist examination.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Specialist licensing examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: LLMs answering board certification exam questions; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM accuracy on specialist exam questions, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.1186/s12909-025-07938-6
    pmid: "41068861"
    authors: Miyamoto Y, Nakaura T, Nakamura H, Hirai T, Iwatsuki M
    year: "2025"
    abstract: |-
      BACKGROUND: Multimodal large language models (LLMs) have the capability to process and integrate both text and image data, offering promising applications in the medical field. This study aimed to evaluate the performance of representative multimodal LLMs in the 2023 Japanese Surgical Specialist Examination, with a focus on image-based questions across various surgical subspecialties.  METHODS: A total of 98 examination questions, including 43 image-based questions, from the 2023 Japanese Surgical Specialist Examination were administered to three multimodal LLMs: GPT-4 Omni, Claude 3.5 Sonnet, and Gemini Pro 1.5. Each model's performance was assessed under two conditions: with and without images. Statistical analysis was conducted using McNemar's test to evaluate the significance of accuracy differences between the two conditions.  RESULTS: Among the three LLMs, Claude 3.5 Sonnet achieved the highest overall accuracy at 84.69%, exceeding the passing threshold of 80%, which is consistent with the standard set by the Japan Surgical Society for board certification. GPT-4 Omni closely approached the threshold with an accuracy of 79.59%, while Gemini Pro 1.5 scored 61.22%. Claude 3.5 Sonnet demonstrated the highest accuracy in four of six subspecialties for image-based questions and was the only model to show a statistically significant improvement with image inclusion (76.74% with images vs. 62.79% without images, p = 0.041). By contrast, GPT-4 Omni and Gemini Pro 1.5 did not exhibit significant performance changes with image inclusion.

      CONCLUSION: Claude 3.5 Sonnet outperformed the other models in most surgical subspecialties for image-based questions and was the only model to benefit significantly from image inclusion. These findings suggest that multimodal LLMs, particularly Claude 3.5 Sonnet, hold promise as diagnostic and educational support tools in surgical domains, and that variation in visual reasoning capabilities may account for model-level differences in image-based performance.
    mergedFrom:
      - source: pubmed
        pmid: "41068861"
        doi: 10.1186/s12909-025-07938-6
    finalDecision: exclude
  - title: "Performance of GPT-4o and o1-Pro on United Kingdom Medical Licensing Assessment-style items: a comparative study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; benchmarks LLM performance on written licensing exam MCQs (UKMLA)."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam-item performance study for licensing preparation, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.3352/jeehp.2025.22.30
    pmid: "41068056"
    authors: Vakili B, Ahmad A, Zolfaghari M
    year: "2025"
    abstract: |-
      PURPOSE: Large language models (LLMs) such as ChatGPT, and their potential to support autonomous learning for licensing exams like the UK Medical Licensing Assessment (UKMLA), are of growing interest. However, empirical evaluations of artificial intelligence (AI) performance against the UKMLA standard remain limited.

      METHODS: We evaluated the performance of 2 recent ChatGPT versions, GPT-4o and o1-Pro, on a curated set of 374 UKMLA-style single-best-answer items spanning diverse medical specialties. Statistical comparisons using McNemar's test assessed the significance of differences between the 2 models. Specialties were analyzed to identify domain-specific variation. In addition, 20 image-based items were evaluated.

      RESULTS: GPT-4o achieved an accuracy of 88.8%, while o1-Pro achieved 93.0%. McNemar's test revealed a statistically significant difference in favor of o1-Pro. Across specialties, both models demonstrated excellent performance in surgery, psychiatry, and infectious diseases. Notable differences arose in dermatology, respiratory medicine, and imaging, where o1-Pro consistently outperformed GPT-4o. Nevertheless, isolated weaknesses in general practice were observed. The analysis of image-based items showed 75% accuracy for GPT-4o and 90% for o1-Pro (P=0.25).

      CONCLUSION: ChatGPT shows strong potential as an adjunct learning tool for UKMLA preparation, with both models achieving scores above the calculated pass mark. This underscores the promise of advanced AI models in medical education. However, specialty-specific inconsistencies suggest AI tools should complement, rather than replace, traditional study methods.
    mergedFrom:
      - source: pubmed
        pmid: "41068056"
        doi: 10.3352/jeehp.2025.22.30
    finalDecision: exclude
  - title: "Death of the Personal Statement: Qualitative Comparison Between Human-Authored and Artificial Intelligence-Generated Medical School Admissions Essays."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About AI-generated admissions essays, not about assessment or feedback of learners in clinical settings"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    doi: 10.1097/XCS.0000000000001602
    pmid: "41051105"
    authors: Vaccaro M, Sharma I, Espina-Rey A, Lyman N, Palacios C, Zhang Y, Mehta A, Leto Barone A, Kellogg B
    year: "2026"
    abstract: |-
      BACKGROUND: Medical school admissions committees use personal statements as subjective measures of applicants' values, motivations, and experiences. Students at varying levels of education report that they would or do use artificial intelligence (AI) in their academic pursuits. It has not been investigated how effectively medical school admissions committees can distinguish between AI-generated and human-authored personal statements, or whether the use of AI provides students an advantage.

      STUDY DESIGN: Human-authored personal statements were retrieved from the 2019 application cycle (before AI chatbots). ChatGPT 4.0 was used to generate personal statements from summaries of the human-authored essays. In a prospective, single-blind, randomized controlled trial, medical school application readers evaluated unique combinations of AI-generated and human-authored personal statements for essay quality (on a 7-point Likert scale), speculated authorship (human or AI), and speculation confidence (on a 5-point Likert scale). ZeroGPT, an AI-detection tool, also speculated authorship.

      RESULTS: Seventeen medical school application readers completed 325 scoring rubrics across 309 essays. Readers demonstrated a 56% accuracy in correctly identifying authorship, whereas ZeroGPT showed 91% accuracy. Readers were more likely to assume human authorship of higher-scoring essays (Mann-Whitney U test, p < 0.001). AI-generated essays scored better than their human-authored counterparts (Wilcoxon signed-rank test, p = 0.020) with mean scores of 5.02 ± 1.21 and 4.67 ± 1.33, respectively.

      CONCLUSIONS: AI-generated personal statements were rated more highly and were nearly indistinguishable to medical school application readers. Despite ZeroGPT's higher accuracy in detecting AI use, its rate of false positives remains unacceptably high for use in medical school admissions. The role of personal statements in medical school admissions requires urgent reconsideration to maintain credibility.
    mergedFrom:
      - source: pubmed
        pmid: "41051105"
        doi: 10.1097/XCS.0000000000001602
    finalDecision: exclude
  - title: Bilingual comparison of the performance of GPT-4o and GPT-4 on ophthalmology residency examination questions.
    reviews:
      - decision: uncertain
        comment: GPT models on ophthalmology residency exam; abstract needed to determine WBA relevance
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks LLM accuracy on ophthalmology board exam questions in English and French; not about assessment or feedback of learner performance."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Board-exam performance comparison does not address workplace-based assessment or analysis of clinical observation documentation."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1016/j.jfo.2025.104650
    pmid: "41045610"
    authors: Shvartz E, Attal L, Zur O, Nujeidat Z, Plopsky G, Bahir D
    year: "2025"
    abstract: |-
      OBJECTIVE: To evaluate and compare the performance of GPT-4 and the newer GPT-4o in both English and French on ophthalmology board examination questions, assessing accuracy across various subspecialties and question formats, with a focus on image analysis.

      METHODS: A dataset of 600 multiple-choice questions from certification-level board examinations covering 12 subspecialties and diverse content was carefully translated and tested in both English and French using GPT-4 and GPT-4o with analyses by examination years, question type, and processing of various image inputs, ensuring a comprehensive evaluation. Performance of human residents from 2021-2023 was used for comparison. Statistical analyses, including χ2 tests and odds ratio calculations, compared accuracy across models.

      RESULTS: GPT-4o in English achieved the highest accuracy (74.5%), approaching human resident performance, while its French counterpart scored 67.4%. GPT-4 scored 62.3% and 64.4% in English and French, respectively, both significantly lower than GPT-4o (P<0.001). Text-based questions showed consistently higher accuracy across all models, with English GPT-4o leading at 82.5%. Image-based questions revealed similar performance for English and French GPT-4o, both outperforming the GPT-4 models.

      CONCLUSIONS: GPT-4o outperforms GPT-4 in both English and French, underscoring its potential for ophthalmology use in both languages. While limitations remain, particularly in image-based diagnostics and language-specific nuances, these models are paving the way for a future where artificial intelligence supports and enhances human expertise in both education and patient care.
    mergedFrom:
      - source: pubmed
        pmid: "41045610"
        doi: 10.1016/j.jfo.2025.104650
    finalDecision: exclude
  - title: "Artificial intelligence versus physical medicine and rehabilitation residents: Can ChatGPT compete in clinical exam performance?"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Study benchmarks ChatGPT performance on PM&R in-training examinations rather than using AI to assess learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on LLM performance on in-training exam MCQs rather than workplace-based assessment or feedback from clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1002/pmrj.70032
    pmid: "41041823"
    authors: Ayy&#x131;ld&#x131;z A, &#x130;nceo&#x11f;lu S, Kuran B, &#xd6;ne&#x15f; K
    year: "2026"
    abstract: |-
      BACKGROUND: Artificial intelligence has begun to replace human power in many areas today.

      OBJECTIVE: To assess the performance of Chat Generative Pretrained Transformer (ChatGPT) on examinations administered to physical medicine and rehabilitation (PM&R) residents.

      DESIGN: Cross-sectional study.

      SETTING: Tertiary-care training and research hospital, department of physical medicine and rehabilitation.

      PARTICIPANTS: ChatGPT-4o and PM&R residents.

      INTERVENTION: ChatGPT was presented with questions from the annual nationwide in-training exams administered to PM&R residents at different postgraduate years. The exam is a national requirement for the majority of PM&R residents in Turkey and is administered annually.

      MAIN OUTCOME MEASURES: The responses to these multiple-choice questions were evaluated as correct or incorrect, and ChatGPT's performance was then compared to that of the residents of each postgraduate year (PGY) term. The time taken by ChatGPT to answer each question was also recorded. Additionally, its learning ability was assessed by reasking the questions it initially answered incorrectly, this time providing the correct answers to evaluate improvement.

      RESULTS: ChatGPT received a score of 88 out of 100 points in the PGY1 exam, 84 points in the PGY2 exam, 78 points in the PGY3 exam, and 80 points in the PGY4 exam. When compared with the performance distribution of residents, ChatGPT ranked in the 40th-50th percentile for PGY1, 70th-80th percentile for PGY2, 30th-40th percentile for PGY3, and 40th-50th percentile for PGY4. It has been demonstrated that ChatGPT has achieved a learning rate of 65%.

      CONCLUSION: Despite the potential of ChatGPT to surpass PM&R physicians in terms of learning capabilities and extensive knowledge network, several functional limitations remain. In its current form, it is not capable of replacing a physician, especially in the field of PM&R, where clinical examination and patient interaction play a critical role.
    mergedFrom:
      - source: pubmed
        pmid: "41041823"
        doi: 10.1002/pmrj.70032
    finalDecision: exclude
  - title: Application of DeepSeek-assisted problem-based learning in hematology residency training.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment/feedback or analysis of observation records; DeepSeek used as a PBL teaching aid in hematology residency, not for assessment or feedback generation"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates PBL curriculum support, not WBA assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1186/s12909-025-07852-x
    pmid: "41039507"
    authors: Hou J, An F, Qin H, Zhang L, Wang J, Zhang C, Fan D
    year: "2025"
    abstract: |-
      OBJECTIVES: This study aimed to evaluate the efficacy of integrating the open-source large language model (LLM) DeepSeek into problem-based learning (PBL) curriculum for hematology residency training.

      METHODS: This non-randomized controlled trial included two groups of 30 s-year hematology residents each. One group received traditional PBL instruction, while the other's PBL was assisted by DeepSeek. Both groups participated in in-person PBL sessions across two identical hematology courses. The DeepSeek-assisted PBL group utilized DeepSeek V3 and R1 models, along with an AI-facilitated web search and integrated output after automatic information filtering and analysis during their in-person PBL sessions. Learning outcomes were assessed via a post-course survey evaluating effectiveness, credibility, reliability, and engagement. Students also completed five standardized examinations covering analysis and diagnostic decision-making, procedural skills, communication, interdisciplinary integration, and emergency management/ethical considerations.

      RESULTS: The study demonstrated significant advantages of DeepSeek-assisted PBL over traditional PBL across multiple competency domains, including case analysis effectiveness, feedback quality, course structure, and clinical reasoning. Participants also reported stronger curriculum alignment with current guidelines and enhanced capacity for generating clinical insights during discussions. Academically, the DeepSeek-assisted PBL group outperformed in four out of five competency domains (Exams I, III, IV, V), achieving higher total examination scores. However, no significant difference emerged in clinical skills (Exam II), nor did DeepSeek enhance interactive elements based on survey results. Notably, the DeepSeek-assisted PBL group also expressed greater concerns about the potential inaccuracies in artificial intelligence-generated medical advice.

      CONCLUSION: Integrating DeepSeek into the PBL curriculum may improve clinical competence, diagnostic reasoning, and learner engagement in hematology residency training. These findings suggest that open-source LLMs like DeepSeek may offer scalable and cost-effective support tools to augment traditional medical education. Further study is needed to explore artificial intelligence tools for enhancing interactive elements and procedural skills.
    mergedFrom:
      - source: pubmed
        pmid: "41039507"
        doi: 10.1186/s12909-025-07852-x
    finalDecision: exclude
  - title: "ChatGPT: A Useful Tool for Medical Students in Radiology Education?"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT accuracy on imaging appropriateness criteria against ACR guidelines and student performance; not about using AI for assessment or feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM answers to imaging-appropriateness questions, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.1111/tct.70220
    pmid: "41036648"
    authors: Sirag M, Moloney B
    year: "2025"
    abstract: |-
      BACKGROUND: Large language models (LLMs) such as ChatGPT are increasingly being explored as educational tools in medical education, particularly in radiology. This study evaluated the accuracy of ChatGPT in recommending appropriate imaging investigations across diverse clinical scenarios, with a focus on its potential as an educational tool for medical students and junior doctors.

      METHODS: ChatGPT-4 (March 2024 version) was presented with a 12-case questionnaire derived from the American College of Radiology's Appropriateness Criteria (ACR-AC). One topic was selected from each of 10 diagnostic sections and two from the interventional section. The model's recommendations were compared with those published by the ACR-AC, which are based on expert consensus. The same questionnaire was also completed by 160 final-year medical students and junior doctors, and their collective performance was compared to ChatGPT.

      RESULTS: ChatGPT achieved a 100% concordance rate (12/12 scenarios) with expert panel recommendations. In contrast, the student/doctor cohort achieved a 68.0% concordance rate. The difference was statistically significant (p < 0.05).

      CONCLUSIONS: ChatGPT demonstrated high accuracy in recommending appropriate imaging investigations in a structured, guideline-based setting. These findings suggest that LLMs may serve as a valuable adjunct in radiology education, particularly in supporting imaging decision making among less experienced clinicians. However, further validation in real-world clinical environments is warranted.
    mergedFrom:
      - source: pubmed
        pmid: "41036648"
        doi: 10.1111/tct.70220
    finalDecision: exclude
  - title: Will artificial intelligence improve residents' quality of life without compromising healthcare quality? A pediatric point-of-view.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: 'Point-of-view' indicates commentary/perspective, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Point-of-view article, not an original research article, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    doi: 10.1186/s13052-025-02073-w
    pmid: "41035039"
    authors: Corsello A, Pegoraro F, Spatuzzo M, Santangelo A
    year: "2025"
    abstract: |-
      BACKGROUND: The integration of artificial intelligence (AI) and advanced large language models in medical education and clinical practice is reshaping healthcare. These technologies have significant potential to enhance training experience and quality of life for medical residents. By automating routine tasks such as documentation and preliminary data analysis, AI-driven models can significantly reduce the workload, enabling residents to focus more on direct patient care and hands-on learning opportunities.

      MAIN BODY: AI-driven support in diagnostics and decision-making may also reduce diagnostic errors, fostering a safer and more efficient healthcare environment. Furthermore, by alleviating administrative burdens, AI could play a critical role in mitigating resident burnout, contributing to a more resilient healthcare workforce and ultimately improving the continuity and quality of patient care. However, the adoption of AI in medical practice poses challenges. Automation risks reducing essential clinical skills, and over-reliance on AI may impact on professional autonomy and the development of diagnostic capacities. Concerns also persist regarding biased data, data security, legal issues, and the transparency in AI-driven decision-making processes.

      CONCLUSION: Addressing these challenges requires collaboration among healthcare professionals, AI developers and policymakers, as well as ethical frameworks and country-specific regulations. Only through a balanced and collaborative approach can we unlock AI's full potential to create a more efficient, equitable, and patient-centered healthcare system.
    mergedFrom:
      - source: pubmed
        pmid: "41035039"
        doi: 10.1186/s13052-025-02073-w
    finalDecision: exclude
  - title: "Impact of Prompt Engineering on the Performance of ChatGPT Variants Across Different Question Types in Medical Student Examinations: Cross-Sectional Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Benchmarks LLM performance on medical student midterm/final exams (academic knowledge tests), not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Conducted in exam-based medical school context, not a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.2196/78320
    pmid: "41032724"
    authors: Hsieh M, Wang T, Su P, Chou M
    year: "2025"
    abstract: |-
      BACKGROUND: Large language models such as ChatGPT (OpenAI) have shown promise in medical education assessments, but the comparative effects of prompt engineering across optimized variants and relative performance against medical students remain unclear.

      OBJECTIVE: This study aims to systematically evaluate the impact of prompt engineering on five ChatGPT variants (GPT-3.5, GPT-4.0, GPT-4o, GPT-4o1-mini, and GPT-4o1) and benchmark their performance against fourth-year medical students in midterm and final examinations.

      METHODS: A 100-item examination dataset covering multiple choice questions, short answer questions, clinical case analysis, and image-based questions was administered to each model under no-prompt and prompt-engineering conditions over 5 independent runs. Student cohort scores (N=143) were collected for comparison. Responses were scored using standardized rubrics, converted to percentages, and analyzed in SPSS Statistics (v29.0) with paired t tests and Cohen d (P<.05).

      RESULTS: Baseline midterm scores ranged from 59.2% (GPT-3.5) to 94.1% (GPT-4o1), and final scores ranged from 55% to 92.4%. Fourth-year students averaged 89.4% (midterm) and 80.2% (final). Prompt engineering significantly improved GPT-3.5 (10.6%, P<.001) and GPT-4.0 (3.2%, P=.002) but yielded negligible gains for optimized variants (P=.07-.94). Optimized models matched or exceeded student performance on both exams.

      CONCLUSIONS: Prompt engineering enhances early-generation model performance, whereas advanced variants inherently achieve near-ceiling accuracy, surpassing medical students. As large language models mature, emphasis should shift from prompt design to model selection, multimodal integration, and critical use of artificial intelligence as a learning companion.
    mergedFrom:
      - source: pubmed
        pmid: "41032724"
        doi: 10.2196/78320
    finalDecision: exclude
  - title: "Performance of ChatGPT-4 on the French Board of Plastic Reconstructive and Aesthetic Surgery written exam: a descriptive study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT performance on French board written exam, standardized exam-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study benchmarks ChatGPT-4 performance on French plastic surgery board examination questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Written board exam benchmarking is not workplace-based clinical education assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.3352/jeehp.2025.22.27
    pmid: "41022586"
    authors: Dejean-Bouyer E, Kanlagna A, Thuau F, Perrot P, Lancien U
    year: "2025"
    abstract: |-
      PURPOSE: This study aims to evaluate the performance of Chat Generative Pre-Trained Transformer 4 (ChatGPT-4) on the French Board of Plastic, Reconstructive, and Aesthetic Surgery written examination and to assess its role as a supplementary resource in helping medical students prepare for the qualification examination in plastic surgery.

      METHODS: This descriptive study evaluated ChatGPT-4's performance on 213 items from the October 2024 French Board of Plastic, Reconstructive, and Aesthetic Surgery written examination. Responses were assessed for accuracy, logical reasoning, internal and external information use, and were categorized for fallacies by independent reviewers. Statistical analyses included chi-square tests and Fisher's exact test for significance.

      RESULTS: ChatGPT-4 answered all questions across the 10 modules, achieving an overall accuracy rate of 77.5%. The model applied logical reasoning in 98.1% of the questions, utilized internal information in 94.4%, and incorporated external information in 91.1%.

      CONCLUSION: ChatGPT-4 performs satisfactorily on the French Board of Plastic, Reconstructive, and Aesthetic Surgery written examination. Its accuracy met the minimum passing standards for the exam. While responses generally align with expected knowledge, careful verification remains necessary, particularly for questions involving image interpretation. As artificial intelligence continues to evolve, ChatGPT-4 is expected to become an increasingly reliable tool for medical education. At present, it remains a valuable resource for assisting plastic surgery residents in their training.
    mergedFrom:
      - source: pubmed
        pmid: "41022586"
        doi: 10.3352/jeehp.2025.22.27
    finalDecision: exclude
  - title: A bi-linguistic comparative analysis of ChatGPT-4, Gemini, and Claude performance on Polish medical-dental final examinations.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks LLM accuracy on Polish medical-dental final examinations; not about assessment or feedback of learner performance."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Final-exam chatbot comparison is outside workplace-based assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1038/s41598-025-17030-0
    pmid: "41006462"
    authors: W&#xf3;jcik D, Adamiak O, Czerepak G, Tokarczuk O, Szalewski L
    year: "2025"
    abstract: "In the realm of medical education, the utility of chatbots is being explored with growing interest. One pertinent area of investigation is the performance of these models on standardized medical examinations, which are crucial for certifying the knowledge and readiness of healthcare professionals. In Poland, dental and medical students have to pass crucial exams known as LDEK (Medical-Dental Final Examination) and LEK (Medical Final Examination) exams respectively. The primary objective of this study was to conduct a comparative analysis of chatbots: ChatGPT-4, Gemini and Claude to evaluate their accuracy in answering exam questions of the LDEK and the Medical-Dental Verification Examination (LDEW), using queries in both English and Polish. The analysis of Generalized Linear Mixed-Effects Model, which compared chatbots within question groups, showed that the chatbot Claude achieved the highest probability of accuracy for all question groups except the area of prosthetic dentistry compared to ChatGPT-4 and Gemini. In addition, the probability of a correct answer to questions in the field of integrated medicine was higher than in the field of dentistry for all chatbots in both prompt languages. Our results demonstrated that Claude achieved the highest accuracy in all areas analysed and outperformed other chatbots. This suggests that Claude has significant potential to support the medical education of dental students. This study showed that the performance of chatbots varied depending on the prompt language and the specific field. This highlights the importance of considering language and specialty when selecting a chatbot for educational purposes."
    mergedFrom:
      - source: pubmed
        pmid: "41006462"
        doi: 10.1038/s41598-025-17030-0
    finalDecision: exclude
  - title: "Pharmacy students' perceptions and experience using AI tools to complete a drug information assignment: A thematic analysis."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Pharmacy students completed a drug information assignment in a classroom elective course, not in a clinical/workplace context."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Educational activity is a course assignment and reflections, not a workplace-based or clinical education assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1016/j.cptl.2025.102491
    pmid: "41005185"
    authors: Alexander K, Egan C, Feild C
    year: "2026"
    abstract: |-
      BACKGROUND: Faculty redesigned a drug information (DI) assignment to incorporate generative artificial intelligence (AI), encouraging the responsible use of AI to craft DI responses. The purpose of this study was to explore pharmacy students' perceptions of using AI tools in DI assignments and the value of AI-enabled assignments in health professions education.

      EDUCATIONAL ACTIVITY: A qualitative thematic analysis was conducted of third-year pharmacy students' narrative reflections after completing an AI-enabled team DI assignment in a Spring 2024 critical care elective. The steps to complete the DI assignment were: 1) create an AI-generated response to a DI question, 2) critique the AI output, 3) complete an individual literature search, 4) synthesize a final team DI submission, 5) reflect. Student AI use at baseline and anticipated use following the assignment were collected via a pre-/post- course survey.

      EVALUATION FINDINGS: Twenty-one students were enrolled in the course and submitted narrative reflections on the DI assignment. Most students stated that prior to the assignment they had never employed the use of AI for professional or academic reasons (n = 14/20; 70 %). Two themes, describing benefits and challenges, and six sub-themes were identified from the reflective statements: user benefits, awareness of AI and its capabilities, quality of the AI response, skepticism, user responsibility, and limitations of AI.

      ANALYSIS OF EDUCATIONAL ACTIVITY: Through narrative reflection, students identified benefits and challenges of integrating AI into a DI assignment. While AI enhanced efficiency, students emphasized the importance of ensuring the validity and detail of the AI-generated DI question response.
    mergedFrom:
      - source: pubmed
        pmid: "41005185"
        doi: 10.1016/j.cptl.2025.102491
    finalDecision: exclude
  - title: "Performance Comparison of Cutting-Edge Large Language Models on the ACR In-Training Examination: An Update for 2025."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on ACR in-training examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks multiple LLMs on radiology in-training exam (DXIT) questions; not about using AI for assessment or feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: In-training exam benchmarking study without workplace-based assessment feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.1016/j.acra.2025.09.008
    pmid: "40998658"
    authors: Young A, Paloka R, Islam A, Prasanna P, Hill V, Payne D
    year: "2025"
    abstract: |-
      PURPOSE: This study represents a continuation of prior work by Payne et al. evaluating large language model (LLM) performance on radiology board-style assessments, specifically the ACR diagnostic radiology in-training examination (DXIT). Building upon earlier findings with GPT-4, we assess the performance of newer, cutting-edge models, such as GPT-4o, GPT-o1, GPT-o3, Claude, Gemini, and Grok on standardized DXIT questions. In addition to overall performance, we compare model accuracy on text-based versus image-based questions to assess multi-modal reasoning capabilities. As a secondary aim, we investigate the potential impact of data contamination by comparing model performance on original versus revised image-based questions.

      METHODS: Seven LLMs - GPT-4, GPT-4o, GPT-o1, GPT-o3, Claude 3.5 Sonnet, Gemini 1.5 Pro, and Grok 2.0-were evaluated using 106 publicly available DXIT questions. Each model was prompted using a standardized instruction set to simulate a radiology resident answering board-style questions. For each question, the model's selected answer, rationale, and confidence score were recorded. Unadjusted accuracy (based on correct answer selection) and logic-adjusted accuracy (based on clinical reasoning pathways) were calculated. Subgroup analysis compared model performance on text-based versus image-based questions. Additionally, 63 image-based questions were revised to test novel reasoning while preserving the original diagnostic image to assess the impact of potential training data contamination.

      RESULTS: Across 106 DXIT questions, GPT-o1 demonstrated the highest unadjusted accuracy (71.7%), followed closely by GPT-4o (69.8%) and GPT-o3 (68.9%). GPT-4 (59.4%) and Grok 2.0 exhibited similar scores (59.4% and 52.8%). Claude 3.5 Sonnet had the lowest unadjusted accuracies (34.9%). Similar trends were observed for logic-adjusted accuracy, with GPT-o1 (60.4%), GPT-4o (59.4%), and GPT-o3 (59.4%) again outperforming other models, while Grok 2.0 and Claude 3.5 Sonnet lagged behind (34.0% and 30.2%, respectively). GPT-4o's performance was significantly higher on text-based questions compared to image-based ones. Unadjusted accuracy for the revised DXIT questions was 49.2%, compared to 56.1% on matched original DXIT questions. Logic-adjusted accuracy for the revised DXIT questions was 40.0% compared to 44.4% on matched original DXIT questions. No significant difference in performance was observed between original and revised questions.

      CONCLUSION: Modern LLMs, especially those from OpenAI, demonstrate strong and improved performance on board-style radiology assessments. Comparable performance on revised prompts suggests that data contamination may have played a limited role. As LLMs improve, they hold strong potential to support radiology resident learning through personalized feedback and board-style question review.
    mergedFrom:
      - source: pubmed
        pmid: "40998658"
        doi: 10.1016/j.acra.2025.09.008
    finalDecision: exclude
  - title: "Faculty versus artificial intelligence chatbot: a comparative analysis of multiple-choice question quality in physiology."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; compares faculty vs AI-generated physiology MCQs for undergraduate preclinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Study assesses AI-generated physiology MCQ quality, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1152/advan.00197.2025
    pmid: "40981738"
    authors: Dhanvijay A, Kumari A, Pinjar M, Kumari A, Ganguly A, Priya A, Juhi A, Gupta P, Mondal H
    year: "2025"
    abstract: "Multiple-choice questions (MCQs) are widely used for assessment in medical education. While human-generated MCQs benefit from pedagogical insight, creating high-quality items is time intensive. With the advent of artificial intelligence (AI), tools like DeepSeek R1 offer potential for automated MCQ generation, though their educational validity remains uncertain. With this background, this study compared the psychometric quality of Physiology MCQs generated by faculty and an AI chatbot. A total of 200 MCQs were developed following the standard syllabus and question design guidelines: 100 by the Physiology faculty and 100 by the AI chatbot DeepSeek R1. Fifty questions from each group were randomly selected and administered to undergraduate medical students in 2 hours. Item analysis was conducted postassessment using difficulty index (DIFI), discrimination index (DI), and nonfunctional distractors (NFDs). Statistical comparisons were made using t tests or nonparametric equivalents, with significance at P < 0.05. Chatbot-generated MCQs had a significantly higher DIFI (0.64 ± 0.22) than faculty MCQs (0.47 ± 0.19; P < 0.0001). No significant difference in DI was found between the groups (P = 0.17). Faculty MCQs had significantly fewer NFDs (median 0) compared to chatbot MCQs (median 1; P = 0.0063). AI-generated MCQs demonstrated comparable discrimination ability but were generally easier and contained more ineffective distractors. While chatbots show promise in MCQ generation, further refinement is needed to improve distractor quality and item difficulty. AI can complement but not yet replace human expertise in assessment design.NEW & NOTEWORTHY This study contributes to the growing research on artificial intelligence (AI)- versus faculty-generated multiple-choice questions in Physiology. Psychometric analysis showed that AI-generated items were generally easier but had comparable discrimination ability to faculty-authored questions, while containing more nonfunctional distractors. By focusing on Physiology, this work offers discipline-specific insights and underscores both the potential and current limitations of AI in assessment development."
    mergedFrom:
      - source: pubmed
        pmid: "40981738"
        doi: 10.1152/advan.00197.2025
    finalDecision: exclude
  - title: "Evaluating the Potential and Accuracy of ChatGPT-3.5 and 4.0 in Medical Licensing and In-Training Examinations: Systematic Review and Meta-Analysis."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Systematic review evaluating LLM accuracy on standardized medical licensing and in-training examinations."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Systematic review of licensing/in-training exam accuracy, not workplace-based assessment/feedback on clinical observations."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.2196/68070
    pmid: "40973108"
    authors: Jaleel A, Aziz U, Farid G, Zahid Bashir M, Mirza T, Khizar Abbas S, Aslam S, Sikander R
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) has significantly impacted health care, medicine, and radiology, offering personalized treatment plans, simplified workflows, and informed clinical decisions. ChatGPT (OpenAI), a conversational AI model, has revolutionized health care and medical education by simulating clinical scenarios and improving communication skills. However, inconsistent performance across medical licensing examinations and variability between countries and specialties highlight the need for further research on contextual factors influencing AI accuracy and exploring its potential to enhance technical proficiency and soft skills, making AI a reliable tool in patient care and medical education.

      OBJECTIVE: This systematic review aims to evaluate and compare the accuracy and potential of ChatGPT-3.5 and 4.0 in medical licensing and in-training residency examinations across various countries and specialties.

      METHODS: A systematic review and meta-analysis were conducted, adhering to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. Data were collected from multiple reputable databases (Scopus, PubMed, JMIR Publications, Elsevier, BMJ, and Wiley Online Library), focusing on studies published from January 2023 to July 2024. Analysis specifically targeted research assessing ChatGPT's efficacy in medical licensing exams, excluding studies not related to this focus or published in languages other than English. Ultimately, 53 studies were included, providing a robust dataset for comparing the accuracy rates of ChatGPT-3.5 and 4.0.

      RESULTS: ChatGPT-4 outperformed ChatGPT-3.5 in medical licensing exams, achieving a pooled accuracy of 81.8%, compared to ChatGPT-3.5's 60.8%. In in-training residency exams, ChatGPT-4 achieved an accuracy rate of 72.2%, compared to 57.7% for ChatGPT-3.5. The forest plot presented a risk ratio of 1.36 (95% CI 1.30-1.43), demonstrating that ChatGPT-4 was 36% more likely to provide correct answers than ChatGPT-3.5 across both medical licensing and residency exams. These results indicate that ChatGPT-4 significantly outperforms ChatGPT-3.5, but the performance advantage varies depending on the exam type. This highlights the importance of targeted improvements and further research to optimize ChatGPT-4's performance in specific educational and clinical settings.

      CONCLUSIONS: ChatGPT-4.0 and 3.5 show promising results in enhancing medical education and supporting clinical decision-making, but they cannot replace the comprehensive skill set required for effective medical practice. Future research should focus on improving AI's capabilities in interpreting complex clinical data and enhancing its reliability as an educational resource.
    mergedFrom:
      - source: pubmed
        pmid: "40973108"
        doi: 10.2196/68070
    finalDecision: exclude
  - title: "ChatGPT's progress over time: A longitudinal enhancing biostatistical problem-solving in medical education."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates ChatGPT's accuracy in solving biostatistics problems over time, an academic/classroom context, not workplace-based or clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Higher-education biostatistics problem-solving context, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1177/14604582251381260
    pmid: "40968760"
    authors: Ignjatovi&#x107; A, An&#x111;elkovi&#x107; Apostolovi&#x107; M, Stevanovi&#x107; L, Radovanovi&#x107; P, Topalovi&#x107; M, Filipovi&#x107; T, Ota&#x161;evi&#x107; S
    year: "2025"
    abstract: "Objective: ChatGPT has been recognised as a potentially transformative tool in higher education by enhancing the teaching and learning process. Cross-sectional evaluations have acknowledged this potential. This study evaluates ChatGPT's performance in solving specific biostatistical problems, focusing on accuracy, stability, and reproducibility, and explores its potential as a reliable educational tool in medical education. Methods: The correlation analysis task from Statistics at Square One by Swinscow and Campbell was chosen for its foundational role in biostatistics. Between October 2023 and March 2024, and July 2024, GPT-3.5 and GPT-4 were tested for accuracy in 12 parameters. Results: A statistically significant change in correct response rates was established in repeated measurements in the period October 2023, March 2024, and July 2024 for GPT-3.5 (Q = 100.99, p < 0.001), GPT-4.0 (Q = 89.55, p < 0.001), respectively. The significant GPT-3.5 improvement was established between March 2024/July 2024 (p = 0.004), and between October 2023 and July 2024 (p = 0.008). The significant GPT-4.0 improvement was established between October 2023 and March 2024 (p = 0.004), and between October 2023 and July 2024 (p = 0.026). Conclusion: Over 9 months, GPT-4 demonstrated rapid and consistent improvements, achieving perfect accuracy by March 2024. Although this study documented ChatGPT's advancement within 9 months, ChatGPT should be positioned as a supplementary tool in higher education classrooms, in the presence of educators, to enhance the learning process."
    mergedFrom:
      - source: pubmed
        pmid: "40968760"
        doi: 10.1177/14604582251381260
    finalDecision: exclude
  - title: "Comparative performance of neurosurgery-specific, peer-reviewed versus general AI chatbots in bilingual board examinations: evaluating accuracy, consistency, and error minimization strategies."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on board examinations, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Evaluates LLM accuracy on neurosurgery board exam MCQs, not about using AI for learner assessment or feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on chatbot accuracy on board-style exam questions, not assessment/feedback generation or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1007/s00701-025-06628-y
    pmid: "40924209"
    authors: "&#xc7;amlar M, Sevgi U, Erol G, Karaka&#x15f; F, Do&#x11f;ruel Y, G&#xfc;ng&#xf6;r A"
    year: "2025"
    abstract: |-
      BACKGROUND: Recent studies suggest that large language models (LLMs) such as ChatGPT are useful tools for medical students or residents when preparing for examinations. These studies, especially those conducted with multiple-choice questions, emphasize that the level of knowledge and response consistency of the LLMs are generally acceptable; however, further optimization is needed in areas such as case discussion, interpretation, and language proficiency. Therefore, this study aimed to evaluate the performance of six distinct LLMs for Turkish and English neurosurgery multiple-choice questions and assess their accuracy and consistency in a specialized medical context.

      METHODS: A total of 599 multiple-choice questions drawn from Turkish Board examinations and an English neurosurgery question bank were presented to six LLMs (ChatGPT-o1pro, ChatGPT-4, AtlasGPT, Gemini, Copilot, and ChatGPT-3.5). Correctness rates were compared using the proportion z-test, and inter-model consistency was examined using Cohen's kappa.

      RESULTS: ChatGPT-o1pro, ChatGPT-4, and AtlasGPT demonstrated relatively high accuracy for Single Best Answer-Recall of Knowledge (SBA-R), Single Best Answer-Interpretative Application of Knowledge (SBA-I), and True/False question types; however, performance notably decreased for questions with images, with some models leaving many unanswered items.

      CONCLUSION: Our findings suggest that GPT-4-based models and AtlasGPT can handle specialized neurosurgery questions at a near-expert level for SBA-R, SBA-I, and True/False formats. Nevertheless, all models exhibit notable limitations in questions with images, indicating that these tools remain supplementary rather than definitive solutions for neurosurgical training and decision-making.
    mergedFrom:
      - source: pubmed
        pmid: "40924209"
        doi: 10.1007/s00701-025-06628-y
    finalDecision: exclude
  - title: "Influence of medical educational background on the diagnostic quality of ChatGPT-4 responses in internal medicine: A pilot study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates how user expertise affects ChatGPT-4 diagnostic output quality; does not use generative AI for assessment/feedback of learners' clinical performance."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates diagnostic responses to written cases, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1111/eci.70113
    pmid: "40922522"
    authors: Gilardi N, Ballabio M, Ravera F, Ferrando L, Stabile M, Bellodi A, Talerico G, Cigolini B, Genova C, Carbone F, Montecucco F, Bracco C, Ballestrero A, Zoppoli G
    year: "2025"
    abstract: This pilot study evaluated the influence of medical background on the diagnostic quality of ChatGPT-4's responses in Internal Medicine. Third-year students, residents and specialists summarised five complex NEJM clinical cases before querying ChatGPT-4. Diagnostic ranking, assessed by independent experts, revealed that residents significantly outperformed students (OR 2.33, p = .007); though overall performance was low. These findings indicate that user expertise and concise case summaries are critical for optimising AI diagnostics, highlighting the need for enhanced AI training and user interaction strategies.
    mergedFrom:
      - source: pubmed
        pmid: "40922522"
        doi: 10.1111/eci.70113
    finalDecision: exclude
  - title: "The effects of ChatGPT-driven blended teaching model on nursing rounds: A quasi-experimental study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: ChatGPT is used as a teaching tool during nursing rounds, not for assessment, feedback generation, or analysis of observation records. The study measures the educational impact of a teaching intervention, not AI-driven assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on ChatGPT-enabled teaching rounds, not AI for assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.1016/j.nepr.2025.104545
    pmid: "40912023"
    authors: Yin J, Hao X, Xing G, Xu M
    year: "2025"
    abstract: |-
      AIM: This study aims to evaluate the application effect of Chat Generative Pre-trained Transformer (ChatGPT)-driven blended teaching model in nursing rounds.

      BACKGROUND: Traditional teacher-centered nursing rounds often lead to passive learning and low efficiency. It remains uncertain whether ChatGPT-based nursing rounds is superior to traditional teaching in nursing rounds.

      DESIGN: A quasi-experimental study was conducted, involving pretest and post-test assessments with both a study group and a control group.

      METHODS: A total of 124 nursing students from three tertiary hospitals were divided into two groups. The teaching intervention (TI) group, consisting of 64 participants, received training using a ChatGPT-driven teaching rounds. In contrast, the traditional teaching (TT) group, with 60 participants, underwent conventional nursing rounds. The impact of these two teaching methods on nursing rounds was compared through pre- and post-training evaluations of professional skills and questionnaires.

      RESULTS: Following the intervention, nursing students in the TI group exhibited enhanced clinical competency and critical thinking ability compared with those in the control group (p < 0.05 for both). Furthermore, the learning efficiency of students in the TI group was elevated and they provided a more favorable evaluation of the innovative ward round teaching method.

      CONCLUSION: Traditional nursing rounds adopt a conventional method, whereas the ChatGPT-integrated blended teaching model revolutionizes them. It boosts students' clinical skills, critical thinking and scenario comprehension, enabling them to tackle complex cases and make sound judgments. Additionally, it optimizes learning efficiency, saving time and facilitating targeted teaching.
    mergedFrom:
      - source: pubmed
        pmid: "40912023"
        doi: 10.1016/j.nepr.2025.104545
    finalDecision: exclude
  - title: "Development of a Clinical Clerkship Mentor Using Generative AI and Evaluation of Its Effectiveness in a Medical Student Trial Compared to Student Mentors: 2-Part Comparative Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: AI-CCM serves as a mentoring/advising tool for clinical clerkship challenges (workload, communication, emotional stress), not for assessment, feedback on clinical performance, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:51.586000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Describes AI mentoring/support, not assessment, feedback generation on observed performance, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:58.822000+00:00
    doi: 10.2196/76702
    pmid: "40907969"
    authors: Ebihara H, Kasai H, Shimizu I, Shikino K, Tajima H, Kimura Y, Ito S
    year: "2025"
    abstract: |-
      BACKGROUND: At the beginning of their clinical clerkships (CCs), medical students face multiple challenges related to acquiring clinical and communication skills, building professional relationships, and managing psychological stress. While mentoring and structured feedback are known to provide critical support, existing systems may not offer sufficient and timely guidance owing to the faculty's limited availability. Generative artificial intelligence, particularly large language models, offers new opportunities to support medical education by providing context-sensitive responses.

      OBJECTIVE: This study aimed to develop a generative artificial intelligence CC mentor (AI-CCM) based on ChatGPT and evaluate its effectiveness in supporting medical students' clinical learning, addressing their concerns, and supplementing human mentoring. The secondary objective was to compare AI-CCM's educational value with responses from senior student mentors.

      METHODS: We conducted 2 studies. In study 1, we created 5 scenarios based on challenges that students commonly encountered during CCs. For each scenario, 5 senior student mentors and AI-CCM generated written advice. Five medical education experts evaluated these responses using a rubric to assess accuracy, practical utility, educational appropriateness (5-point Likert scale), and safety (binary scale). In study 2, a total of 17 fourth-year medical students used AI-CCM for 1 week during their CCs and completed a questionnaire evaluating its usefulness, clarity, emotional support, and impact on communication and learning (5-point Likert scale) informed by the technology acceptance model.

      RESULTS: All results indicated that AI-CCM achieved higher mean scores than senior student mentors. AI-CCM responses were rated higher in educational appropriateness (4.2, SD 0.7 vs 3.8, SD 1.0; P=.001). No significant differences with senior student mentors were observed in accuracy (4.4, SD 0.7 vs 4.2, SD 0.9; P=.11) or practical utility (4.1, SD 0.7 vs 4.0, SD 0.9; P=.35). No safety concerns were identified in AI-CCM responses, whereas 2 concerns were noted in student mentors' responses. Scenario-specific analysis revealed that AI-CCM performed substantially better in emotional and psychological stress scenarios. In the student trial, AI-CCM was rated as moderately useful (mean usefulness score 3.9, SD 1.1), with positive evaluations for clarity (4.0, SD 0.9) and emotional support (3.8, SD 1.1). However, aspects related to feedback guidance (2.9, SD 0.9) and anxiety reduction (3.2, SD 1.0) received more neutral ratings. Students primarily consulted AI-CCM regarding learning workload and communication difficulties; few students used it to address emotional stress-related issues.

      CONCLUSIONS: AI-CCM has the potential to serve as a supplementary educational partner during CCs, offering comparable support to that of senior student mentors in structured scenarios. Despite challenges of response latency and limited depth in clinical content, AI-CCM was received well by and accessible to students who used ChatGPT's free version. With further refinements, including specialty-specific content and improved responsiveness, AI-CCM may serve as a scalable, context-sensitive support system in clinical medical education.
    mergedFrom:
      - source: pubmed
        pmid: "40907969"
        doi: 10.2196/76702
    finalDecision: exclude
  - title: "From algorithms to operating room: can large language models master China's attending anesthesiology exam? A cross-sectional evaluation."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on anesthesiology exam, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Study benchmarks LLM performance on an anesthesiology attending physician examination."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Examines LLM performance on attending physician exam question banks, not workplace-based assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1097/JS9.0000000000003406
    pmid: "40905848"
    authors: He Q, Tan Z, Niu W, Chen D, Zhang X, Qin F, Yuan J
    year: "2026"
    abstract: |-
      OBJECTIVE: The performance of large language models (LLMs) in complex clinical reasoning tasks is not well established. This study compares ChatGPT (GPT-3.5 and GPT-4) and DeepSeek (DeepSeek-V3 and DeepSeek-R1) in the Chinese anesthesiology attending physician examination (CAAPE), aiming to set artificial intelligence (AI) benchmarks in medical assessments and enhance AI-driven medical education.

      METHODS: This cross-sectional study assessed 4 iterations of 2 major LLMs on the 2025 CAAPE question bank (5647 questions). Testing employed diverse querying strategies and languages, with subgroup analyses by subspecialty, knowledge type, and question format. The focus was on LLM performance in clinical and logical reasoning tasks, measuring accuracy, error types, and response times.

      RESULTS: DeepSeek-R1 (70.6-73.4%) and GPT-4 (68.6-70.3%) outperformed DeepSeek-V3 (53.1-55.5%) and GPT-3.5 (52.2-55.7%) across all strategies. System role (SR) improved performance, while joint response degraded it. DeepSeek-R1 outperformed GPT-4 in complex subspecialties, reaching peak accuracy (73.4%) under SR combined initial response. Generative Pre-trained Transformers (GPT) models performed better with English than Chinese queries. All models excelled in basic knowledge and Type A1 questions but struggled with clinical scenarios and advanced reasoning. Despite DeepSeek-R1's stronger performance, its response time was longer. Errors were primarily logical and informational (over 70%), with more than half being high-risk clinical errors.

      CONCLUSION: LLMs show promise in complex clinical reasoning but risk critical errors in high-risk settings. While useful for education and decision support, their error potential must be carefully assessed in high-stakes environments.
    mergedFrom:
      - source: pubmed
        pmid: "40905848"
        doi: 10.1097/JS9.0000000000003406
    finalDecision: exclude
  - title: Comparative evaluation of large language models performance in medical education using urinary system histology assessment.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on histology assessment (preclinical), not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Histology assessment in medical education is not clearly workplace-based clinical education"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    doi: 10.1038/s41598-025-17571-4
    pmid: "40883524"
    authors: Szab&#xf3; A, Laein G
    year: "2025"
    abstract: "Large language models (LLMs) show potential for medical education, but their domain-specific capabilities need systematic evaluation. This study presents a comparative assessment of thirteen LLMs in urinary system histology education. Using a multi-dimensional framework, we evaluated models across two tasks: answering 65 validated multiple-choice questions (MCQs) and generating clinical scenarios with assessment items. The two tasks operationalize distinct constructs and were analyzed independently; scores are not directly comparable across tasks. For MCQ performance, we assessed accuracy along with explanation quality through relevance and comprehensiveness metrics. For scenario generation, we evaluated Quality, Complexity, Relevance, Correctness, and Variety dimensions. Performance varied substantially across models and tasks, with ChatGPT-o1 achieving highest MCQ accuracy (96.31 ± 17.85%) and Claude-3.5 demonstrating superior clinical scenario generation capabilities (91.4% of maximum possible score). For task 1, all models significantly outperformed random guessing with large effect sizes. Statistical analyses revealed significant differences in consistency across multiple attempts and dimensional performance, with most models showing higher Correctness than Quality scores in scenario generation. Term frequency analysis revealed significant content imbalances across all models, with systematic overemphasis of certain anatomical structures and complete omission of others. Our findings demonstrate that while LLMs show considerable promise for medical education, their reliable implementation requires matching specific models to appropriate educational tasks, implementing verification mechanisms, and recognizing their current limitations in generating pedagogically balanced content."
    mergedFrom:
      - source: pubmed
        pmid: "40883524"
        doi: 10.1038/s41598-025-17571-4
    finalDecision: exclude
  - title: "Development and Validation of a Large Language Model-Based System for Medical History-Taking Training: Prospective Multicase Study on Evaluation Stability, Human-AI Consistency, and Transparency."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments (LLM-based virtual standardized patients for history-taking training, not real clinical encounters)"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: History-taking assessment occurs with virtual simulated patients only."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.2196/73419
    pmid: "40882613"
    authors: Liu Y, Shi C, Wu L, Lin X, Chen X, Zhu Y, Tan H, Zhang W
    year: "2025"
    abstract: |-
      BACKGROUND: History-taking is crucial in medical training. However, current methods often lack consistent feedback and standardized evaluation and have limited access to standardized patient (SP) resources. Artificial intelligence (AI)-powered simulated patients offer a promising solution; however, challenges such as human-AI consistency, evaluation stability, and transparency remain underexplored in multicase clinical scenarios.

      OBJECTIVE: This study aimed to develop and validate the AI-Powered Medical History-Taking Training and Evaluation System (AMTES), based on DeepSeek-V2.5 (DeepSeek), to assess its stability, human-AI consistency, and transparency in clinical scenarios with varying symptoms and difficulty levels.

      METHODS: We developed AMTES, a system using multiple strategies to ensure dialog quality and automated assessment. A prospective study with 31 medical students evaluated AMTES's performance across 3 cases of varying complexity: a simple case (cough), a moderate case (frequent urination), and a complex case (abdominal pain). To validate our design, we conducted systematic baseline comparisons to measure the incremental improvements from each level of our design approach and tested the framework's generalizability by implementing it with an alternative large language model (LLM) Qwen-Max (Qwen AI; version 20250409), under a zero-modification condition.

      RESULTS: A total of 31 students practiced with our AMTES. During the training, students generated 8606 questions across 93 history-taking sessions. AMTES achieved high dialog accuracy: 98.6% (SD 1.5%) for cough, 99.0% (SD 1.1%) for frequent urination, and 97.9% (SD 2.2%) for abdominal pain, with contextual appropriateness exceeding 99%. The system's automated assessments demonstrated exceptional stability and high human-AI consistency, supported by transparent, evidence-based rationales. Specifically, the coefficients of variation (CV) were low across total scores (0.87%-1.12%) and item-level scoring (0.55%-0.73%). Total score consistency was robust, with the intraclass correlation coefficients (ICCs) exceeding 0.923 across all scenarios, showing strong agreement. The item-level consistency was remarkably high, consistently above 95%, even for complex cases like abdominal pain (95.75% consistency). In systematic baseline comparisons, the fully-processed system improved ICCs from 0.414/0.500 to 0.923/0.972 (moderate and complex cases), with all CVs ≤1.2% across the 3 cases. A zero-modification implementation of our evaluation framework with an alternative LLM (Qwen-Max) achieved near-identical performance, with the item-level consistency rates over 94.5% and ICCs exceeding 0.89. Overall, 87% of students found AMTES helpful, and 83% expressed a desire to use it again in the future.

      CONCLUSIONS: Our data showed that AMTES demonstrates significant educational value through its LLM-based virtual SPs, which successfully provided authentic clinical dialogs with high response accuracy and delivered consistent, transparent educational feedback. Combined with strong user approval, these findings highlight AMTES's potential as a valuable, adaptable, and generalizable tool for medical history-taking training across various educational contexts.
    mergedFrom:
      - source: pubmed
        pmid: "40882613"
        doi: 10.2196/73419
    finalDecision: exclude
  - title: Comparative evaluation of AI platforms "Google Gemini 2.5 Flash, Google Gemini 2.0 Flash, DeepSeek V3 and ChatGPT 4o" in solving multiple-choice questions from different subtopics of anatomy.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks AI platforms on anatomy MCQs; not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: LLM accuracy on anatomy MCQs is not workplace-based assessment or feedback analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1007/s00276-025-03707-8
    pmid: "40885861"
    authors: Singal A, Goyal S
    year: "2025"
    abstract: |-
      PURPOSE: The rise of artificial intelligence (AI) based large language models (LLMs) had a profound impact on medical education. Given the widespread use of multiple-choice questions (MCQs) in anatomy education, it is likely that such queries are commonly directed to AI tools. The current study compared the accuracy level of different AI platforms for solving MCQs from various subtopics in Anatomy.

      METHODS: A total of 55 MCQs from different subtopics of Anatomy were enquired using Google Gemini 2.0 Flash, Google Gemini Flash 2.5 DeepSeek V3 and ChatGPT 4o AI platforms. The accuracy rate was calculated. Chi-square test and Fisher's exact test was employed for statistical analysis.

      RESULTS: Overall accuracy of 95.9% was observed across all platforms. When ranked by performance, Google Gemini 2.5 Flash performed the best, followed by Google Gemini 2.0, with Chat GPT 4o and DeepSeek V3. No statistically significant performance difference among different AI models was observed. General Anatomy was identified as the most challenging area across all the models.

      CONCLUSIONS: All models performed exceptionally well on anatomy-focused MCQs. Google Gemini models show superior overall performance. However, certain errors persist that cannot be overlooked, highlighting the continued need for human oversight and expert validation. As per best available literature, this is the first study to include MCQS from different anatomy subtopics and to compare the performance of DeepSeek and Google Gemini Flash 2.5 for the task.
    mergedFrom:
      - source: pubmed
        pmid: "40885861"
        doi: 10.1007/s00276-025-03707-8
    finalDecision: exclude
  - title: "Assessing LLM-generated vs. expert-created clinical anatomy MCQs: a student perception-based comparative study in medical education."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Compares LLM-generated vs expert-written anatomy MCQs in a classroom/academic setting, not workplace-based or clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Classroom-based MCQ development/evaluation study, not workplace-based assessment in clinical settings."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1080/10872981.2025.2554678
    pmid: "40884796"
    authors: Elzayyat M, Mohammad J, Zaqout S
    year: "2025"
    abstract: Large language models (LLMs) such as ChatGPT and Gemini are increasingly used to generate educational content in medical education, including multiple-choice questions (MCQs), but their effectiveness compared to expert-written questions remains underexplored, particularly in anatomy. We conducted a cross-sectional, mixed-methods study involving Year 2-4 medical students at Qatar University, where participants completed and evaluated three anonymized MCQ sets-authored by ChatGPT, Google-Gemini, and a clinical anatomist-across 17 quality criteria. Descriptive and chi-square analyses were performed, and optional feedback was reviewed thematically. Among 48 participants, most rated the three MCQ sources as equally effective, although ChatGPT was more often preferred for helping students identify and confront their knowledge gaps through challenging distractors and diagnostic insight, while expert-written questions were rated highest for deeper analytical thinking. A significant variation in preferences was observed across sources (χ² (64) = 688.79, p < .001). Qualitative feedback emphasized the need for better difficulty calibration and clearer distractors in some AI-generated items. Overall, LLM-generated anatomy MCQs can closely match expert-authored ones in learner-perceived value and may support deeper engagement, but expert review remains critical to ensure clarity and alignment with curricular goals. A hybrid AI-human workflow may provide a promising path for scalable, high-quality assessment design in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "40884796"
        doi: 10.1080/10872981.2025.2554678
    finalDecision: exclude
  - title: Using Pretrained Large Language Models for AI-Driven Assessment in Medical Education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; uses LLMs to grade student written responses in a classroom-based military medical ethics course"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Conducted in a classroom ethics course rather than a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.1097/ACM.0000000000006207
    pmid: "40865045"
    authors: Cole J, Duncan J, Cole R
    year: "2025"
    abstract: |-
      PROBLEM: Assessing students in competency-based medical education can be time-consuming and demanding for faculty, especially with large classes and complex topics. Traditional methods can lead to inconsistencies and a lack of targeted feedback. Innovative and accessible solutions to improve the efficiency, objectivity, and effectiveness of assessment in medical education are needed.

      APPROACH: From September 2024 to February 2025, the authors piloted the use of large language models (LLMs) with retrieval-augmented generation to assess students' understanding of moral injury. The authors selected and uploaded 6 seminal articles on moral injury within military and veteran populations to Google Gemini 1.5 Pro. They tasked the same LLM with creating a grading rubric based on these articles to assess 165 student responses in a military medical ethics course (Uniformed Services University of the Health Sciences). The authors uploaded both the generated rubric and the student responses to each of 3 LLMs (Google Gemini 1.5 Pro, Google Gemini 2.0 Flash, and OpenAI ChatGPT-4o) with a prompt to generate scores for the student responses.

      OUTCOMES: In the authors' expert opinion, an LLM (Google Gemini 1.5 Pro) successfully generated a grading rubric that captured the nuances of moral injury and its implications for military medical practice. The LLMs' scoring accuracy was compared against 2 experienced educators to generate validity evidence. The best-performing model, OpenAI ChatGPT-4o, demonstrated an interrater reliability of 0.77 and 0.68 for reviewers 1 and 2, respectively, indicating a higher level of agreement between the LLM and both individual reviewers than between the 2 reviewers (0.57).

      NEXT STEPS: While this approach shows promise, faculty oversight is necessary to ensure ethical accountability and address potential biases. Further research is needed to optimize the integration of AI and human capabilities in assessment to ultimately enhance the quality of health care professional education and improve patient outcomes.
    mergedFrom:
      - source: pubmed
        pmid: "40865045"
        doi: 10.1097/ACM.0000000000006207
      - source: scopus
        doi: 10.1097/ACM.0000000000006207
        scopusId: SCOPUS_ID:105014920817
    finalDecision: exclude
  - title: Leveraging Large Language Models to Advance Certification, Physician Learning, and Diagnostic Excellence.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not original research; commentary on ABFM's plans to integrate LLMs into certification platform without empirical data"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary article, not original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.3122/jabfm.2024.240385R1
    pmid: "40854705"
    authors: Wang T, Price D, Bazemore A
    year: "2025"
    abstract: Diagnostic errors are a significant challenge in health care, often resulting from gaps in physicians' knowledge and misalignment between confidence and diagnostic accuracy. Traditional educational methods have not sufficiently addressed these issues. This commentary explores how large language models (LLMs), a subset of artificial intelligence, can enhance diagnostic education by improving learning transfer and physicians' diagnostic accuracy. The American Board of Family Medicine (ABFM) is integrating LLMs into its Continuous Knowledge Self-Assessment (CKSA) platform to generate high-quality cloned diagnostic questions, implement effective spaced repetition strategies, and provide personalized feedback. By leveraging LLMs for efficient question generation and individualized learning, the initiative aims to transform continuous certification and lifelong learning, ultimately enhancing diagnostic accuracy and patient care.
    mergedFrom:
      - source: pubmed
        pmid: "40854705"
        doi: 10.3122/jabfm.2024.240385R1
    finalDecision: exclude
  - title: "Evaluating large language models as graders of medical short answer questions: a comparative analysis with expert human graders."
    reviews:
      - decision: uncertain
        comment: LLMs as graders of medical questions; may involve assessment with validity/reliability data
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study evaluates LLMs grading SAQs in pre-clinical basic science subjects (anatomy, histology, embryology, physiology)."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Grading classroom basic-science short-answer questions is outside workplace-based clinical education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.1080/10872981.2025.2550751
    pmid: "40849930"
    authors: Bolgova O, Ganguly P, Ikram M, Mavrych V
    year: "2025"
    abstract: "The assessment of short-answer questions (SAQs) in medical education is resource-intensive, requiring significant expert time. Large Language Models (LLMs) offer potential for automating this process, but their efficacy in specialized medical education assessment remains understudied. To evaluate the capability of five LLMs to grade medical SAQs compared to expert human graders across four distinct medical disciplines. This study analyzed 804 student responses across anatomy, histology, embryology, and physiology. Three faculty members graded all responses. Five LLMs (GPT-4.1, Gemini, Claude, Copilot, DeepSeek) evaluated responses twice: first using their learned representations to generate their own grading criteria (A1), then using expert-provided rubrics (A2). Agreement was measured using Cohen's Kappa and Intraclass Correlation Coefficient (ICC). Expert-expert agreement was substantial across all questions (average Kappa: 0.69, ICC: 0.86), ranging from moderate (SAQ2: 0.57) to almost perfect (SAQ4: 0.87). LLM performance varied dramatically by question type and model. The highest expert-LLM agreement was observed for Claude on SAQ3 (Kappa: 0.61) and DeepSeek on SAQ2 (Kappa: 0.53). Providing expert criteria had inconsistent effects, significantly improving some model-question combinations while decreasing others. No single LLM consistently outperformed others across all domains. LLM strictness in grading unsatisfactory responses varied substantially from experts. LLMs demonstrated domain-specific variations in grading capabilities. The provision of expert criteria did not consistently improve performance. While LLMs show promise for supporting medical education assessment, their implementation requires domain-specific considerations and continued human oversight."
    mergedFrom:
      - source: pubmed
        pmid: "40849930"
        doi: 10.1080/10872981.2025.2550751
    finalDecision: exclude
  - title: "Large language models underperform in European general surgery board examinations: a comparative study with experts and surgical residents."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment/feedback of learners; evaluates LLM performance on European general surgery board exam questions compared to human surgeons"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Board-exam performance comparison, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1186/s12909-025-07856-7
    pmid: "40849634"
    authors: G&#xfc;l M
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) has become a transformative tool in medical education and assessment. Despite advancements, AI models such as GPT-4o demonstrate variable performance on high-stakes examinations. This study compared the performance of four AI models (Llama-3, Gemini, GPT-4o, and Copilot) with specialists and residents on European General Surgery Board test questions, focusing on accuracy across question formats, lengths, and difficulty levels.

      METHODS: A total of 120 multiple-choice questions were systematically sampled from the General Surgery Examination and Board Review question bank using a structured randomization protocol. The questions were administered via Google Forms to four large language models (Llama-3, GPT-4o, Gemini, and Copilot) and 30 surgeons (15 board-certified specialists and 15 residents) under timed, single-session conditions. Participant demographics (age, gender, years of experience) were recorded. Questions were categorized by word count (short, medium, long) and by difficulty level (easy, moderate, hard), rated by three independent board-certified surgeons. Group accuracy rates were compared using ANOVA with appropriate post-hoc tests, and 95% confidence intervals were reported.

      RESULTS: Board-certified surgeons achieved the highest accuracy rate at 81.6% (95% CI: 78.9-84.3), followed by surgical residents at 69.9% (95% CI: 66.7-73.1). Among large language models (LLMs), Llama-3 demonstrated the best performance with an accuracy of 65.8% (95% CI: 62.4-69.2), whereas Copilot showed the lowest performance at 51.7% (95% CI: 48.1-55.3). LLM performance declined significantly as item difficulty and length increased, particularly for Copilot (68.3% on short vs. 36.4% on long questions, p < 0.001). In contrast, human participants maintained relatively stable accuracy across difficulty levels. Notably, only Llama-3 ranked within the human performance range, placing 26th among 30 surgeons, while all other LLMs failed to surpass the 60% accuracy threshold (p < 0.001).

      CONCLUSION: Current LLMs underperform compared to human specialists when faced with questions requiring high-level medical knowledge, reinforcing their current role as supplementary tools in surgical education rather than replacements for expert clinical judgment.
    mergedFrom:
      - source: pubmed
        pmid: "40849634"
        doi: 10.1186/s12909-025-07856-7
    finalDecision: exclude
  - title: The performance of ChatGPT on medical image-based assessments and implications for medical education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates GPT-4/GPT-4o performance on image-based USMLE exam questions"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM performance on image-based licensing exam items, not workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.1186/s12909-025-07752-0
    pmid: "40849473"
    authors: Yang X, Chen W
    year: "2025"
    abstract: |-
      BACKGROUND: Generative artificial intelligence (AI) tools like ChatGPT (OpenAI) have garnered significant attention for their potential in fields such as medical education; however, their performance of large language and vision models on medical test items involving images remains underexplored, limiting their broader educational utility. This study aims to evaluate the performance of GPT-4 and GPT-4 Omni (GPT-4o), accessed via the ChatGPT platform, on image-based United States Medical Licensing Examination (USMLE) sample items, to explore their implications for medical education.

      METHODS: We identified all image-based questions from the USMLE Step 1 and Step 2 Clinical Knowledge sample item sets. Prompt engineering techniques were applied to generate responses from GPT-4 and GPT-4o. Each model was independently tested, with accuracy calculated based on the proportion of correct answers. In addition, we explored the application of these models in case-based teaching scenarios involving medical images.

      RESULTS: A total of 38 image-based questions spanning multiple medical disciplines-including dermatology, cardiology, and gastroenterology-were included in the analysis. GPT-4 achieved an accuracy rate of 73.4% (95% CI, 57.0% to 85.5%), while GPT-4o outperformed it with an accuracy of 89.5% (95% CI, 74.4% to 96.1%), with a numerically higher accuracy but no statistically significant difference (P = 0.137). The two models showed substantial disagreement in their classification of question complexity. In exploratory case-based teaching scenarios, GPT-4o was able to analyze and revise incorrect responses with logical reasoning. Moreover, it demonstrated potential to assist educators in designing structured lesson plans focused on core clinical knowledge areas, though human oversight remained essential.

      CONCLUSION: This study demonstrates that GPT models can accurately answer image-based medical examination questions, with GPT-4o exhibiting numerically higher performance. Prompt engineering further enables their use in instructional planning. While these models hold promise for enhancing medical education, expert supervision remains critical to ensure the accuracy and reliability of AI-generated content.
    mergedFrom:
      - source: pubmed
        pmid: "40849473"
        doi: 10.1186/s12909-025-07752-0
    finalDecision: exclude
  - title: The role of generative AI tools in case-based learning and teaching evaluation of medical biochemistry.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Medical biochemistry case-based learning is classroom/laboratory-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting (case-based classroom learning)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1186/s12909-025-07567-z
    pmid: "40847414"
    authors: Li L, Zhang W, Zhang K, Yang Y, Wang L, Zuo L, Sun Y, Peng Q
    year: "2025"
    abstract: |-
      BACKGROUND: Medical biochemistry, a fundamental course in medical education, has a complex and expanding knowledge base. Traditional teaching methods often fail to meet students' needs for in-depth understanding and personalized learning. Students can become overwhelmed by the vast array of biochemical concepts, reactions, and molecular structures.

      OBJECTIVE: This study aims to explore the potential of generative AI tools as teaching assistants in medical biochemistry, particularly in CBL (Case-Based Learning) settings where their application is currently limited.

      METHODS: We conducted a comparative study involving a control group (N = 40) and an experimental group (N = 39) to assess the impact of AI tools on CBL learning. We analyzed students' performance and compared evaluations of their work by both teachers and AI tools. Additionally, a questionnaire was used to gauge the effects of AI tools on case study learning.

      RESULTS: The experimental group using AI tools showed significantly better performance than the control group. The former completed case assignments faster (2.6 h vs. 5.5 h, P < 0.05) and achieved higher exam scores (77.3 ± 4.3 vs. 66.5 ± 5.4, P < 0.05). AI-based grading on students' assignments closely matched teachers' evaluations on them (P > 0.05), demonstrating reliability in assessment. Students rated AI highly for basic knowledge acquisition (Q4M = 9.18) but noted limitations in complex clinical reasoning (Q11M = 4.20) and innovative thinking (Q12M = 3.90). Key concerns of using AI included that AI reduced interaction between teachers and students (Q1M = 7.17) and standardized AI outputs led to homogenized learning (Q6M = 6.56). Despite these drawbacks, students' acceptance of AI increased significantly after the trial (5.5 to 7.6, P < 0.05).

      CONCLUSION: Generative AI tools have significantly enhanced learning efficiency and performance in CBL teaching of medical biochemistry, shortened task completion time and improved examination scores. Although there are limitations in the cultivation of innovative thinking and interaction between teachers and students, students' acceptance of AI has increased. Therefore, AI should serve as a supplement to traditional teaching to balance the learning efficiency and creative thinking of students.
    mergedFrom:
      - source: pubmed
        pmid: "40847414"
        doi: 10.1186/s12909-025-07567-z
    finalDecision: exclude
  - title: Performance of ChatGPT-4 Omni and Gemini 1.5 Pro on Ophthalmology-Related Questions in the Turkish Medical Specialty Exam.
    reviews:
      - decision: uncertain
        comment: LLMs on specialty exam; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates LLM performance on a medical specialty licensing exam (MCQs)"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on LLM performance on specialty exam MCQs, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.4274/tjo.galenos.2025.27895
    pmid: "40838476"
    authors: Sabaner M, Yozgat Z
    year: "2025"
    abstract: |-
      OBJECTIVES: To evaluate the response and interpretative capabilities of two pioneering artificial intelligence (AI)-based large language model (LLM) platforms in addressing ophthalmology-related multiple-choice questions (MCQs) from Turkish Medical Specialty Exams.

      MATERIALS AND METHODS: MCQs from a total of 37 exams held between 2006-2024 were reviewed. Ophthalmology-related questions were identified and categorized into sections. The selected questions were asked to the ChatGPT-4o and Gemini 1.5 Pro AI-based LLM chatbots in both Turkish and English with specific prompts, then re-asked without any interaction. In the final step, feedback for incorrect responses were generated and all questions were posed a third time.

      RESULTS: A total of 220 ophthalmology-related questions out of 7312 MCQs were evaluated using both AI-based LLMs. A mean of 6.47±2.91 (range: 2-13) MCQs was taken from each of the 33 parts (32 full exams and the pooled 10% of exams shared between 2022 and 2024). After the final step, ChatGPT-4o achieved higher accuracy in both Turkish (97.3%) and English (97.7%) compared to Gemini 1.5 Pro (94.1% and 93.2%, respectively), with a statistically significant difference in English (p=0.039) but not in Turkish (p=0.159). There was no statistically significant difference in either the inter-AI comparison of sections or interlingual comparison.

      CONCLUSION: While both AI platforms demonstrated robust performance in addressing ophthalmology-related MCQs, ChatGPT-4o was slightly superior. These models have the potential to enhance ophthalmological medical education, not only by accurately selecting the answers to MCQs but also by providing detailed explanations.
    mergedFrom:
      - source: pubmed
        pmid: "40838476"
        doi: 10.4274/tjo.galenos.2025.27895
    finalDecision: exclude
  - title: Surgeon, Trainee, or GPT? A Blinded Multicentric Study of AI-Augmented Operative Notes.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: uncertain
        comment: Generative AI (GPT) used for operative note generation with quality/accuracy assessment comparing resident, attending, GPT-only, and hybrid notes. Relates to AI-assisted clinical documentation in surgical training with reliability data (inter-rater reliability). Unclear whether this constitutes assessment of learner documentation skills or purely documentation automation. Forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates operative note generation quality/efficiency, not assessment/feedback of learner performance or analysis of observation records for WBA."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records for learner evaluation. Compares operative note documentation quality across author types using publicly available surgical teaching videos, evaluating documentation quality rather than learner performance."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17T07:34:40.555Z
      - decision: exclude
        comment: "Exclusion criterion 2: Studies AI-assisted operative note generation quality/efficiency, not assessment, feedback, or analysis of learner observation records for educational evaluation."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17T07:34:43.172Z
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates operative note documentation quality, not assessment/feedback of learner performance."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1002/lary.70063
    pmid: "40833187"
    authors: Hack S, Attal R, Locatelli G, Scotta G, Maniaci A, Parisi F, van der Poel N, Van Daele M, Garcia-Lliberos A, Rodriguez-Prado C, Chiesa-Estomba C, Andueza-Guembe M, Cobb P, Zalzal H, Saibene A
    year: "2026"
    abstract: |-
      OBJECTIVES: Clear, complete operative documentation is essential for surgical safety, continuity of care, and medico-legal standards. Large language models such as ChatGPT offer promise for automating clinical documentation; however, their performance in operative note generation, particularly in surgical subspecialties, remains underexplored. This study aimed to compare the quality, accuracy, and efficiency of operative notes authored by a surgical resident, attending surgeon, GPT alone, and an attending surgeon using GPT as a writing aid.

      METHODS: Five publicly available otolaryngologic procedures were selected. For each procedure, four operative notes were generated, one by a resident, one by an attending, one by GPT alone, and one by a hybrid of attending plus GPT. Ten blinded otolaryngologists (five residents, five attendings) independently reviewed all 20 notes. Reviewers scored each note across eight domains using a five-point scale, assigned a final approval rating, and provided qualitative feedback. Writing time was recorded to assess documentation efficiency.

      RESULTS: Hybrid notes written by an attending surgeon with GPT assistance received the highest average domain scores and the highest "as is" approval rate (79%), outperforming all other groups. GPT-only notes were the fastest to generate but had the lowest approval rate (23%) and the highest incidence of both omissions and overdocumentation. Writing time was significantly reduced in both AI-assisted groups compared to human-only authorship. Inter-rater reliability among reviewers was moderate to high across most domains.

      CONCLUSION: In this limited dataset, hybrid human-AI collaboration outperformed both human-only and AI-only authorship in operative documentation. These findings support GPT-assisted documentation to improve operative note efficiency and consistency.

      LEVEL OF EVIDENCE: N/A.
    mergedFrom:
      - source: pubmed
        pmid: "40833187"
        doi: 10.1002/lary.70063
      - source: scopus
        doi: 10.1002/lary.70063
        scopusId: SCOPUS_ID:105013759462
    finalDecision: exclude
  - title: "A Virtual Simulator to Improve Weight-Related Communication Skills for Health Care Professionals: Mixed Methods Pre-Post Pilot Feasibility Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Study is centered on a virtual simulator, indicating simulation-only assessment."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: The study does not use generative AI/LLMs; VITAL-COMS is a digital simulation tool. Generative AI is only mentioned as a future development direction in the conclusions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:51.586000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Simulation training tool is not an application of generative AI/LLMs in the evaluated intervention."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:58.822000+00:00
    doi: 10.2196/65949
    pmid: "40815779"
    authors: Quigley F, Ryan L, Bond R, McAloon T, Zheng H, Moorhead A
    year: "2025"
    abstract: |-
      BACKGROUND: Discussing weight remains a sensitive and often avoided topic in health care, despite rising prevalence of obesity and calls for earlier, more compassionate interventions. Many health care professionals report inadequate training and low confidence to discuss weight, while patients often describe feeling stigmatized or dismissed. Digital simulation offers a promising route to build communication skills through supporting repeatable and reflective practice in a safe space. VITAL-COMS (Virtual Training and Assessment for Communication Skills) is a novel simulation tool designed to support health care professionals in navigating weight-related conversations with greater understanding and skill.

      OBJECTIVE: This study aimed to assess the potential of VITAL-COMS as a digital simulation training tool to improve weight-related communication skills among health care professionals.

      METHODS: A mixed-method feasibility study was conducted online via Zoom (Zoom Video Communications) between January to July 2021, with UK-based nurses, doctors, and dietitians. The intervention comprised educational videos and 2 simulated patient scenarios with real-time verbal interaction. Pre- and posttraining self-assessments of communication skills and conversation length were collected. Participants also completed a feasibility questionnaire. Descriptive statistics were used to analyze the feasibility questionnaire, and open-ended feedback was analyzed using content analysis. Paired-samples t tests were used to assess changes in communication skills and conversation length before and post training.

      RESULTS: In total, 31 participants completed the study. There was a statistically significant improvement in self-assessed communication skills following training (mean difference=3.9; 95% CI, 2.54-5.26; t30=-5.76, P=.001, Cohen d=1.03). Mean conversation length increased significantly in both scenarios: in the female patient scenario, from 3.73 (SD 1.36) to 6.08 (SD 2.26) minutes, with a mean difference of 2.35 minutes (95% CI, 1.71-2.99; t30=7.49, P=.001, Cohen d=1.34); and in the male scenario, from 3.61 (SD 1.12) to 5.65 (SD 1.76) minutes, a mean difference of 2.03 minutes (95% CI, 1.51-2.55; t30=8.03, P=.001, Cohen d=1.44). Participants rated the simulation positively, with 97% (95% CI 90%-100%) supporting wider use in health care and 84% (95% CI 71%-97%) reporting emotional engagement. Content analysis of feedback generated two themes: (1) adapting to this form of learning and (2) recognizing the potential of simulation to support reflective, skills-based training. A minority, 13% (95% CI 1%-25%) expressed a preference for alternative learning methods.

      CONCLUSIONS: VITAL-COMS was feasible to implement and acceptable to a diverse group of health care professionals. Participants demonstrated significant improvements in self-assessed communication skills and patient-scenario engagement. The simulation was perceived as realistic, emotionally engaging, and well-suited for training in sensitive conversations. These findings support further development and integration of VITAL-COMS into health education programs. Next steps include the translation of the insights identified in this study to inform a tool supported by generative artificial intelligence.
    mergedFrom:
      - source: pubmed
        pmid: "40815779"
        doi: 10.2196/65949
    finalDecision: exclude
  - title: Evaluating Hospital Course Summarization by an Electronic Health Record-Based Large Language Model.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on clinical summarization, not assessment/feedback/analysis of learner observation records in WBA."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates EHR-embedded LLM for clinical documentation (hospital course summarization in discharge summaries); not about assessment or feedback of learner clinical performance"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Clinical documentation summarization study, not workplace-based learner assessment or feedback analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1001/jamanetworkopen.2025.26339
    pmid: "40802185"
    authors: Small W, Austrian J, O'Donnell L, Burk-Rafel J, Hochman K, Goodman A, Zaretsky J, Martin J, Johnson S, Major V, Jones S, Henke C, Verplanke B, Osso J, Larson I, Saxena A, Mednick A, Simonis C, Han J, Kesari R, Wu X, Heery L, Desel T, Baskharoun S, Figman N, Farooq U, Shah K, Jahan N, Kim J, Testa P, Feldman J
    year: "2025"
    abstract: |-
      IMPORTANCE: Hospital course (HC) summarization represents an increasingly onerous discharge summary component for physicians. Literature supports large language models (LLMs) for HC summarization, but whether physicians can effectively partner with electronic health record-embedded LLMs to draft HCs is unknown.

      OBJECTIVES: To compare the editing effort required by time-constrained resident physicians to improve LLM- vs physician-generated HCs toward a novel 4Cs (complete, concise, cohesive, and confabulation-free) HC.

      DESIGN, SETTING, AND PARTICIPANTS: Quality improvement study using a convenience sample of 10 internal medicine resident editors, 8 hospitalist evaluators, and randomly selected general medicine admissions in December 2023 lasting 4 to 8 days at New York University Langone Health.

      EXPOSURES: Residents and hospitalists reviewed randomly assigned patient medical records for 10 minutes. Residents blinded to author type who edited each HC pair (physician and LLM) for quality in 3 minutes, followed by comparative ratings by attending hospitalists.

      MAIN OUTCOMES AND MEASURES: Editing effort was quantified by analyzing the edits that occurred on the HC pairs after controlling for length (percentage edited) and the degree to which the original HCs' meaning was altered (semantic change). Hospitalists compared edited HC pairs with A/B testing on the 4Cs (5-point Likert scales converted to 10-point bidirectional scales).

      RESULTS: Among 100 admissions, compared with physician HCs, residents edited a smaller percentage of LLM HCs (LLM mean [SD], 31.5% [16.6%] vs physicians, 44.8% [20.0%]; P < .001). Additionally, LLM HCs required less semantic change (LLM mean [SD], 2.4% [1.6%] vs physicians, 4.9% [3.5%]; P < .001). Attending physicians deemed LLM HCs to be more complete (mean [SD] difference LLM vs physicians on 10-point bidirectional scale, 3.00 [5.28]; P < .001), similarly concise (mean [SD], -1.02 [6.08]; P = .20), and cohesive (mean [SD], 0.70 [6.14]; P = .60), but with more confabulations (mean [SD], -0.98 [3.53]; P = .002). The composite scores were similar (mean [SD] difference LLM vs physician on 40-point bidirectional scale, 1.70 [14.24]; P = .46).

      CONCLUSIONS AND RELEVANCE: Electronic health record-embedded LLM HCs required less editing than physician-generated HCs to approach a quality standard, resulting in HCs that were comparably or more complete, concise, and cohesive, but contained more confabulations. Despite the potential influence of artificial time constraints, this study supports the feasibility of a physician-LLM partnership for writing HCs and provides a basis for monitoring LLM HCs in clinical practice.
    mergedFrom:
      - source: pubmed
        pmid: "40802185"
        doi: 10.1001/jamanetworkopen.2025.26339
      - source: scopus
        doi: 10.1001/jamanetworkopen.2025.26339
        scopusId: SCOPUS_ID:105013416970
    finalDecision: exclude
  - title: "Pediatric surgical trainees and artificial intelligence: a comparative analysis of DeepSeek, Copilot, Google Bard and pediatric surgeons' performance on the European Pediatric Surgical In-Training Examinations (EPSITE)."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Evaluates LLM performance on standardized pediatric surgery in-training exam questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares LLM accuracy on in-training exam items rather than workplace-based assessment/feedback on clinical observations."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.1007/s00383-025-06104-9
    pmid: "40781215"
    authors: Gnatzy R, Lacher M, Cascio S, M&#xfc;nsterer O, Wagner R, Aubert O
    year: "2025"
    abstract: |-
      OBJECTIVE: Large language models (LLMs) have advanced rapidly, but their utility in pediatric surgery remains uncertain. This study assessed the performance of three AI models-DeepSeek, Microsoft Copilot (GPT-4) and Google Bard-on the European Pediatric Surgery In-Training Examination (EPSITE).

      METHODS: We evaluated model performance using 294 EPSITE questions from 2021 to 2023. Data for Copilot and Bard were collected in early 2024, while DeepSeek was assessed in 2025. Responses were compared to those of pediatric surgical trainees. Statistical analyses determined performance differences.

      RESULTS: DeepSeek achieved the highest accuracy (85.0%), followed by Copilot (55.4%) and Bard (48.0%). Pediatric surgical trainees averaged 60.1%. Performance differences were statistically significant (p < 0.0001). DeepSeek significantly outperformed both human trainees and other models (p < 0.0001), while Bard was consistently outperformed by trainees across all training levels (p < 0.01). Sixth-year trainees performed better than Copilot (p < 0.05). Copilot and Bard failed to answer a small portion of questions (3.4% and 4.7%, respectively) due to ethical concerns or perceived lack of correct choices. The time gap between model assessments reflects the rapid evolution of LLMs, contributing to the superior performance of newer models like DeepSeek.

      CONCLUSION: LLMs show variable performance in pediatric surgery, with newer models like DeepSeek demonstrating marked improvement. These findings highlight the rapid progression of LLM capabilities and emphasize the need for ongoing evaluation before clinical integration, especially in high-stakes decision-making contexts.
    mergedFrom:
      - source: pubmed
        pmid: "40781215"
        doi: 10.1007/s00383-025-06104-9
    finalDecision: exclude
  - title: Accuracy of ChatGPT Responses to Examinations for Senior Medical Information Technicians in Japan.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT accuracy on examination questions for medical information technicians; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are senior medical information technicians, not medical/health-professional learners in scope"
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.3233/SHTI251215
    pmid: "40776232"
    authors: Ishida K, Seto R, Hanada E
    year: "2025"
    abstract: This study recruited senior medical information technicians in Japan to evaluate the performance of ChatGPT. The average correct response rates for multiple-choice and written questions were 82.1% and 76.3%, respectively. Although ChatGPT-generated plausible sentences, it failed to describe the appropriate role of medical information technicians in short essay exams.
    mergedFrom:
      - source: pubmed
        pmid: "40776232"
        doi: 10.3233/SHTI251215
    finalDecision: exclude
  - title: ChatGPT-4's Level of Dermatological Knowledge Based on Board Examination Review Questions and Bloom's Taxonomy.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT-4 on dermatology board review questions; benchmarks AI on exam questions, not about assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Reports ChatGPT performance on board review questions rather than workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.2196/74085
    pmid: "40773772"
    authors: Tai H, Kovarik C
    year: "2025"
    abstract: Our study demonstrated the ability of ChatGPT-4 to answer 77.5% of all sampled text-based board review type questions correctly. Questions requiring the recall of factual information were answered correctly most often, with slight decreases in correctness as higher-order thinking requirements increased. Improvements to ChatGPT's visual diagnostics capabilities will be required before it can be used reliably for clinical decision-making and visual diagnostics.
    mergedFrom:
      - source: pubmed
        pmid: "40773772"
        doi: 10.2196/74085
    finalDecision: exclude
  - title: Artificial intelligence assisted automated short answer question scoring tool shows high correlation with human examiner markings.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in workplace-based or clinical education setting; GPT-4 used to score SAQs in a Systematic Pharmacology course (classroom-based assessment)"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on classroom short-answer exam scoring, not WBA assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1186/s12909-025-07718-2
    pmid: "40764994"
    authors: Seneviratne H, Manathunga S
    year: "2025"
    abstract: |-
      BACKGROUND: Optimizing the skill of answering Short answer questions (SAQ) in medical undergraduates with personalized feedback is challenging. With the increasing number of students and staff shortages this task is becoming practically difficult. Hence, we aimed to develop automated SAQ scoring tool (ASST) using artificial intelligence (AI) to evaluate written answers with feedback.

      METHODS: This study investigated the use of large language models (LLMs) for automated SAQ scoring, specifically following rubrics. Rubric is a set of guidelines or criteria used to evaluate or grade assignments, which is provided by instructors. We focused on short answer questions from the Systematic Pharmacology course, along with model responses and rubrics shared with the LLM. The LLM analyzed student answers by extracting key parts, scoring them based on the rubric criteria, and providing feedback. The evaluation process relied on GPT-4, where the final score was determined by averaging results across five sampled runs. To validate the method, human examiners also graded the same answers, and the marks were compared to measure correlation.

      RESULTS: Across 30 student answers, AES scores showed high correlation with independent human examiner markings (correlation coefficients of 0.93 and 0.96). An intra-class correlation coefficient of 0.94 indicate excellent inter-rater reliability between the LLM and human examiners.

      CONCLUSION: AI assisted automated SAQ scoring tool markings highly correlate with the human examiner markings which shows promise for transparent, flexible automated grading with granular feedback. This approach enabling criteria-based assessment could have valuable applications in education, reducing grading burdens on instructors while providing students with more granular, actionable feedback on their written work.
    mergedFrom:
      - source: pubmed
        pmid: "40764994"
        doi: 10.1186/s12909-025-07718-2
    finalDecision: exclude
  - title: "Utility of Generative Artificial Intelligence for Japanese Medical Interview Training: Randomized Crossover Pilot Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments; participants interacted with an AI-simulated patient or a standardized patient, not in a real clinical workplace."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Training and assessment are based on simulated patient stations rather than workplace-based clinical assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.2196/77332
    pmid: "40749190"
    authors: Hirosawa T, Yokose M, Sakamoto T, Harada Y, Tokumasu K, Mizuta K, Shimizu T
    year: "2025"
    abstract: |-
      BACKGROUND: The medical interview remains a cornerstone of clinical training. There is growing interest in applying generative artificial intelligence (AI) in medical education, including medical interview training. However, its utility in culturally and linguistically specific contexts, including Japanese, remains underexplored. This study investigated the utility of generative AI for Japanese medical interview training.

      OBJECTIVE: This pilot study aimed to evaluate the utility of generative AI as a tool for medical interview training by comparing its performance with that of traditional face-to-face training methods using a simulated patient.

      METHODS: We conducted a randomized crossover pilot study involving 20 postgraduate year 1-2 physicians from a university hospital. Participants were randomly allocated into 2 groups. Group A began with an AI-based station on a case involving abdominal pain, followed by a traditional station with a standardized patient presenting chest pain. Group B followed the reverse order, starting with the traditional station for abdominal pain and subsequently within the AI-based station for the chest pain scenario. In the AI-based stations, participants interacted with a GPT-configured platform that simulated patient behaviors. GPTs are customizable versions of ChatGPT adapted for specific purposes. The traditional stations involved face-to-face interviews with a simulated patient. Both groups used identical, standardized case scenarios to ensure uniformity. Two independent evaluators, blinded to the study conditions, assessed participants' performances using 6 defined metrics: patient care and communication, history taking, physical examination, accuracy and clarity of transcription, clinical reasoning, and patient management. A 6-point Likert scale was used for scoring. The discrepancy between the evaluators was resolved through discussion. To ensure cultural and linguistic authenticity, all interviews and evaluations were conducted in Japanese.

      RESULTS: AI-based stations scored lower across most categories, particularly in patient care and communication, than traditional stations (4.48 vs 4.95; P=.009). However, AI-based stations demonstrated comparable performance in clinical reasoning, with a nonsignificant difference (4.43 vs 4.85; P=.10).

      CONCLUSIONS: The comparable performance of generative AI in clinical reasoning highlights its potential as a complementary tool in medical interview training. One of its main advantages lies in enabling self-learning, allowing trainees to independently practice interviews without the need for simulated patients. Nonetheless, the lower scores in patient care and communication underline the importance of maintaining traditional methods that capture the nuances of human interaction. These findings support the adoption of hybrid training models that combine generative AI with conventional approaches to enhance the overall effectiveness of medical interview training in Japan.
    mergedFrom:
      - source: pubmed
        pmid: "40749190"
        doi: 10.2196/77332
    finalDecision: exclude
  - title: '"Digital Clinicians" Performing Obesity Medication Self-Injection Education: Feasibility Randomized Controlled Trial.'
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment or feedback of learners; about patient education for self-injection"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are not medical/health-professional learners; title indicates patient education context."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    doi: 10.2196/63503
    pmid: "40737494"
    authors: Coleman S, Lynch C, Worlikar H, Kelly E, Loveys K, Simpkin A, Walsh J, Broadbent E, Finucane F, O' Keeffe D
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) chatbots have shown competency in a range of areas, including clinical note taking, diagnosis, research, and emotional support. An obesity epidemic, alongside a growth in novel injectable pharmacological solutions, has put a strain on limited resources.

      OBJECTIVE: This study aimed to investigate the use of a chatbot integrated with a digital avatar to create a "digital clinician." This was used to provide mandatory patient education for those beginning semaglutide once-weekly self-administered injections for the treatment of overweight and obesity at a national center.

      METHODS: A "digital clinician" with facial and vocal recognition technology was generated with a bespoke 10- to 15-minute clinician-validated tutorial. A feasibility randomized controlled noninferiority trial compared knowledge test scores, self-efficacy, consultation satisfaction, and trust levels between those using the AI-powered clinician avatar onsite and those receiving conventional semaglutide education from nursing staff. Attitudes were recorded immediately after the intervention and again at 2 weeks after the education session.

      RESULTS: A total of 43 participants were recruited, 27 to the intervention group and 16 to the control group. Patients in the "digital clinician" group were significantly more knowledgeable postconsultation (median 10, IQR 10-11 vs median 8, IQR 7-9.3; P<.001). Patients in the control group were more satisfied with their consultation (median 7, IQR 6-7 vs median 7, IQR 7-7; P<.001) and had more trust in their education provider (median 7, IQR 4.8-7 vs median 7, IQR 7-7; P<.001). There was no significant difference in reported levels of self-efficacy (P=.57). 81% (22/27) participants in the intervention group said they would use the resource in their own time.

      CONCLUSIONS: Bespoke AI chatbots integrated with digital avatars to create a "digital clinician" may perform health care education in a clinical environment. They can ensure higher levels of knowledge transfer yet are not as trusted as their human counterparts. "Digital clinicians" may have the potential to aid the redistribution of resources, alleviating pressure on bariatric services and health care systems, the extent to which remains to be determined in future studies.
    mergedFrom:
      - source: pubmed
        pmid: "40737494"
        doi: 10.2196/63503
      - source: scopus
        doi: 10.2196/63503
        scopusId: SCOPUS_ID:105015199475
    finalDecision: exclude
  - title: A Randomized Controlled Trial of a Deep Language Learning Model-Based Simulation Tool for Undergraduate Medical Students in Surgery.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: Simulation tool for medical students, assessment conducted in simulated environment rather than workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is based on a simulation tool"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
    doi: 10.1016/j.jsurg.2025.103629
    pmid: "40729832"
    authors: McCarrick C, McEntee P, Boland P, Donnelly S, O'Meara Y, Heneghan H, Cahill R
    year: "2025"
    abstract: |-
      INTRODUCTION: Effective communication is a critical skill for surgeons that commences often with history-taking. While simulation-based training is utilized to enhance these skills, recent advancements in artificial intelligence (AI), especially deep language learning models (DLM), offer new opportunities. This study evaluates the integration of DLM as a simulated patient (SP) into surgical history-taking training for senior medical students during clinical rotations.

      METHODS: A randomized controlled trial was conducted with surgery module students. Participants were divided into control and intervention groups, the former receiving standard experiential learning and the latter adding 3 structured sessions with DLM (ChatGPT, Open AI) as SP (with interaction texts submitted for tutor evaluation). All students underwent Objective Structured Clinical Examination (OSCE) of history-taking with a human SP and blinded assessor blinded by group for baseline competency ascertainment and again after either intervention or a similar time of standard learning. Intervention group students were anonymously surveyed to assess communication confidence and perspectives on DLM as SP.

      RESULTS: After initial pilot trialing, ninety students participated formally with 45 assigned to each arm via randomized cluster sampling. DLM-content was uniformly appropriate. Baseline scores were similar but significantly increased in the intervention group alone (p < 0.001, 0.37v0.19 Cohen D education effect size). 62% of students completed the survey, a majority (57%) articulating increased confidence, rich detail in DLM histories (72%) and would use again (95%).

      CONCLUSIONS: DLM effectively enhanced surgical history-taking skills. These findings indicate AI can serve as a valuable tool for student development alongside clinical learning.
    mergedFrom:
      - source: pubmed
        pmid: "40729832"
        doi: 10.1016/j.jsurg.2025.103629
    finalDecision: exclude
  - title: Can AI Outperform Human Aspirants? Evaluating 3 ChatGPT Models on the Spanish FIR and BIR Specialized Health Examinations.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on standardized health examinations, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates LLM performance on pharmacy/biology licensing exam MCQs"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam-question benchmarking study (FIR/BIR) without workplace-based assessment or analysis of clinical observation documentation."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.1093/jalm/jfaf098
    pmid: "40726369"
    authors: Verd&#xfa; G, Rayo A, Fabregat-Bolufer A
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) models are increasingly used in academic and clinical settings that require information synthesis and decision-making. This study explores the performance, accuracy, and reproducibility of 3 OpenAI models-GPT-4o Mini, GPT-4o, and GPT-o1-when applied to the 2023 Spanish FIR (Pharmaceutical Internal Resident) and BIR (Biologist Internal Resident) exams. By assessing their capabilities on these highly specialized tests, we aim to evaluate their potential as reliable tools for academic preparation and clinical support.

      METHODS: Each model was prompted with 200 questions from the 2023 FIR and BIR exams, respectively. The analysis evaluated overall accuracy, official exam scoring, and predicted ranking. Subanalyses focused on multimodal image-based questions and clinical cases. Reproducibility was assessed by retesting all questions from both exams using the Cohen Kappa and McNemar tests.

      RESULTS: After the first attempt, GPT-o1 achieved the highest accuracy (92% on FIR, 97.0% on BIR), securing top positions in both exams. GPT-4o performed exceptionally (87% on FIR, 97.5% on BIR), surpassing all human candidates on BIR and ranking third on FIR. GPT-4o Mini, while strong (80.5% on FIR, 93.0% on BIR), struggled with complex or image-reliant questions. The reproducibility analysis showed GPT-o1's tendency to correct previous mistakes on retesting, while GPT-4o and GPT-4o Mini more consistently repeated initial answers.

      CONCLUSIONS: These models, particularly GPT-o1, outperformed human examinees, supporting AI integration in exam preparation and clinical training. However, limitations persist in multimodal understanding and specialized subdomains. Human oversight remains essential to ensure reliability in laboratory and clinical practice.
    mergedFrom:
      - source: pubmed
        pmid: "40726369"
        doi: 10.1093/jalm/jfaf098
    finalDecision: exclude
  - title: "Evaluating AI-generated examination papers in periodontology: a comparative study with human-designed counterparts."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Classroom examination paper generation, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Evaluates AI-generated standardized examination papers for undergraduate dental education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Conducted in undergraduate classroom testing, not a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.1186/s12909-025-07706-6
    pmid: "40702472"
    authors: Ma X, Pan W, Yu X
    year: "2025"
    abstract: |-
      OBJECTIVE: This study systematically evaluates the performance of artificial intelligence (AI)-generated examinations in periodontology education, comparing their quality, student outcomes, and practical applications with those of human-designed examinations.

      METHODS: A randomized controlled trial was conducted with 126 undergraduate dental students, who were divided into AI (n = 63) and human (n = 63) test groups. The AI-generated examination was developed using GPT-4, while the human examination was derived from the 2024 institutional final exam. Both assessments covered identical content from Periodontology (5th Edition) and included 90 multiple-choice questions (MCQs) across five formats: A1: Single-sentence best choice; A2: Case summary best choice; A3: Case group best choice; A4: Case chain best choice; X: Multiple correct options. Psychometric properties (reliability, validity, difficulty, discrimination) and student feedback were analyzed using split-half reliability, content coverage analysis, factor analysis, and 5-point Likert scales.

      RESULTS: The AI examination demonstrated superior content coverage (81.3% vs. 72.4%) and significantly higher total scores (79.34 ± 6.93 vs. 73.17 ± 9.57, p = 0.027). However, it showed significantly lower discrimination indices overall (0.35 vs. 0.49, p = 0.004). Both examinations exhibited adequate split-half reliability (AI = 0.81, human = 0.84) and comparable difficulty distributions (AI: easy 40.0%, moderate 46.7%, difficult 13.3%; human: easy 30.0%, moderate 50.0%, difficult 20.0%; p = 0.274). Student feedback revealed significantly lower ratings for the AI test in terms of perceived difficulty appropriateness (3.53 ± 1.03 vs. 4.19 ± 0.76, p < 0.001), knowledge coverage (3.67 ± 0.89 vs. 4.19 ± 0.72, p < 0.001), and learning inspiration (3.79 ± 0.90 vs. 4.25 ± 0.67, p = 0.001).

      CONCLUSION: While AI-generated examinations improve content breadth and efficiency, their limited clinical contextualization and discrimination constrain their use in high-stakes applications. A hybrid "AI-human collaborative generation" framework, integrating medical knowledge graphs for contextual optimization, is proposed to balance automation with assessment precision. This study provides empirical evidence for the role of AI in enhancing dental education assessment systems.
    mergedFrom:
      - source: pubmed
        pmid: "40702472"
        doi: 10.1186/s12909-025-07706-6
    finalDecision: exclude
  - title: Evaluating the Use of ChatGPT 3.5 and Bard as Self-Assessment Tools for Short Answer Questions in Undergraduate Ophthalmology.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates AI-generated answers to university examination questions for undergraduate self-assessment"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Self-assessment SAQ study in undergraduate coursework, not workplace-based assessment or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.7759/cureus.86288
    pmid: "40688974"
    authors: Khake A, Gokhale S, Dindore P, Khake S, Desai M
    year: "2025"
    abstract: |-
      OBJECTIVE: This study aimed to evaluate the efficacy of ChatGPT 3.5 and Google Bard as tools for self-assessment of short answer questions (SAQs) in ophthalmology for undergraduate medical students.

      METHODOLOGY: A total of 261 SAQs were randomly selected from previous university examination papers and publicly available ophthalmology question banks. The questions were classified according to the competency-based medical education (CBME) curriculum of the National Medical Commission (NMC) of India into three categories: short note task-oriented questions (SNTO, n = 169), short note reasoning questions (SNRQ, n = 15), and applied aspect SAQs (SN Applied, n = 77). Image-based questions were excluded. Three ophthalmologists collaboratively developed model answers for each question. The same questions were then submitted to ChatGPT 3.5 and Google Bard. The AI-generated responses were independently evaluated by three ophthalmologists using a 3-point scale based on correct diagnosis, accuracy of content, and relevance. The scores were compiled, and the data were analyzed to compare the overall and category-wise performance of the two AI tools.

      RESULTS: Out of a total possible score of 783 (261 questions × 3 points), ChatGPT 3.5 scored 696 (88.8%), while Bard scored 685 (87.5%). Although the overall performance difference was insignificant, ChatGPT 3.5 performed significantly better in the SNTO category. However, both AI tools produced poor-quality or inadequate answers for a subset of questions: 50 (19%) by ChatGPT 3.5 and 44 (16.8%) by Bard. Some responses lacked essential information, even for high-yield topics.

      CONCLUSION: ChatGPT 3.5 and Bard can generate accurate and relevant responses to ophthalmology SAQs in most cases. ChatGPT 3.5 demonstrated slightly better performance, particularly for task-oriented questions, suggesting it may be a more effective tool for undergraduate students' self-assessment. However, due to a notable error rate (~20%), AI-generated responses should not be used in isolation and must be cross-referenced with standard textbooks. These tools best suit rapid information retrieval during the early study phases.
    mergedFrom:
      - source: pubmed
        pmid: "40688974"
        doi: 10.7759/cureus.86288
    finalDecision: exclude
  - title: "Generative Artificial Intelligence (AI) in Medical Education: A Narrative Review of the Challenges and Possibilities for Future Professionalism."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Narrative review, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article, systematic review, meta-analysis, or preprint"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    doi: 10.7759/cureus.86316
    pmid: "40688936"
    authors: Komasawa N, Yokohira M
    year: "2025"
    abstract: The rapid emergence of generative artificial intelligence (AI) is reshaping the landscape of medical education and healthcare. Unlike traditional AI, which focuses on classification or prediction, generative AI can create novel content-such as clinical notes, patient education materials, and simulated interactions-based on large-scale data. This capacity offers significant opportunities for personalized learning, clinical efficiency, and patient engagement. However, the integration of generative AI also introduces complex challenges, including ethical ambiguity, misinformation, accountability, data privacy risks, and potential erosion of critical thinking skills. These risks are especially salient in educational settings, where future physicians are still developing their professional identities. In this narrative review, we examine the dual role of generative AI as both a transformative tool and a source of ethical and professional disruption. We analyze its benefits and challenges across educational and clinical domains and argue that the traditional model of medical professionalism must evolve in response. Drawing on international literature and diverse cultural contexts in medical education, we propose a redefined framework for AI-era professionalism-one that integrates technological fluency with enduring humanistic values such as empathy, integrity, and accountability. This review offers AI-integrated medical professionalism to prepare future physicians to use generative AI responsibly, ethically, and in service of patient-centered care.
    mergedFrom:
      - source: pubmed
        pmid: "40688936"
        doi: 10.7759/cureus.86316
    finalDecision: exclude
  - title: Quality of Human Expert vs Large Language Model-Generated Multiple-Choice Questions in the Field of Mechanical Ventilation.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates AI-generated MCQs for critical care education, not workplace-based assessment"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Study of AI-generated MCQ quality for education, not assessment/feedback tied to workplace clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.1016/j.chest.2025.07.005
    pmid: "40684906"
    authors: Safadi S, Amirahmadi R, Tlimat A, Rovinski R, Sun J, Lee B, Seam N, Critical Care Education Research Consortium
    year: "2025"
    abstract: |-
      BACKGROUND: Although mechanical ventilation (MV) is a critical competency in critical care training, standardized methods for assessing MV-related knowledge are lacking. Traditional multiple-choice question (MCQ) development is resource intensive, and prior studies have suggested that generative AI tools could streamline question creation. However, the quality of AI-generated MCQs remains unclear.

      RESEARCH QUESTION: Are MCQs generated by ChatGPT noninferior to human expert (HE)-created questions in terms of quality and relevance for MV education?

      STUDY DESIGN AND METHODS: Three key MV topics were selected: Equation of Motion and Ohm's Law, Tau and Auto-PEEP, and Oxygenation. Fifteen learning objectives were used to generate 15 AI-written MCQs via a standardized prompt with ChatGPT-o1 (preview model; made available September 12, 2024). A group of 31 faculty experts, all of whom regularly teach MV, evaluated both AI- and HE-generated MCQs. Each MCQ was assessed based on its alignment with learning objectives, accuracy of chosen answer, clarity of the question stem, plausibility of distractor options, and difficulty level. The faculty members were blinded to the provenance of the MCQ questions. The noninferiority margin was predefined as 15% of the total possible score (-3.45).

      RESULTS: AI-generated MCQs were statistically noninferior to the HE-written MCQs (95% upper CI, [-1.15, ∞]). In additions, respondents were unable to reliably differentiate AI-generated MCQs from HE-written MCQs (P = .32).

      INTERPRETATION: Our results suggest that AI-generated MCQs using ChatGPT-o1 are comparable in quality to those written by HEs. Given the time and resource-intensive nature of human MCQ development, AI-assisted question generation may serve as an efficient and scalable alternative for medical education assessment, even in highly specialized domains such as MV.
    mergedFrom:
      - source: pubmed
        pmid: "40684906"
        doi: 10.1016/j.chest.2025.07.005
    finalDecision: exclude
  - title: "The Evolving Role of Artificial Intelligence in Plastic Surgery Education: Insights From Program Directors and Residents."
    reviews:
      - decision: uncertain
        comment: AI in surgical education; may involve assessment or feedback
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 5: Cross-sectional survey of AI perceptions/policies in plastic surgery residency; no data on validity, reliability, acceptability, or educational impact of AI-based assessment itself."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Survey of AI perceptions/policies in training, not generative AI for assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.1016/j.jsurg.2025.103622
    pmid: "40682993"
    authors: Genovese A, Prabha S, Gomez-Cabello C, Haider S, Borna S, Trabilsy M, Forte A
    year: "2025"
    abstract: |-
      OBJECTIVE: To assess the current state of artificial intelligence (AI) policies, educational resources, and perceptions within U.S. plastic surgery residency programs from the perspectives of program directors (PDs) and residents.

      DESIGN: Cross-sectional study using 2 anonymized surveys to evaluate AI-related policies, current use, educational tools, perceived barriers, and attitudes toward AI use in surgical education and residency applications.

      SETTING: Plastic surgery residency programs across the United States PARTICIPANTS: Program directors (n = 77) were invited via email, with 24 (31%) responding. Residents (n = 89) were recruited via social media; 1 resident per program was randomly selected to ensure institutional diversity, with 23 (26%) completing the survey.

      RESULTS: Institutional adoption of AI was limited. Only 8% of PDs reported screening residency applications for AI-generated content, and 88% indicated their programs had no formal policies on AI use. AI-based educational tools were available in 13% of programs, 21% offered AI ethics training, and 8% reported using AI to assess surgical skill. Barriers included lack of expertise (65%), data privacy concerns (52%), cost (48%), and limited evidence of efficacy (48%). In contrast, residents reported substantial independent AI use (50%). Residents used platforms such as ChatGPT (50%), Google Gemini, Microsoft Copilot, and Claude (each 9%)-often to generate clinical explanations (43%), procedural guides (17%), and differential diagnoses (13%). One resident also reported undergoing AI-based surgical skill assessment. Despite this engagement, 74% stated their programs lacked AI-related educational resources. Residents expressed moderate trust in AI (mean 5.26/10), stating it "probably" or "definitely" has a place in their education (86%).

      CONCLUSIONS: A marked discrepancy exists between institutional policies and resident usage of AI in plastic surgery education. As residents adopt these tools independently, there is an urgent need for evidence-based guidelines, validated resources, and structured implementation to ensure safe, effective integration into surgical training.
    mergedFrom:
      - source: pubmed
        pmid: "40682993"
        doi: 10.1016/j.jsurg.2025.103622
    finalDecision: exclude
  - title: Comparing ChatGPT and medical student performance in a real image-based Radiology and Applied Physics in Medicine exam.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; compares ChatGPT with students on a radiology practical exam with radiological images."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam performance comparison study, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.1016/j.rxeng.2025.101638
    pmid: "40675728"
    authors: Salvador R, Vas D, Oleaga L, Matute-Gonz&#xe1;lez M, Castillo-Fortu&#xf1;o &, Setoain X, Nicolau C
    year: "2025"
    abstract: |-
      INTRODUCTION: Artificial intelligence models can provide textual answers to a wide range of questions, including medical questions. Recently, these models have incorporated the ability to interpret and answer image-based questions, and this includes radiological images. The main objective of this study is to analyse the performance of ChatGPT-4o compared to third-year medical students in a Radiology and Applied Physics in Medicine practical exam. We also intend to assess the capacity of ChatGPT to interpret medical images and answer related questions.

      MATERIALS AND METHODS: Thirty-three students set an exam of 10 questions on radiological and nuclear medicine images. Exactly the same exam in the same format was given to ChatGPT (version GPT-4) without prior training. The exam responses were evaluated by professors who were unaware of which exam corresponded to which respondent type. The Mann-Whitney U test was used to compare the results of the two groups.

      RESULTS: The students outperformed ChatGPT on eight questions. The students' average final score was 7.78, while ChatGPT's was 6.05, placing it in the 9th percentile of the students' grade distribution.

      DISCUSSION: ChatGPT demonstrates competent performance in several areas, but students achieve better grades, especially in the interpretation of images and contextualised clinical reasoning, where students' training and practical experience play an essential role. Improvements in AI models are still needed to achieve human-like capabilities in interpreting radiological images and integrating clinical information.
    mergedFrom:
      - source: pubmed
        pmid: "40675728"
        doi: 10.1016/j.rxeng.2025.101638
    finalDecision: exclude
  - title: Comparison of applicability, difficulty, and discrimination indices of multiple-choice questions on medical imaging generated by different AI-based chatbots.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Title indicates examination item analysis without workplace-based clinical setting"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study evaluates AI-generated MCQs for medical imaging education in classroom/exam settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on AI-generated MCQ creation and psychometrics, not assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1016/j.radi.2025.103087
    pmid: "40674889"
    authors: Karahan B, Emekli E
    year: "2025"
    abstract: |-
      INTRODUCTION: Creating high-quality multiple-choice questions (MCQs) is vital in health education, particularly in fields like medical imaging. AI-based chatbots have emerged as a tool to automate this process. This study evaluates the applicability, difficulty, and discrimination indices of MCQs generated by various AI chatbots for medical imaging education.

      METHODS: 80 MCQs were generated by seven AI-based chatbots (Claude 3, Claude 3.5, ChatGPT-3.5, ChatGPT-4.0, Copilot, Gemini, Turin Q, and Writesonic) using lecture materials. These questions were evaluated for relevance, accuracy, and originality by radiology faculty, and then administered to 56 students and 12 research assistants. The questions were analyzed using Miller's Pyramid to assess cognitive levels, with difficulty and discrimination indices calculated.

      DISCUSSION: AI-based chatbots generated MCQs suitable for medical imaging education, with 72.5 % of the questions deemed appropriate. Most questions assessed recall (79.31 %), suggesting that AI models excel at generating basic knowledge questions but struggle with higher cognitive skills. Differences in question quality were noted between chatbots, with Claude 3 being the most reliable. The difficulty index averaged 0.62, indicating a moderate level of difficulty, but some models produced easier questions.

      CONCLUSION: AI chatbots show promise for automating MCQ creation in health education, though most questions focus on recall. For AI to fully support health education, further development is needed to improve question quality, especially in higher cognitive domains.

      IMPLICATION FOR PRACTICE: AI-based chatbots can support educators in generating MCQs, especially for assessing basic knowledge in medical imaging. While useful for saving time, expert review remains essential to ensure question quality and to address higher-level cognitive skills. Integrating AI tools into assessment workflows may enhance efficiency, provided there is appropriate oversight.
    mergedFrom:
      - source: pubmed
        pmid: "40674889"
        doi: 10.1016/j.radi.2025.103087
    finalDecision: exclude
  - title: Ten tips to harnessing generative AI for high-quality MCQS in medical education assessment.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: 'Ten tips' format indicates a guidelines/tips article, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Title indicates a tips/article format rather than original research or systematic review"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    doi: 10.1080/10872981.2025.2532682
    pmid: "40674697"
    authors: Magzoub M, Zafar I, Munshi F, Shersad F
    year: "2025"
    abstract: Generating high quality MCQs is time consuming and expensive. Many strategies are applied to produce high quality items including sharing of item banks, training of item writers and automatic item generation (AIG). Generative AI, when used with precision, has proven to reduce significantly both cost and time without compromising quality. Medical educators encounter numerous obstacles when using AI to generate MCQs of good quality. We searched the fast and recent growing medical education literature for articles related to the use of AI in generating high quality MCQs. Additionally, the development of these tips was guided by our own institutional experience.  We created 10 tips for MCQ generation using AI to assist MCQ item writers in both undergraduate and graduate medical education.
    mergedFrom:
      - source: pubmed
        pmid: "40674697"
        doi: 10.1080/10872981.2025.2532682
    finalDecision: exclude
  - title: Evaluating the Agreement Between ChatGPT and the Clinical Competency Committee in Assigning ACGME Milestones for Family Medicine Residents.
    reviews:
      - decision: uncertain
        comment: ChatGPT for ACGME milestone assignment for residents; directly relevant to generative AI in workplace-based assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: include
        comment: "Generative AI (ChatGPT) used to assign ACGME milestone levels based on written faculty feedback for family medicine residents. Directly relevant: LLM applied to workplace-based assessment (milestone assignment from clinical feedback) with correlation/agreement data compared to CCC."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
      - decision: include
        comment: Uses ChatGPT to assign ACGME milestone levels based on written faculty feedback from clinical observations of family medicine residents. Workplace-based setting (residency clinical practice). Presents agreement data (Pearson correlation, concordance correlation coefficients) comparing ChatGPT with the Clinical Competency Committee. Directly applies generative AI to assessment of learners in a workplace-based context.
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:54.870000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:03.048000+00:00
    doi: 10.22454/FamMed.2025.363712
    pmid: "40663416"
    authors: Partin M, Dambro A, Newman R, Shang Y, Kong L, Clebak K
    year: "2025"
    abstract: |-
      BACKGROUND AND OBJECTIVES: Although artificial intelligence models have existed for decades, the demand for application of these tools within health care and especially medical education are exponentially expanding. Pressure is mounting to increase direct observation and faculty feedback for resident learners, which can create administrative burdens for a Clinical Competency Committee (CCC). This study aimed to assess the feasibility of utilizing a large language model (ChatGPT) in family medicine residency evaluation by comparing the agreement between ChatGPT and the CCC for the Accreditation Council for Graduate Medical Education (ACGME) family medicine milestone levels and examining potential biases in milestone assignment.

      METHODS: Written faculty feedback for 24 residents from July 2022 to December 2022 at our institution was collated and de-identified. Using standardized prompts for each query, we used ChatGPT to assign milestone levels based on faculty feedback for 11 ACGME subcompetencies. We analyzed these levels for correlation and agreement between actual levels assigned by the CCC.

      RESULTS: Using Pearson's correlation coefficient, we found an overall positive and strong correlation between ChatGPT and the CCC for competencies of patient care, medical knowledge, communication, and professionalism. We found no significant difference in correlation or mean difference in milestone level between male and female residents. No significant difference existed between residents with a high faculty feedback word count versus a low word count.

      CONCLUSIONS: This study demonstrates the feasibility for tools like ChatGPT to assist in the evaluation process of family medicine residents without apparent bias based on gender or word count.
    mergedFrom:
      - source: pubmed
        pmid: "40663416"
        doi: 10.22454/FamMed.2025.363712
      - source: scopus
        doi: 10.22454/FamMed.2025.363712
        scopusId: SCOPUS_ID:105008334993
    finalDecision: include
  - title: "Empowering standardized residency training in China through large language models: problem analysis and solutions."
    reviews:
      - decision: uncertain
        comment: LLMs for residency training; may involve assessment or feedback components in workplace-based setting
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not original research; described as a commentary paper exploring potential of LLMs without empirical data"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/discussion paper, not original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1080/07853890.2025.2516695
    pmid: "40662377"
    authors: Li Z, Fu Q, Zhao Y, Ren W
    year: "2025"
    abstract: |-
      BACKGROUND: China's standardized residency training (SRT) faces challenges such as uneven distribution of resources, inadequate clinical skill training, subjective assessment methods, and high burnout rates among physician. The aim of this commentary paper is to explore the underlying causes and impacts of these challenges, analyze the potential of large language models (LLMs) to address these issues, and discuss the ethical concerns they raise.

      DISCUSSION: Uneven economic development in China has led to disparities in SRT resources and faculty distribution, with residents facing deficiencies in clinical training, research, and communication skills. Heavy workloads and nontechnical tasks exacerbate resident burnout and stagnation, hindering growth and skill development. LLMs have the potential to transform traditional education and learning modes by optimizing teaching resources, delivering real-time medical knowledge, and simulating clinical scenarios, thus effectively bridging gaps between training bases. As virtual mentors, LLMs can provide real-time guidance and personalized feedback, enhancing individuals' clinical and research skills. LLMs also make assessments more objective and improve nontechnical clinical task efficiency, reducing burnout, and increasing job satisfaction. However, their integration creates ethical challenges around information accuracy, privacy protection, biases, and academic misconduct.

      CONCLUSION: LLMs offer innovative solutions to China's SRT challenges. However, ethical issues must be carefully addressed, viewing LLMs as a potent complement to traditional medical education, and maintain the predominant role of conventional educational methods.
    mergedFrom:
      - source: pubmed
        pmid: "40662377"
        doi: 10.1080/07853890.2025.2516695
    finalDecision: exclude
  - title: The application of problem-based learning (PBL) guided by ChatGPT in clinical education in the Department of Nephrology.
    reviews:
      - decision: uncertain
        comment: ChatGPT in clinical education; may involve assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on teaching approach (PBL), not assessment, feedback, or analysis of observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment/feedback or analysis of observation records; ChatGPT used as a PBL teaching tool in nephrology, not for assessment or feedback generation"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: PBL teaching intervention not focused on WBA assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1186/s12909-025-07427-w
    pmid: "40660193"
    authors: Tong X, Hu Y, Long Y, Zhang Q, Yang Y, Yuan J, Zha Y
    year: "2025"
    abstract: |-
      BACKGROUND: Nephrology, a complex and specialized medical field, has seen significant advancements, yet traditional teaching methods remain outdated and less effective. This study explores the integration of Problem-Based Learning (PBL) guided by ChatGPT in the Department of Nephrology at Guizhou Provincial People's Hospital to enhance medical education.

      OBJECTIVE: To assess the impact of ChatGPT-guided Problem-Based Learning (PBL) on medical students' education and satisfaction in the Nephrology Department.

      METHODS: Fifty-four clinical resident doctors were divided into an experimental group, using ChatGPT-guided PBL, and a control group, using traditional methods. Both groups were assessed through theoretical and clinical practice exams, teaching satisfaction surveys, and self-assessments of teaching effectiveness.

      RESULTS: Results indicated the experimental group achieved significantly higher scores in both theoretical and clinical assessments. Additionally, they reported higher satisfaction and effectiveness in learning. Despite spending more time in pre-class preparation, the overall learning time did not increase due to reduced post-class review time facilitated by ChatGPT.

      CONCLUSIONS: This study demonstrates the potential of ChatGPT-integrated PBL to enhance learning outcomes, satisfaction, and efficiency in medical education. Future research should include larger samples and longer follow-up to validate these findings.
    mergedFrom:
      - source: pubmed
        pmid: "40660193"
        doi: 10.1186/s12909-025-07427-w
    finalDecision: exclude
  - title: Assessment of ChatGPT-3.5 performance on the medical genetics specialist exam.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Tests ChatGPT-3.5 on Polish medical genetics specialist exam questions; benchmarks AI on exam questions, not about assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT on specialist exam questions and does not study workplace-based assessment/feedback or analysis of observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1093/labmed/lmaf038
    pmid: "40654165"
    authors: Paruzel K, Ordak M
    year: "2025"
    abstract: |-
      INTRODUCTION: Artificial intelligence is increasingly used in medical education and testing. ChatGPT, developed by OpenAI, has shown mixed results on various medical exams, but its performance in medical laboratory genetics remains unknown.

      METHODS: This study assessed ChatGPT-3.5 using 456 publicly available questions from the Polish national specialist exam in medical laboratory genetics. Questions were categorized by topic and complexity (simple vs complex) and submitted to ChatGPT 3 times. Accuracy and consistency were statistically evaluated.

      RESULTS: ChatGPT correctly answered 59% of the 456 exam questions, which was statistically significant (P < .001). Accuracy differed by category: 71% for calculation-based questions, approximately 60% for genetic methods and genetic alterations, and only 37% for clinical case-based questions. Question complexity also affected performance: Simple questions had 63% accuracy, while complex questions yielded 43% (P = .001). No statistically significant differences were found across 3 repeated sessions, with performance remaining stable over time (P = .43).

      DISCUSSION: ChatGPT-3.5 demonstrated moderate accuracy and stable performance on a specialist exam in medical genetics. Although it may support education in this field, the tool's limitations in complex, domain-specific reasoning suggest the need for further development before broader implementation.
    mergedFrom:
      - source: pubmed
        pmid: "40654165"
        doi: 10.1093/labmed/lmaf038
    finalDecision: exclude
  - title: Evaluating the Performance of Large Language Models on Multispecialty FRCS Section 1 Questions.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on standardized exam questions, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Tests four LLMs on FRCS Section 1 surgical exam questions; benchmarks AI on exam questions, not about assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM performance on FRCS exam questions, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1016/j.jss.2025.06.026
    pmid: "40652730"
    authors: Ahmed H
    year: "2025"
    abstract: |-
      INTRODUCTION: Large language models (LLMs) have increasingly demonstrated utility in medical education and professional examinations. However, their reliability, accuracy, and consistency in answering complex surgical questions remain unclear. This study aims to assess the accuracy, consistency, and intermodel reliability of four widely used LLMs, ChatGPT 4o, Google Gemini, Perplexity AI, and Microsoft Copilot, in answering Fellowship of the Royal Colleges of Surgeons Section 1 single best answer questions.

      METHODS: A total of 50 single best answer-type questions from the official Joint Committee on Intercollegiate Examinations sample set, covering ten surgical specialties, were presented to each LLM three times in independent sessions to prevent memory effects. Accuracy (correct versus incorrect responses), response consistency across repeated trials, and intermodel reliability were evaluated.

      RESULTS: ChatGPT had the highest accuracy (81.33%, 122/150, P < 0.0001), followed by Gemini (69.33%), Perplexity (64%), and Copilot (59.33%). ChatGPT achieved 100% accuracy in cardiothoracic Surgery and neurosurgery, whereas Gemini performed poorly in neurosurgery (40%) and urology (20%). Otolaryngology and plastic surgery had lower accuracy across all models. Gemini and Perplexity showed the highest consistency (90%). Intermodel reliability was low (Fleiss' Kappa = 0.127, P < 0.0001), with cardiothoracic surgery having the highest agreement (0.401) and oral and maxillofacial surgery the lowest (-0.0992).

      CONCLUSIONS: ChatGPT performed best overall, whereas other models showed variable accuracies and lower agreement. Although Gemini and Perplexity demonstrated high internal consistency, intermodel reliability was limited. The study findings suggest that, although promising, these tools should be used with care in Fellowship of the Royal Colleges of Surgeons surgical assessments.
    mergedFrom:
      - source: pubmed
        pmid: "40652730"
        doi: 10.1016/j.jss.2025.06.026
    finalDecision: exclude
  - title: "Large language models in medical education: a comparative cross-platform evaluation in answering histological questions."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Benchmarks LLM accuracy on histology MCQs, not about using AI for learner assessment or WBA feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Histology MCQ benchmarking study without workplace-based assessment, feedback generation, or clinical observation-record analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1080/10872981.2025.2534065
    pmid: "40651009"
    authors: Mavrych V, Yousef E, Yaqinuddin A, Bolgova O
    year: "2025"
    abstract: "Large language models (LLMs) have shown promising capabilities across medical disciplines, yet their performance in basic medical sciences remains incompletely characterized. Medical histology, requiring factual knowledge and interpretative skills, provides a unique domain for evaluating AI capabilities in medical education. To evaluate and compare the performance of five current LLMs: GPT-4.1, Claude 3.7 Sonnet, Gemini 2.0 Flash, Copilot, and DeepSeek R1 on correctly answering medical histology multiple choice questions (MCQs). This cross-sectional comparative study used 200 USMLE-style histology MCQs across 20 topics. Each LLM completed all the questions in three separate attempts. Performance metrics included accuracy rates, test-retest reliability (ICC), and topic-specific analysis. Statistical analysis employed ANOVA with post-hoc Tukey's tests and two-way mixed ANOVA for system-topic interactions. All LLMs achieved exceptionally high accuracy (Mean 91.1%, SD 7.2). Gemini performed best (92.0%), followed by Claude (91.5%), Copilot (91.0%), GPT-4 (90.8%), and DeepSeek (90.3%), with no significant differences between systems (p > 0.05). Claude showed the highest reliability (ICC = 0.931), followed by GPT-4 (ICC = 0.882). Complete accuracy and reproducibility (100%) were detected in Histological Methods, Blood and Hemopoiesis, and Circulatory System, while Muscle tissue (76.0%) and Lymphoid System (84.7%) presented the greatest challenges. LLMs demonstrate exceptional accuracy and reliability in answering histological MCQs, significantly outperforming other medical disciplines. Minimal inter-system variability suggests technological maturity, though topic-specific challenges and reliability concerns indicate the continued need for human expertise. These findings reflect rapid AI advancement and identify histology as particularly suitable for AI-assisted medical education.Clinical trial number: The clinical trial number is not pertinent to this study as it does not involve medicinal products or therapeutic interventions."
    mergedFrom:
      - source: pubmed
        pmid: "40651009"
        doi: 10.1080/10872981.2025.2534065
    finalDecision: exclude
  - title: "Feasibility study of using GPT for history-taking training in medical education: a randomized clinical trial."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environment; GPT-simulated patient encounters for history-taking training with structured clinical examination as outcome measure."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is based on GPT-simulated patient interactions and structured examination, i.e., simulated environment only."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.1186/s12909-025-07614-9
    pmid: "40640776"
    authors: Wang Z, Fan T, Li M, Zhu N, Wang X
    year: "2025"
    abstract: |-
      BACKGROUNDS: Traditional methods of teaching history-taking in medical education are limited by scalability and resource intensity. This study aims to assess the effectiveness of simulated patient interactions based on a custom-designed Generative Pre-trained Transformer (GPT) model, developed using OpenAI's ChatGPT GPTs platform, in enhancing medical students' history-taking skills compared to traditional role-playing methods.

      METHODS: A total of 56 medical students were randomly assigned into two groups: an GPT group using GPT-simulated patients and a control group using traditional role-playing. Pre- and post-training assessments were conducted using a structured clinical examination to measure students' abilities in history collection, clinical reasoning, communication skills, and professional behavior. Additionally, students' evaluations of the educational effectiveness, satisfaction, and recommendation likelihood were assessed.

      RESULTS: The GPT-simulation group showed significantly higher post-training scores in the structured clinical examination compared to the control group (86.79 ± 5.46,73.64 ± 4.76, respectively, P < 0.001). Students in the GPT group exhibited higher enthusiasm for learning, greater self-directed learning motivation, and better communication feedback abilities compared to the control group (P < 0.05). Additionally, the student satisfaction survey revealed that the GPT group rated higher on the diversity of diseases encountered, ease of use, and likelihood of recommending the training compared to the control group (P < 0.05).

      CONCLUSIONS: GPT-based history-taking training effectively enhances medical students' history-taking skills, providing a solid foundation for the application of artificial intelligence (AI) in medical education.

      CLINICAL TRIAL NUMBER: NCT06766383.
    mergedFrom:
      - source: pubmed
        pmid: "40640776"
        doi: 10.1186/s12909-025-07614-9
    finalDecision: exclude
  - title: Leveraging ChatGPT for Enhancing Learning in Radiology Resident Education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: ChatGPT generates board-style practice MCQs from lecture transcripts for resident education; not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Study evaluates AI-generated practice questions, not assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1016/j.acra.2025.06.019
    pmid: "40628645"
    authors: Zheng A, Barker C, Ferrante S, Squires J, Branstetter Iv B, Hughes M
    year: "2025"
    abstract: |-
      RATIONALE AND OBJECTIVES: Chat generative pre-trained transformer (ChatGPT) is a generative artificial intelligence chatbot based on a LLM at the forefront of technological development with promising applications in medical education. This study aims to evaluate the use of ChatGPT in generating board-style practice questions for radiology resident education.

      MATERIALS AND METHODS: Multiple-choice questions (MCQs) were generated by ChatGPT from resident lecture transcripts using a custom prompt. 17 of the ChatGPT-generated MCQs were selected for inclusion in the study and randomly combined with 11 attending radiologist-written MCQs. For each MCQ, the 21 participating radiology residents answered the MCQ, rated the MCQ from 1-10 on effectiveness in reinforcing lecture material, and responded whether they thought an attending radiologist at their institution wrote the MCQ versus an alternative source.

      RESULTS: Perceived MCQ quality was not significantly different between ChatGPT-generated (M=6.93, SD=0.29) and attending radiologist-written MCQs (M=7.08, SD=0.51) (p=0.15). MCQ correct answer percentages did not significantly differ between ChatGPT-generated (M=57%, SD=20%) and attending radiologist-written MCQs (M=59%, SD=25%) (p=0.78). The percentage of MCQs thought to be written by an attending radiologist was significantly different between ChatGPT-generated (M=57%, SD=13%) and attending radiologist-written MCQs (M=71%, SD=20%) (p=0.04).

      CONCLUSION: LLMs such as ChatGPT demonstrate potential in generating and presenting educational material for radiology education, and their use should be explored further on a larger scale.
    mergedFrom:
      - source: pubmed
        pmid: "40628645"
        doi: 10.1016/j.acra.2025.06.019
    finalDecision: exclude
  - title: "ChatGPT performance in answering medical residency questions in nephrology: a pilot study in Brazil."
    reviews:
      - decision: uncertain
        comment: ChatGPT on residency questions; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT performance on nephrology residency exam multiple-choice questions; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1590/2175-8239-JBN-2024-0254en
    pmid: "40623208"
    authors: Feitosa Filho H, Furtado J, Eul&#xe1;lio E, Ribeiro P, Paiva L, Correia M, Silva J&#xfa;nior G
    year: "2025"
    abstract: |-
      OBJECTIVE: This study evaluated the performance of ChatGPT 4 and 3.5 versions in answering nephrology questions from medical residency exams in Brazil.

      METHODS: A total of 411 multiple-choice questions, with and without images, were analyzed, organized into four main themes: chronic kidney disease (CKD), hydroelectrolytic and acid-base disorders (HABD), tubulointerstitial diseases (TID), and glomerular diseases (GD). Questions with images were answered only by ChatGPT-4. Statistical analysis was performed using the chi-square test.

      RESULTS: ChatGPT-4 achieved an overall accuracy of 79.80%, while ChatGPT-3.5 achieved 56.29%, with a statistically significant difference (p < 0.001). In the main themes, ChatGPT-4 performed better in HABD (79.11% vs. 55.17%), TID (88.23% vs. 52.23%), CKD (75.51% vs. 61.95%), and DG (79.31% vs. 55.29%), all with p < 0.001. ChatGPT-4 presented an accuracy of 81.49% in questions without images and 54.54% in questions with images, with an accuracy of 60% for electrocardiogram analysis. This study is limited by the small number of image-based questions and the use of outdated examination items, reducing its ability to assess visual diagnostic skills and current clinical relevance. Furthermore, addressing only 4 areas of Nephrology may not fully represent the breadth of nephrology practice.

      CONCLUSION: ChatGPT-3.5 was found to have limitations in nephrology reasoning compared to ChatGPT-4, evidencing gaps in knowledge. The study suggests that further exploration is needed in other nephrology themes to improve the use of these AI tools.
    mergedFrom:
      - source: pubmed
        pmid: "40623208"
        doi: 10.1590/2175-8239-JBN-2024-0254en
    finalDecision: exclude
  - title: How well does GPT-4 perform on an emergency medicine board exam? A comparative assessment.
    reviews:
      - decision: uncertain
        comment: GPT-4 on emergency medicine board exam; abstract needed to determine WBA relevance
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Study benchmarks GPT-4 on a written licensing exam rather than using AI to assess or provide feedback to learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on LLM performance on written board examination questions, not assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1007/s43678-025-00951-0
    pmid: "40610779"
    authors: Almehairi N, Clark G, Davis S
    year: "2025"
    abstract: |-
      BACKGROUND: Recent advancements in artificial intelligence have shown promise in enhancing diagnostic precision within healthcare sectors. In emergency departments, artificial intelligence has demonstrated potential for improving triage, guiding the choice of radiologic imaging and crafting individualized medical notes and discharge summaries, including tailored care plans. Advances in generative artificial intelligence have led to the development of sophisticated models such as OpenAI's GPT-4. This study assessed the ability of generative artificial intelligence in diagnosis and management in emergency medicine. Specifically, we compared GPT-4 with the performance of emergency medicine trainees in Canada, as gauged by the Canadian In-Training Examination.

      METHODS: We compared the performance of emergency medicine residents to GPT-4 on the Canadian in-training exams for the years 2021 and 2022. Each question was entered into a fresh GPT-4 chat and the first response was recorded without any prompting. GPT-4's responses were then assessed using the same marking grid that is employed for evaluating medical trainees. We then compared GPT-4'sscores to the average scores of each post-graduate year (PGY) level of residents across all FRCPC training programs. Ethical approval was obtained, then Canadian In-Training Examination committee provided exam questions and anonymized national results.

      RESULTS: The participants in this study included 389 residents in 2021 and 333 residents in the 2022 exams. In 2021, mean trainee scores increased progressively across the levels, with PGY1 trainees scoring 48.0% (SD 15.6), PGY2 at 56.2% (SD 14.7), PGY3 at 59.8% (SD 16.7), PGY4 at 67.2% (12.3), and PGY5 at 70.1% (SD 12.5), whereas GPT-4 scored 88.7%. In 2022, a similar pattern, with PGY1 scoring 46.3% (SD 14.7), PGY2 at 51.8% (SD 14.7), PGY3 at 58.2% (SD 14.3), PGY4 at 66.2% (SD 15.3), and PGY5 at 64.3% (SD 8.5), while GPT-4 scored 82.0%.

      CONCLUSIONS: GPT-4 has shown impressive capabilities, surpassing the performance of medical trainees at different postgraduate levels in the clinical written exam. These findings highlight the potential of artificial intelligence to serve as a valuable support tool in medical practice. However, it should be used with caution and must not substitute for established, evidence-based medical resources.
    mergedFrom:
      - source: pubmed
        pmid: "40610779"
        doi: 10.1007/s43678-025-00951-0
    finalDecision: exclude
  - title: "Comparative Performance of Medical Students, ChatGPT-3.5 and ChatGPT-4.0 in Answering Questions From a Brazilian National Medical Exam: Cross-Sectional Questionnaire Study."
    reviews:
      - decision: uncertain
        comment: ChatGPT vs medical students on national exam; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Compares ChatGPT with medical students on a national medical exam (Progress Test), not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: National medical exam question performance is not workplace-based or clinical education assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.2196/66552
    pmid: "40607498"
    authors: Rodrigues Alessi M, Gomes H, Oliveira G, Lopes de Castro M, Grenteski F, Miyashiro L, do Valle C, Tozzini Tavares da Silva L, Okamoto C
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence has advanced significantly in various fields, including medicine, where tools like ChatGPT (GPT) have demonstrated remarkable capabilities in interpreting and synthesizing complex medical data. Since its launch in 2019, GPT has evolved, with version 4.0 offering enhanced processing power, image interpretation, and more accurate responses. In medicine, GPT has been used for diagnosis, research, and education, achieving significant milestones like passing the United States Medical Licensing Examination. Recent studies show that GPT 4.0 outperforms earlier versions and even medical students on medical exams.

      OBJECTIVE: This study aimed to evaluate and compare the performance of GPT versions 3.5 and 4.0 on Brazilian Progress Tests (PT) from 2021 to 2023, analyzing their accuracy compared to medical students.

      METHODS: A cross-sectional observational study was conducted using 333 multiple-choice questions from the PT, excluding questions with images and those nullified or repeated. All questions were presented sequentially without modification to their structure. The performance of GPT versions was compared using statistical methods and medical students' scores were included for context.

      RESULTS: There was a statistically significant difference in total performance scores across the 2021, 2022, and 2023 exams between GPT-3.5 and GPT-4.0 (P=.03). However, this significance did not remain after Bonferroni correction. On average, GPT v3.5 scored 68.4%, whereas v4.0 achieved 87.2%, reflecting an absolute improvement of 18.8% and a relative increase of 27.4% in accuracy. When broken down by subject, the average scores for GPT-3.5 and GPT-4.0, respectively, were as follows: surgery (73.5% vs 88.0%, P=.03), basic sciences (77.5% vs 96.2%, P=.004), internal medicine (61.5% vs 75.1%, P=.14), gynecology and obstetrics (64.5% vs 94.8%, P=.002), pediatrics (58.5% vs 80.0%, P=.02), and public health (77.8% vs 89.6%, P=.02). After Bonferroni correction, only basic sciences and gynecology and obstetrics retained statistically significant differences.

      CONCLUSIONS: GPT-4.0 demonstrates superior accuracy compared to its predecessor in answering medical questions on the PT. These results are similar to other studies, indicating that we are approaching a new revolution in medicine.
    mergedFrom:
      - source: pubmed
        pmid: "40607498"
        doi: 10.2196/66552
      - source: scopus
        doi: 10.2196/66552
        scopusId: SCOPUS_ID:105006937733
    finalDecision: exclude
  - title: "Comparison of AI-generated and clinician-designed multiple-choice questions in emergency medicine exam: a psychometric analysis."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Psychometric comparison of AI- vs clinician-generated MCQs administered to EM residents; focuses on exam item generation quality, not workplace-based assessment or feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Psychometric comparison of MCQs, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1186/s12909-025-07528-6
    pmid: "40597998"
    authors: Kaya M, Sonmez E, Halici A, Yildirim H, Coskun A
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) has shown promise in generating multiple-choice questions (MCQs) for medical education, yet the psychometric quality of such items remains underexplored. This study aimed to compare the psychometric properties of MCQs created by ChatGPT-4o and those written by emergency medicine clinicians.

      METHODS: Eighteen emergency medicine residents completed a 100-item examination comprising 50 AI-generated and 50 clinician-authored questions across core emergency medicine topics. Each item was analyzed for difficulty (P_index), discrimination (D_index), and point-biserial correlation (PBCC). Items were also categorized based on standardized index classifications.

      RESULTS: ChatGPT-4o-generated questions exhibited a higher mean difficulty index (P_index: 0.76 ± 0.23) compared to those created by clinicians (0.65 ± 0.24; p = 0.02), indicating that the AI-generated items were generally easier. Participants achieved significantly higher scores on AI-generated items (76.8 ± 8.18) than on clinician-authored questions (67.3 ± 9.65; p = 0.003). The mean discrimination index did not differ significantly between AI-generated (0.172 ± 0.23) and clinician-generated items (0.196 ± 0.26; p = 0.634). Likewise, the mean point-biserial correlation coefficient (PBCC) was nearly identical between the two groups (AI: 0.23 ± 0.28; clinicians: 0.23 ± 0.25; p = 0.99), suggesting similar internal consistency. Categorical analysis revealed that 56% of AI-generated items were classified as "easy," compared to 36% of clinician-designed items. Furthermore, based on PBCC values, 36% of AI-generated items and 24% of clinician items were identified as "problematic" (p = 0.015), indicating a higher rate of psychometric concerns among AI-generated questions.

      CONCLUSION: The findings suggest that AI-generated questions, while generally easier and associated with higher participant scores, may pose psychometric limitations, as evidenced by a greater proportion of items classified as problematic. Although the overall internal consistency and discrimination indices were comparable to clinician-authored items, careful quality control and validation are essential when integrating AI-generated content into assessment frameworks.
    mergedFrom:
      - source: pubmed
        pmid: "40597998"
        doi: 10.1186/s12909-025-07528-6
    finalDecision: exclude
  - title: Student perceptions of GenAI as a virtual tutor to support collaborative research training for health professionals.
    reviews:
      - decision: uncertain
        comment: GenAI as virtual tutor for health professionals; abstract may reveal assessment elements
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: GenAI providing feedback on research ideas in classroom-based small-group activity, not in workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: GenAI is used for research-training tutoring, not assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1186/s12909-025-07390-6
    pmid: "40597036"
    authors: McLaughlin J, Ponte C, Lyons K
    year: "2025"
    abstract: |-
      BACKGROUND: Research and evaluation skills are essential in healthcare education. Instructors frequently employ collaborative learning models to teach these competencies; however, delivering timely and personalized feedback to multiple groups can be a significant challenge. This study aimed to investigate the potential of generative artificial intelligence (GenAI) as a tool for providing feedback on students' research ideas.

      METHODS: We employed GenAI tools to provide personalised formative feedback during a small-group activity focused on helping students formulate research plans. The activity was implemented within two university courses designed for working health professionals. Students were grouped into groups of 5-7 students and provided clear instructions for how to prompt the GenAI for feedback on their research ideas. Participants completed an evaluation survey at the end of the activity that assessed frequency of use, perceived value, utility, and overall satisfaction.

      RESULTS: Half of the participants (n = 64, 85.3% response rate) had never used GenAI before (n = 32). Participants agreed or strongly agreed that the feedback provided by the GenAI was valuable (n = 54, 85%), helped them improve their plan (n = 50, 78%), and increased their interest in GenAI (n = 55, 86%). Participant comments described the feedback as favourable, specified an interest in adapting their plan, and rarely indicated a lack of trust or disagreement with the feedback provided.

      CONCLUSIONS: GenAI offers an efficient and satisfactory approach to providing feedback to students for their research ideas. As a supportive tool, GenAI may foster student learning while enabling educators to rethink how and what they focus their time on, with an eye toward helping students understand more nuanced and complex research concepts. This study highlights the potential of GenAI as a virtual tutor, and the need for additional research to explore its benefits and challenges.
    mergedFrom:
      - source: pubmed
        pmid: "40597036"
        doi: 10.1186/s12909-025-07390-6
    finalDecision: exclude
  - title: "Large language models versus traditional textbooks: optimizing learning for plastic surgery case preparation."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About LLMs for learning/case preparation, not about assessment or feedback"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares LLM-generated educational content with textbook responses for case preparation; not about assessment, feedback, or analysis of learner clinical performance."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares educational content for case preparation, not assessment/feedback processes tied to clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1186/s12909-025-07550-8
    pmid: "40597031"
    authors: Hinson C, Stingl C, Nazerali R
    year: "2025"
    abstract: |-
      BACKGROUND: Large language models (LLMs), such as ChatGPT-4 and Gemini, represent a new frontier in surgical education by offering dynamic, interactive learning experiences. Despite their potential, concerns about the accuracy, depth of knowledge, and bias in LLM responses persist. This study evaluates the effectiveness of LLMs in aiding surgical trainees in plastic and reconstructive surgery through comparison with traditional case-preparation textbooks.

      METHODS: Six representative cases from key areas of plastic and reconstructive surgery-craniofacial, hand, microsurgery, burn, gender-affirming, and aesthetics-were selected. Four types of questions were developed for each case to cover clinical anatomy, indications, contraindications, and complications. Responses from LLMs (ChatGPT-4 and Gemini) and textbooks were compared using surveys distributed to medical students, research fellows, residents, and attending surgeons. Reviewers rated each response on accuracy, thoroughness, usefulness for case preparation, brevity, and overall quality using a 5-point Likert scale. Statistical analyses, including ANOVA and unpaired T-tests, were conducted to assess the differences between LLM and textbook responses.

      RESULTS: A total of 90 surveys were completed. LLM responses were rated as more thorough (p < 0.001) but less concise (p < 0.001) than textbook responses. Textbooks were rated superior for answering questions on contraindications (p = 0.027) and complications (p = 0.014). ChatGPT was perceived as more accurate (p = 0.018), thorough (p = 0.002), and useful (p = 0.026) than Gemini. Gemini was rated lower in quality (p = 0.30) compared to ChatGPT along with being inferior to textbook answers for burn-related questions (p = 0.017) and anatomical questions (p = 0.013).

      CONCLUSION: While LLMs show promise in generating thorough educational content, they require improvement in conciseness, accuracy, and utility for practical case preparation. ChatGPT generally outperforms Gemini, indicating variability in LLM capabilities. Further development should focus on enhancing accuracy and consistency to establish LLMs as reliable tools in medical education and practice.
    mergedFrom:
      - source: pubmed
        pmid: "40597031"
        doi: 10.1186/s12909-025-07550-8
    finalDecision: exclude
  - title: A comparative analysis of DeepSeek R1, DeepSeek-R1-Lite, OpenAi o1 Pro, and Grok 3 performance on ophthalmology board-style questions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks reasoning LLMs on ophthalmology board-style MCQs; not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Board-style question answering study, not workplace-based assessment or feedback on observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1038/s41598-025-08601-2
    pmid: "40595291"
    authors: Shean R, Shah T, Pandiarajan A, Tang A, Bolo K, Nguyen V, Xu B
    year: "2025"
    abstract: The ability of large language models (LLMs) to accurately answer medical board-style questions reflects their potential to benefit medical education and real-time clinical decision-making. With the recent advance to reasoning models, the latest LLMs excel at addressing complex problems in benchmark math and science tests. This study assessed the performance of first-generation reasoning models-DeepSeek's R1 and R1-Lite, OpenAI's o1 Pro, and Grok 3-on 493 ophthalmology questions sourced from the StatPearls and EyeQuiz question banks. o1 Pro achieved the highest overall accuracy (83.4%), significantly outperforming DeepSeek R1 (72.5%), DeepSeek-R1-Lite (76.5%), and Grok 3 (69.2%) (p < 0.001 for all pairwise comparisons). o1 Pro also demonstrated superior performance in questions from eight of nine ophthalmologic subfields, questions of second and third order cognitive complexity, and on image-based questions. DeepSeek-R1-Lite performed the second best, despite relatively small memory requirements, while Grok 3 performed inferiorly overall. These findings demonstrate that the strong performance of the first-generation reasoning models extends beyond benchmark tests to high-complexity ophthalmology questions. While these findings suggest a potential role for reasoning models in medical education and clinical practice, further research is needed to understand their performance with real-world data, their integration into educational and clinical settings, and human-AI interactions.
    mergedFrom:
      - source: pubmed
        pmid: "40595291"
        doi: 10.1038/s41598-025-08601-2
    finalDecision: exclude
  - title: "ChatGPT for Dermatology Students: Studying How Input Format Affects Reliability."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates ChatGPT diagnostic accuracy on dermatological education case studies, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Uses simulated dermatology education case studies rather than workplace-based assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.3233/SHTI250675
    pmid: "40588883"
    authors: Gudiu A, Stoicu-Tivadar L
    year: "2025"
    abstract: The use of ChatGPT has steadily increased over the years following its launch. This study evaluated ChatGPT-4o's diagnostic accuracy in dermatological education case studies, comparing Free Answer and Multiple Choice formats, and assessing the impact of input data. Results showed a better performance in the Free Answer format, as opposed to Multiple Choice. Furthermore, adding patient data to the input images did not improve accuracy. These findings suggest that while ChatGPT-4o can serve as a second-opinion tool in dermatological education, it can do so by supporting, not replacing, the critical thinking process the students should perform when exercising diagnostic capabilities. Proper regulation is needed to ensure ethical and effective implementation of Generative AI in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "40588883"
        doi: 10.3233/SHTI250675
    finalDecision: exclude
  - title: "Evaluating Resident Feedback Using a Large Language Model: Are We Missing Core Competencies?"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: include
        comment: LLM (ChatGPT4o) used to analyze content and quality of narrative feedback from workplace-based assessments (SIMPL-OR, OSATS, end-of-rotation evaluations) for surgical residents; reports concordance with faculty evaluators (validity data)
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
      - decision: include
        comment: "Uses ChatGPT-4o to evaluate narrative feedback from SIMPL-OR (workplace-based assessment of surgical performance), OSATS, and end-of-rotation evaluations for residents. Presents validity data: 90% concordance with faculty raters (kappa=0.94). Analyzes feedback quality and ACGME core competency coverage across 776 entries. Meets all inclusion criteria."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:57.506000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:05.615000+00:00
    doi: 10.1002/lary.32368
    pmid: "40574724"
    authors: Ahmad S, Armache M, Trakimas D, Chen J, Galaiya D
    year: "2025"
    abstract: |-
      OBJECTIVES: Use a large language model (LLM) to examine the content and quality of narrative feedback provided to residents through: (1) an app collecting workplace-based assessments of surgical performance (SIMPL-OR), (2) Objective Structured Assessment of Technical Skills (OSATS), and (3) end-of-rotation (EOR) evaluations.

      METHODS: Narrative feedback provided to residents at a single institution from 2017 to 2021 was examined. Sixty entries (20 of each format) were evaluated by two faculty members on whether they were encouraging, corrective, or specific, and whether they addressed the Core Competencies outlined by the Accreditation Council for Graduate Medical Education. ChatGPT4o was tested on these 60 entries before evaluating the remaining 776 entries.

      RESULTS: ChatGPT evaluated entries with 90% concordance with faculty (κ = 0.94). Within the 776 feedback entries evaluated by ChatGPT, competencies addressed included: patient care (n = 491, 97% vs. 77% vs. 36% for SIMPL-OR, OSATS, EOR respectively, p < 0.001), practice-based learning (n = 175, 32% vs. 23% vs. 16%, p < 0.001), professionalism (n = 168, 1% vs. 6% vs. 40%, p < 0.001), medical knowledge (n = 95, 7% vs. 8% vs. 17%, p < 0.001), interpersonal and communication skills (n = 59, 3% vs. 3% vs. 12%, p < 0.001), and systems-based practice (n = 31, 4% vs. 2% vs. 5%, p = 0.387). Feedback was "encouraging" in 93% of both SIMPL-OR and OSATS, as compared to 84% of EOR (p < 0.001). Feedback was "corrective" in 71% of SIMPL-OR versus 44% of OSATS versus 24% of EOR (p < 0.001), and "specific" in 97% versus 53% versus 15%, respectively (p < 0.001).

      CONCLUSION: Different instruments provided feedback of differing content and quality and a multimodal feedback approach is important.

      LEVEL OF EVIDENCE: N/A.
    mergedFrom:
      - source: pubmed
        pmid: "40574724"
        doi: 10.1002/lary.32368
      - source: scopus
        doi: 10.1002/lary.32368
        scopusId: SCOPUS_ID:105009206495
    finalDecision: include
  - title: Generation of Fundus Fluorescein Angiography Videos for Health Care Data Sharing.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or education; focuses on generating medical images for data sharing"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    doi: 10.1001/jamaophthalmol.2025.1419
    pmid: "40569610"
    authors: Wu X, Wang L, Chen R, Liu B, Zhang W, Yang X, Feng Y, He M, Shi D
    year: "2025"
    abstract: |-
      IMPORTANCE: Medical data sharing faces strict restrictions. Text-to-video generation shows potential for creating realistic medical data while preserving privacy, offering a solution for cross-center data sharing and medical education.

      OBJECTIVE: To develop and evaluate a text-to-video generative artificial intelligence (AI)-driven model that converts the text of reports into dynamic fundus fluorescein angiography (FFA) videos, enabling visualization of retinal vascular and structural abnormalities.

      DESIGN, SETTING, AND PARTICIPANTS: This study retrospectively collected anonymized FFA data from a tertiary hospital in China. The dataset included both the medical records and FFA examinations of patients assessed between November 2016 and December 2019. A text-to-video model was developed and evaluated. The AI-driven model integrated the wavelet-flow variational autoencoder and the diffusion transformer.

      MAIN OUTCOMES AND MEASURES: The AI-driven model's performance was assessed through objective metrics (Fréchet video distance, learned perceptual image patch similarity score, and visual question answering score [VQAScore]). The domain-specific evaluation for the generated FFA videos was measured by the bidirectional encoder representations from transformers score (BERTScore). Image retrieval was evaluated using a Recall@K score. Each video was rated for quality by 3 ophthalmologists on a scale of 1 (excellent) to 5 (very poor).

      RESULTS: A total of 3625 FFA videos were included (2851 videos [78.6%] for training, 387 videos [10.7%] for validation, and 387 videos [10.7%] for testing). The AI-generated FFA videos demonstrated retinal abnormalities from the input text (Fréchet video distance of 2273, a mean learned perceptual image patch similarity score of 0.48 [SD, 0.04], and a mean VQAScore of 0.61 [SD, 0.08]). The domain-specific evaluations showed alignment between the generated videos and textual prompts (mean BERTScore, 0.35 [SD, 0.09]). The Recall@K scores were 0.02 for K = 5, 0.04 for K = 10, and 0.16 for K = 50, yielding a mean score of 0.073, reflecting disparities between AI-generated and real clinical videos and demonstrating privacy-preserving effectiveness. For assessment of visual quality of the FFA videos by the 3 ophthalmologists, the mean score was 1.57 (SD, 0.44).

      CONCLUSIONS AND RELEVANCE: This study demonstrated that an AI-driven text-to-video model generated FFA videos from textual descriptions, potentially improving visualization for clinical and educational purposes. The privacy-preserving nature of the model may address key challenges in data sharing while trying to ensure compliance with confidentiality standards.
    mergedFrom:
      - source: pubmed
        pmid: "40569610"
        doi: 10.1001/jamaophthalmol.2025.1419
      - source: scopus
        doi: 10.1001/jamaophthalmol.2025.1419
        scopusId: SCOPUS_ID:105010101144
    finalDecision: exclude
  - title: Large Language Model Clinical Vignettes and Multiple-Choice Questions for Postgraduate Medical Education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: LLM-generated MCQs for ob-gyn resident training exams; item generation for academic examinations, not workplace-based assessment or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: MCQ authoring/evaluation study in postgraduate education, not workplace-based assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1097/ACM.0000000000006137
    pmid: "40550116"
    authors: Jackson F, Keller N, Kouba I, Kouba W, Bracero L, Blitz M
    year: "2025"
    abstract: |-
      PROBLEM: Clinical vignette-based multiple-choice questions (MCQs) have been used to assess postgraduate medical trainees but require substantial time and effort to develop. Large language models, a type of artificial intelligence (AI), can potentially expedite this task. This report describes prompt engineering techniques used with ChatGPT-4 to generate clinical vignettes and MCQs for obstetrics-gynecology residents and evaluates whether residents and attending physicians can differentiate between human- and AI-generated content.

      APPROACH: The authors generated MCQs using a structured prompt engineering approach, incorporating authoritative source documents and an iterative prompt chaining technique, to refine output quality. Fifty human-generated and 50 AI-generated MCQs were randomly arranged into 10 quizzes (10 questions each). The AI-generated MCQs were developed in August 2024 and surveys conducted in September 2024. Obstetrics-gynecology residents and attending physician faculty members at Northwell Health or Donald and Barbara Zucker School of Medicine at Hofstra/Northwell completed an online survey, answering each MCQ and indicating whether they believed it was human or AI written or if they were uncertain.

      OUTCOMES: Thirty-three participants (16 residents, 17 attendings) completed the survey (80.5% response rate). Respondents correctly identified MCQ authorship a median (interquartile range [IQR]) of 39.1% (30.0%-50.0%) of the time, indicating difficulty in distinguishing human- and AI-generated questions. The median (IQR) correct answer selection rate was 62.3% (50.0%-75.0%) for human-generated MCQs and 64.4% (50.0%-83.3%) for AI-generated MCQs ( P = .74). The difficulty (0.69 vs 0.66, P = .83) and discriminatory (0.42 vs 0.38, P = .90) indexes showed no significant differences, supporting the feasibility of large language model-generated MCQs in medical education.

      NEXT STEPS: Future studies should explore the optimal balance between AI-generated content and expert review, identifying strategies to maximize efficiency without compromising accuracy. The authors will develop practice exams and assess their predictive validity by comparing scores with standardized exam results.
    mergedFrom:
      - source: pubmed
        pmid: "40550116"
        doi: 10.1097/ACM.0000000000006137
    finalDecision: exclude
  - title: Can ChatGPT pass the urology fellowship examination? Artificial intelligence capability in surgical training assessment.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Fellowship licensing examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates ChatGPT on the Australian urology fellowship written examination, an academic exam setting, not workplace-based or clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Fellowship written examination comparison without workplace-based assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1111/bju.16806
    pmid: "40538057"
    authors: Lockhart K, Canagasingham A, Zhong W, Ashrafi D, March B, Cole-Clark D, Grant A, Chung A
    year: "2025"
    abstract: |-
      OBJECTIVES: To assess the performance of ChatGPT compared to human trainees in the Australian Urology written fellowship examination (essay format).

      MATERIALS AND METHODS: Each examination was marked independently by two blinded examining urologists and assessed for: overall pass/failure; proportion of passing questions; and adjusted aggregate score. Examining urologists also made a blinded judgement as to authorship (artificial intelligence [AI] or trainee).

      RESULTS: A total of 20 examination papers were marked; 10 completed by urology trainees and 10 by AI platforms (half each on ChatGPT-3.5 and -4.0). Overall, 9/10 of trainees successfully passed the urology fellowship, whereas only 6/10 of ChatGPT examinations passed (P = 0.3). Of the ChatGPT failing examinations, 3/4 were undertaken by the ChatGPT-3.5 platform. The proportion of passing questions per examination was higher in trainees compared to ChatGPT: mean 89.4% vs 80.9% (P = 0.2). The adjusted aggregate scores of trainees were also higher than those of ChatGPT by a small margin: mean 79.2% vs 78.1% (P = 0.8). ChatGPT-3.5 and ChatGPT-4.0 achieved similar aggregate scores (78.9% and 77.4%, P = 0.8). However, ChatGPT-3.5 had a lower percentage of passing questions per examination: mean 79.6% vs 82.1% (P = 0.8). Two examinations were incorrectly assigned by examining urologists (both trainee candidates perceived to be ChatGPT); therefore, the sensitivity for identifying ChatGPT authorship was 100% and overall accuracy was 91.7%.

      CONCLUSION: Overall, ChatGPT did not perform as well as human trainees in the Australian Urology fellowship written examination. Examiners were able to identify AI-generated answers with a high degree of accuracy.
    mergedFrom:
      - source: pubmed
        pmid: "40538057"
        doi: 10.1111/bju.16806
    finalDecision: exclude
  - title: "Bytes versus brains: A comparative study of AI-generated feedback and human tutor feedback in medical education."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Second-year medical students received AI vs tutor feedback on a written assignment in a classroom/curriculum setting, not clinical/workplace-based assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Conducted within a classroom curriculum written-assignment context, not a workplace-based or clinical training setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1080/0142159X.2025.2519639
    pmid: "40528415"
    authors: Ali M, Harbieh I, Haider K
    year: "2026"
    abstract: |-
      INTRODUCTION: Timely, high-quality feedback is vital in medical education but increasingly difficult due to rising student numbers and limited faculty. Artificial intelligence (AI) tools offer scalable solutions, yet limited research compares their effectiveness with traditional tutor feedback. This study examined the comparative effectiveness of AI-generated feedback versus human tutor feedback within the medical curriculum.

      METHODS: Second-year medical students (n = 108) received two sets of feedback on a written assignment, one from their tutor and one unedited response from ChatGPT. Students assessed each feedback using a structured online questionnaire focused on key feedback quality criteria.

      RESULTS: Eighty-five students (79%) completed the evaluation. Tutor feedback was rated significantly higher in clarity and understandability (p < 0.001), relevance (p < 0.001), actionability (p = 0.009), comprehensiveness (p = 0.001), accuracy and reliability (p = 0.003), and overall usefulness (p < 0.001). However, 62.3% of students indicated that both pieces of feedback complemented each other. Open-ended responses aligned with these quantitative findings.  .

      CONCLUSION: Human tutors currently provide superior feedback in terms of clarity, relevance, and accuracy. Nonetheless, AI-generated feedback shows promise as a complementary tool. A hybrid feedback model integrating AI and human input could enhance the scalability and richness of feedback in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "40528415"
        doi: 10.1080/0142159X.2025.2519639
    finalDecision: exclude
  - title: Simulation-based training significantly improved confidence and clinical skills of resident doctors in acute diabetes management.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About simulation-based training, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:20.264000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title indicates simulation-based training without generative AI or LLM involvement."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:21.767000+00:00
    doi: 10.1111/dme.70068
    pmid: "40526722"
    authors: Persad K, Khatoon A, Buchipudi A, Jie Yee A, Philip N, Sharma A, Sinha A, Salahuddin S, Kempegowda P, DEVI collaboration
    year: "2025"
    abstract: |-
      AIMS: The joint consensus of the American Diabetes Association and the European Association for the Study of Diabetes highlights the need for practical, up-to-date training in acute diabetes care. Therefore, this session aimed to develop an applied learning intervention that identifies key gaps in acute diabetes training, assesses participants' change in confidence in managing these scenarios following the intervention and evaluates participants' satisfaction and the intervention's relevance to practice.

      METHODS: To design the programme, we adopted Kern's six-step curriculum development framework. Clinical experts were interviewed to assess general needs, and students and resident doctors were interviewed to assess targeted needs. The SIMBA (Simulation via Instant Messaging for Bedside Application) model was adopted to develop cases alongside generative AI and expert input. Pre- and post-session surveys assessed participants' confidence, while the latter also assessed satisfaction and relevance to practice.

      RESULTS: 33 participants attended the session. 17 participants completed both pre- and post-session surveys and were included in the study. Simulation performance scores averaged 4.0/5 for history taking, interpretation and clinical judgement; 3.9/5 for physical examination and investigations; and 3.6/5 for management. Confidence in managing acute diabetes scenarios improved significantly (pre vs. post: 33.3% vs. 78.4%, p < 0.001). 94.1% strongly agreed the cases were relevant, and 82.3% preferred this teaching method over traditional approaches.

      CONCLUSION: The model identified the need for targeted education on physical examination, investigations and management specific to acute diabetes scenarios. Simulation-based education significantly enhances confidence and is perceived as highly relevant for training in acute diabetes management.
    mergedFrom:
      - source: pubmed
        pmid: "40526722"
        doi: 10.1111/dme.70068
    finalDecision: exclude
  - title: "Assessment of Large Language Model Performance on Medical School Essay-Style Concept Appraisal Questions: Exploratory Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks Bing Chat/Copilot performance on medical school essay-style questions compared to students; not about using AI for assessment or feedback of learners. Also criterion 4: not workplace-based."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Essay-style exam response study without workplace-based assessment feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.2196/72034
    pmid: "40523238"
    authors: Mehta S, Haddad E, Burke I, Majors A, Maeda R, Burke S, Deshpande A, Nowacki A, Lindenmeyer C, Mehta N
    year: "2025"
    abstract: Bing Chat (subsequently renamed Microsoft Copilot)-a ChatGPT 4.0-based large language model-demonstrated comparable performance to medical students in answering essay-style concept appraisals, while assessors struggled to differentiate artificial intelligence (AI) responses from human responses. These results highlight the need to prepare students and educators for a future world of AI by fostering reflective learning practices and critical thinking.
    mergedFrom:
      - source: pubmed
        pmid: "40523238"
        doi: 10.2196/72034
    finalDecision: exclude
  - title: "Artificial intelligence versus ophthalmology experts: Comparative analysis of responses to blepharitis patient queries."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About AI responses to patient queries compared to experts, not about assessment or feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on AI responses to patient queries, not learner assessment/feedback."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    doi: 10.1177/11206721251350809
    pmid: "40518911"
    authors: Bahir D, Rostov A, Busool Abu Eta Y, Hamed Azzam S, Lockington D, Teichman J, Matsou A, Chan C, Shvartz E, Mimouni M
    year: "2025"
    abstract: "ObjectiveTo assess the accuracy and clinical education value of responses from AI models (GPT-3.5, GPT-4o, Gemini, Gemini Advanced) compared to expert ophthalmologists' answers to common patient questions about blepharitis, and evaluate their potential for patient education and clinical use.MethodsThirteen frequently asked questions about blepharitis were selected. Responses were generated by AI models and compared to expert answers. A panel of ophthalmologists rated each response for correctness and clinical education value using a 7-point Likert scale. The Friedman test with post hoc comparisons was used to identify performance differences.ResultsExpert responses had the highest correctness (6.3) and clinical education value (6.4) scores, especially in complex, context-driven questions. Significant differences were found between expert and AI responses (P < 0.05). Among AI models, GPT-3.5 performed best in simple definitions (correctness: 6.4) but dropped to 5.5 in nuanced cases. GPT-4o followed (5.4), while Gemini and Gemini Advanced scored lower (5.0 and 4.9), especially in diagnostic and treatment contexts.ConclusionsAI models can support patient education by effectively answering basic factual questions about blepharitis. However, their limitations in complex clinical scenarios highlight the continued need for expert input. While promising as educational tools, AI should complement-not replace-clinician guidance in patient care."
    mergedFrom:
      - source: pubmed
        pmid: "40518911"
        doi: 10.1177/11206721251350809
    finalDecision: exclude
  - title: "Evaluating Large Language Models on American Board of Anesthesiology-style Anesthesiology Questions: Accuracy, Domain Consistency, and Clinical Implications."
    reviews:
      - decision: uncertain
        comment: LLMs on board-style questions; abstract may reveal assessment or WBA elements
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Tests five LLMs on ABA BASIC exam questions; benchmarks AI on board exam questions, not about assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Assesses LLM accuracy on board-style questions, not generative AI use for workplace-based assessment/feedback or observation-record analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1053/j.jvca.2025.05.033
    pmid: "40518333"
    authors: Patel S, Ngo V, Wilhelmi B
    year: "2025"
    abstract: "Recent advances in large language models (LLMs) have led to growing interest in their potential applications in medical education and clinical practice. This study evaluated whether five widely used and highly developed LLMs-ChatGPT-4, Gemini, Claude, Microsoft CoPilot, and Meta-could achieve a passing score on the American Board of Anesthesiology (ABA) BASIC Exam. Each model completed three separate sets of 200 multiple-choice questions derived from a widely used review resource, with the content distribution mirroring the ABA BASIC Exam blueprint. All five models demonstrated statistically significant performance above the 70% passing threshold (p < 0.05), with the following averages: ChatGPT-4: 92.0%, Gemini: 89.0%, Claude: 88.3%, Microsoft CoPilot: 91.5%, and Meta: 85.8%. Furthermore, an analysis of variance comparing their mean accuracy scores found no statistically significant difference among them (F = 1.88, p = 0.190). These findings suggest that current LLMs can surpass the minimum competency required for board certification, raising important questions about their future role in medical education and clinical decision-making. Performance on topics central to cardiac, thoracic, and vascular anesthesiology-such as hemodynamic management, cardiopulmonary physiology, and coagulation-was particularly robust, suggesting relevance to both fellowship-level education and complex intraoperative care. While these results highlight the capability of artificial intelligence (AI) to meet standardized medical knowledge benchmarks, their broader implications extend beyond examination performance. As AI continues to evolve, its integration into real-time patient care may transform anesthesiology practice, offering decision-support tools that assist physicians in synthesizing complex clinical data. Further research is needed to explore the reliability, ethical considerations, and real-world applications of AI-driven technologies in patient care settings."
    mergedFrom:
      - source: pubmed
        pmid: "40518333"
        doi: 10.1053/j.jvca.2025.05.033
    finalDecision: exclude
  - title: LLM-Generated multiple choice practice quizzes for preclinical medical students.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM-generated quizzes for preclinical students; preclinical is not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting (preclinical context)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1152/advan.00106.2024
    pmid: "40516963"
    authors: Camarata T, McCoy L, Rosenberg R, Temprine Grellinger K, Brettschnieder K, Berman J
    year: "2025"
    abstract: Multiple choice questions (MCQs) are frequently used in medical education for assessment. Automated generation of MCQs in board-exam format could potentially save significant effort for faculty and generate a wider set of practice materials for student use. The goal of this study was to explore the feasibility of using ChatGPT by OpenAI to generate United States Medical Licensing Exam (USMLE)/Comprehensive Osteopathic Medical Licensing Examination (COMLEX-USA)-style practice quiz items as study aids. Researchers gave second-year medical students studying renal physiology access to a set of practice quizzes with ChatGPT-generated questions. The exam items generated were evaluated by independent experts for quality and adherence to the National Board of Medical Examiners (NBME)/National Board of Osteopathic Medical Examiners (NBOME) guidelines. Forty-nine percent of questions contained item writing flaws, and 22% contained factual or conceptual errors. However, 59/65 (91%) were categorized as a reasonable starting point for revision. These results demonstrate the feasibility of large language model (LLM)-generated practice questions in medical education but only when supervised by a subject matter expert with training in exam item writing.NEW & NOTEWORTHY Practice board exam questions generated by large language models can be made suitable for preclinical medical students by subject-matter experts.
    mergedFrom:
      - source: pubmed
        pmid: "40516963"
        doi: 10.1152/advan.00106.2024
    finalDecision: exclude
  - title: Advancing medical education in cervical cancer control with large language models for multiple-choice question generation.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Uses LLMs to generate MCQs for cervical cancer education; focuses on item generation quality, not assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1080/0142159X.2025.2513419
    pmid: "40504493"
    authors: Chen M, Ma J, Cui X, Dai Q, Hu H, Wu Y, Husaiyin S, Wu A, Qiao Y
    year: "2025"
    abstract: |-
      OBJECTIVE: To explore the feasibility of using large language models (LLMs) to generate multiple-choice questions (MCQs) for cervical cancer control education and compare them with those created by clinicians.

      METHODS: GPT-4o and Baichuan4 generated 40 MCQs each with iteratively refined prompts. Clinicians generated 40 MCQs for comparison. 120 MCQs were evaluated by 12 experts across five dimensions (correctness, clarity and specificity, cognitive level, clinical relevance, explainability) using a 5-point Likert scale. Difficulty and discriminatory power were tested by practitioners. Participants were asked to identify the source of each MCQ.

      RESULTS: Automated MCQs were similar to clinician-generated ones in most dimensions. However, clinician-generated MCQs had a higher cognitive level (4.00±1.08) than those from GPT-4o (3.68±1.07) and Baichuan4 (3.7±1.13). Testing with 312 practitioners revealed no significant differences in difficulty or discriminatory power among clinicians (59.51±24.50, 0.38±0.14), GPT-4o (61.89±25.36, 0.30±0.19), and Baichuan4 (59.79±26.25, 0.33±0.15). Recognition rates for LLM-generated MCQs ranged from 32% to 50%, with experts outperforming general practitioners in identifying the question setters.

      CONCLUSIONS: LLMs can generate MCQs comparable to clinician-generated ones with engineered prompts, though clinicians outperformed in cognitive level. LLM-assisted MCQ generation could enhance efficiency but requires rigorous validation to ensure educational quality.
    mergedFrom:
      - source: pubmed
        pmid: "40504493"
        doi: 10.1080/0142159X.2025.2513419
    finalDecision: exclude
  - title: A comparison of the psychometric properties of GPT-4 versus human novice and expert authors of clinically complex MCQs in a mock examination of Australian medical students.
    reviews:
      - decision: uncertain
        comment: GPT-4 vs human MCQ generation with psychometric comparison; abstract needed to determine WBA context
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not workplace-based assessment; GPT-4 used to generate MCQs for a mock examination of medical students, not WBA"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: MCQ authoring psychometrics study, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1080/0142159X.2025.2513418
    pmid: "40504491"
    authors: Wu H, Lee D, Zerner T, Court-Kowalski S, Devitt P, Palmer E
    year: "2026"
    abstract: |-
      PURPOSE: Creating clinically complex Multiple Choice Questions (MCQs) for medical assessment can be time-consuming . Large language models such as GPT-4, a type of generative artificial intelligence (AI), are a potential MCQ design tool. Evaluating the psychometric properties of AI-generated MCQs is essential to ensuring quality.

      METHODS: A 120-item mock examination was constructed, containing 40 human-generated MCQs at novice item-writer level, 40 at expert level, and 40 AI-generated MCQs. int. All examination items underwent panel review to ensure they tested higher order cognitive skills and met a minimum acceptable standard. The online mock examination was administered to Australian medical students, who were blinded to each item's author.

      RESULTS: 234 medical students completed the examination. Analysis showed acceptable reliability (Cronbach's 0.836). There were no differences in item difficulty or discrimination between AI, Novice, and Expert items. The mean item difficulty was 'easy' and mean item discrimination 'fair' across all groups. AI items had lower distractor efficiency (39%) compared to Novice items (55%, p = 0.035), but no difference to Expert items (48%, p = 0.382).

      CONCLUSIONS: The psychometric properties of AI-generated MCQs are comparable to human-generated MCQs at both novice and expert level. Item quality can be improved across all author groups. AI-generated items should undergo human review to enhance distractor efficiency.
    mergedFrom:
      - source: pubmed
        pmid: "40504491"
        doi: 10.1080/0142159X.2025.2513418
    finalDecision: exclude
  - title: "Anatomy exam model for the circulatory and respiratory systems using GPT-4: a medical school study."
    reviews:
      - decision: uncertain
        comment: GPT-4 for anatomy exams; abstract needed to determine if workplace-based or classroom only
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates GPT-4-generated anatomy MCQs for a classroom examination."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on AI-generated anatomy MCQs and exam psychometrics, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.1007/s00276-025-03667-z
    pmid: "40495075"
    authors: Tekin A, Karamus N, &#xc7;olak T
    year: "2025"
    abstract: |-
      PURPOSE: The study aimed to evaluate the effectiveness of anatomy multiple-choice questions (MCQs) generated by GPT-4, focused on their methodological appropriateness and alignment with the cognitive levels defined by Bloom's revised taxonomy to enhance assessment.

      METHODS: The assessment questions developed for medical students were created utilizing GPT-4, comprising 240 MCQs organized into subcategories consistent with Bloom's revised taxonomy. When designing prompts to create MCQs, details about the lesson's purpose, learning objectives, and students' prior experiences were included to ensure the questions were contextually appropriate. A set of 30 MCQs was randomly selected from the generated questions for testing. A total of 280 students participated in the examination, which assessed the difficulty index of the MCQs, the item discrimination index, and the overall test difficulty level. Expert anatomists examined the taxonomy accuracy of GPT-4's questions.

      RESULTS: Students achieved a median score of 50 (range, 36.67-60) points on the test. The test's internal consistency, assessed by KR-20, was 0.737. The average difficulty of the test was 0.5012. Results show difficulty and discrimination indices for each AI-generated question. Expert anatomists' taxonomy-based classifications matched GPT-4's 26.6%. Meanwhile, 80.9% of students found the questions were clear, and 85.8% showed interest in retaking the assessment exam.

      CONCLUSION: This study demonstrates GPT-4's significant potential for generating medical education exam questions. While it effectively assesses basic knowledge recall, it fails to sufficiently evaluate higher-order cognitive processes outlined in Bloom's revised taxonomy. Future research should consider alternative methods that combine AI with expert evaluation and specialized multimodal models.
    mergedFrom:
      - source: pubmed
        pmid: "40495075"
        doi: 10.1007/s00276-025-03667-z
    finalDecision: exclude
  - title: Evaluating the Accuracy and Performance of ChatGPT-4o in Solving Japanese National Dental Technician Examination.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT performance on national dental technician examination, standardized exam-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study benchmarks ChatGPT-4o performance on dental technician licensing exam questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Study evaluates national licensing exam question answering, not workplace-based clinical education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.1016/j.identj.2025.100847
    pmid: "40494209"
    authors: Fukuda H, Morishita M, Muraoka K, Yamaguchi S, Nakamura T, Habu M, Yoshioka I, Awano S, Ono K
    year: "2025"
    abstract: |-
      INTRODUCTION AND AIMS: With advancements in artificial intelligence (AI), its role in education and examinations is expanding. This study aimed to evaluate the accuracy and response capability of ChatGPT-4o in solving questions from the Japanese National Dental Technician Examination (JNDTE). By analysing the AI's answers to a variety of questions, including both text-based items and those containing visual information across multiple domains, we examined the practicality of using AI as a supportive tool for learning and assessment in dental education.

      MATERIALS AND METHODS: A total of 400 JNDTE questions (2018-2022) were analysed using ChatGPT-4o. Standardized prompts were used, and statistical analysis was conducted using Fisher's exact test.

      RESULTS: The AI demonstrated high accuracy in basic knowledge-based questions but lower accuracy in specialized topics requiring technical skills. Notably, accuracy decreased when visual elements (figures, tables, and photographs) were present. The highest accuracy was observed in 'Dental materials, laboratory equipment, and processing technology' (84.1%), while the lowest was in 'Orthodontic technology' (46.2%) and 'Pediatric dental technology' (50.0%). The AI particularly struggled with interpreting radiographic images and procedural workflows.

      CONCLUSIONS: ChatGPT-4o demonstrated reasonable proficiency with JNDTE-related knowledge, suggesting its potential application as a supplementary tool in dental education. However, its limitations in processing visual and technical content highlight the need for advancements in AI-driven image recognition and simulation-based learning.

      CLINICAL RELEVANCE: The findings of this study highlight the potential application of AI models like ChatGPT-4o as a supplementary tool in dental technician education and examination preparation. Enhancing AI-driven image recognition and simulation-based learning could improve its effectiveness in dental education and clinical training, ultimately contributing to better learning outcomes and skill development for future dental technicians.
    mergedFrom:
      - source: pubmed
        pmid: "40494209"
        doi: 10.1016/j.identj.2025.100847
    finalDecision: exclude
  - title: Curriculum check, 2025-equipping radiology residents for AI challenges of tomorrow.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Appears to be a commentary/perspective on curriculum, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Curriculum-focused title without assessment/feedback analysis of clinical observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    doi: 10.1007/s00261-025-05016-5
    pmid: "40488865"
    authors: Venugopal V, Kumar A, Tan M, Szarf G
    year: "2025"
    abstract: "The exponential rise in the artificial intelligence (AI) tools for medical imaging is profoundly impacting the practice of radiology. With over 1000 FDA-cleared AI algorithms now approved for clinical use-many of them designed for radiologic tasks-the responsibility lies with training institutions to ensure that radiology residents are equipped not only to use AI systems, but to critically evaluate, monitor, respond to their output in a safe, ethical manner. This review proposes a comprehensive framework to integrate AI into radiology residency curricula, targeting both essential competencies required of all residents, optional advanced skills for those interested in research or AI development. Core educational strategies include structured didactic instruction, hands-on lab exposure to commercial AI tools, case-based discussions, simulation-based clinical pathways, teaching residents how to interpret model cards, regulatory documentation. Clinical examples such as stroke triage, Urinary tract calculi detection, AI-CAD in mammography, false-positive detection are used to anchor theory in practice. The article also addresses critical domains of AI governance: model transparency, ethical dilemmas, algorithmic bias, the role of residents in human-in-the-loop oversight systems. It outlines mentorship, faculty development strategies to build institutional readiness, proposes a roadmap to future-proof radiology education. This includes exposure to foundation models, vision-language systems, multi-agent workflows, global best practices in post-deployment AI monitoring. This pragmatic framework aims to serve as a guide for residency programs adapting to the next era of radiology practice."
    mergedFrom:
      - source: pubmed
        pmid: "40488865"
        doi: 10.1007/s00261-025-05016-5
    finalDecision: exclude
  - title: "'I, robot, can help you'. Applications of Generative Artificial Intelligence in RANZCP psychiatry training."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/opinion piece examining how GenAI could be integrated into psychiatry training; not original research with empirical data."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/discussion article on potential GenAI applications in training, not original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.1177/10398562251344470
    pmid: "40484732"
    authors: Gill S, Galletly Oam C
    year: "2025"
    abstract: ObjectiveThe growing availability and sophistication of online generative artificial intelligence (GenAI) tools, such as ChatGPT, necessitates an exploration of their applications within the RANZCP Fellowship Training Program. This article examines how GenAI could be integrated into various areas, including trainee selection, workplace-based assessments, the Scholarly Project (SP), and the Psychotherapy Written Case (PWC). While GenAI offers benefits, trainees must understand that there are limitations on how these can be used with patient data. They should also be aware of the ethical and legal considerations surrounding the use of purpose-built medical scribe programs. For this paper, ChatGPT was employed to enhance language quality, without contributing to content generation.ConclusionThe accessibility and ease of use of GenAI present both opportunities and challenges. Given the limitations of AI detection software, the RANZCP will need to adapt to GenAI rather than attempt to regulate its use. Revisions to assessment methods, particularly those relying on written assignments, will be needed. As this is a rapidly evolving field, training programs should be continually updated to remain relevant.
    mergedFrom:
      - source: pubmed
        pmid: "40484732"
        doi: 10.1177/10398562251344470
      - source: scopus
        doi: 10.1177/10398562251344470
        scopusId: SCOPUS_ID:105008061611
    finalDecision: exclude
  - title: A large language model improves clinicians' diagnostic performance in complex critical illness cases.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Cases were collected from the literature, not from actual clinical workplace encounters; evaluates diagnostic accuracy on literature-based vignettes rather than workplace-based assessment or feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1186/s13054-025-05468-7
    pmid: "40481529"
    authors: Wu X, Huang Y, He Q
    year: "2025"
    abstract: |-
      BACKGROUND: Large language models (LLMs) have demonstrated potential in assisting clinical decision-making. However, studies evaluating LLMs' diagnostic performance on complex critical illness cases are lacking. We aimed to assess the diagnostic accuracy and response quality of an artificial intelligence (AI) model, and evaluate its potential benefits in assisting critical care residents with differential diagnosis of complex cases.

      METHODS: This prospective comparative study collected challenging critical illness cases from the literature. Critical care residents from tertiary teaching hospitals were recruited and randomly assigned to non-AI-assisted physician and AI-assisted physician groups. We selected a reasoning model, DeepSeek-R1, for our study. We evaluated the model's response quality using Likert scales, and we compared the diagnostic accuracy and efficiency between groups.

      RESULTS: A total of 48 cases were included. Thirty-two critical care residents were recruited, with 16 residents assigned to each group. Each resident handled an average of 3 cases. DeepSeek-R1's responses received median Likert grades of 4.0 (IQR 4.0-5.0; 95% CI 4.0-4.5) for completeness, 5.0 (IQR 4.0-5.0; 95% CI 4.5-5.0) for clarity, and 5.0 (IQR 4.0-5.0; 95% CI 4.0-5.0) for usefulness. The AI model's top diagnosis accuracy was 60% (29/48; 95% CI 0.456-0.729), with a median differential diagnosis quality score of 5.0 (IQR 4.0-5.0; 95% CI 4.5-5.0). Top diagnosis accuracy was 27% (13/48; 95% CI 0.146-0.396) in the non-AI-assisted physician group versus 58% (28/48; 95% CI 0.438-0.729) in the AI-assisted physician group. Median differential quality scores were 3.0 (IQR 0-5.0; 95% CI 2.0-4.0) without and 5.0 (IQR 3.0-5.0; 95% CI 3.0-5.0) with AI assistance. The AI model showed higher diagnostic accuracy than residents, and AI assistance significantly improved residents' accuracy. The residents' diagnostic time significantly decreased with AI assistance (median, 972 s; IQR 570-1320; 95% CI 675-1200) versus without (median, 1920 s; IQR 1320-2640; 95% CI 1710-2370).

      CONCLUSIONS: For diagnostically difficult critical illness cases, DeepSeek-R1 generates high-quality information, achieves reasonable diagnostic accuracy, and significantly improves residents' diagnostic accuracy and efficiency. Reasoning models are suggested to be promising diagnostic adjuncts in intensive care units.
    mergedFrom:
      - source: pubmed
        pmid: "40481529"
        doi: 10.1186/s13054-025-05468-7
    finalDecision: exclude
  - title: "Performance of DeepSeek-R1 and ChatGPT-4o on the Chinese National Medical Licensing Examination: A Comparative Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks LLM accuracy on licensing exam MCQs; not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam question-answering performance study, not workplace-based assessment or feedback analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1007/s10916-025-02213-z
    pmid: "40459679"
    authors: Wu J, Wang Z, Qin Y
    year: "2025"
    abstract: Large Language Models (LLMs) have a significant impact on medical education due to their advanced natural language processing capabilities. ChatGPT-4o (Chat Generative Pre-trained Transformer), a mainstream Western LLM, demonstrates powerful multimodal abilities. DeepSeek-R1, a newly released free and open-source LLM from China, demonstrates capabilities on par with ChatGPT-4o across various domains. This study aims to evaluate the performance of DeepSeek-R1 and ChatGPT-4o on the Chinese National Medical Licensing Examination (CNMLE) and explore the performance differences of LLMs from distinct linguistic environments in Chinese medical education. We evaluated both LLMs using 600 multiple-choice questions from the written part of 2024 CNMLE, covering four units. The questions were categorized into low- and high-difficulty groups according to difficulty. The primary outcome was the overall accuracy rate of each LLM. The secondary outcomes included accuracy within each of the four units and within the two difficulty-level groups. DeepSeek-R1 achieved a statistically significantly higher overall accuracy of 92.0% compared to ChatGPT-4o's 87.2% (P < 0.05). In the low-difficulty group, DeepSeek-R1 demonstrated an accuracy rate of 95.9%, which was significantly higher than ChatGPT-4o's 92.0% (P < 0.05). No statistically significant differences were observed between the models in any of the four units or in the high-difficulty group (P > 0.05). DeepSeek-R1 demonstrated a performance advantage on CNMLE.
    mergedFrom:
      - source: pubmed
        pmid: "40459679"
        doi: 10.1007/s10916-025-02213-z
    finalDecision: exclude
  - title: Assessing medical students' readiness for artificial intelligence after pre-clinical training.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About assessing students' readiness/attitudes toward AI, not about using AI for assessment or feedback"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses on AI readiness training, not a generative AI/LLM assessment application."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    doi: 10.1186/s12909-025-07008-x
    pmid: "40457325"
    authors: AlZaabi A, Masters K
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) is becoming increasingly relevant in healthcare, necessitating healthcare professionals' proficiency in its use. Medical students and practitioners require fundamental understanding and skills development to manage data, oversee AI tools and make informed decisions based on AI applications. Integrating AI into medical education is essential to meet this demand.

      METHOD: This cross-sectional study aimed to evaluate the level of undergraduate medical students' readiness for AI as they enter their clinical years at Sultan Qaboos University's College of Medicine and Health Sciences. The students' readiness was assessed after being exposed to various AI related topics in several courses in the preclinical phases of the medical curriculum. The Medical Artificial Intelligence Readiness Scale For Medical Students (MAIRS-MS) questionnaire was used as the study instrument.

      RESULTS: A total of 84 out of 115 students completed the questionnaire (73.04% response rate). Of these, 45 (53.57%) were female while 39 (46.43%) were male. The cognition section, which evaluated the participants' cognitive preparedness in terms of knowledge of medical AI terminology, the logic behind AI applications, and data science, received the lowest score (Mean = 3.52). Conversely, the vision section of the questionnaire, which assessed the participants' capacity to comprehend the limitations and potential of medical AI, and anticipate opportunities and risks displayed the highest level of preparedness, had the highest score (Mean = 3.90). Notably, there were no statistically significant differences in AI competency scores by gender or academic year.

      CONCLUSION: This study's findings suggest while medical students demonstrate a moderate level of AI-readiness as they enter their clinical years, significant gaps remain, particularly in cognitive areas such as understanding AI terminology, logic, and data science. The majority of students use ChatGPT as their AI tool, with a notable difference in attitudes between tech-savvy and non-tech-savvy individuals. Further efforts are needed to improve students' competency in evaluating AI tools. Medical schools should consider integrating AI into their curricula to enhance students' preparedness for future medical practice. Assessing students' readiness for AI in healthcare is crucial for identifying knowledge and skills gaps and guiding future training efforts.
    mergedFrom:
      - source: pubmed
        pmid: "40457325"
        doi: 10.1186/s12909-025-07008-x
    finalDecision: exclude
  - title: "Macy Foundation Innovation Report Part I: Current Landscape of Artificial Intelligence in Medical Education."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Foundation report, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Innovation report/publication type is not original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    doi: 10.1097/ACM.0000000000006107
    pmid: "40456178"
    authors: Boscardin C, Abdulnour R, Gin B
    year: "2025"
    abstract: "The rapid emergence of artificial intelligence (AI), including generative large language models, offers transformative opportunities in medical education. This proliferation has generated numerous speculative discussions about AI's promise but has been limited in delivering a comprehensive analysis to distinguish evidence-based utility from hype while identifying context-specific limitations.In this first part of a 2-part innovation report, commissioned by the Josiah Macy Jr. Foundation to inform the discussions at a conference on AI in medical education, the authors synthesize the landscape of AI in medical education, underscoring both its potential advantages and inherent challenges. To map the AI landscape, they reviewed 455 articles that targeted 5 medical education domains: (1) admissions, (2) classroom-based learning and teaching, (3) workplace-based learning and teaching, (4) assessment, feedback, and certification, and (5) program evaluation and research.In admissions, AI-driven strategies facilitated holistic applicant reviews through predictive modeling, natural language processing, and large language model-based chatbots. Preclinical learning benefited from AI-powered virtual patients and curriculum design tools that managed expanding medical knowledge and supported robust student practice. Within clinical learning, AI aided diagnostic and interpretive processes, prompting medical education curricula to demand relevant AI competency and literacy frameworks. A few studies reported that assessment and feedback processes became more efficient through automated grading and advanced analytics, which reduced faculty workload and offered timely, targeted feedback. Program evaluation and research gained additional insights using AI on careers, diversity, and performance metrics of faculty and learners, improving resource allocations and guiding evidence-based approaches.Despite these possibilities, bias in AI algorithms, concerns about transparency, inadequate ethical guidelines, and risks of over-reliance highlighted the need for cautious, informed AI implementation. By mapping AI tasks to medical education applications, the authors provide a framework for understanding and leveraging AI's potential while addressing technical, ethical, and human-factor complexities in this evolving field."
    mergedFrom:
      - source: pubmed
        pmid: "40456178"
        doi: 10.1097/ACM.0000000000006107
      - source: scopus
        doi: 10.1097/ACM.0000000000006107
        scopusId: SCOPUS_ID:105007446142
    finalDecision: exclude
  - title: "Performance of GPT-4 in oral and maxillofacial surgery board exams: challenges in specialized questions."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates GPT-4 on OMFS board exam MCQs from a question bank, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Board exam question-bank performance study is not workplace-based clinical education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.1007/s10006-025-01412-9
    pmid: "40447863"
    authors: Warwas F, Heim N
    year: "2025"
    abstract: |-
      PURPOSE: The aim of this study was to evaluate the performance of GPT-4 in answering oral and maxillofacial surgery (OMFS) board exam questions, given its success in other medical specializations.

      METHODS: A total of 250 multiple-choice questions were randomly selected from an established OMFS question bank, covering a broad range of topics such as craniofacial trauma, oncological procedures, orthognathic surgery, and general surgical principles. GPT-4's responses were assessed for accuracy, and statistical analysis was performed to compare its performance across different topics.

      RESULTS: GPT-4 achieved an overall accuracy of 62% in answering the OMFS board exam questions. The highest accuracies were observed in Pharmacology (92.8%), Anatomy (73.3%), and Mucosal Lesions (70.8%). Conversely, the lowest accuracies were noted in Dental Implants (37.5%), Orthognathic Surgery (38.5%), and Reconstructive Surgery (42.9%). Statistical analysis indicated significant variability in performance across different topics, with GPT-4 performing better in general topics compared to specialized ones.

      CONCLUSION: GPT-4 demonstrates a promising ability to answer OMFS board exam questions, particularly in general medical topics. However, its performance in highly specialized areas reveals significant limitations. These findings suggest that while GPT-4 can be a useful tool in medical education, further enhancements are needed for its application in specialized medical fields.
    mergedFrom:
      - source: pubmed
        pmid: "40447863"
        doi: 10.1007/s10006-025-01412-9
    finalDecision: exclude
  - title: "Chatbots' Role in Generating Single Best Answer Questions for Undergraduate Medical Student Assessment: Comparative Analysis."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in workplace-based or clinical education setting; AI chatbots used to generate SBA questions for undergraduate medical student course assessment, not WBA"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on AI generation of exam questions, not assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.2196/69521
    pmid: "40446330"
    authors: Abouzeid E, Wassef R, Jawwad A, Harris P
    year: "2025"
    abstract: |-
      BACKGROUND: Programmatic assessment supports flexible learning and individual progression but challenges educators to develop frequent assessments reflecting different competencies. The continuous creation of large volumes of assessment items, in a consistent format and comparatively restricted time, is laborious. The application of technological innovations, including artificial intelligence (AI), has been tried to address this challenge. A major concern raised is the validity of the information produced by AI tools, and if not properly verified, it can produce inaccurate and therefore inappropriate assessments.

      OBJECTIVE: This study was designed to examine the content validity and consistency of different AI chatbots in creating single best answer (SBA) questions, a refined format of multiple choice questions better suited to assess higher levels of knowledge, for undergraduate medical students.

      METHODS: This study followed 3 steps. First, 3 researchers used a unified prompt script to generate 10 SBA questions across 4 chatbot platforms. Second, assessors evaluated the chatbot outputs for consistency by identifying similarities and differences between users and across chatbots. With 3 assessors and 10 learning objectives, the maximum possible score for any individual chatbot was 30. Third, 7 assessors internally moderated the questions using a rating scale developed by the research team to evaluate scientific accuracy and educational quality.

      RESULTS: In response to the prompts, all chatbots generated 10 questions each, except Bing, which failed to respond to 1 prompt. ChatGPT-4 exhibited the highest variation in question generation but did not fully satisfy the "cover test." Gemini performed well across most evaluation criteria, except for item balance, and relied heavily on the vignette for answers but showed a preference for one answer option. Bing scored low in most evaluation areas but generated appropriately structured lead-in questions. SBA questions from GPT-3.5, Gemini, and ChatGPT-4 had similar Item Content Validity Index and Scale Level Content Validity Index values, while the Krippendorff alpha coefficient was low (0.016). Bing performed poorly in content clarity, overall validity, and item construction accuracy. A 2-way ANOVA without replication revealed statistically significant differences among chatbots and domains (P<.05). However, the Tukey-Kramer HSD (honestly significant difference) post hoc test showed no significant pairwise differences between individual chatbots, as all comparisons had P values >.05 and overlapping CIs.

      CONCLUSIONS: AI chatbots can aid the production of questions aligned with learning objectives, and individual chatbots have their own strengths and weaknesses. Nevertheless, all require expert evaluation to ensure their suitability for use. Using AI to generate SBA prompts us to reconsider Bloom's taxonomy of the cognitive domain, which traditionally positions creation as the highest level of cognition.
    mergedFrom:
      - source: pubmed
        pmid: "40446330"
        doi: 10.2196/69521
      - source: scopus
        doi: 10.2196/69521
        scopusId: SCOPUS_ID:105008535824
    finalDecision: exclude
  - title: "The Growing Role of Artificial Intelligence in Surgical Education: ChatGPT Undertakes the Australian Generic Surgical Sciences Examination."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on surgical sciences examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT performance on a surgical board examination; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted on simulated examination questions only."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.1111/ans.70186
    pmid: "40444677"
    authors: Guo A, Canagasingham A, Rasiah K, Chalasani V, Mundy J, Chung A
    year: "2025"
    abstract: |-
      BACKGROUND: Large language models have undergone vast development in recent years. The advent of large language models such as ChatGPT may play an important role in enhancing future medical education.

      METHODS: To evaluate the accuracy and performance of ChatGPT in the Generic Surgical Sciences Examination, we constructed a sample examination used to assess ChatGPT. Questions were sourced from a past questions bank and formatted to mirror the structure and layout of the examination. The performance of ChatGPT was assessed based on a predefined answer key recorded earlier.

      RESULTS: ChatGPT scored a total of 468 marks out of a maximum total of 644 marks, scoring a final percentage of 72.7% across all sections tested. ChatGPT performed best in the physiology section, scoring 77.9%, followed by pathology, scoring 75.0%, and scored lowest in the anatomy section with 66.3%. When scoring was analyzed by question type, it was identified that ChatGPT performed best in the type "A" questions (multiple choice), scoring a total of 75%, which was followed closely by its performance in type "X" questions (true or false), where ChatGPT scored 73.2%. However, ChatGPT only scored 43.8% when answering type "B" questions (establishing a relationship between two statements).

      CONCLUSION: Our results demonstrate that ChatGPT completed the Generic Surgical Sciences Examination with accuracy exceeding the required threshold for a pass in this examination. However, the large language model struggled with certain question types and sections. Overall, further research regarding the utility of ChatGPT in surgical education is required, and caution should be exercised with its use, as it remains in its infancy stages.
    mergedFrom:
      - source: pubmed
        pmid: "40444677"
        doi: 10.1111/ans.70186
    finalDecision: exclude
  - title: "Evaluating Large Language Models for Enhancing Radiology Specialty Examination: A Comparative Study with Human Performance."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on radiology specialty examination, standardized exam-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLMs on radiology specialty exam questions for exam design and question difficulty assessment, not about assessment/feedback of clinical performance in workplace settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Specialty exam benchmarking study without workplace-based assessment in clinical training."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1016/j.acra.2025.05.023
    pmid: "40436713"
    authors: Liu H, Chen S, Wang W, Lee C, Hsu H, Shen S, Chiou H, Lee W
    year: "2025"
    abstract: |-
      RATIONALE AND OBJECTIVES: The radiology specialty examination assesses clinical decision-making, image interpretation, and diagnostic reasoning. With the expansion of medical knowledge, traditional test design faces challenges in maintaining accuracy and relevance. Large language models (LLMs) demonstrate potential in medical education. This study evaluates LLM performance in radiology specialty exams, explores their role in assessing question difficulty, and investigates their reasoning processes, aiming to develop a more objective and efficient framework for exam design.

      MATERIALS AND METHODS: This study compared the performance of LLMs and human examinees in a radiology specialty examination. Three LLMs (GPT-4o, o1-preview, and GPT-3.5-turbo-1106) were evaluated under zero-shot conditions. Exam accuracy, examinee accuracy, discrimination index, and point-biserial correlation were used to assess LLMs' ability to predict question difficulty and reasoning processes. The data provided by the Taiwan Radiological Society ensures comparability between AI and human performance.

      RESULTS: As for accuracy, GPT-4o (88.0%) and o1-preview (90.9%) outperformed human examinees (76.3%), whereas GPT-3.5-turbo-1106 showed significantly lower accuracy (50.2%). Question difficulty analysis revealed that newer LLMs excel in solving complex questions, while GPT-3.5-turbo-1106 exhibited greater performance variability. Discrimination index and point-biserial Correlation analyses demonstrated that GPT-4o and o1-preview accurately identified key differentiating questions, closely mirroring human reasoning patterns. These findings suggest that advanced LLMs can assess medical examination difficulty, offering potential applications in exam standardization and question evaluation.

      CONCLUSION: This study evaluated the problem-solving capabilities of GPT-3.5-turbo-1106, GPT-4o, and o1-preview in a radiology specialty examination. LLMs should be utilized as tools for assessing exam question difficulty and assisting in the standardized development of medical examinations.
    mergedFrom:
      - source: pubmed
        pmid: "40436713"
        doi: 10.1016/j.acra.2025.05.023
    finalDecision: exclude
  - title: Impact of generative artificial intelligence on the professional qualities curriculum and reflection essays during physician training.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Discussion/commentary article about the impact of generative AI on reflection essays, not original research presenting empirical data."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/discussion article rather than original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.1111/imj.70087
    pmid: "40434153"
    authors: Wang L
    year: "2025"
    abstract: The professional development curriculum is a vital component in physician training and is important for helping trainees develop as competent and reflective practitioners. The advent of generative artificial intelligence (AI) will likely have a significant impact on the assessment of the professional qualities curriculum in physician training. The development of professional qualities as well as self-reflection is currently assessed through personal reflection essays. These reflection essays are particularly vulnerable to new developments in generative AI, which are capable of producing high-quality outputs that can pass as trainee-created responses. Generative AI-produced reflection essays are becoming increasingly difficult to detect using AI-detection software. The challenge remains how best to incorporate AI into learning while safeguarding the attainment and assessment of learning goals within the multiple domains of professional development. This article discusses several options for safeguarding the professional qualities curriculum in medical education and vocational training. Discussion-based activities may very well prove to be a suitable potential replacement for the current professional reflection essay.
    mergedFrom:
      - source: pubmed
        pmid: "40434153"
        doi: 10.1111/imj.70087
    finalDecision: exclude
  - title: "Exploring the Application Capability of ChatGPT as an Instructor in Skills Education for Dental Medical Students: Randomized Controlled Trial."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in a simulated/laboratory environment (desktop virtual reality for dental skills), not workplace-based."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted in a preclinical simulated/virtual environment without a workplace-based component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.2196/68538
    pmid: "40424023"
    authors: Huang S, Wen C, Bai X, Li S, Wang S, Wang X, Yang D
    year: "2025"
    abstract: |-
      BACKGROUND: Clinical operative skills training is a critical component of preclinical education for dental students. Although technology-assisted instruction, such as virtual reality and simulators, is increasingly being integrated, direct guidance from instructors remains the cornerstone of skill development. ChatGPT, an advanced conversational artificial intelligence model developed by OpenAI, is gradually being used in medical education.

      OBJECTIVE: This study aimed to compare the effects of ChatGPT-assisted skill learning on performance, cognitive load, self-efficacy, learning motivation, and spatial ability, with the aim of evaluating the potential of ChatGPT in clinical operative skills education.

      METHODS: In this study, 187 undergraduate dental students recruited from a first-class university in China were randomly divided into a ChatGPT group and a blank control group. Among them, the control group used videos for skill acquisition, and the ChatGPT group used ChatGPT in addition to the videos. After 1 week of intervention, skills were tested using desktop virtual reality, and cognitive load was measured by recording changes in pupil diameter with an eye tracker. In addition, a spatial ability test was administered to analyze the effect of ChatGPT on those with different spatial abilities. Finally, a questionnaire was also used to assess cognitive load and self-efficacy during the learning process.

      RESULTS: A total of 192 dental undergraduates from a top-tier Chinese university were initially recruited for the experiment by October 25, 2024. Following eye-tracking calibration procedures, 5 participants were excluded, resulting in 187 eligible students successfully completing the experimental protocol by November 2, 2024. Following a short-term intervention administered through randomized allocation, superior performance (ChatGPT group: mean 73.12, SD 10.06; control group: mean 65.54, SD 12.48; P<.001) was observed among participants in the ChatGPT group, along with higher levels of self-efficacy (P=.04) and learning motivation (P=.02). In addition, cognitive load was lower in the ChatGPT group according to eye-tracking measures (ChatGPT group: mean 0.137, SD 0.036; control group: mean 0.312, SD 0.032; P<.001). The analysis of the learning performance of participants with different spatial abilities in the 2 modalities showed that compared to the learners with high spatial abilities (ChatGPT group: mean 76.58, SD 9.23; control group: mean 73.89, SD 11.75; P=.22), those with low spatial abilities (ChatGPT group: mean 70.20, SD 10.71; control group: mean 55.41, SD 13.31; P<.001) were more positively influenced by ChatGPT.

      CONCLUSIONS: ChatGPT has performed outstandingly in assisting dental skill learning, and the study supports the integration of ChatGPT into skills teaching and provides new ideas for modernizing skill teaching.

      TRIAL REGISTRATION: ClinicalTrials.gov NCT06942130；https://clinicaltrials.gov/study/NCT06942130.
    mergedFrom:
      - source: pubmed
        pmid: "40424023"
        doi: 10.2196/68538
    finalDecision: exclude
  - title: "Assessing ChatGPT-4's performance on the US prosthodontic exam: impact of fine-tuning and contextual prompting vs. base knowledge, a cross-sectional study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: US prosthodontic licensing exam, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks base and fine-tuned ChatGPT-4 on prosthodontic exam questions; not about using AI for assessment or feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM performance on mock exam questions rather than workplace-based assessment feedback or analysis of observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.1186/s12909-025-07371-9
    pmid: "40410713"
    authors: Dashti M, Khosraviani F, Azimi T, Hefzi D, Ghasemi S, Fahimipour A, Zare N, Khurshid Z, Habib S
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI), such as ChatGPT-4 from OpenAI, has the potential to transform medical education and assessment. However, its effectiveness in specialized fields like prosthodontics, especially when comparing base to fine-tuned models, remains underexplored. This study evaluates the performance of ChatGPT-4 on the US National Prosthodontic Resident Mock Exam in its base form and after fine-tuning. The aim is to determine whether fine-tuning improves the AI's accuracy in answering specialized questions.

      METHODS: An official sample questions from the 2021 US National Prosthodontic Resident Mock Exam was used, obtained from the American College of Prosthodontists. A total of 150 questions were initially considered, and resources were available for 106 questions. Both the base and fine-tuned models of ChatGPT-4 were tested under simulated exam conditions. Performance was assessed by comparing correct and incorrect responses. The Chi-square test was used to analyze accuracy, with significance set at p < 0.05. The Kappa coefficient was calculated to measure agreement between the models' responses.

      RESULTS: The base model of ChatGPT-4 correctly answered 62.7% of the 150 questions. For the 106 questions with resources, the fine-tuned model answered 73.6% correctly. The Chi-square test showed a significant improvement in performance after fine-tuning (p < 0.001). The Kappa coefficient was 0.39, indicating moderate agreement between the models (p < 0.001). Performance varied by topic, with lower accuracy in areas such as Implant Prosthodontics, Removable Prosthodontics, and Occlusion, though the fine-tuned model consistently outperformed the base model.

      CONCLUSIONS: Fine-tuning ChatGPT-4 with specific resources significantly enhances its accuracy in answering specialized prosthodontic exam questions. While the base model provides a solid baseline, fine-tuning is essential for improving AI performance in specialized fields. However, certain topics may require more targeted training to achieve higher accuracy.
    mergedFrom:
      - source: pubmed
        pmid: "40410713"
        doi: 10.1186/s12909-025-07371-9
    finalDecision: exclude
  - title: "Modern artificial intelligence and large language models in graduate medical education: a scoping review of attitudes, applications & practice."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Broad scoping review of AI in GME covering attitudes, recruitment, board exams, clinical decision-making, and more; not specifically focused on AI for assessment/feedback/analysis of observation records in WBA."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Scoping review (not an original research article, systematic review, meta-analysis, or preprint per protocol)."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.1186/s12909-025-07321-5
    pmid: "40394586"
    authors: Verghese B, Iyer C, Borse T, Cooper S, White J, Sheehy R
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) holds transformative potential for graduate medical education (GME), yet, a comprehensive exploration of AI's applications, perceptions, and limitations in GME is lacking.

      OBJECTIVE: To map the current literature on AI in GME, identifying prevailing perceptions, applications, and research gaps to inform future research, policy discussions, and educational practices through a scoping review.

      METHODS: Following the Joanna Briggs Institute guidelines and the PRISMA-ScR checklist a comprehensive search of multiple databases up to February 2024 was performed to include studies addressing AI interventions in GME.

      RESULTS: Out of 1734 citations, 102 studies met the inclusion criteria, conducted across 16 countries, predominantly from North America (72), Asia (14), and Europe (6). Radiology had the highest number of publications (21), followed by general surgery (11) and emergency medicine (8). The majority of studies were published in 2023. Several key thematic areas emerged from the literature. Initially, perceptions of AI in graduate medical education (GME) were mixed, but have increasingly shifted toward a more favorable outlook, particularly as the benefits of AI integration in education become more apparent. In assessments, AI demonstrated the ability to differentiate between skill levels and offer meaningful feedback. It has also been effective in evaluating narrative comments to assess resident performance. In the domain of recruitment, AI tools have been applied to analyze letters of recommendation, applications, and personal statements, helping identify potential biases and improve equity in candidate selection. Furthermore, large language models consistently outperformed average candidates on board certification and in-training examinations, indicating their potential utility in standardized assessments. Finally, AI tools showed promise in enhancing clinical decision-making by supporting trainees with improved diagnostic accuracy and efficiency.

      CONCLUSIONS: This scoping review provides a comprehensive overview of applications and limitations of AI in GME but is limited with potential biases, study heterogeneity, and evolving nature of AI.
    mergedFrom:
      - source: pubmed
        pmid: "40394586"
        doi: 10.1186/s12909-025-07321-5
    finalDecision: exclude
  - title: "The Performance of AI in Dermatology Exams: The Exam Success and Limits of ChatGPT."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Dermatology exams are standardized examinations, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:20.264000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Exam performance focus is not clearly workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:21.767000+00:00
    doi: 10.1111/jocd.70244
    pmid: "40387311"
    authors: G&#xf6;&#xe7;er G&#xfc;rok N, &#xd6;zt&#xfc;rk S
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence holds significant potential in dermatology.

      OBJECTIVES: This study aimed to explore the potential and limitations of artificial intelligence applications in dermatology education by evaluating ChatGPT's performance on questions from the dermatology residency exam.

      METHOD: In this study, the dermatology residency exam results for ChatGPT versions 3.5 and 4.0 were compared with those of resident doctors across various seniority levels. Dermatology resident doctors were categorized into four seniority levels based on their education, and a total of 100 questions-25 multiple-choice questions for each seniority level-were included in the exam. The same questions were also administered to ChatGPT versions 3.5 and 4.0, and the scores were analyzed statistically.

      RESULTS: ChatGPT 3.5 performed poorly, especially when compared to senior residents. Second (p = 0.038), third (p = 0.041), and fourth-year senior resident physicians (p = 0.020) scored significantly higher than ChatGPT 3.5. ChatGPT 4.0 showed similar performance compared to first- and third-year senior resident physicians, but performed worse in comparison to second (p = 0.037) and fourth-year senior resident physicians (p = 0.029). Both versions scored lower as seniority and exam difficulty increased. ChatGPT 3.5 passed the first and second-year exams but failed the third and fourth-year exams. ChatGPT 4.0 passed the first, second, and third-year exams but failed the fourth-year exam. These findings suggest that ChatGPT was not on par with senior resident physicians, particularly on topics requiring advanced knowledge; however, version 4.0 proved to be more effective than version 3.5.

      CONCLUSION: In the future, as ChatGPT's language support and knowledge of medicine improve, it can be used more effectively in educational processes.
    mergedFrom:
      - source: pubmed
        pmid: "40387311"
        doi: 10.1111/jocd.70244
    finalDecision: exclude
  - title: Using large language models (LLMs) to apply analytic rubrics to score post-encounter notes.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: LLMs score post-encounter notes from OSCEs, which are conducted entirely in simulated environments (standardized patient encounters), not workplace-based."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:51.586000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Study is based on OSCE post-encounter notes from a simulated assessment environment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:58.822000+00:00
    doi: 10.1080/0142159X.2025.2504106
    pmid: "40380943"
    authors: Runyon C
    year: "2025"
    abstract: |-
      BACKGROUND: Large language models (LLMs) show promise in medical education. This study examines LLMs' ability to score post-encounter notes (PNs) from Objective Structured Clinical Examinations (OSCEs) using an analytic rubric. The goal was to evaluate and refine methods for accurate, consistent scoring.

      METHODS: Seven LLMs scored five PNs representing varying levels of performance, including an intentionally incorrect PN. An iterative experimental design tested different prompting strategies and temperature settings, a parameter controlling LLM response creativity. Scores were compared to expected rubric-based results.

      RESULTS: Consistently accurate scoring required multiple rounds of prompt refinement. Simple prompting led to high variability, which improved with structured approaches and low-temperature settings. LLMs occasionally made errors calculating total scores, necessitating external calculation. The final approach yielded consistently accurate scores across all models.

      CONCLUSIONS: LLMs can reliably apply analytic rubrics to PNs with careful prompt engineering and process refinement. This study illustrates their potential as scalable, automated scoring tools in medical education, though further research is needed to explore their use with holistic rubrics. These findings demonstrate the utility of LLMs in assessment practices.
    mergedFrom:
      - source: pubmed
        pmid: "40380943"
        doi: 10.1080/0142159X.2025.2504106
    finalDecision: exclude
  - title: "Artificial Intelligence in Narrative Feedback Analysis for Competency-Based Medical Education: A Review."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: uncertain
        comment: Review of AI/NLP for analyzing narrative feedback in CBME. Covers relevant topic (AI for feedback analysis in medical education) but unclear whether it is a systematic review or narrative review, and whether it focuses on generative AI specifically or broader NLP. Forwarded to full-text screening to verify study type and scope.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Review article not identified as a systematic review or meta-analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article, systematic review, or meta-analysis. Narrative literature review of 8 studies on AI/NLP for analyzing narrative feedback in CBME. Additionally, reviewed studies primarily use traditional NLP/ML rather than generative AI (criterion 1)."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17T07:34:40.555Z
      - decision: exclude
        comment: "Exclusion criterion 1: Review centers on general AI/NLP methods (e.g., conventional NLP, clustering, sentiment analysis) rather than generative AI/LLMs as the core intervention."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17T07:34:43.172Z
      - decision: exclude
        comment: "Exclusion criterion 7: Narrative literature review (8 studies), not SR/MA. Also criterion 1 (conventional NLP/ML, not generative AI). Study design exclusion (Cr.7) takes priority."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.3233/SHTI250637
    pmid: "40380740"
    authors: Khan S, Taiyara J, Zary N, Otaki F
    year: "2025"
    abstract: Competency-Based Medical Education (CBME) generates large volumes of qualitative data in the form of narrative feedback. Traditional qualitative analysis methods face limitations in managing this data's scale and complexity. This review explores the applications, impact, and challenges of Artificial Intelligence (AI), particularly Natural Language Processing (NLP), for analyzing and visualizing medical student performance feedback within CBME contexts. We conducted a comprehensive search of PubMed and Google Scholar databases, identifying key studies that met our inclusion criteria. Our findings highlight how AI can enhance traditional analysis methods by automating theme extraction, reducing educator workload, and improving feedback evaluation processes. The review also addresses challenges associated with AI implementations, including contextual limitations and the need for human oversight. We conclude by emphasizing AI's transformative potential in CBME while identifying critical areas for further research to ensure effective integration into educational workflows.
    mergedFrom:
      - source: pubmed
        pmid: "40380740"
        doi: 10.3233/SHTI250637
    finalDecision: exclude
  - title: "Bridging AI and Medical Expertise: ChatGPT's Success on the Medical Specialization Residency Admission Exam in Spain."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on residency admission examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates ChatGPT performance on the national MIR residency admission exam in Spain; not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Residency admission exam performance is not a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.3233/SHTI250544
    pmid: "40380650"
    authors: Leis A, Mayer M, Mayer A
    year: "2025"
    abstract: The growing use of Artificial Intelligence (AI) in healthcare, particularly focusing on the potential of generative AI models like ChatGPT-4 is a trending topic. The study examines how ChatGPT-4 performed on the national Medicine Residency exam in Spain, a highly selective test for accessing the medical specialization training program called MIR. ChatGPT-4 answered 210 questions, including 25 that required image interpretation. The chatbot correctly answered 150 out of 200 questions, achieving an estimated ranking of around 1900-2300 out of 11,577 candidates. This performance would allow access to most medical specialties in Spain. No significant differences were found between questions requiring image analysis and those that did not, but ChatGPT struggled with more difficult questions, showing a higher error rate for complex problems just like a human being. Despite its potential as an educational and problem-solving tool, the study highlights ChatGPT's limitations, including occasional "AI hallucinations" (incorrect or nonsensical answers) and variability in responses when questions were repeated. The study emphasizes that while AI tools such as ChatGPT can assist in education and medical tasks, they cannot replace qualified healthcare professionals, and their output requires careful verification.
    mergedFrom:
      - source: pubmed
        pmid: "40380650"
        doi: 10.3233/SHTI250544
    finalDecision: exclude
  - title: Response Analysis of Health Care Information Technologists in Japan Using Chat Generative Pretrained Transformer.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are health care information technologists, not medical/health-professional learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Title does not indicate assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.3233/SHTI250499
    pmid: "40380607"
    authors: Ishida K, Hanada E
    year: "2025"
    abstract: This study used the default model of ChatGPT on health care information technologist exams in Japan to evaluate the reliability of this model for medical information knowledge in non-English languages. The average correct response rates for all questions in the health care, information technology, and health information systems fields were 86%, 93% and 81%, respectively. ChatGPT lacked knowledge of standards and laws; therefore, the accuracy rates for related questions were low. Thus, the study concludes that careful attention is required when using ChatGPT, because several of the explanations it provides lack correct descriptions.
    mergedFrom:
      - source: pubmed
        pmid: "40380607"
        doi: 10.3233/SHTI250499
    finalDecision: exclude
  - title: "The Current Landscape of Artificial Intelligence in Plastic Surgery Education and Training: A Systematic Review."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; systematic review focuses on AI performance on exams, teaching assistance, and educational content generation in plastic surgery education, not workplace-based assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on general AI applications in plastic surgery education, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1016/j.jsurg.2025.103519
    pmid: "40378641"
    authors: Genovese A, Borna S, Gomez-Cabello C, Haider S, Prabha S, Trabilsy M, Forte A
    year: "2025"
    abstract: |-
      OBJECTIVE: Artificial intelligence (AI) shows promise in surgery, but its role in plastic surgery education remains underexplored. This review evaluates the current landscape of AI in plastic surgery education.

      DESIGN: A systematic search was conducted on August 11, 2024, across PubMed, CINAHL, IEEE, Scopus, Web of Science, and Google Scholar using terms related to AI, plastic surgery, and education. Original research articles focusing on AI in plastic surgery education were included, excluding correspondence, reviews, book chapters, theses, corrections, and non-peer-reviewed or non-English articles. Two investigators independently screened studies and synthesized data. ROBINS-I was used to assess bias.

      RESULTS: Fifteen studies were included, with 13 evaluating large language models (LLMs) such as ChatGPT, Microsoft Bing, and Google Bard. ChatGPT-4 outperformed other models on In-Service Examinations (average score of 72.7%) and demonstrated potential as a teaching assistant in plastic surgery education. AI-generated personal statements were comparable to human-written ones. However, ChatGPT showed inaccuracies in generating surgical protocols. ChatGPT demonstrated its ability to provide qualitative predictions, forecasting survey results that indicated limited current use of AI in plastic surgery education but support for further AI research. a study combined ChatGPT with DALL-E 2, a generative model, to create acceptable educational images. Machine learning was used in 1 study for evaluating surgical skill and providing real-time feedback during liposuction. Nine studies had low risk of bias, while 6 had moderate risk.

      CONCLUSIONS: AI demonstrates potential as an educational tool in plastic surgery. However, limitations of evidence, such as AI model uncertainties, introduce ambiguity. While AI cannot replicate the expertise of seasoned surgeons, it shows promise for foundational learning and skill assessment. Developing authenticity guidelines and enhancing AI capabilities are essential for its effective, ethical integration into plastic surgery education.
    mergedFrom:
      - source: pubmed
        pmid: "40378641"
        doi: 10.1016/j.jsurg.2025.103519
    finalDecision: exclude
  - title: "Twelve tips: Using generative AI to create and optimize content for virtual patient simulations."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: 'Twelve tips' format is a guidelines/tips article, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Focuses on virtual patient simulation rather than workplace-based assessment context."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    doi: 10.1080/0142159X.2025.2501252
    pmid: "40372387"
    authors: Moser M, Posel N, Ganescu O, Fleiszer D
    year: "2025"
    abstract: Generative Artificial Intelligence (GenAI) is increasingly being used in medical education, including the creation of content for clinical virtual patient simulation (VPS). We share practical insights drawn from our experience as medical educators, designed to help other educators integrate GenAI tools such as ChatGPT, Gemini, or Claude. We provide 12 practical tips that illustrate how GenAI can be used for the creation of content for VPS to streamline case development and increase realism for our learners. GenAI saves resources of time and expense by streamlining case creation, including creating patient images, clinical reports, and assessment questions. Clinical content experts can now concentrate on defining the case focus and on final-stage review, improving efficiency. It simplifies the development of effective scoring rubrics and meaningful formative feedback, enhancing learner engagement and clinical reasoning. By reducing the time needed to create VPS content, GenAI can allow educators to focus more on educational objectives and feedback to programs and learners. While GenAI is new and has limitations, these are likely to be addressed in the near future. As GenAI evolves, it will continue to transform VPS through improved realism, streamlined case creation, and more informative feedback, further supporting its role in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "40372387"
        doi: 10.1080/0142159X.2025.2501252
    finalDecision: exclude
  - title: "AI usage among medical students in Palestine: a cross-sectional study and demonstration of AI-assisted research workflows."
    reviews:
      - decision: uncertain
        comment: AI usage by medical students; could touch on assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on AI usage/research workflows, not assessment, feedback, or analysis of observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Survey of AI usage patterns and perceptions among medical students."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Describes general AI use and perceptions, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.1186/s12909-025-07272-x
    pmid: "40355851"
    authors: Yousef M, Deeb S, Alhashlamon K
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial Intelligence (AI) is transforming medical education globally, offering solutions to challenges such as resource limitations and limited clinical exposure. However, its integration in resource-constrained settings like Palestine remains underexplored. This study evaluates the prevalence, impact, and challenges of AI use among Palestinian medical students, focusing on academic performance, clinical competence, and research productivity.

      METHODS: A cross-sectional study was conducted among 590 medical students from Palestinian universities. Data were collected using a validated electronic questionnaire, covering demographics, AI usage patterns, and perceived impacts across academic, clinical, and research domains. Initial analysis was conducted using AI tools, specifically ChatGPT, to facilitate insights and structure the results effectively. Statistical analyses were performed using IBM SPSS v27 to validate findings. Statistical significance was set at p < 0.05. The draft underwent detailed reviews by the research team to confirm accuracy and validity.

      RESULTS: AI adoption was high, with 87% of students frequently using tools like ChatGPT (76%) and virtual simulators (26%). Students reported significant improvements in academic performance (mean score: 4.2, SD = 0.7) and research productivity (mean score: 4.5, SD = 0.6), particularly in literature reviews and data analysis. Clinical competence received moderate ratings (mean score: 3.6, SD = 0.8), reflecting AI's limited role in practical skill development. Time management was highly rated (mean score: 4.6, SD = 0.5), highlighting AI's ability to automate repetitive tasks. Challenges included ethical concerns, data accuracy, and limited AI literacy, with 91% lacking formal AI training.

      CONCLUSION: AI demonstrates significant potential to enhance medical education in resource-constrained settings by improving academic outcomes and research efficiency. ChatGPT played a critical role in this study, not only as a tool used by participants but also in the research process itself, including data analysis and manuscript drafting. These findings were cross-verified using SPSS to ensure robustness. Despite its promise, limitations in practical clinical applications and technical understanding highlight the need for targeted AI literacy programs and ethical guidelines. This study underscores the importance of integrating AI into medical curricula to address existing gaps and maximize its benefits in similar global contexts.
    mergedFrom:
      - source: pubmed
        pmid: "40355851"
        doi: 10.1186/s12909-025-07272-x
    finalDecision: exclude
  - title: "Performance of large language models on Thailand’s national medical licensing examination: a cross-sectional study."
    reviews:
      - decision: uncertain
        comment: LLMs on national licensing exam; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates LLM performance on Thailand's national medical licensing exam, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Licensing exam preparation context without a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.3352/jeehp.2025.22.16
    pmid: "40354784"
    authors: Saowaprut P, Wabina R, Yang J, Siriwat L
    year: "2025"
    abstract: |-
      PURPOSE: This study aimed to evaluate the feasibility of general-purpose large language models (LLMs) in addressing inequities in medical licensure exam preparation for Thailand’s National Medical Licensing Examination (ThaiNLE), which currently lacks standardized public study materials.

      METHODS: We assessed 4 multi-modal LLMs (GPT-4, Claude 3 Opus, Gemini 1.0/1.5 Pro) using a 304-question ThaiNLE Step 1 mock examination (10.2% image-based), applying deterministic API configurations and 5 inference repetitions per model. Performance was measured via micro- and macro-accuracy metrics compared against historical passing thresholds.

      RESULTS: All models exceeded passing scores, with GPT-4 achieving the highest accuracy (88.9%; 95% confidence interval, 88.7–89.1), surpassing Thailand’s national average by more than 2 standard deviations. Claude 3.5 Sonnet (80.1%) and Gemini 1.5 Pro (72.8%) followed hierarchically. Models demonstrated robustness across 17 of 20 medical domains, but variability was noted in genetics (74.0%) and cardiovascular topics (58.3%). While models demonstrated proficiency with images (Gemini 1.0 Pro: +9.9% vs. text), text-only accuracy remained superior (GPT4o: 90.0% vs. 82.6%).

      CONCLUSION: General-purpose LLMs show promise as equitable preparatory tools for ThaiNLE Step 1. However, domain-specific knowledge gaps and inconsistent multi-modal integration warrant refinement before clinical deployment.
    mergedFrom:
      - source: pubmed
        pmid: "40354784"
        doi: 10.3352/jeehp.2025.22.16
    finalDecision: exclude
  - title: "Comparative analysis of LLMs performance in medical embryology: A cross-platform study of ChatGPT, Claude, Gemini, and Copilot."
    reviews:
      - decision: uncertain
        comment: LLMs on medical embryology questions; abstract needed to determine assessment context
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks LLMs on USMLE-style embryology MCQs; not about using AI for assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Cross-platform benchmarking on embryology MCQs is not about workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1002/ase.70044
    pmid: "40350555"
    authors: Bolgova O, Ganguly P, Mavrych V
    year: "2025"
    abstract: "Integrating artificial intelligence, particularly large language models (LLMs), into medical education represents a significant new step in how medical knowledge is accessed, processed, and evaluated. The objective of this study was to conduct a comprehensive analysis comparing the performance of advanced LLM chatbots in different topics of medical embryology courses. Two hundred United States Medical Licensing Examination (USMLE)-style multiple-choice questions were selected from the course exam database and distributed across 20 topics. The results of 3 attempts by GPT-4o, Claude, Gemini, Copilot, and GPT-3.5 to answer the assessment items were evaluated. Statistical analyses included intraclass correlation coefficients for reliability, one-way and two-way mixed ANOVAs for performance comparisons, and post hoc analyses. Effect sizes were calculated using Cohen's f and eta-squared (η2). On average, the selected chatbots correctly answered 78.7% ± 15.1% of the questions. GPT-4o and Claude performed best, correctly answering 89.7% and 87.5% of the questions, respectively, without a statistical difference in their performance (p = 0.238). The performance of other chatbots was significantly lower (p < 0.01): Copilot (82.5%), Gemini (74.8%), and GPT-3.5 (59.0%). Test-retest reliability analysis showed good reliability for GPT-4o (ICC = 0.803), Claude (ICC = 0.865), and Gemini (ICC = 0.876), with moderate reliability for Copilot and GPT-3.5. This study suggests that AI models like GPT-4o and Claude show promise for providing tailored embryology instruction, though instructor verification remains essential."
    mergedFrom:
      - source: pubmed
        pmid: "40350555"
        doi: 10.1002/ase.70044
    finalDecision: exclude
  - title: Analysis of ChatGPT-4's performance on ophthalmology questions from the MIR exam.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: MIR examination questions, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT-4 performance on ophthalmology MIR exam questions; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1016/j.oftale.2025.05.002
    pmid: "40350063"
    authors: Monera Lucas C, Mora Caballero C, Escolano Serrano J, Machan A, Castilla Mart&#xed;nez G, Romero Valero D, Campello Lluch J
    year: "2025"
    abstract: |-
      PURPOSE: To evaluate the performance of ChatGPT in solving clinical scenarios in ophthalmology, specifically questions from the specialty exams for Resident Medical Interns (MIR).

      DESIGN: Cross-sectional design for evaluating a diagnostic tool.

      METHOD: Ophthalmology questions from the MIR exams from the 2010-2023 sessions were collected. The performance of ChatGPT in successfully answering the questions was calculated. The results were also compared with those obtained by ophthalmology professionals. Additionally, sensitivity, specificity, and positive and negative probability coefficients were calculated.

      RESULTS: A total of 54 questions were collected, with those from the subspecialty "Retina" being the most frequent. ChatGPT's overall score was 90.2%, with a sensitivity of 92.59% and a specificity of 96.8%. The average concordance with the evaluators' answers was 86.41%. The agreement between the evaluators was 79.62%.

      CONCLUSIONS: ChatGPT-4 is a useful tool for solving clinical scenarios and theoretical questions in ophthalmology. Proper use of the tool, supervised by professionals, can help optimize the care processes for ophthalmology patients.
    mergedFrom:
      - source: pubmed
        pmid: "40350063"
        doi: 10.1016/j.oftale.2025.05.002
    finalDecision: exclude
  - title: "Chatgpt vs traditional pedagogy: a comparative study in urological learning."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; compares learning methods (ChatGPT vs lecture) for medical students in a classroom setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Classroom teaching intervention, not a workplace-based or clinical education assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.1007/s00345-025-05654-w
    pmid: "40338279"
    authors: Digiacomo A, Orsini A, Cicchetti R, Spadano L, De Santis S, Di Sessa L, Vitale M, Di Nicola M, Tamborino F, Basconi M, De Archangelis R, Salzano G, Dello Stritto G, Lannutti P, Schips L, Marchioni M
    year: "2025"
    abstract: |-
      PURPOSE: Technological evolution is radically changing medical learning models. We evaluated the learning outcomes of urological concepts using ChatGPT, traditional lecture and combined approach.

      METHODS: We conducted a randomized triple-blind study on 121 medical students with no previous formal curriculum in urology. Students were randomly divided into three study classes with different learning methods: ChatGPT, Lecture and ChatGPT + Lecture. The "adrenal glands" were randomly extracted as the subject of the lessons. Students were evaluated using a thirty-question test.

      RESULTS: The evaluation test median score was higher for students who underwent ChatGPT + Lecture compared with those who had only ChatGPT (10 vs. 12, p = 0.007). Such differences remained statistically significant also in multivariable models adjusting according to year of course, gender and previous ChatGPT experience (estimate: 2.6, p-value = 0.002). For most of the questions (about 70%), the proportion of students correctly answering was higher in the ChatGPT + Lecture learning groups than in the other groups.

      CONCLUSION: ChatGPT loses its potential if used without a previous background. The limits of scientific reliability persist and a teacher-guided method is still essential. ChatGPT + traditional lecture gives more effective results than the single traditional lecture also allowing a better use of the chatbot.
    mergedFrom:
      - source: pubmed
        pmid: "40338279"
        doi: 10.1007/s00345-025-05654-w
    finalDecision: exclude
  - title: Exploring the role of artificial intelligence in Turkish orthopedic progression exams.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarking AI models on orthopedic specialty exam questions; not about using AI for assessment/feedback of learners in workplace settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM answers on progression exam questions rather than workplace-based assessment or feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.5152/j.aott.2025.24090
    pmid: "40337975"
    authors: Ayik G, Kolac U, Aksoy T, Yilmaz A, Sili M, Tokgozoglu M, Huri G
    year: "2025"
    abstract: |-
      OBJECTIVE: The aim of this study was to evaluate and compare the performance of the artificial intelligence (AI) models ChatGPT-3.5, ChatGPT-4, and Gemini on the Turkish Specialization Training and Development Examination (UEGS) to determine their utility in medical education and their potential to improve patient care.

      METHODS: This retrospective study analyzed responses of ChatGPT-3.5, ChatGPT-4, and Gemini to 1000 true or false questions from UEGS administered over 5 years (2018-2023). Questions, encompassing 9 orthopedic subspecialties, were categorized by 2 independent residents, with discrepancies resolved by a senior author. Artificial intelligence models were restarted for each query to prevent data retention. Performance was evaluated by calculating net scores and comparing them to orthopedic resident scores obtained from the Turkish Orthopedics and Traumatology Education Council (TOTEK) database. Statistical analyses included chi-squared tests, Bonferroni-adjusted Z tests, Cochran's Q test, and receiver operating characteristic (ROC) analysis to determine the optimal question length for AI accuracy. All AI responses were generated independently without retaining prior information.

      RESULTS: Significant di!erences in AI tool accuracy were observed across di!erent years and subspecialties (P < .001). ChatGPT-4 consistently outperformed other models, achieving the highest overall accuracy (95% in specific subspecialties). Notably, ChatGPT-4 demonstrated superior performance in Basic and General Orthopedics and Foot and Ankle Surgery, while Gemini and ChatGPT-3.5 showed variability in accuracy across topics and years. Receiver operating characteristic analysis revealed a significant relationship between shorter letter counts and higher accuracy for ChatGPT-4 (P=.002). ChatGPT-4 showed significant negative correlations between letter count and accuracy across all years (r="0.099, P=.002), outperformed residents in basic and general orthopedics (P=.015) and trauma (P=.012), unlike other AI models.

      CONCLUSION: The findings underscore the advancing role of AI in the medical field, with ChatGPT-4 demonstrating significant potential as a tool for medical education and clinical decision-making. Continuous evaluation and refinement of AI technologies are essential to enhance their educational and clinical impact.
    mergedFrom:
      - source: pubmed
        pmid: "40337975"
        doi: 10.5152/j.aott.2025.24090
    finalDecision: exclude
  - title: "Comparing ChatGPT-4 and a Paediatric Intensive Care Specialist in Responding to Medical Education Questions: A Multicenter Evaluation."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Compares AI and specialist responses to educational questions from a trainee forum."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on answering educational questions, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.1111/jpc.70080
    pmid: "40331496"
    authors: Yitzhaki S, Peled N, Kaplan E, Kadmon G, Nahum E, Gendler Y, Weissbach A
    year: "2025"
    abstract: |-
      OBJECTIVE: To compare the performance of the Generative Pre-trained Transformer model 4 (ChatGPT-4) with that of a paediatric intensive care unit (PICU) specialist in responding to open-ended medical education questions.

      METHODS: A comparative analysis was conducted using 100 educational questions sourced from a PICU trainee WhatsApp forum, covering factual knowledge and clinical reasoning. Ten PICU specialists from multiple tertiary paediatric centres independently evaluated 20 sets of paired responses from ChatGPT-4 and a PICU specialist (the original respondent to the forum questions), assessing overall superiority, completeness, accuracy, and integration potential.

      RESULTS: After excluding one question requiring a visual aid, 198 paired evaluations were made (96 factual knowledge and 102 clinical reasoning). ChatGPT-4's responses were significantly longer than those of the PICU specialist (median words: 189 vs. 41; p < 0.0001). ChatGPT-4 was preferred in 60% of factual knowledge comparisons (p < 0.001), while the PICU specialist's responses were preferred in 67% of clinical reasoning comparisons (p < 0.0001). ChatGPT-4 demonstrated superior completeness in factual knowledge (p = 0.02) but lower accuracy in clinical reasoning (p < 0.0001). Integration of both answers was favoured in 37% of cases (95% CI, 31%-44%).

      CONCLUSIONS: ChatGPT-4 shows promise as a tool for factual medical education in the PICU, excelling in completeness. However, it requires oversight in clinical reasoning tasks, where the PICU specialist's responses remain superior. Expert review is essential before using ChatGPT-4 independently in PICU education and in other similarly underexplored medical fields.
    mergedFrom:
      - source: pubmed
        pmid: "40331496"
        doi: 10.1111/jpc.70080
    finalDecision: exclude
  - title: "The answer may vary: large language model response patterns challenge their use in test item analysis."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; investigates LLM utility for predicting psychometric properties of anesthesiology MCQs, not workplace-based assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLMs for MCQ test-item psychometrics, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1080/0142159X.2025.2497891
    pmid: "40319392"
    authors: Buhl L
    year: "2025"
    abstract: |-
      INTRODUCTION: The validation of multiple-choice question (MCQ)-based assessments typically requires administration to a test population, which is resource-intensive and practically demanding. Large language models (LLMs) are a promising tool to aid in many aspects of assessment development, including the challenge of determining the psychometric properties of test items. This study investigated whether LLMs could predict the difficulty and point biserial indices of MCQs, potentially alleviating the need for preliminary analysis in a test population.

      METHODS: Sixty MCQs developed by subject matter experts in anesthesiology were presented one hundred times each to five different LLMs (ChatGPT-4o, o1-preview, Claude 3.5 Sonnet, Grok-2, and Llama 3.2) and to clinical fellows. Response patterns were analyzed, and difficulty indices (proportion of correct responses) and point biserial indices (item-test score correlation) were calculated. Spearman correlation coefficients were used to compare difficulty and point biserial indices between the LLMs and fellows.

      RESULTS: Marked differences in response patterns were observed among LLMs: ChatGPT-4o, o1-preview, and Grok-2 showed variable responses across trials, while Claude 3.5 Sonnet and Llama 3.2 gave consistent responses. The LLMs outperformed fellows with mean scores of 58% to 85% compared to 57% for the fellows. Three LLMs showed a weak correlation with fellow difficulty indices (r = 0.28-0.29), while the two highest scoring models showed no correlation. No LLM predicted the point biserial indices.

      DISCUSSION: These findings suggest LLMs have limited utility in predicting MCQ performance metrics. Notably, higher-scoring models showed less correlation with human performance, suggesting that as models become more powerful, their ability to predict human performance may decrease. Understanding the consistency of an LLM's response pattern is critical for both research methodology and practical applications in test development. Future work should focus on leveraging the language-processing capabilities of LLMs for overall assessment optimization (e.g., inter-item correlation) rather than predicting item characteristics.
    mergedFrom:
      - source: pubmed
        pmid: "40319392"
        doi: 10.1080/0142159X.2025.2497891
    finalDecision: exclude
  - title: Is AI the future of evaluation in medical education?? AI vs. human evaluation in objective structured clinical examination.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: AI evaluation in OSCE, which is a simulated assessment environment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    doi: 10.1186/s12909-025-07241-4
    pmid: "40312328"
    authors: Tekin M, Yurdal M, Toraman &, Korkmaz G, Uysal &
    year: "2025"
    abstract: |-
      BACKGROUND: Objective Structured Clinical Examinations (OSCEs) are widely used in medical education to assess students' clinical and professional skills. Recent advancements in artificial intelligence (AI) offer opportunities to complement human evaluations. This study aims to explore the consistency between human and AI evaluators in assessing medical students' clinical skills during OSCE.

      METHODS: This cross-sectional study was conducted at a state university in Turkey, focusing on pre-clinical medical students (Years 1, 2, and 3). Four clinical skills-intramuscular injection, square knot tying, basic life support, and urinary catheterization-were evaluated during OSCE at the end of the 2023-2024 academic year. Video recordings of the students' performances were assessed by five evaluators: a real-time human assessor, two video-based expert human assessors, and two AI-based systems (ChatGPT-4o and Gemini Flash 1.5). The evaluations were based on standardized checklists validated by the university. Data were collected from 196 students, with sample sizes ranging from 43 to 58 for each skill. Consistency among evaluators was analyzed using statistical methods.

      RESULTS: AI models consistently assigned higher scores than human evaluators across all skills. For intramuscular injection, the mean total score given by AI was 28.23, while human evaluators averaged 25.25. For knot tying, AI scores averaged 16.07 versus 10.44 for humans. In basic life support, AI scores were 17.05 versus 16.48 for humans. For urinary catheterization, mean scores were similar (AI: 26.68; humans: 27.02), but showed considerable variance in individual criteria. Inter-rater consistency was higher for visually observable steps, while auditory tasks led to greater discrepancies between AI and human evaluators.

      CONCLUSIONS: AI shows promise as a supplemental tool for OSCE evaluation, especially for visually based clinical skills. However, its reliability varies depending on the perceptual demands of the skill being assessed. The higher and more uniform scores given by AI suggest potential for standardization, yet refinement is needed for accurate assessment of skills requiring verbal communication or auditory cues.
    mergedFrom:
      - source: pubmed
        pmid: "40312328"
        doi: 10.1186/s12909-025-07241-4
    finalDecision: exclude
  - title: "Diagnostic accuracy of ChatGPT-4 in orthopedic oncology: a comparative study with residents."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluating ChatGPT-4 diagnostic accuracy on histology slides vs. residents; this is about AI diagnostic capability, not about using AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares diagnostic interpretation accuracy on histology slides, not assessment/feedback or analysis of workplace clinical observation records for learners."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.1016/j.knee.2025.04.004
    pmid: "40311171"
    authors: Baker H, Aggarwal S, Kalidoss S, Hess M, Haydon R, Strelzow J
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) is increasingly being explored for its potential role in medical diagnostics. ChatGPT-4, a large language model (LLM) with image analysis capabilities, may assist in histopathological interpretation, but its accuracy in musculoskeletal oncology remains untested. This study evaluates ChatGPT-4's diagnostic accuracy in identifying musculoskeletal tumors from histology slides compared to orthopedic surgery residents.

      METHODS: A comparative study was conducted using 24 histology slides randomly selected from an orthopedic oncology registry. Five teams of orthopedic surgery residents (PGY-1 to PGY-5) participated in a diagnostic competition, providing their best diagnosis for each slide. ChatGPT-4 was tested separately using identical histology images and clinical vignettes, with two independent attempts. Statistical analyses, including one-way ANOVA and independent t-tests were performed to compare diagnostic accuracy.

      RESULTS: Orthopedic residents significantly outperformed ChatGPT-4 in diagnosing musculoskeletal tumors. The mean diagnostic accuracy among resident teams was 55%, while ChatGPT-4 achieved 25% on its first attempt and 33% on its second attempt. One-way ANOVA revealed a significant difference in accuracy across groups (F = 8.51, p = 0.033). Independent t-tests confirmed that residents performed significantly better than ChatGPT-4 (t = 5.80, p = 0.0004 for first attempt; t = 4.25, p = 0.0028 for second attempt). Both residents and ChatGPT-4 struggled with specific cases, particularly soft tissue sarcomas.

      CONCLUSIONS: ChatGPT-4 demonstrated limited accuracy in interpreting histopathological slides compared to orthopedic residents. While AI holds promise for medical diagnostics, its current capabilities in musculoskeletal oncology remain insufficient for independent clinical use. These findings should be viewed as exploratory rather than confirmatory, and further research with larger, more diverse datasets is needed to assess AI's role in histopathology. Future studies should investigate AI-assisted workflows, refine prompt engineering, and explore AI models specifically trained for histopathological diagnosis.
    mergedFrom:
      - source: pubmed
        pmid: "40311171"
        doi: 10.1016/j.knee.2025.04.004
    finalDecision: exclude
  - title: "Applications of Natural Language Processing in Otolaryngology: A Scoping Review."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses on general NLP, not clearly generative AI/LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Broad scoping review of NLP applications in otolaryngology; not specifically about assessment, feedback, or analysis of observation records in workplace-based assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Scoping review of broad NLP applications in otolaryngology, not focused on assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1002/lary.32198
    pmid: "40309961"
    authors: Banyi N, Ma B, Amanian A, Bur A, Abdalkhani A
    year: "2025"
    abstract: |-
      OBJECTIVE: To review the current literature on the applications of natural language processing (NLP) within the field of otolaryngology.

      DATA SOURCES: MEDLINE, EMBASE, SCOPUS, Cochrane Library, Web of Science, and CINAHL.

      METHODS: The preferred reporting Items for systematic reviews and meta-analyzes extension for scoping reviews checklist was followed. Databases were searched from the date of inception up to Dec 26, 2023. Original articles on the application of language-based models to otolaryngology patient care and research, regardless of publication date, were included. The studies were classified under the 2011 Oxford CEBM levels of evidence.

      RESULTS: One-hundred sixty-six papers with a median publication year of 2024 (range 1982, 2024) were included. Sixty-one percent (102/166) of studies used ChatGPT and were published in 2023 or 2024. Sixty studies used NLP for clinical education and decision support, 42 for patient education, 14 for electronic medical record improvement, 5 for triaging, 4 for trainee education, 4 for patient monitoring, 3 for telemedicine, and 1 for medical translation. For research, 37 studies used NLP for extraction, classification, or analysis of data, 17 for thematic analysis, 5 for evaluating scientific reporting, and 4 for manuscript preparation.

      CONCLUSION: The role of NLP in otolaryngology is evolving, with ChatGPT passing OHNS board simulations, though its clinical application requires improvement. NLP shows potential in patient education and post-treatment monitoring. NLP is effective at extracting data from unstructured or large data sets. There is limited research on NLP in trainee education and administrative tasks. Guidelines for NLP use in research are critical.
    mergedFrom:
      - source: pubmed
        pmid: "40309961"
        doi: 10.1002/lary.32198
      - source: scopus
        doi: 10.1002/lary.32198
        scopusId: SCOPUS_ID:105004215037
    finalDecision: exclude
  - title: "Large language models for generating script concordance test in obstetrics and gynecology: ChatGPT and Claude."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: About generating SCT items for assessing clinical reasoning, not about assessment/feedback/analysis of clinical observation records in workplace settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on generating SCT exam items, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.1080/0142159X.2025.2497888
    pmid: "40305090"
    authors: Yap&#x131;c&#x131; Co&#x15f;kun Z, K&#x131;yak Y, Co&#x15f;kun &, Budako&#x11f;lu I, &#xd6;zdemir &
    year: "2025"
    abstract: |-
      OBJECTIVE: To evaluate the performance of large language models (ChatGPT-4o and Claude 3.5 Sonnet) to generate script concordance test (SCT) items for assessing clinical reasoning in obstetrics and gynecology.

      METHODS: This cross-sectional study involved the generation of SCT items for five common diagnostic topics in obstetrics and gynecology in primary care settings. A total of 16 panelists evaluated the AI-generated SCT items against 11 predefined criteria. Descriptive statistics were used to compare the models' performance across criteria.

      RESULTS: ChatGPT-4o had an overall agreement rate of 90.57% for SCT items meeting the quality criteria, while Claude 3.5 Sonnet achieved 91.48%. The criterion with the lowest scores was "The scenario is of appropriate difficulty for medical students," with ChatGPT-4o rated at 71.25% and Claude 3.5 Sonnet at 76.25%.

      CONCLUSION: Large language models can generate SCT items that effectively assess clinical reasoning; however, further refinement is required to ensure the appropriate level of difficulty for medical students. These findings highlight the potential of AI to enhance the efficiency of SCT generation in obstetrics and gynecology within primary care settings.
    mergedFrom:
      - source: pubmed
        pmid: "40305090"
        doi: 10.1080/0142159X.2025.2497888
    finalDecision: exclude
  - title: Performance of GPT-4 Turbo and GPT-4o in Korean Society of Radiology In-Training Examinations.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: GPT performance on radiology in-training examinations, standardized exam-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks GPT-4 Turbo and GPT-4o performance on radiology in-training exam questions; does not use generative AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: In-training exam accuracy study, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.3348/kjr.2024.1096
    pmid: "40288896"
    authors: Choi A, Kim H, Choi M, Ramasamy S, Kim Y, Jung S
    year: "2025"
    abstract: |-
      OBJECTIVE: Despite the potential of large language models for radiology training, their ability to handle image-based radiological questions remains poorly understood. This study aimed to evaluate the performance of the GPT-4 Turbo and GPT-4o in radiology resident examinations, to analyze differences across question types, and to compare their results with those of residents at different levels.

      MATERIALS AND METHODS: A total of 776 multiple-choice questions from the Korean Society of Radiology In-Training Examinations were used, forming two question sets: one originally written in Korean and the other translated into English. We evaluated the performance of GPT-4 Turbo (gpt-4-turbo-2024-04-09) and GPT-4o (gpt-4o-2024-11-20) on these questions with the temperature set to zero, determining the accuracy based on the majority vote from five independent trials. We analyzed their results using the question type (text-only vs. image-based) and benchmarked them against nationwide radiology residents' performance. The impact of the input language (Korean or English) on model performance was examined.

      RESULTS: GPT-4o outperformed GPT-4 Turbo for both image-based (48.2% vs. 41.8%, P = 0.002) and text-only questions (77.9% vs. 69.0%, P = 0.031). On image-based questions, GPT-4 Turbo and GPT-4o showed comparable performance to that of 1st-year residents (41.8% and 48.2%, respectively, vs. 43.3%, P = 0.608 and 0.079, respectively) but lower performance than that of 2nd- to 4th-year residents (vs. 56.0%-63.9%, all P ≤ 0.005). For text-only questions, GPT-4 Turbo and GPT-4o performed better than residents across all years (69.0% and 77.9%, respectively, vs. 44.7%-57.5%, all P ≤ 0.039). Performance on the English- and Korean-version questions showed no significant differences for either model (all P ≥ 0.275).

      CONCLUSION: GPT-4o outperformed the GPT-4 Turbo in all question types. On image-based questions, both models' performance matched that of 1st-year residents but was lower than that of higher-year residents. Both models demonstrated superior performance compared to residents for text-only questions. The models showed consistent performances across English and Korean inputs.
    mergedFrom:
      - source: pubmed
        pmid: "40288896"
        doi: 10.3348/kjr.2024.1096
    finalDecision: exclude
  - title: "AI-generated questions for urological competency assessment: a prospective educational study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: uncertain
        comment: AI generates MCQs used during clinical urology rotations to assess intern competency; workplace-based context and validity data present, but unclear if this constitutes WBA of clinical performance vs. knowledge testing during rotations. Forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Uses LLMs to generate exam questions, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. AI (ChatGPT/Gemini) generated MCQs used as knowledge tests administered at three time points during a urology clinical rotation. The assessment itself is a written MCQ exam, not a workplace-based assessment involving direct observation of clinical performance. WBA requires observation of learners in clinical practice; MCQ exams are knowledge tests regardless of when administered."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:52.271000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on AI-generated multiple-choice exam questions, not feedback/assessment of clinical observation records."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:00.184000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: AI generates MCQ exam questions, not assessment/feedback of clinical observation. Even if administered during rotation, MCQ knowledge tests are not WBA."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1186/s12909-025-07202-x
    pmid: "40281621"
    authors: Ba&#x15f;arano&#x11f;lu M, Akbay E, Erdem E
    year: "2025"
    abstract: |-
      BACKGROUND: The integration of artificial intelligence (AI) in medical education assessment remains largely unexplored, particularly in specialty-specific evaluations during clinical rotations. Traditional question development methods are time-intensive and often struggle to keep pace with evolving medical knowledge. This study evaluated the effectiveness of AI-generated questions in assessing urological competency among medical interns during a standardized clinical rotation.

      METHODS: Two state-of-the-art AI language models (ChatGPT and Gemini) generated 300 multiple-choice questions across six urological subspecialties. Seven experienced urologists, each with over 10 years of clinical practice and active involvement in resident training programs, independently evaluated the questions using a modified Delphi approach with standardized scoring rubrics. The evaluation criteria encompassed technical accuracy based on current clinical guidelines, clinical relevance to core rotation objectives, construct validity assessed through cognitive task analysis, and alignment with rotation objectives. Questions achieving consensus approval from at least five experts were retained, resulting in 100 validated questions.

      RESULTS: From the initial cohort of 45 eligible interns, 42 completed all three assessment points (93.3% completion rate). Performance improved significantly from baseline (mean: 45.2%, 95% CI: 42.6-47.8%) through mid-rotation (mean: 62.8%, 95% CI: 60.4-65.2%) to final assessment (mean: 78.4%, 95% CI: 76.5-80.3%). The technical accuracy was comparable between AI platforms (ChatGPT: 84.3%, Gemini: 83.8%, p = 0.86). Clinical scenario questions demonstrated better discrimination than recall questions (mean indices: 0.28 vs. 0.14, p < 0.001). Subspecialty performance varied, with highest scores in uro-oncology (mean: 82.6%, 95% CI: 80.2-85.0%) and endourology (mean: 79.4%, 95% CI: 77.0-81.8%).

      CONCLUSIONS: AI-generated questions showed appropriate technical accuracy and difficulty levels for assessing clinical competency in urology. While promising for formative assessment, particularly with clinical scenarios, current limitations in discrimination capability suggest careful consideration for high-stakes testing. The strong correlation between clinical exposure and improved performance validates their effectiveness in measuring knowledge acquisition. These findings support the potential integration of AI-generated questions in specialty-specific assessment, though careful implementation with expert oversight and continuous validation remains essential.

      CLINICAL TRIAL NUMBER: Not applicable.
    mergedFrom:
      - source: pubmed
        pmid: "40281621"
        doi: 10.1186/s12909-025-07202-x
    finalDecision: exclude
  - title: "The role of artificial intelligence in medical education: an evaluation of Large Language Models (LLMs) on the Turkish Medical Specialty Training Entrance Exam."
    reviews:
      - decision: uncertain
        comment: LLMs on specialty training exam; abstract may reveal WBA-relevant elements
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks LLM accuracy on Turkish medical specialty training entrance exam questions; not about assessment or feedback of learner performance."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Specialty entrance-exam evaluation does not address workplace-based assessment or analysis of observed clinical performance."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1186/s12909-025-07148-0
    pmid: "40281510"
    authors: Ko&#xe7;ak M, O&#x11f;uz A, Ak&#xe7;al&#x131; Z
    year: "2025"
    abstract: |-
      OBJECTIVE: To evaluate the performance of advanced large language models (LLMs)-OpenAI-ChatGPT 4, Google AI-Gemini 1.5 Pro, Cohere-Command R + and Meta AI-Llama 3 70B on questions from the Turkish Medical Specialty Training Entrance Exam (2021, 1st semester) and analyze their answers for user interpretability in languages other than English.

      METHODS: The study used questions from the Basic Medical Sciences and Clinical Medical Sciences exams of the Turkish Medical Specialty Training Entrance Exam held on March 21, 2021. The 240 questions were presented to the LLMs in Turkish, and their responses were evaluated based on the official answers published by the Student Selection and Placement Centre.

      RESULTS: ChatGPT 4 was the best-performing model with an overall accuracy of 88.75%. Llama 3 70B followed closely with 79.17% accuracy. Gemini 1.5 Pro achieved 78.13% accuracy, while Command R + lagged with 50% accuracy. ChatGPT 4 demonstrated strengths in both basic and clinical medical science questions. Performance varied across question difficulties, with ChatGPT 4 maintaining high accuracy even on the most challenging questions.

      CONCLUSIONS: GPT-4 and Llama 3 70B achieved satisfactory results on the Turkish Medical Specialty Training Entrance Exam, demonstrating their potential as safe sources for basic medical sciences and clinical medical sciences knowledge in languages other than English. These LLMs could be valuable resources for medical education and clinical support in non-English speaking areas. However, Gemini 1.5 Pro and Command R + show potential but need significant improvement to compete with the best-performing models.
    mergedFrom:
      - source: pubmed
        pmid: "40281510"
        doi: 10.1186/s12909-025-07148-0
    finalDecision: exclude
  - title: "Harnessing advanced large language models in otolaryngology board examinations: an investigation using python and application programming interfaces."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Tests LLM performance on board examinations, not about assessment or feedback of human learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting (board examination context)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1007/s00405-025-09404-x
    pmid: "40281318"
    authors: Hoch C, Funk P, Guntinas-Lichius O, Volk G, L&#xfc;ers J, Hussain T, Wirth M, Schmidl B, Wollenberg B, Alfertshofer M
    year: "2025"
    abstract: |-
      PURPOSE: This study aimed to explore the capabilities of advanced large language models (LLMs), including OpenAI's GPT-4 variants, Google's Gemini series, and Anthropic's Claude series, in addressing highly specialized otolaryngology board examination questions. Additionally, the study included a longitudinal assessment of GPT-3.5 Turbo, which was evaluated using the same set of questions one year ago to identify changes in its performance over time.

      METHODS: We utilized a question bank comprising 2,576 multiple-choice and single-choice questions from a German online education platform tailored for otolaryngology board certification preparation. The questions were submitted to 11 different LLMs, including GPT-3.5 Turbo, GPT-4 variants, Gemini models, and Claude models, through Application Programming Interfaces (APIs) using Python scripts, facilitating efficient data collection and processing.

      RESULTS: GPT-4o demonstrated the highest accuracy among all models, particularly excelling in categories such as allergology and head and neck tumors. While the Claude models showed competitive performance, they generally lagged behind the GPT-4 variants. A comparison of GPT-3.5 Turbo's performance revealed a significant decline in accuracy over the past year. Newer LLMs displayed varied performance levels, with single-choice questions consistently yielding higher accuracy than multiple-choice questions across all models.

      CONCLUSION: While newer LLMs show strong potential in addressing specialized medical content, the observed decline in GPT-3.5 Turbo's performance over time underscores the necessity for continuous evaluation. This study highlights the critical need for ongoing optimization and efficient API usage to improve LLMs potential for applications in medical education and certification.
    mergedFrom:
      - source: pubmed
        pmid: "40281318"
        doi: 10.1007/s00405-025-09404-x
    finalDecision: exclude
  - title: "The Role of Natural Language Processing in Graduate Medical Education: A Scoping Review."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses on NLP broadly, not specifically generative AI/LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: uncertain
        comment: Scoping review covers NLP in GME including automated performance feedback and competency assessment in residency training. Unclear whether generative AI (vs conventional NLP) is the primary focus; requires full-text review to determine if generative AI applications for WBA are specifically addressed.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses broadly on NLP applications and is not specifically about generative AI/LLMs in the target assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: This scoping review focuses primarily on conventional NLP and traditional ML techniques (bag-of-words, LDA, SVM, TF-IDF, VADER sentiment analysis) applied in GME, not specifically on generative AI or LLMs. While LLMs are mentioned peripherally (GPT performance on exam questions), the core studies reviewed use traditional NLP/ML methods. The review does not focus on generative AI applied to WBA."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:53.576000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses primarily on NLP/ML broadly rather than generative AI-specific applications."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:01.509000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses on conventional NLP/ML (BoW, LDA, SVM, TF-IDF, VADER), not generative AI/LLMs."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.7759/cureus.81078
    pmid: "40271343"
    authors: Janumpally R
    year: "2025"
    abstract: The rapid evolution of artificial intelligence, particularly in the form of natural language processing (NLP) and large language models (LLMs), presents new opportunities to enhance graduate medical education (GME). NLP technologies have the potential to improve residency training programs by automating performance feedback, personalizing learning pathways, and identifying competency gaps. However, the integration of these technologies also raises challenges related to privacy, ethical considerations, and algorithmic bias. This review provides a comprehensive evaluation of the application and impact of NLP in GME. A scoping review of the literature was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Relevant studies from 2018 to 2024 were identified using databases such as PubMed, Scopus, Web of Science, and Google Scholar. Inclusion criteria focused on peer-reviewed studies evaluating NLP applications in residency training programs across various specialties. Data were extracted from 20 studies, and key themes were synthesized to assess the educational, technological, and ethical implications of NLP in GME. The review identified several key areas where NLP is transforming GME. These include automated performance evaluation systems, sentiment analysis of narrative feedback, personalized learning recommendations, and competency assessment algorithms. NLP technologies demonstrated significant potential in reducing administrative workload, improving assessment accuracy, and enhancing the personalization of residency training. However, studies also highlighted concerns regarding algorithmic biases and the need for transparent, ethical frameworks to ensure fair implementation. The integration of NLP in GME offers significant opportunities to streamline educational processes and enhance trainee development. Automated feedback systems can reduce subjective biases and provide more actionable insights for residents. Additionally, NLP applications can identify early indicators of residents at risk of underperformance and support timely interventions. However, the adoption of these technologies requires careful consideration of ethical and legal implications, particularly around data privacy and fairness. NLP has the potential to revolutionize GME by improving the quality and efficiency of residency training programs. While the technology offers promising benefits, further research is needed to address ethical challenges and ensure responsible implementation. Interdisciplinary collaboration between educators, informaticians, and ethicists will be critical to fully realize the potential of NLP in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "40271343"
        doi: 10.7759/cureus.81078
    finalDecision: exclude
  - title: Evaluating the performance of GPT-3.5, GPT-4, and GPT-4o in the Chinese National Medical Licensing Examination.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates GPT models on Chinese national medical licensing exam questions; benchmarks AI on exam questions, not about assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares GPT models on licensing exam performance, not workplace-based assessment, feedback, or observation-record analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1038/s41598-025-98949-2
    pmid: "40269046"
    authors: Luo D, Liu M, Yu R, Liu Y, Jiang W, Fan Q, Kuang N, Gao Q, Yin T, Zheng Z
    year: "2025"
    abstract: This study aims to compare and evaluate the performance of GPT-3.5, GPT-4, and GPT-4o in the 2020 and 2021 Chinese National Medical Licensing Examination (NMLE), exploring their potential value in medical education and clinical applications. Six hundred original test questions from the 2020 and 2021 NMLE (covering five types of questions) were selected and input into GPT-3.5, GPT-4, and GPT-4o for response. The accuracy of the models across different question types and units was recorded and analyzed. Statistical methods were employed to compare the performance differences among the three models. GPT-4o demonstrated significantly higher overall accuracy than GPT-4 and GPT-3.5 (P < 0.001). In the 2020 and 2021 exams, GPT-4o achieved accuracy rates of 84.2% and 88.2%, respectively, with the highest accuracy observed in questions related to the digestive system (Unit 3), reaching 94.75%. GPT-4 showed moderate performance, while GPT - 3.5 had the lowest accuracy. Additionally, GPT-4o exhibited a clear advantage in complex question formats, such as case analysis questions (A3/A4 type) and standard matching questions (B1 type). GPT-4o outperformed its predecessors in the NMLE, demonstrating exceptional comprehension and problem-solving abilities in non-English medical examinations. This study provides important insights into the application and promotion of generative AI in medical education and clinical practice.
    mergedFrom:
      - source: pubmed
        pmid: "40269046"
        doi: 10.1038/s41598-025-98949-2
    finalDecision: exclude
  - title: Comparison of ChatGPT plus (version 4.0) and pretrained AI model (Orthopod) on orthopaedic in-training exam (OITE).
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study benchmarks LLMs on orthopaedic in-training exam practice questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: In-training exam question answering does not evaluate workplace-based assessment or feedback in clinical settings."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.1016/j.surge.2025.04.004
    pmid: "40263060"
    authors: Magruder M, Miskiewicz M, Rodriguez A, Ng M, Abdelgawad A
    year: "2025"
    abstract: |-
      INTRODUCTION: Recent advancements in large language model (LLM) artificial intelligence (AI) systems, like ChatGPT, have showcased ability in answering standardized examination questions, but their performance is variable. The goal of this study was to compare the performance of standard ChatGPT-4 with a custom-trained ChatGPT model taking the Orthopaedic Surgery In-Training Examination (OITE).

      METHODS: Practice questions for the 2022 OITE, made available on the AAOS-ResStudy website (aaos.org/education/examinations/ResStudy), were used for this study. Question stems were uploaded to both standard ChatGPT-4 and the custom-trained ChatGPT model (Orthopod), and the responses were documented as correct or incorrect. For questions containing media elements, screenshots were converted to PNG files and uploaded to ChatGPT. Evaluation of the AI's performance included descriptive statistics to determine the percent of questions answered correctly or incorrectly.

      RESULTS: Two-hundred and seven questions were analyzed with both ChatGPT 4.0 and Orthopod. ChatGPT correctly answered 73.43 % (152/207) of the questions, while Orthopod correctly answered 71.01 % (147/207) of the questions. There was no significant difference in performance of either language model based on inclusion of media or question category.

      CONCLUSION: ChatGPT 4.0 and Orthopod correctly answered 73.43 % and 71.01 % of OITE practice questions correctly. Both systems provided well-reasoned answers in response to multiple choice questions. The thoughtfully articulated responses and well-supported explanations offered by both systems may prove to be a valuable educational resource for orthopedic residents as they prepare for upcoming board-style exams.

      LEVEL OF EVIDENCE: IV.
    mergedFrom:
      - source: pubmed
        pmid: "40263060"
        doi: 10.1016/j.surge.2025.04.004
    finalDecision: exclude
  - title: Assessing the performance of ChatGPT-4o on the Turkish Orthopedics and Traumatology Board Examination.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates ChatGPT on board exam MCQs, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Exam question benchmarking does not occur in a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.52312/jdrs.2025.1958
    pmid: "40235408"
    authors: Ya&#x11f;ar H, G&#xfc;m&#xfc;&#x15f;o&#x11f;lu E, Mert Asfuro&#x11f;lu Z
    year: "2025"
    abstract: |-
      OBJECTIVES: This study aims to assess the overall performance of ChatGPT version 4-omni (GPT-4o) on the Turkish Orthopedics and Traumatology Board Examination (TOTBE) using actual examinees as a reference point to evaluate and compare the performance of GPT-4o with that of human participants.

      MATERIALS AND METHODS: In this study, GPT-4o was tested with multiple-choice questions that formed the first step of 14 TOTBEs conducted between 2010 and 2023. The assessment of image-based questions was conducted separately for all exams. The questions were classified based on the subspecialties for the five exams (2010-2014). The performance of GPT-4o was assessed and compared to those of actual examinees of the TOTBE.

      RESULTS: The mean total score of GPT-4o was 70.2±5.64 (range, 61 to 84), whereas that of actual examinees was 58±3.28 (range, 53.6 to 64.6). Considering accuracy rates, GPT-4o demonstrated 62% accuracy on image-based questions and 70% accuracy on text-based questions. It also demonstrated superior performance in the field of basic sciences, whereas actual examinees performed better in the specialty of reconstruction. Both GPT-4o and actual examinees exhibited the lowest scores in the subspecialty of lower extremity and foot.

      CONCLUSION: Our study results showed that GPT-4o performed well on the TOTBE, particularly in basic sciences. While it demonstrated accuracy comparable to actual examinees in some areas, these findings highlight its potential as a helpful tool in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "40235408"
        doi: 10.52312/jdrs.2025.1958
    finalDecision: exclude
  - title: "Assessing ChatGPT 4.0's Capabilities in the United Kingdom Medical Licensing Examination (UKMLA): A Robust Categorical Analysis."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: GPT-4 performance on UK medical licensing exam questions; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted on licensing-exam style questions in a simulated setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.1038/s41598-025-97327-2
    pmid: "40234701"
    authors: Casals-Farre O, Baskaran R, Singh A, Kaur H, Ul Hoque T, de Almeida A, Coffey M, Hassoulas A
    year: "2025"
    abstract: Advances in the various applications of artificial intelligence will have important implications for medical training and practice. The advances in ChatGPT-4 alongside the introduction of the medical licensing assessment (MLA) provide an opportunity to compare GPT-4's medical competence against the expected level of a United Kingdom junior doctor and discuss its potential in clinical practice. Using 191 freely available questions in MLA style, we assessed GPT-4's accuracy with and without offering multiple-choice options. We compared single and multi-step questions, which targeted different points in the clinical process, from diagnosis to management. A chi-squared test was used to assess statistical significance. GPT-4 scored 86.3% and 89.6% in papers one-and-two respectively. Without the multiple-choice options, GPT's performance was 61.5% and 74.7% in papers one-and-two respectively. There was no significant difference between single and multistep questions, but GPT-4 answered 'management' questions significantly worse than 'diagnosis' questions with no multiple-choice options (p = 0.015). GPT-4's accuracy across categories and question structures suggest that LLMs are competently able to process clinical scenarios but remain incapable of understanding these clinical scenarios. Large-Language-Models incorporated into practice alongside a trained practitioner may balance risk and benefit as the necessary robust testing on evolving tools is conducted.
    mergedFrom:
      - source: pubmed
        pmid: "40234701"
        doi: 10.1038/s41598-025-97327-2
    finalDecision: exclude
  - title: The performance of ChatGPT and ERNIE Bot in surgical resident examinations.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares LLM performance on surgical resident examination questions; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1016/j.ijmedinf.2025.105906
    pmid: "40220627"
    authors: Guo S, Li G, Du W, Situ F, Li Z, Lei J
    year: "2025"
    abstract: |-
      STUDY PURPOSE: To assess the application of these two large language models (LLMs) for surgical resident examinations and to compare the performance of these LLMs with that of human residents.

      STUDY DESIGN: In this study, 596 questions with a total of 183,556 responses were first included from the Medical Vision World, an authoritative medical education platform across China. Both Chinese prompted and non-prompted questions were input into ChatGPT-4.0 and ERNIE Bot-4.0 to compare their performance in a Chinese question database. Additionally, we screened another 210 surgical questions with detailed response results from 43 residents to compare the performance of residents and these two LLMs.

      RESULTS: There were no significant differences in the correctness of the responses to the 596 questions with or without prompts between the two LLMs (ChatGPT-4.0: 68.96 % [without prompt], 71.14 % [with prompts], p = 0.411; ERNIE Bot-4.0: 78.36 % [without prompt], 78.86 % [with prompts], p = 0.832), but ERNIE Bot-4.0 displayed higher correctness than ChatGPT-4.0 did (with prompts: p = 0.002; without prompts: p < 0.001). For another 210 questions with prompts, the two LLMs, especially ERNIE Bot-4.0 (ranking in the top 95 % of the 43 residents' scores), significantly outperformed the residents.

      CONCLUSIONS: The performance of ERNIE Bot-4.0 was superior to that of ChatGPT-4.0 and that of residents on surgical resident examinations in a Chinese question database.
    mergedFrom:
      - source: pubmed
        pmid: "40220627"
        doi: 10.1016/j.ijmedinf.2025.105906
    finalDecision: exclude
  - title: "Large Language Models in Biochemistry Education: Comparative Evaluation of Performance."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on biochemistry education (preclinical), not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Biochemistry education context is not workplace-based clinical education"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    doi: 10.2196/67244
    pmid: "40209205"
    authors: Bolgova O, Shypilova I, Mavrych V
    year: "2025"
    abstract: |-
      BACKGROUND: Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs), have started a new era of innovation across various fields, with medicine at the forefront of this technological revolution. Many studies indicated that at the current level of development, LLMs can pass different board exams. However, the ability to answer specific subject-related questions requires validation.

      OBJECTIVE: The objective of this study was to conduct a comprehensive analysis comparing the performance of advanced LLM chatbots-Claude (Anthropic), GPT-4 (OpenAI), Gemini (Google), and Copilot (Microsoft)-against the academic results of medical students in the medical biochemistry course.

      METHODS: We used 200 USMLE (United States Medical Licensing Examination)-style multiple-choice questions (MCQs) selected from the course exam database. They encompassed various complexity levels and were distributed across 23 distinctive topics. The questions with tables and images were not included in the study. The results of 5 successive attempts by Claude 3.5 Sonnet, GPT-4-1106, Gemini 1.5 Flash, and Copilot to answer this questionnaire set were evaluated based on accuracy in August 2024. Statistica 13.5.0.17 (TIBCO Software Inc) was used to analyze the data's basic statistics. Considering the binary nature of the data, the chi-square test was used to compare results among the different chatbots, with a statistical significance level of P<.05.

      RESULTS: On average, the selected chatbots correctly answered 81.1% (SD 12.8%) of the questions, surpassing the students' performance by 8.3% (P=.02). In this study, Claude showed the best performance in biochemistry MCQs, correctly answering 92.5% (185/200) of questions, followed by GPT-4 (170/200, 85%), Gemini (157/200, 78.5%), and Copilot (128/200, 64%). The chatbots demonstrated the best results in the following 4 topics: eicosanoids (mean 100%, SD 0%), bioenergetics and electron transport chain (mean 96.4%, SD 7.2%), hexose monophosphate pathway (mean 91.7%, SD 16.7%), and ketone bodies (mean 93.8%, SD 12.5%). The Pearson chi-square test indicated a statistically significant association between the answers of all 4 chatbots (P<.001 to P<.04).

      CONCLUSIONS: Our study suggests that different AI models may have unique strengths in specific medical fields, which could be leveraged for targeted support in biochemistry courses. This performance highlights the potential of AI in medical education and assessment.
    mergedFrom:
      - source: pubmed
        pmid: "40209205"
        doi: 10.2196/67244
    finalDecision: exclude
  - title: Online extended focused assessment with sonography for trauma (EFAST) course enhanced knowledge and perceived confidence among medical trainees during the COVID-19 pandemic disaster.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About ultrasound (EFAST) training course, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    doi: 10.1186/s13017-025-00604-8
    pmid: "40205423"
    authors: Cevik A, Abu-Zidan F
    year: "2025"
    abstract: |-
      BACKGROUND: The COVID-19 pandemic disrupted medical education worldwide, prompting the need for innovative e-learning solutions. This study evaluated the effectiveness of an online extended Focused Assessment with Sonography in Trauma (EFAST) course, delivered via the International Emergency Medicine Education Project's platform, to improve participants' knowledge and perceived confidence in EFAST procedure.

      METHODS: A prospective observational study was conducted between May 17, 2020, and December 20, 2023. Pre- and post-course quizzes and surveys were used to assess participants' knowledge and confidence. Participant demographics, quiz scores, and survey responses were collected. Quantitative data were analysed using the Wilcoxon Signed-Rank test and Cohen's d to evaluate knowledge improvement and confidence changes. Thematic analysis of qualitative feedback was performed with the assistance of large language model AI tools for emerging themes.

      RESULTS: 1758 participants enrolled in the course. From 111 countries, 1515 started the course, and 1190 (78.6%) reached the final exam stage, with 96.1% achieving a passing score. 66.4% indicated they had never attended a prior ultrasound course. Most (81.1%) were medical students, interns, or residents. 36.5% of participants were from low- or lower-middle-income countries. 1175 (77.6%) participants completed both the pre- and post-course formative knowledge quizzes. The median (IQR) scores were 53.3 (40.0-66.7) pre-course and 86.7 (73.3-93.3) post-course (p < 0.001, effect size: -0.958). 771 participants completed both pre- and post-course surveys. Participants' median (IQR) confidence in EFAST increased from 5 (3-7) to 8 (7-10) (p < 0.001, effect size: -0.844). Qualitative feedback showed that participants found the course practical, well-structured, and effective. They suggested improving video quality and simplifying content for clarity and engagement.

      CONCLUSION: The online EFAST course enhanced participants' knowledge and perceived confidence, demonstrating the potential of online clinical education during global crises.
    mergedFrom:
      - source: pubmed
        pmid: "40205423"
        doi: 10.1186/s13017-025-00604-8
    finalDecision: exclude
  - title: "Generative AI vs. human expertise: a comparative analysis of case-based rational pharmacotherapy question generation."
    reviews:
      - decision: uncertain
        comment: Generative AI for question generation; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Using AI to generate pharmacotherapy exam questions for classroom assessments; not in a workplace-based clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Study centers on AI generation of exam questions rather than workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.1007/s00228-025-03838-2
    pmid: "40205076"
    authors: G&#xfc;vel M, K&#x131;yak Y, Varan H, Sezen&#xf6;z B, Co&#x15f;kun &, Uluo&#x11f;lu C
    year: "2025"
    abstract: |-
      PURPOSE: This study evaluated the performance of three generative AI models-ChatGPT- 4o, Gemini 1.5 Advanced Pro, and Claude 3.5 Sonnet-in producing case-based rational pharmacology questions compared to expert educators.

      METHODS: Using one-shot prompting, 60 questions (20 per model) addressing essential hypertension and type 2 diabetes subjects were generated. A multidisciplinary panel categorized questions by usability (no revisions needed, minor or major revisions required, or unusable). Subsequently, 24 AI-generated and 8 expert-created questions were asked to 103 medical students in a real-world exam setting. Performance metrics, including correct response rate, discrimination index, and identification of nonfunctional distractors, were analyzed.

      RESULTS: No statistically significant differences were found between AI-generated and expert-created questions, with mean correct response rates surpassing 50% and discrimination indices consistently equal to or above 0.20. Claude produced the highest proportion of error-free items (12/20), whereas ChatGPT exhibited the fewest unusable items (5/20). Expert revisions required approximately one minute per AI-generated question, representing a substantial efficiency gain over manual question preperation. Nonetheless, 19 out of 60 AI-generated questions were deemed unusable, highlighting the necessity of expert oversight.

      CONCLUSION: Large language models can profoundly accelerate the development of high-quality assessment questions in medical education. However, expert review remains critical to address lapses in reliability and validity. A hybrid model, integrating AI-driven efficiencies with rigorous expert validation, may offer an optimal approach for enhancing educational outcomes.
    mergedFrom:
      - source: pubmed
        pmid: "40205076"
        doi: 10.1007/s00228-025-03838-2
    finalDecision: exclude
  - title: "Using a Hybrid of AI and Template-Based Method in Automatic Item Generation to Create Multiple-Choice Questions in Medical Education: Hybrid AIG."
    reviews:
      - decision: uncertain
        comment: AI for automatic item generation in medical education; may involve generative AI for assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on item generation, not assessment/feedback or analysis of clinical observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Methodological study on hybrid AI/template-based MCQ generation for medical education; not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Method for generating MCQ templates, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.2196/65726
    pmid: "40184548"
    authors: K&#x131;yak Y, Kononowicz A
    year: "2025"
    abstract: |-
      BACKGROUND: Template-based automatic item generation (AIG) is more efficient than traditional item writing but it still heavily relies on expert effort in model development. While nontemplate-based AIG, leveraging artificial intelligence (AI), offers efficiency, it faces accuracy challenges. Medical education, a field that relies heavily on both formative and summative assessments with multiple choice questions, is in dire need of AI-based support for the efficient automatic generation of items.

      OBJECTIVE: We aimed to propose a hybrid AIG to demonstrate whether it is possible to generate item templates using AI in the field of medical education.

      METHODS: This is a mixed-methods methodological study with proof-of-concept elements. We propose the hybrid AIG method as a structured series of interactions between a human subject matter expert and AI, designed as a collaborative authoring effort. The method leverages AI to generate item models (templates) and cognitive models to combine the advantages of the two AIG approaches. To demonstrate how to create item models using hybrid AIG, we used 2 medical multiple-choice questions: one on respiratory infections in adults and another on acute allergic reactions in the pediatric population.

      RESULTS: The hybrid AIG method we propose consists of 7 steps. The first 5 steps are performed by an expert in a customized AI environment. These involve providing a parent item, identifying elements for manipulation, selecting options and assigning values to elements, and generating the cognitive model. After a final expert review (Step 6), the content in the template can be used for item generation through a traditional (non-AI) software (Step 7). We showed that AI is capable of generating item templates for AIG under the control of a human expert in only 10 minutes. Leveraging AI in template development made it less challenging.

      CONCLUSIONS: The hybrid AIG method transcends the traditional template-based approach by marrying the "art" that comes from AI as a "black box" with the "science" of algorithmic generation under the oversight of expert as a "marriage registrar". It does not only capitalize on the strengths of both approaches but also mitigates their weaknesses, offering a human-AI collaboration to increase efficiency in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "40184548"
        doi: 10.2196/65726
    finalDecision: exclude
  - title: Generating learning guides for medical education with LLMs and statistical analysis of test results.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Uses LLMs to generate personalized feedback from a formative knowledge-based progress test (PTM), not in a workplace-based or clinical education setting. The assessment is a standardized MCQ test, not clinical observation."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:51.586000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on progress-test feedback generation rather than workplace-based or clinical education context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:58.822000+00:00
    doi: 10.1186/s12909-025-06978-2
    pmid: "40158187"
    authors: Rosell&#xf3; Atanet I, Tomova M, Sieg M, Sehy V, M&#xe4;der P, M&#xe4;rz M
    year: "2025"
    abstract: |-
      BACKGROUND: The Progress Test Medizin (PTM) is a formative test for medical students issued twice a year by the Charité-Universitätsmedizin Berlin. The PTM provides a numerical feedback based on a global view of the strengths and weaknesses of students. This feedback can benefit from more fine-grained information, pinpointing the topics where students need to improve, as well as advice on what they should learn in light of their results. The scale of the PTM, taken by more than 10,000 participants every academic semester, makes it necessary to automate this task.

      METHODS: We have developed a seven-step approach based on large language models and statistical analysis to fulfil the purpose of this study. Firstly, a large language model (ChatGPT 4.0) identified keywords in the form of MeSH terms from all 200 questions of one PTM run. These keywords were checked against the list of medical terms included in the Medical Subject Headings (MeSH) thesaurus published by the National Library of Medicine (NLM). Meanwhile, answer patterns of PTM questions were also analysed to find empirical relationships between questions. With this information, we obtained series of questions related to specific MeSH terms and used them to develop a framework that allowed us to assess the performance of PTM participants and compose personalized feedback structured around a curated list of medical topics.

      RESULTS: We used data from a past PTM to simulate the generation of personalized feedback for 1,401 test participants, thereby producing specific information about their knowledge regarding a number of topics ranging from 34 to 243. Substantial knowledge gaps were found in 14.67% to 21.76% of rated learning topics, depending on the benchmarking set considered.

      CONCLUSION: We designed and tested a method to generate student feedback covering up to 243 medical topics defined by MeSH terms. The feedback generated with data from students in later stages of their studies was more detailed, as they tend to face more questions matching their knowledge level.
    mergedFrom:
      - source: pubmed
        pmid: "40158187"
        doi: 10.1186/s12909-025-06978-2
    finalDecision: exclude
  - title: "Comparing diagnostic skills in endodontic cases: dental students versus ChatGPT-4o."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Compares ChatGPT-4o with dental students on endodontic diagnostic MCQs, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is based on simulated case MCQs, with no workplace-based component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.1186/s12903-025-05857-y
    pmid: "40158110"
    authors: Durmazpinar P, Ekmekci E
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) technologies have revolutionized fields such as economics, law, and healthcare. Large language models, like ChatGPT, have shown significant potential in dentistry, supporting diagnostic accuracy, treatment planning, and education. However, earlier versions of ChatGPT were limited to text-based data. The latest multimodal model, ChatGPT-4o, introduced in 2024, now processes text, images, audio, and video, enabling broader applications in clinical education. This study evaluates ChatGPT-4o's diagnostic accuracy in endodontic cases, comparing it with dental students' performance.

      MATERIALS AND METHODS: This study included two groups of dental students: 3rd-year and 5th-year, alongside ChatGPT-4o. Participants answered 15 multiple-choice questions designed using radiographs, clinical photographs, and patient histories. These questions, based on the American Association of Endodontists' Clinical Guidelines, were administered via Google Forms for students and ChatGPT-4o. Responses were categorized as correct, incorrect, or unanswered. Data were analyzed statistically.

      RESULTS: ChatGPT-4o demonstrated a higher accuracy rate and lower error rate compared to students, with 91.4% correct and 8.2% incorrect responses. Third-year students had a correct response rate of 60.8%, while fifth-year students achieved 79.5%. A statistically significant difference was found between the study groups in terms of correct response rates (p < 0.05), with ChatGPT outperforming both student groups (p < 0.001). Additionally, fifth-year students showed a higher correct response rate compared to third-year students.

      CONCLUSION: ChatGPT-4o demonstrates significant potential as a diagnostic support tool in dental education, particularly in endodontics. Its high diagnostic accuracy and consistency highlight its value as an innovative ( application in clinical training and decision-making.
    mergedFrom:
      - source: pubmed
        pmid: "40158110"
        doi: 10.1186/s12903-025-05857-y
    finalDecision: exclude
  - title: Can Artificial Intelligence Coach Faculty to Utilize Growth Mindset Language? A Qualitative Analysis of Feedback Statements.
    reviews:
      - decision: uncertain
        comment: AI coaching faculty on feedback; directly relevant to feedback in assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Focuses on faculty coaching rather than medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: include
        comment: Generative AI used to modify feedback statements from fixed to growth mindset language in surgery clerkship and SIMPL (workplace-based) evaluations. Presents data on quality and detectability of AI-modified feedback. Directly addresses AI-assisted feedback enhancement in WBA context.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
      - decision: include
        comment: "Uses Google Chrome generative AI to modify fixed mindset feedback statements into growth mindset language for surgical education. Feedback originates from workplace-based settings (surgery clerkship, SIMPL OR evaluations for residents). Presents data on AI accuracy in converting feedback language, reviewer identification rates, and qualitative analysis of AI-generated vs original statements. Meets criteria: generative AI applied to workplace-based assessment feedback, with validity/acceptability data."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:56.143000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:04.368000+00:00
    doi: 10.1016/j.jss.2025.01.029
    pmid: "40153901"
    authors: Furey M, Stemrich R, Westfall-Snyder J, Gupta T, Rapp M, Hoffman R
    year: "2025"
    abstract: |-
      INTRODUCTION: Feedback is at the core of competency-based medical education. Learner perceptions of the evaluation process influence how feedback is utilized. Systems emphasize a fixed mindset, prioritizing evaluation over growth. Embracing growth mindset culture, the belief that ability is acquired through effort and human capabilities can be developed over time, will allow learners to gain greater benefits from feedback. Transitioning from fixed mindset language (FML) to growth mindset language (GML) will require faculty training. Artificial intelligence (AI) can assist faculty with incorporating GML concepts in written feedback. The aim of this study was to assess the ability of AI to assist in changing FML feedback statements into statements with GML.

      METHODS: A qualitative study was performed utilizing a sample of 83 summative and formative feedback statements provided to students (37) and residents (46) from surgery clerkship and national SIMPL-inguinal hernia evaluations. Of these 83 statements, a reviewer coded 41 statements as using GML and 42 using FML. Original statements identified as using FML were entered into the Google Chrome "Help me write" tool, a writing aid using Generative AI. The AI tool was prompted with the statement "rewrite using growth mindset language:," followed by an original FML statement. A dataset containing a combination of AI-altered and original statements, 99 statements in all, was provided to two additional blinded reviewers trained in GML concepts. Reviewers evaluated statements as predominantly GML or FML and commented on their perception of AI use in statements. Reviewer agreement was adjudicated by the original coder.

      RESULTS: Of the 41 original GML statements, coders correctly identified 37 (90.2%) as using GML. Of the 26 original FML statements, coders correctly identified all 26 (100%) as using FML. Of the AI-modified FML to GML statements, coders correctly identified 17 of 18 (94.4%) as using GML. They correctly identified 56.3% as AI-modified and 44.8% as not AI-modified statements. They disagreed on AI use in 39.4% of statements. AI-assistance was unrecognized in 16 (8.1%) statements and mistaken for use in 47 (23.7%) statements.

      CONCLUSIONS: AI was successful at modifying FML statements into feedback containing GML, and in a way that was not obviously AI-generated. This proof-of-concept study demonstrates that AI can be a helpful tool for faculty to increase the use of GML in written feedback. While AI cannot perfectly create GML feedback without initial input and understanding from faculty, it does serve as a promising educational aid. As the body of work on using GML in surgical education grows, the better AI can assist in the generation of quality feedback.
    mergedFrom:
      - source: pubmed
        pmid: "40153901"
        doi: 10.1016/j.jss.2025.01.029
      - source: scopus
        doi: 10.1016/j.jss.2025.01.029
        scopusId: SCOPUS_ID:105000866677
    finalDecision: include
  - title: A Review of Large Language Models in Medical Education, Clinical Decision Support, and Healthcare Administration.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: 'A Review' in title indicates this is a review article, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article, systematic review, or meta-analysis; this is a narrative/comprehensive review of LLM applications across medical education, clinical decision support, and healthcare administration."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Narrative review article rather than an original research study, systematic review, meta-analysis, or preprint meeting this review's criteria."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.3390/healthcare13060603
    pmid: "40150453"
    authors: Vrdoljak J, Boban Z, Vilovi&#x107; M, Kumri&#x107; M, Bo&#x17e;i&#x107; J
    year: "2025"
    abstract: "Background/Objectives: Large language models (LLMs) have shown significant potential to transform various aspects of healthcare. This review aims to explore the current applications, challenges, and future prospects of LLMs in medical education, clinical decision support, and healthcare administration. Methods: A comprehensive literature review was conducted, examining the applications of LLMs across the three key domains. The analysis included their performance, challenges, and advancements, with a focus on techniques like retrieval-augmented generation (RAG). Results: In medical education, LLMs show promise as virtual patients, personalized tutors, and tools for generating study materials. Some models have outperformed junior trainees in specific medical knowledge assessments. Concerning clinical decision support, LLMs exhibit potential in diagnostic assistance, treatment recommendations, and medical knowledge retrieval, though performance varies across specialties and tasks. In healthcare administration, LLMs effectively automate tasks like clinical note summarization, data extraction, and report generation, potentially reducing administrative burdens on healthcare professionals. Despite their promise, challenges persist, including hallucination mitigation, addressing biases, and ensuring patient privacy and data security. Conclusions: LLMs have transformative potential in medicine but require careful integration into healthcare settings. Ethical considerations, regulatory challenges, and interdisciplinary collaboration between AI developers and healthcare professionals are essential. Future advancements in LLM performance and reliability through techniques such as RAG, fine-tuning, and reinforcement learning will be critical to ensuring patient safety and improving healthcare delivery."
    mergedFrom:
      - source: pubmed
        pmid: "40150453"
        doi: 10.3390/healthcare13060603
      - source: scopus
        doi: 10.3390/healthcare13060603
        scopusId: SCOPUS_ID:105001405937
    finalDecision: exclude
  - title: "Evaluating the value of AI-generated questions for USMLE step 1 preparation: A study using ChatGPT-3.5."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates AI-generated MCQs for a written licensing exam (USMLE Step 1)."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on AI-generated exam questions for test preparation, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.1080/0142159X.2025.2478872
    pmid: "40146672"
    authors: Balu A, Prvulovic S, Fernandez Perez C, Kim A, Donoho D, Keating G
    year: "2025"
    abstract: |-
      PURPOSE: Students are increasingly relying on artificial intelligence (AI) for medical education and exam preparation. However, the factual accuracy and content distribution of AI-generated exam questions for self-assessment have not been systematically investigated.

      METHODS: Curated prompts were created to generate multiple-choice questions matching the USMLE Step 1 examination style. We utilized ChatGPT-3.5 to generate 50 questions and answers based upon each prompt style. We manually examined output for factual accuracy, Bloom's Taxonomy, and category within the USMLE Step 1 content outline.

      RESULTS: ChatGPT-3.5 generated 150 multiple-choice case-style questions and selected an answer. Overall, 83% of generated multiple questions had no factual inaccuracies and 15% contained one to two factual inaccuracies. With simple prompting, common themes included deep venous thrombosis, myocardial infarction, and thyroid disease. Topic diversity improved by separating content topic generation from question generation, and specificity to Step 1 increased by indicating that "treatment" questions were not desired.

      CONCLUSION: We demonstrate that ChatGPT-3.5 can successfully generate Step 1 style questions with reasonable factual accuracy, and this method may be used by medical students preparing for USMLE examinations. While AI-generated questions demonstrated adequate factual accuracy, targeted prompting techniques should be used to overcome ChatGPT's bias towards particular medical conditions.
    mergedFrom:
      - source: pubmed
        pmid: "40146672"
        doi: 10.1080/0142159X.2025.2478872
    finalDecision: exclude
  - title: "Accuracy of LLMs in medical education: evidence from a concordance test with medical teacher."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Tests LLM accuracy on USMLE MCQs compared to a medical teacher, not about using AI for learner assessment or WBA feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Accuracy comparison on USMLE MCQs, not assessment/feedback generation or analysis of clinical observation records in WBA."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1186/s12909-025-07009-w
    pmid: "40140805"
    authors: Mishra V, Lurie Y, Mark S
    year: "2025"
    abstract: |-
      BACKGROUND: There is an unprecedented increase in the use of Generative AI in medical education. There is a need to assess these models' accuracy to ensure patient safety. This study assesses the accuracy of ChatGPT, Gemini, and Copilot in answering multiple-choice questions (MCQs) compared to a qualified medical teacher.

      METHODS: This study randomly selected 40 Multiple Choice Questions (MCQs) from past United States Medical Licensing Examination (USMLE) and asked for answers to three LLMs: ChatGPT, Gemini, and Copilot. The results of an LLM are then compared with those of a qualified medical teacher and with responses from other LLMs. The Fleiss' Kappa Test was used to determine the concordance between four responders (3 LLMs + 1 Medical Teacher). In case of poor agreement between responders, Cohen's Kappa test was performed to assess the agreement between responders.

      RESULTS: ChatGPT demonstrated the highest accuracy (70%, Cohen's Kappa = 0.84), followed by Copilot (60%, Cohen's Kappa = 0.69), while Gemini showed the lowest accuracy (50%, Cohen's Kappa = 0.53). The Fleiss' Kappa value of -0.056 indicated significant disagreement among all four responders.

      CONCLUSION: The study provides an approach for assessing the accuracy of different LLMs. The study concludes that ChatGPT is far superior (70%) to other LLMs when asked medical questions across different specialties, while contrary to expectations, Gemini (50%) performed poorly. When compared with medical teachers, the low accuracy of LLMs suggests that general-purpose LLMs should be used with caution in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "40140805"
        doi: 10.1186/s12909-025-07009-w
    finalDecision: exclude
  - title: Accuracy and quality of ChatGPT-4o and Google Gemini performance on image-based neurosurgery board questions.
    reviews:
      - decision: uncertain
        comment: LLMs on neurosurgery board questions; abstract needed to determine WBA relevance
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on neurosurgery board exam image-based questions; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted on image-based board-preparation question banks only."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.1007/s10143-025-03472-7
    pmid: "40131528"
    authors: Sau S, George D, Singh R, Kohli G, Li A, Jalal M, Singh A, Furst T, Rahmani R, Vates G, Stone J
    year: "2025"
    abstract: "Large-language models (LLMs) have shown the capability to effectively answer medical board examination questions. However, their ability to answer imagebased questions has not been examined. This study sought to evaluate the performance of two LLMs (GPT-4o and Google Gemini) on an image-based question bank designed for neurosurgery board examination preparation. The accuracy of LLMs was tested using 379 image-based questions from The Comprehensive Neurosurgery Board Preparation Book: Illustrated Questions and Answers and Neurosurgery Practice Questions and Answers. LLMs were asked to answer all questions on their own and provide an explanation for their chosen answer. The problem-solving order of questions and quality of LLM responses was evaluated by senior neurological surgery residents who have passed the American Board of Neurological Surgery (ABNS) primary examination. First order questions assess anatomy, second-order questions require diagnostic reasoning, and third-order questions test deeper clinical knowledge by inferring diagnoses and related facts, evaluating the model's ability to recall and apply medical concepts. Chi-squared tests and independent-samples t-tests were conducted to measure performance differences between LLMs. On the image-based question bank, GPT-4o and Gemini achieved correct score percentages of 51.45% (95% CI: 46.43-56.44%) and 39.58% (95% CI: 34.78-44.58%), respectively. GPT-4o significantly outperformed Gemini overall (P = 0.0013), particularly in pathology/histology (P = 0.036) and radiology (P = 0.014). GPT-4o also performed better on second-order questions (56.52% vs. 41.85%, P = 0.0067) and had a higher average response quality rating (2.77 vs. 2.31, P = 0.000002). On a question bank with 379 image-based questions designed for neurosurgery board preparation, GPT-4o obtained a score of 51.45% and outperformed Gemini. GPT-4o not only achieved higher accuracy but also provided higher-quality responses compared to Gemini. In comparison to previous studies on LLM performance of board-style questions, image-based question performance was lower, indicating LLMs may struggle with machine vision/medical image interpretation tasks."
    mergedFrom:
      - source: pubmed
        pmid: "40131528"
        doi: 10.1007/s10143-025-03472-7
    finalDecision: exclude
  - title: Can ChatGPT Generate Acceptable Case-Based Multiple-Choice Questions for Medical School Anatomy Exams? A Pilot Study on Item Difficulty and Discrimination.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study evaluates ChatGPT-generated anatomy MCQs for medical school exams in a classroom/exam setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Addresses generation of exam MCQs and item metrics, not workplace-based assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1002/ca.24271
    pmid: "40129054"
    authors: K&#x131;yak Y, Soylu A, Co&#x15f;kun &, Budako&#x11f;lu I, Peker T
    year: "2025"
    abstract: Developing high-quality multiple-choice questions (MCQs) for medical school exams is effortful and time-consuming. In this study, we investigated the ability of ChatGPT to generate case-based anatomy MCQs with acceptable levels of item difficulty and discrimination for medical school exams. We used ChatGPT to generate case-based anatomy MCQs for an endocrine and urogenital system exam based on a framework for artificial intelligence (AI)-assisted item generation. The questions were evaluated by experts, approved by the department, and administered to 502 second-year medical students (372 Turkish-language, 130 English-language). The items were analyzed to determine the discrimination and difficulty indices. The item discrimination indices ranged from 0.29 to 0.54, indicating acceptable differentiation between high- and low-performing students. All items in Turkish (six out of six) and five out of six in English met the higher discrimination threshold (≥ 0.30) required for large-scale standardized tests. The item difficulty indices ranged from 0.41 to 0.89, most items falling within the moderate difficulty range (0.20-0.80). Therefore, it was concluded that ChatGPT can generate case-based anatomy MCQs with acceptable psychometric properties, offering a promising tool for medical educators. However, human expertise remains crucial for reviewing and refining AI-generated assessment items. Future research should explore AI-generated MCQs across various anatomy topics and investigate different AI models for question generation.
    mergedFrom:
      - source: pubmed
        pmid: "40129054"
        doi: 10.1002/ca.24271
    finalDecision: exclude
  - title: "Evaluating podcasts as a tool for OSCE training: a randomized trial using generative AI-powered simulation."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: OSCE training using generative AI-powered simulation, entirely simulated environment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment appears to be conducted in simulation (OSCE training)."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.1186/s12909-025-06675-0
    pmid: "40128772"
    authors: Dupont V, Guerrot D, Bouazzi L, Figueres L, Pers Y, Guenou E, Barbe C, Yauy K
    year: "2025"
    abstract: |-
      INTRODUCTION: Objective Structured Clinical Examinations (OSCEs) are critical for assessing clinical competencies in medical education. While traditional teaching methods remain prevalent, this study introduces an innovative approach by evaluating the effectiveness of an OSCE preparation podcast in improving medical students' OSCE performance using nephrology as a proof of concept. This novel method offers a flexible and accessible format for supplementary learning, potentially revolutionizing medical education.

      METHODS: A mono-centric randomized controlled trial was conducted among 50 fourth-year medical students. Participants were randomly assigned to either the podcast intervention group or a control group. Both groups completed six nephrology-specific OSCE stations on DocSimulator, a generative AI-powered virtual patient platform. Scores from three baseline and three post-intervention OSCE stations were compared. The primary outcome was the change in OSCE scores. Secondary outcomes included interest in nephrology and students' self-reported competence in nephrology-related skills.

      RESULTS: The baseline OSCE scores did not differ between the two groups (23.8 ± 3.9 vs. 23.3 ± 5.3; p = 0.77). After the intervention, the podcast group demonstrated a significantly higher OSCE score compared to the control group (27.6 ± 3.6 vs. 23.6 ± 5.0; p = 0.002) with a greater improvement in OSCE scores (+ 3.52[0.7,6.5] vs. -1.22[-3,5.5]; p = 0.03). While the podcast did not increase students' intention to specialize in nephrology (4.2% vs. 4.0%; p = 0.99), it significantly improved their confidence in nephrology-related clinical skills (41.7% vs. 16%, p = 0.04). 68% of students in the podcast group found OSCE training podcast useful for their OSCE preparation, and 96% reported they would use it again.

      CONCLUSIONS: The use of an OSCE preparation podcast significantly enhanced students' performance in AI-based simulations and confidence in nephrology clinical competencies. Podcasts represent a valuable supplementary tool for medical education, providing flexibility and supporting diverse learning styles.

      TRIAL REGISTRATION: Not applicable.
    mergedFrom:
      - source: pubmed
        pmid: "40128772"
        doi: 10.1186/s12909-025-06675-0
    finalDecision: exclude
  - title: "Performance of Plug-In Augmented ChatGPT and Its Ability to Quantify Uncertainty: Simulation Study on the German Medical Board Examination."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is explicitly a simulation study"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on board examination questions; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted on simulated board-exam questions only."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.2196/58375
    pmid: "40116759"
    authors: Madrid J, Diehl P, Selig M, Rolauffs B, Hans F, Busch H, Scheef T, Benning L
    year: "2025"
    abstract: |-
      BACKGROUND: The GPT-4 is a large language model (LLM) trained and fine-tuned on an extensive dataset. After the public release of its predecessor in November 2022, the use of LLMs has seen a significant spike in interest, and a multitude of potential use cases have been proposed. In parallel, however, important limitations have been outlined. Particularly, current LLMs encounter limitations, especially in symbolic representation and accessing contemporary data. The recent version of GPT-4, alongside newly released plugin features, has been introduced to mitigate some of these limitations.

      OBJECTIVE: Before this background, this work aims to investigate the performance of GPT-3.5, GPT-4, GPT-4 with plugins, and GPT-4 with plugins using pretranslated English text on the German medical board examination. Recognizing the critical importance of quantifying uncertainty for LLM applications in medicine, we furthermore assess this ability and develop a new metric termed "confidence accuracy" to evaluate it.

      METHODS: We used GPT-3.5, GPT-4, GPT-4 with plugins, and GPT-4 with plugins and translation to answer questions from the German medical board examination. Additionally, we conducted an analysis to assess how the models justify their answers, the accuracy of their responses, and the error structure of their answers. Bootstrapping and CIs were used to evaluate the statistical significance of our findings.

      RESULTS: This study demonstrated that available GPT models, as LLM examples, exceeded the minimum competency threshold established by the German medical board for medical students to obtain board certification to practice medicine. Moreover, the models could assess the uncertainty in their responses, albeit exhibiting overconfidence. Additionally, this work unraveled certain justification and reasoning structures that emerge when GPT generates answers.

      CONCLUSIONS: The high performance of GPTs in answering medical questions positions it well for applications in academia and, potentially, clinical practice. Its capability to quantify uncertainty in answers suggests it could be a valuable artificial intelligence agent within the clinical decision-making loop. Nevertheless, significant challenges must be addressed before artificial intelligence agents can be robustly and safely implemented in the medical domain.
    mergedFrom:
      - source: pubmed
        pmid: "40116759"
        doi: 10.2196/58375
    finalDecision: exclude
  - title: "Integrating AI into clinical education: evaluating general practice trainees' proficiency in distinguishing AI-generated hallucinations and impacting factors."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Assesses trainees' ability to detect AI hallucinations (AI literacy), not about using generative AI for assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on detecting AI hallucinations rather than assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.1186/s12909-025-06916-2
    pmid: "40108629"
    authors: Zhou J, Zhang J, Wan R, Cui X, Liu Q, Guo H, Shi X, Fu B, Meng J, Yue B, Zhang Y, Zhang Z
    year: "2025"
    abstract: |-
      OBJECTIVE: To assess the ability of General Practice (GP) Trainees to detect AI-generated hallucinations in simulated clinical practice, ChatGPT-4o was utilized. The hallucinations were categorized into three types based on the accuracy of the answers and explanations: (1) correct answers with incorrect or flawed explanations, (2) incorrect answers with explanations that contradict factual evidence, and (3) incorrect answers with correct explanations.

      METHODS: This multi-center, cross-sectional survey study involved 142 GP Trainees, all of whom were undergoing General Practice Specialist Training and volunteered to participate. The study evaluated the accuracy and consistency of ChatGPT-4o, as well as the Trainees' response time, accuracy, sensitivity (d'), and response tendencies (β). Binary regression analysis was used to explore factors affecting the Trainees' ability to identify errors generated by ChatGPT-4o.

      RESULTS: A total of 137 participants were included, with a mean age of 25.93 years. Half of the participants were unfamiliar with AI, and 35.0% had never used it. ChatGPT-4o's overall accuracy was 80.8%, which slightly decreased to 80.1% after human verification. However, the accuracy for professional practice (Subject 4) was only 57.0%, and after human verification, it dropped further to 44.2%. A total of 87 AI-generated hallucinations were identified, primarily occurring at the application and evaluation levels. The mean accuracy of detecting these hallucinations was 55.0%, and the mean sensitivity (d') was 0.39. Regression analysis revealed that shorter response times (OR = 0.92, P = 0.02), higher self-assessed AI understanding (OR = 0.16, P = 0.04), and more frequent AI use (OR = 10.43, P = 0.01) were associated with stricter error detection criteria.

      CONCLUSIONS: The study concluded that GP trainees faced challenges in identifying ChatGPT-4o's errors, particularly in clinical scenarios. This highlights the importance of improving AI literacy and critical thinking skills to ensure effective integration of AI into medical education.
    mergedFrom:
      - source: pubmed
        pmid: "40108629"
        doi: 10.1186/s12909-025-06916-2
    finalDecision: exclude
  - title: "Chat GPT, Gemini or Meta AI: A comparison of AI platforms as a tool for answering higher-order questions in microbiology."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Title indicates classroom microbiology question-answering context, not workplace-based clinical education"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; compares LLM performance on curriculum-based microbiology exam questions in a purely academic context."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Conducted on microbiology curriculum question papers, not in a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.4103/jpgm.jpgm_775_24
    pmid: "40107971"
    authors: Roy R, Gupta S, Das D, Chowdhury P
    year: "2025"
    abstract: |-
      INTRODUCTION: Artificial intelligence (AI) platforms have achieved a noteworthy role in various fields of medical sciences, ranging from medical education to clinical diagnostics and treatment. ChatGPT, Gemini, and Meta AI are some large language models (LLMs) that have gained immense popularity among students for solving questions from different branches of education.

      MATERIALS AND METHODS: A cross-sectional study was conducted in the Department of Microbiology to assess the performance of ChatGPT, Gemini, and Meta AI in answering higher-order questions from various competencies of the microbiology curriculum (MI 1 to 8), according to CBME guidelines. Sixty higher-order questions were compiled from university question papers of two universities. Their responses were assessed by three faculty members from the department.

      RESULTS: The mean rank scores of ChatGPT, Gemini, and Meta AI were found to be 102.76, 108.5, and 60.23 by Evaluator 1; 106.03, 88.5, and 76.95 by Evaluator 2; and 104.85, 85.6, and 81.04, respectively, indicating lowest overall mean rank score for Meta AI. ChatGPT had the highest mean score in MI 2,3,5,6,7, and 8 competencies, while Gemini had a higher score for MI 1 and 4 competencies. A qualitative assessment of the three platforms was also performed. ChatGPT provided elaborative responses, some responses from Gemini lacked certain significant points, and Meta AI gave answers in bullet points.

      CONCLUSIONS: Both ChatGPT and Gemini have created vast databases to correctly respond to higher-order queries in medical microbiology in comparison to Meta AI. Our study is the first of its kind to compare these three popular LLM platforms for microbiology.
    mergedFrom:
      - source: pubmed
        pmid: "40107971"
        doi: 10.4103/jpgm.jpgm_775_24
    finalDecision: exclude
  - title: "Performance of ChatGPT-4 on Taiwanese Traditional Chinese Medicine Licensing Examinations: Cross-Sectional Study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Licensing examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; benchmarks GPT-4 on TCM licensing exam questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Study compares model performance with licensed doctors, not medical/health-professional learners."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.2196/58897
    pmid: "40106227"
    authors: Tseng L, Lu Y, Tseng L, Chen Y, Chen H
    year: "2025"
    abstract: |-
      BACKGROUND: The integration of artificial intelligence (AI), notably ChatGPT, into medical education, has shown promising results in various medical fields. Nevertheless, its efficacy in traditional Chinese medicine (TCM) examinations remains understudied.

      OBJECTIVE: This study aims to (1) assess the performance of ChatGPT on the TCM licensing examination in Taiwan and (2) evaluate the model's explainability in answering TCM-related questions to determine its suitability as a TCM learning tool.

      METHODS: We used the GPT-4 model to respond to 480 questions from the 2022 TCM licensing examination. This study compared the performance of the model against that of licensed TCM doctors using 2 approaches, namely direct answer selection and provision of explanations before answer selection. The accuracy and consistency of AI-generated responses were analyzed. Moreover, a breakdown of question characteristics was performed based on the cognitive level, depth of knowledge, types of questions, vignette style, and polarity of questions.

      RESULTS: ChatGPT achieved an overall accuracy of 43.9%, which was lower than that of 2 human participants (70% and 78.4%). The analysis did not reveal a significant correlation between the accuracy of the model and the characteristics of the questions. An in-depth examination indicated that errors predominantly resulted from a misunderstanding of TCM concepts (55.3%), emphasizing the limitations of the model with regard to its TCM knowledge base and reasoning capability.

      CONCLUSIONS: Although ChatGPT shows promise as an educational tool, its current performance on TCM licensing examinations is lacking. This highlights the need for enhancing AI models with specialized TCM training and suggests a cautious approach to utilizing AI for TCM education. Future research should focus on model improvement and the development of tailored educational applications to support TCM learning.
    mergedFrom:
      - source: pubmed
        pmid: "40106227"
        doi: 10.2196/58897
    finalDecision: exclude
  - title: "A tutorial activity for students to experience generative artificial intelligence: students' perceptions and actions."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Classroom activity where students critique a ChatGPT-generated exam answer and discuss GenAI perceptions; not about using AI for assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Conducted in a classroom tutorial context and not in a workplace-based or clinical education assessment setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1152/advan.00245.2024
    pmid: "40105755"
    authors: Reinke N, Parkinson A, Kafer G
    year: "2025"
    abstract: Freely accessible generative artificial intelligence (GenAI) poses challenges to physiology education regarding learning and academic integrity. Although many studies have explored the capabilities of GenAI to complete assessments, few have implemented educative activities to highlight GenAI risks and benefits or explored physiology students' perceptions and uses of GenAI. Our study implemented a learning activity, designed using constructivist principles, to allow physiology students to explore GenAI and consider its use in assessment tasks. The activity engaged students (n = 236) enrolled in a second-year physiology subject over 2 years. The activity began with students being directed to critique a sample exam answer as a form of content revision. The answer had been covertly generated by ChatGPT, and it lacked depth and contained some hallucinated facts. Students then engaged in discussion about the use of GenAI for university study and assessment. Questions were used to stimulate thought and discussion, and student responses were collected via Padlet (492 posts). Thematic analysis of the comments highlighted students' beliefs about using GenAI and perceived benefits and risks. There was a general trend of increasing acceptance of using GenAI, and using it for assessment, over time. Students were concerned about breaching academic integrity guidelines, information accuracy and sources, and the negative effect it might have on their learning. At the conclusion of the activity, the revelation that ChatGPT wrote the sample exam answer reinforced the need for responsible GenAI use.NEW & NOTEWORTHY Constructivist learning tenets were used to guide the design of a critical evaluation learning activity about GenAI, to enable physiology students to make informed decisions regarding the use of GenAI in their learning and assessment. The trend of increasing acceptance of GenAI coincided with increasing student beliefs about uses of GenAI being perceived as responsible. Student concerns about academic integrity and ethical considerations persisted, yet academic misconduct cases increased.
    mergedFrom:
      - source: pubmed
        pmid: "40105755"
        doi: 10.1152/advan.00245.2024
    finalDecision: exclude
  - title: "Impact of Clinical Decision Support Systems on Medical Students' Case-Solving Performance: Comparison Study with a Focus Group."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not clearly about generative AI or LLMs (clinical decision support systems)."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates medical students using ChatGPT as a clinical decision support tool for case-solving; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on case-solving with CDSS tools, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.2196/55709
    pmid: "40101183"
    authors: Montagna M, Chiabrando F, De Lorenzo R, Rovere Querini P, Medical Students
    year: "2025"
    abstract: |-
      BACKGROUND: Health care practitioners use clinical decision support systems (CDSS) as an aid in the crucial task of clinical reasoning and decision-making. Traditional CDSS are online repositories (ORs) and clinical practice guidelines (CPG). Recently, large language models (LLMs) such as ChatGPT have emerged as potential alternatives. They have proven to be powerful, innovative tools, yet they are not devoid of worrisome risks.

      OBJECTIVE: This study aims to explore how medical students perform in an evaluated clinical case through the use of different CDSS tools.

      METHODS: The authors randomly divided medical students into 3 groups, CPG, n=6 (38%); OR, n=5 (31%); and ChatGPT, n=5 (31%); and assigned each group a different type of CDSS for guidance in answering prespecified questions, assessing how students' speed and ability at resolving the same clinical case varied accordingly. External reviewers evaluated all answers based on accuracy and completeness metrics (score: 1-5). The authors analyzed and categorized group scores according to the skill investigated: differential diagnosis, diagnostic workup, and clinical decision-making.

      RESULTS: Answering time showed a trend for the ChatGPT group to be the fastest. The mean scores for completeness were as follows: CPG 4.0, OR 3.7, and ChatGPT 3.8 (P=.49). The mean scores for accuracy were as follows: CPG 4.0, OR 3.3, and ChatGPT 3.7 (P=.02). Aggregating scores according to the 3 students' skill domains, trends in differences among the groups emerge more clearly, with the CPG group that performed best in nearly all domains and maintained almost perfect alignment between its completeness and accuracy.

      CONCLUSIONS: This hands-on session provided valuable insights into the potential perks and associated pitfalls of LLMs in medical education and practice. It suggested the critical need to include teachings in medical degree courses on how to properly take advantage of LLMs, as the potential for misuse is evident and real.
    mergedFrom:
      - source: pubmed
        pmid: "40101183"
        doi: 10.2196/55709
    finalDecision: exclude
  - title: While GPT-3.5 is unable to pass the Physician Licensing Exam in Taiwan, GPT-4 successfully meets the criteria.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Tests ChatGPT performance on Taiwan's Physician Licensing Exam, not about using AI for learner assessment or WBA feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Licensing-exam question answering study, not workplace-based assessment feedback or analysis of observation documentation."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1097/JCMA.0000000000001225
    pmid: "40083047"
    authors: Chen T, Lin K, Lin M, Chang H, Chen Y, Chen T
    year: "2025"
    abstract: |-
      BACKGROUND: This study investigates the performance of ChatGPT-3.5 and ChatGPT-4 in answering medical questions from Taiwan's Physician Licensing Exam, ranging from basic medical knowledge to specialized clinical topics. It aims to understand these artificial intelligence (AI) models' capabilities in a non-English context, specifically traditional Chinese.

      METHODS: The study incorporated questions from the Taiwan Physician Licensing Exam in 2022, excluding image-based queries. Each question was manually input into ChatGPT, and responses were compared with official answers from Taiwan's Ministry of Examination. Differences across specialties and question types were assessed using the Kruskal-Wallis and Fisher's exact tests.

      RESULTS: ChatGPT-3.5 achieved an average accuracy of 67.7% in basic medical sciences and 53.2% in clinical medicine. Meanwhile, ChatGPT-4 significantly outperformed ChatGPT-3.5, with average accuracies of 91.9% and 90.7%, respectively. ChatGPT-3.5 scored above 60.0% in seven out of 10 basic medical science subjects and three of 14 clinical subjects, while ChatGPT-4 scored above 60.0% in every subject. The type of question did not significantly affect accuracy rates.

      CONCLUSION: ChatGPT-3.5 showed proficiency in basic medical sciences but was less reliable in clinical medicine, whereas ChatGPT-4 demonstrated strong capabilities in both areas. However, their proficiency varied across different specialties. The type of question had minimal impact on performance. This study highlights the potential of AI models in medical education and non-English languages examination and the need for cautious and informed implementation in educational settings due to variability across specialties.
    mergedFrom:
      - source: pubmed
        pmid: "40083047"
        doi: 10.1097/JCMA.0000000000001225
    finalDecision: exclude
  - title: Comparison of ChatGPT-4, Copilot, Bard and Gemini Ultra on an Otolaryngology Question Bank.
    reviews:
      - decision: uncertain
        comment: LLMs on otolaryngology questions; abstract needed to determine WBA relevance
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Study compares LLM performance on an otolaryngology question bank rather than using AI to assess learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares LLM accuracy on question-bank MCQs, not workplace-based assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1111/coa.14302
    pmid: "40081900"
    authors: Ramchandani R, Guo E, Mostowy M, Kreutz J, Sahlollbey N, Carr M, Chung J, Caulley L
    year: "2025"
    abstract: |-
      OBJECTIVE: To compare the performance of Google Bard, Microsoft Copilot, GPT-4 with vision (GPT-4) and Gemini Ultra on the OTO Chautauqua, a student-created, faculty-reviewed otolaryngology question bank.

      STUDY DESIGN: Comparative performance evaluation of different LLMs.

      SETTING: N/A.

      PARTICIPANTS: N/A.

      METHODS: Large language models (LLMs) are being extensively tested in medical education. However, their accuracy and effectiveness remain understudied, particularly in otolaryngology. This study involved inputting 350 single-best-answer multiple choice questions, including 18 image-based questions, into four LLMS. Questions were sourced from six independent question banks related to (a) rhinology, (b) head and neck oncology, (c) endocrinology, (d) general otolaryngology, (e) paediatrics, (f) otology, (g) facial plastics, reconstruction and (h) trauma. LLMs were instructed to provide an output reasoning for their answers, the length of which was recorded.

      RESULTS: Aggregate and subgroup analysis revealed that Gemini (79.8%) outperformed the other LLMs, followed by GPT-4 (71.1%), Copilot (68.0%), and Bard (65.1%) in accuracy. The LLMs had significantly different average response lengths, with Bard (x̄ = 1685.24) being the longest and no difference between GPT-4 (x̄ = 827.34) and Copilot (x̄ = 904.12). Gemini's longer responses (x̄ =1291.68) included explanatory images and links. Gemini and GPT-4 correctly answered image-based questions (n = 18), unlike Copilot and Bard, highlighting their adaptability and multimodal capabilities.

      CONCLUSION: Gemini outperformed the other LLMs in terms of accuracy, followed by GPT-4, Copilot and Bard. GPT-4, although it has the second-highest accuracy, provides concise and relevant explanations. Despite the promising performance of LLMs, medical learners should cautiously assess accuracy and decision-making reliability.
    mergedFrom:
      - source: pubmed
        pmid: "40081900"
        doi: 10.1111/coa.14302
    finalDecision: exclude
  - title: "Evaluation of radiology residents' reporting skills using large language models: an observational study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: include
        comment: LLMs (GPT-4o) used to evaluate and provide feedback on radiology residents' clinical reports in a workplace training setting, with comparison to board-certified radiologist revisions and tracking of skill development over time. Meets all inclusion criteria.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
      - decision: include
        comment: "Uses generative AI (GPT-4o, Claude-3.5 Sonnet, Claude-3 Opus) to evaluate and provide feedback on radiology resident reports from actual clinical practice. Presents validity evidence (kappa agreement with board-certified radiologists) and educational impact data (tracking reporting skill development over time using Wilcoxon tests). Workplace-based setting: real patient CT/MRI reports written during residency rotations."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 03:36:32.323000+00:00
      - decision: include
        comment: Uses GPT-4o to evaluate and provide feedback-relevant analysis of real resident radiology reports in clinical training, with validity/agreement and longitudinal educational-impact data.
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 03:36:33.873000+00:00
    doi: 10.1007/s11604-025-01764-y
    pmid: "40056344"
    authors: Atsukawa N, Tatekawa H, Oura T, Matsushita S, Horiuchi D, Takita H, Mitsuyama Y, Omori A, Shimono T, Miki Y, Ueda D
    year: "2025"
    abstract: |-
      PURPOSE: Large language models (LLMs) have the potential to objectively evaluate radiology resident reports; however, research on their use for feedback in radiology training and assessment of resident skill development remains limited. This study aimed to assess the effectiveness of LLMs in revising radiology reports by comparing them with reports verified by board-certified radiologists and to analyze the progression of resident's reporting skills over time.

      MATERIALS AND METHODS: To identify the LLM that best aligned with human radiologists, 100 reports were randomly selected from 7376 reports authored by nine first-year radiology residents. The reports were evaluated based on six criteria: (1) addition of missing positive findings, (2) deletion of findings, (3) addition of negative findings, (4) correction of the expression of findings, (5) correction of the diagnosis, and (6) proposal of additional examinations or treatments. Reports were segmented into four time-based terms, and 900 reports (450 CT and 450 MRI) were randomly chosen from the initial and final terms of the residents' first year. The revised rates for each criterion were compared between the first and last terms using the Wilcoxon Signed-Rank test.

      RESULTS: Among the three LLMs-ChatGPT-4 Omni (GPT-4o), Claude-3.5 Sonnet, and Claude-3 Opus-GPT-4o demonstrated the highest level of agreement with board-certified radiologists. Significant improvements were noted in Criteria 1-3 when comparing reports from the first and last terms (Criteria 1, 2, and 3; P < 0.001, P = 0.023, and P = 0.004, respectively) using GPT-4o. No significant changes were observed for Criteria 4-6. Despite this, all criteria except for Criteria 6 showed progressive enhancement over time.

      CONCLUSION: LLMs can effectively provide feedback on commonly corrected areas in radiology reports, enabling residents to objectively identify and improve their weaknesses and monitor their progress. Additionally, LLMs may help reduce the workload of radiologists' mentors.
    mergedFrom:
      - source: pubmed
        pmid: "40056344"
        doi: 10.1007/s11604-025-01764-y
    finalDecision: include
  - title: "Introducing AI as members of script concordance test expert reference panel: A comparative analysis."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; Script Concordance Test is a written test format assessing clinical reasoning, not workplace-based assessment"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Script concordance vignette testing does not provide a workplace-based assessment component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.1080/0142159X.2025.2473620
    pmid: "40056088"
    authors: Sallam M, Abouzeid E
    year: "2025"
    abstract: |-
      BACKGROUND: The Script Concordance Test (SCT) is increasingly used in professional development to assess clinical reasoning, with linear progression in SCT performance observed as clinical experience increases. One challenge in implementing SCT is the potential burnout of expert reference panel (ERP) members. To address this, we introduced ChatGPT as panel members. The aim was to enhance the efficiency of SCT creation while maintaining educational content quality and to explore the effectiveness of different models as reference panels.

      METHODOLOGY: A quasi-experimental comparative design was employed, involving all undergraduate medical students and faculty members enrolled in the Ophthalmology clerkship. Two groups involved Traditional ERP which consisted of 15 experts, diversified in clinical experience: 5 senior residents, 5 lecturers, and 5 professors and AI-Generated ERP which is a panel generated using ChatGPT and o1 preview, designed to mirror diverse clinical opinions based on varying experience levels.

      RESULTS: Experts consistently achieved the highest mean scores across most vignettes, with ChatGPT-4 and o1 scores generally slightly lower. Notably, the o1 mean scores were closer to those of experts compared to ChatGPT-4. Significant differences were observed between ChatGPT-4 and o1 scores in certain vignettes. These values indicate a strong level of consistency, suggesting that both experts and AI models provided highly reliable ratings.

      CONCLUSION: These findings suggest that while AI models cannot replace human experts, they can be effectively used to train students, enhance reasoning skills, and help narrow the gap between student and expert performance.
    mergedFrom:
      - source: pubmed
        pmid: "40056088"
        doi: 10.1080/0142159X.2025.2473620
    finalDecision: exclude
  - title: "ChatGPT's Performance on Portuguese Medical Examination Questions: Comparative Analysis of ChatGPT-3.5 Turbo and ChatGPT-4o Mini."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on medical examination questions, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates ChatGPT performance on a national medical licensing exam (PNA); not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Exam-question performance study outside workplace-based or clinical education settings."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.2196/65108
    pmid: "40043219"
    authors: Prazeres F
    year: "2025"
    abstract: |-
      BACKGROUND: Advancements in ChatGPT are transforming medical education by providing new tools for assessment and learning, potentially enhancing evaluations for doctors and improving instructional effectiveness.

      OBJECTIVE: This study evaluates the performance and consistency of ChatGPT-3.5 Turbo and ChatGPT-4o mini in solving European Portuguese medical examination questions (2023 National Examination for Access to Specialized Training; Prova Nacional de Acesso à Formação Especializada [PNA]) and compares their performance to human candidates.

      METHODS: ChatGPT-3.5 Turbo was tested on the first part of the examination (74 questions) on July 18, 2024, and ChatGPT-4o mini on the second part (74 questions) on July 19, 2024. Each model generated an answer using its natural language processing capabilities. To test consistency, each model was asked, "Are you sure?" after providing an answer. Differences between the first and second responses of each model were analyzed using the McNemar test with continuity correction. A single-parameter t test compared the models' performance to human candidates. Frequencies and percentages were used for categorical variables, and means and CIs for numerical variables. Statistical significance was set at P<.05.

      RESULTS: ChatGPT-4o mini achieved an accuracy rate of 65% (48/74) on the 2023 PNA examination, surpassing ChatGPT-3.5 Turbo. ChatGPT-4o mini outperformed medical candidates, while ChatGPT-3.5 Turbo had a more moderate performance.

      CONCLUSIONS: This study highlights the advancements and potential of ChatGPT models in medical education, emphasizing the need for careful implementation with teacher oversight and further research.
    mergedFrom:
      - source: pubmed
        pmid: "40043219"
        doi: 10.2196/65108
    finalDecision: exclude
  - title: GPT-4 generated answer rationales to multiple choice assessment questions in undergraduate medical education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Undergraduate multiple-choice assessment context, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: AI-generated answer rationales for pre-clerkship MCQs; not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on generating MCQ rationales, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1186/s12909-025-06862-z
    pmid: "40038669"
    authors: Ch'en P, Day W, Pekson R, Barrientos J, Burton W, Ludwig A, Jariwala S, Cassese T
    year: "2025"
    abstract: |-
      BACKGROUND: Pre-clerkship medical students benefit from practice questions that provide rationales for answer choices. Creating these rationales is a time-intensive endeavor. Therefore, not all practice multiple choice questions (MCQ) have corresponding explanations to aid learning. The authors examined artificial intelligence's (AI) potential to create high-quality answer rationales for clinical vignette-style MCQs.

      METHODS: The authors conducted a single-center pre-post intervention survey study in August 2023 assessing 8 pre-clerkship course director (CD) attitudes towards GPT-4 generated answer rationales to clinical vignette style MCQs. Ten MCQs from each course's question bank were selected and input into GPT-4 with instructions to select the best answer and generate rationales for each answer choice. CDs were provided their unmodified GPT-4 interactions to assess the accuracy, clarity, appropriateness, and likelihood of implementation of the rationales. CDs were asked about time spent reviewing and making necessary modifications, satisfaction, and receptiveness in using GPT-4 for this purpose.

      RESULTS: GPT-4 correctly answered 75/80 (93.8%) questions on the first attempt. CDs were receptive to using GPT-4 for rationale generation and all were satisfied with the generated rationales. CDs determined that the majority of rationales were very accurate (77.5%), very clear (83.8%) and very appropriate (93.8%). Most rationales could be implemented with little or no modification (88.3%). All CDs would implement AI-generated answer rationales with CD editorial insights. Most CDs (75%) took ≤ 4 min to review a set of generated rationales for a question.

      CONCLUSION: GPT-4 is an acceptable and feasible tool for generating accurate, clear and appropriate answer rationales for MCQs in medical education. Future studies should examine students' feedback to generated rationales and further explore generating rationales for question with media. The authors plan to explore the implementation of this technological application at their medical school, including logistics and training to create a streamlined process that benefits both learners and educators.

      CLINICAL TRIAL: Not applicable; not a clinical trial.
    mergedFrom:
      - source: pubmed
        pmid: "40038669"
        doi: 10.1186/s12909-025-06862-z
    finalDecision: exclude
  - title: Comparitive performance of artificial intelligence-based large language models on the orthopedic in-training examination.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates LLM performance on orthopedic in-training examination (OITE) questions"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Tests LLM performance on in-training exam questions rather than workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.1177/10225536241268789
    pmid: "40028745"
    authors: Xu A, Singh M, Balmaceno-Criss M, Oh A, Leigh D, Daher M, Alsoof D, McDonald C, Diebo B, Daniels A
    year: "2025"
    abstract: |-
      BACKGROUND: Large language models (LLMs) have many clinical applications. However, the comparative performance of different LLMs on orthopedic board style questions remains largely unknown.

      METHODS: Three LLMs, OpenAI's GPT-4 and GPT-3.5, and Google Bard, were tested on 189 official 2022 Orthopedic In-Training Examination (OITE) questions. Comparative analyses were conducted to assess their performance against orthopedic resident scores and on higher-order, image-associated, and subject category-specific questions.

      RESULTS: GPT-4 surpassed the passing threshold for the 2022 OITE, performing at the level of PGY-3 to PGY-5 (p = .149, p = .502, and p = .818, respectively) and outperforming GPT-3.5 and Bard (p < .001 and p = .001, respectively). While GPT-3.5 and Bard did not meet the passing threshold for the exam, GPT-3.5 performed at the level of PGY-1 to PGY-2 (p = .368 and p = .019, respectively) and Bard performed at the level of PGY-1 to PGY-3 (p = .440, .498, and 0.036, respectively). GPT-4 outperformed both Bard and GPT-3.5 on image-associated (p = .003 and p < .001, respectively) and higher-order questions (p < .001). Among the 11 subject categories, all models performed similarly regardless of the subject matter. When individual LLM performance on higher-order questions was assessed, no significant differences were found compared to performance on first order questions (GPT-4 p = .139, GPT-3.5 p = .124, Bard p = .319). Finally, when individual model performance was assessed on image-associated questions, only GPT-3.5 performed significantly worse compared to performance on non-image-associated questions (p = .045).

      CONCLUSION: The AI-based LLM GPT-4, exhibits a robust ability to correctly answer a diverse range of OITE questions, exceeding the minimum score for the 2022 OITE, and outperforming predecessor GPT-3.5 and Google Bard.
    mergedFrom:
      - source: pubmed
        pmid: "40028745"
        doi: 10.1177/10225536241268789
    finalDecision: exclude
  - title: Can ChatGPT-4 perform as a competent physician based on the Chinese critical care examination?
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT-4 performance on critical care examination questions; not about assessment or feedback of learner performance."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam-question benchmark does not involve workplace-based assessment or clinical observation-record analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1016/j.jcrc.2024.155010
    pmid: "40023616"
    authors: Wang X, Tang J, Feng Y, Tang C, Wang X
    year: "2025"
    abstract: |-
      BACKGROUND: The use of ChatGPT in medical applications is of increasing interest. However, its efficacy in critical care medicine remains uncertain. This study aims to assess ChatGPT-4's performance in critical care examination, providing insights into its potential as a tool for clinical decision-making.

      METHODS: A dataset from the Chinese Health Professional Technical Qualification Examination for Critical Care Medicine, covering four components-fundamental knowledge, specialized knowledge, professional practical skills, and related medical knowledge-was utilized. ChatGPT-4 answered 600 questions, which were evaluated by critical care experts using a standardized rubric.

      RESULTS: ChatGPT-4 achieved a 73.5 % success rate, surpassing the 60 % passing threshold in four components, with the highest accuracy in fundamental knowledge (81.94 %). ChatGPT-4 performed significantly better on single-choice questions than on multiple-choice questions (76.72 % vs. 51.32 %, p < 0.001), while no significant difference was observed between case-based and non-case-based questions.

      CONCLUSION: ChatGPT demonstrated notable strengths in critical care examination, highlighting its potential for supporting clinical decision-making, information retrieval, and medical education. However, caution is required regarding its potential to generate inaccurate responses. Its application in critical care must therefore be carefully supervised by medical professionals to ensure both the accuracy of the information and patient safety.
    mergedFrom:
      - source: pubmed
        pmid: "40023616"
        doi: 10.1016/j.jcrc.2024.155010
    finalDecision: exclude
  - title: Exploring the potential of artificial intelligence chatbots in prosthodontics education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Study benchmarks AI chatbot performance on prosthodontics exam questions rather than using AI for learner assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Assesses chatbot accuracy on residency exam questions rather than workplace-based assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1186/s12909-025-06849-w
    pmid: "40016760"
    authors: Eraslan R, Ayata M, Yagci F, Albayrak H
    year: "2025"
    abstract: |-
      BACKGROUND: The purpose of this study was to evaluate the performance of widely used artificial intelligence (AI) chatbots in answering prosthodontics questions from the Dentistry Specialization Residency Examination (DSRE).

      METHODS: A total of 126 DSRE prosthodontics questions were divided into seven subtopics (dental morphology, materials science, fixed dentures, removable partial dentures, complete dentures, occlusion/temporomandibular joint, and dental implantology). Questions were translated into English by the authors, and this version of the questions were asked to five chatbots (ChatGPT-3.5, Gemini Advanced, Claude Pro, Microsoft Copilot, and Perplexity) within a 7-day period. Statistical analyses, including chi-square and z-tests, were performed to compare accuracy rates across the chatbots and subtopics at a significance level of 0.05.

      RESULTS: The overall accuracy rates for the chatbots were as follows: Copilot (73%), Gemini (63.5%), ChatGPT-3.5 (61.1%), Claude Pro (57.9%), and Perplexity (54.8%). Copilot significantly outperformed Perplexity (P = 0.035). However, no significant differences in accuracy were found across subtopics among chatbots. Questions on dental implantology had the highest accuracy rate (75%), while questions on removable partial dentures had the lowest (50.8%).

      CONCLUSION: Copilot showed the highest accuracy rate (73%), significantly outperforming Perplexity (54.8%). AI models demonstrate potential as educational support tools but currently face limitations in serving as reliable educational tools across all areas of prosthodontics. Future advancements in AI may lead to better integration and more effective use in dental education.
    mergedFrom:
      - source: pubmed
        pmid: "40016760"
        doi: 10.1186/s12909-025-06849-w
    finalDecision: exclude
  - title: "Novel Evaluation Metric and Quantified Performance of ChatGPT-4 Patient Management Simulations for Early Clinical Education: Experimental Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Explicitly based on patient management simulations"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: ChatGPT-based clinical simulations for preclinical medical students; assessment conducted entirely in simulated environment, not workplace-based."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted in simulated patient-management scenarios only, without a workplace-based clinical component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.2196/66478
    pmid: "40013991"
    authors: Scherr R, Spina A, Dao A, Andalib S, Halaseh F, Blair S, Wiechmann W, Rivera R
    year: "2025"
    abstract: |-
      BACKGROUND: Case studies have shown ChatGPT can run clinical simulations at the medical student level. However, no data have assessed ChatGPT's reliability in meeting desired simulation criteria such as medical accuracy, simulation formatting, and robust feedback mechanisms.

      OBJECTIVE: This study aims to quantify ChatGPT's ability to consistently follow formatting instructions and create simulations for preclinical medical student learners according to principles of medical simulation and multimedia educational technology.

      METHODS: Using ChatGPT-4 and a prevalidated starting prompt, the authors ran 360 separate simulations of an acute asthma exacerbation. A total of 180 simulations were given correct answers and 180 simulations were given incorrect answers. ChatGPT was evaluated for its ability to adhere to basic simulation parameters (stepwise progression, free response, interactivity), advanced simulation parameters (autonomous conclusion, delayed feedback, comprehensive feedback), and medical accuracy (vignette, treatment updates, feedback). Significance was determined with χ² analyses using 95% CIs for odds ratios.

      RESULTS: In total, 100% (n=360) of simulations met basic simulation parameters and were medically accurate. For advanced parameters, 55% (200/360) of all simulations delayed feedback, while the Correct arm (157/180, 87%) delayed feedback was significantly more than the Incorrect arm (43/180, 24%; P<.001). A total of 79% (285/360) of simulations concluded autonomously, and there was no difference between the Correct and Incorrect arms in autonomous conclusion (146/180, 81% and 139/180, 77%; P=.36). Overall, 78% (282/360) of simulations gave comprehensive feedback, and there was no difference between the Correct and Incorrect arms in comprehensive feedback (137/180, 76% and 145/180, 81%; P=.31). ChatGPT-4 was not significantly more likely to conclude simulations autonomously (P=.34) and provide comprehensive feedback (P=.27) when feedback was delayed compared to when feedback was not delayed.

      CONCLUSIONS: These simulations have the potential to be a reliable educational tool for simple simulations and can be evaluated by a novel 9-part metric. Per this metric, ChatGPT simulations performed perfectly on medical accuracy and basic simulation parameters. It performed well on comprehensive feedback and autonomous conclusion. Delayed feedback depended on the accuracy of user inputs. A simulation meeting one advanced parameter was not more likely to meet all advanced parameters. Further work must be done to ensure consistent performance across a broader range of simulation scenarios.
    mergedFrom:
      - source: pubmed
        pmid: "40013991"
        doi: 10.2196/66478
    finalDecision: exclude
  - title: "Artificial intelligence versus neurologists: A comparative study on multiple sclerosis expertise."
    reviews:
      - decision: uncertain
        comment: AI vs neurologists; may involve clinical assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Not focused on medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Tests AI medical knowledge against neurologists using MCQs, not about learner assessment or feedback in clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares AI and clinicians on multiple-choice questions, not assessment/feedback generation or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1016/j.clineuro.2025.108785
    pmid: "40010238"
    authors: Yaman Kula A, Durmaz &#xc7;eli K N, &#xd6;zben S, Yatmazo&#x11f;lu &#xc7;eti N M, K&#xf6;seo&#x11f;lu M
    year: "2025"
    abstract: |-
      INTRODUCTION: Multiple sclerosis (MS) is an autoimmune neurodegenerative disease affecting the central nervous system. MS diagnosis is complex, requiring magnetic resonance imaging and cerebrospinal fluid analysis due to the lack of definitive biomarkers. Although treatment advancements have reduced disability, diagnostic and therapeutic challenges remain, even among MS-specialized neurologists. Artificial Intelligence (AI) tools, which analyze large datasets, are increasingly used in healthcare, especially for diagnosis and treatment.

      OBJECTIVES: This study aims to assess the accuracy and scope of knowledge regarding MS, focusing on diagnosis, treatment options and management strategies, as tested among neurologists and AI bots.

      METHODS: Twenty multiple-choice questions, developed by MS-experienced neurology academics, were administered to 37 neurology specialists and 79 neurology residents in Turkey. The same questions were posed to AI platforms, including ChatGPT-4.0, GPT-4o, Gemini 1.5 Pro, Claude 3.5, Perplexity, and Perplexity Pro.

      RESULTS: Neurology specialists answered 12.05 ± 4.01 questions correctly on average, while residents scored 9.08 ± 3.41. Among residents with more than two years of training, the correct answer rate improved to 11.96 ± 3.5. Specialists active in MS clinics scored significantly higher than other neurologists (17.67 ± 1.75). AI platforms scored between 14 and 19 out of 20; with an average of 17.0 ± 1.79 with Claude 3.5 scoring highest.

      CONCLUSION: The findings suggest AI holds promise in supporting MS diagnosis and treatment, though challenges remain in nuanced cases. While AI complements neurologists, further studies are essential to understand its potential and limitations.

      DATA AVAILABILITY: Aggregated data will be shared upon written request to the corresponding author.
    mergedFrom:
      - source: pubmed
        pmid: "40010238"
        doi: 10.1016/j.clineuro.2025.108785
    finalDecision: exclude
  - title: Quality assurance and validity of AI-generated single best answer questions.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: About AI-generated exam questions (single best answer), exam-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: AI-generated SBA questions for formative medical school exams; not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: AI-generated SBA item development/psychometrics, not workplace-based assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1186/s12909-025-06881-w
    pmid: "40001164"
    authors: Ahmed A, Kerr E, O'Malley A
    year: "2025"
    abstract: |-
      BACKGROUND: Recent advancements in generative artificial intelligence (AI) have opened new avenues in educational methodologies, particularly in medical education. This study seeks to assess whether generative AI might be useful in addressing the depletion of assessment question banks, a challenge intensified during the Covid-era due to the prevalence of open-book examinations, and to augment the pool of formative assessment opportunities available to students. While many recent publications have sought to ascertain whether AI can achieve a passing standard in existing examinations, this study investigates the potential for AI to generate the exam itself. This research utilized a commercially available AI large language model (LLM), OpenAI GPT-4, to generate 220 single best answer (SBA) questions, adhering to Medical Schools Council Assessment Alliance guidelines the and a selection of Learning Outcomes (LOs) of the Scottish Graduate-Entry Medicine (ScotGEM) program. All questions were assessed by an expert panel for accuracy and quality. A total of 50 AI-generated and 50 human-authored questions were used to create two 50-item formative SBA examinations for Year 1 and Year 2 ScotGEM students. Each exam, delivered via the Speedwell eSystem, comprised 25 AI-generated and 25 human-authored questions presented in random order. Students completed the online, closed-book exams on personal devices under exam conditions that reflected summative examinations. The performance of both AI-generated and human-authored questions was evaluated, focusing on facility and discrimination index as key metrics. The screening process revealed that 69% of AI-generated SBAs were fit for inclusion in the examinations with little or no modifications required. Modifications, when necessary, were predominantly due to reasons such as the inclusion of "all of the above" options, usage of American English spellings, and non-alphabetized answer choices. 31% of questions were rejected for inclusion in the examinations, due to factual inaccuracies and non-alignment with students' learning. When included in an examination, post hoc statistical analysis indicated no significant difference in performance between the AI- and human- authored questions in terms of facility and discrimination index.

      DISCUSSION AND CONCLUSION: The outcomes of this study suggest that AI LLMs can generate SBA questions that are in line with best-practice guidelines and specific LOs. However, a robust quality assurance process is necessary to ensure that erroneous questions are identified and rejected. The insights gained from this research provide a foundation for further investigation into refining AI prompts, aiming for a more reliable generation of curriculum-aligned questions. LLMs show significant potential in supplementing traditional methods of question generation in medical education. This approach offers a viable solution to rapidly replenish and diversify assessment resources in medical curricula, marking a step forward in the intersection of AI and education.
    mergedFrom:
      - source: pubmed
        pmid: "40001164"
        doi: 10.1186/s12909-025-06881-w
    finalDecision: exclude
  - title: Evolving resource use for self-directed learning in physiology among first-year medical students in a classroom setting.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on self-directed learning resource use in a classroom setting"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    doi: 10.1152/advan.00236.2024
    pmid: "39993016"
    authors: Mondal H
    year: "2025"
    abstract: Self-directed learning (SDL) is integral to medical education. It helps in fostering critical thinking, independence, and problem-solving skills. With advancements in technology, digital tools like search engines, interactive content, and large language model (LLM) chatbots have become supplementary tools to traditional materials such as textbooks. However, limited data exist on SDL resource preferences among medical students in India since the inception of LLMs like ChatGPT. To address this, 64 medical students participated in a classroom-based SDL session on anemia. Students freely selected resources during a 40-min preparation period, followed by 20 min of writing responses to a total of five higher-order knowledge questions (i.e., questions started with "Explain why"). Postsession, they anonymously reported their resource use. Among 63 valid responses, 46.03% used one resource, 39.68% used two, and 14.29% used three. Search engines (61.9%) and LLM chatbots (60.32%) were the most frequently used, followed by textbooks (26.98%), with less reliance on notes, journals, and videos. Hence, there is a growing preference for search engines and LLM chatbots as an educational tool in self-directed learning in a classroom setting.NEW & NOTEWORTHY This study reports a shift in educational resource use for self-directed learning (SDL) among medical students after introduction of large language model (LLM) chatbots. Students use multiple sources, with digital tools like search engines and LLM chatbots nearly matching each other as the most preferred resources. Despite the accessibility and efficiency of digital tools, traditional resources like textbooks remain relevant, though less frequently chosen.
    mergedFrom:
      - source: pubmed
        pmid: "39993016"
        doi: 10.1152/advan.00236.2024
    finalDecision: exclude
  - title: Differentiating between GPT-generated and human-written feedback for radiology residents.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: include
        comment: Generative AI (GPT-3.5) used to generate feedback for radiology residents in a CBME diagnostic radiology program (workplace-based). Compares quality of AI-generated vs human-written narrative feedback, presenting data on distinguishability and quality. Relevant to AI-generated feedback in WBA.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
      - decision: include
        comment: GPT-3.5 used to generate synthetic narrative feedback for radiology residents in a CBME EPA-based assessment program. Workplace-based setting (EPA assessments from a Canadian Diagnostic Radiology residency). Presents data on quality comparison of human vs AI feedback, accuracy of source differentiation, and inter-rater agreement (Cohen's kappa, Fleiss' kappa).
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:52.271000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:00.184000+00:00
    doi: 10.1067/j.cpradiol.2025.02.002
    pmid: "39984362"
    authors: Zhou Z, Rizwan A, Rogoza N, Chung A, Kwan B
    year: "2025"
    abstract: |-
      PURPOSE: Recent competency-based medical education (CBME) implementation within Canadian radiology programs has required faculty to conduct more assessments. The rise of narrative feedback in CBME, coinciding with the rise of large language models (LLMs), raises questions about the potential of these models to generate informative comments matching human experts and associated challenges. This study compares human-written feedback to GPT-3.5-generated feedback for radiology residents, and how well raters can differentiate between these sources.

      METHODS: Assessments were completed by 28 faculty members for 10 residents within a Canadian Diagnostic Radiology program (2019-2023). Comments were extracted from Elentra, de-identified, and parsed into sentences, of which 110 were randomly selected for analysis. 11 of these comments were entered into GPT-3.5, generating 110 synthetic comments that were mixed with actual comments. Two faculty raters and GPT-3.5 read each comment to predict whether it was human-written or GPT-generated.

      RESULTS: Actual comments from humans were often longer and more specific than synthetic comments, especially when describing clinical procedures and patient interactions. Source differentiation was more difficult when both feedback types were similarly vague. Low agreement (k=-0.237) between responses provided by GPT-3.5 and humans was observed. Human raters were also more accurate (80.5 %) at identifying actual and synthetic comments than GPT-3.5 (50 %).

      CONCLUSION: Currently, GPT-3.5 cannot match human experts in delivering specific, nuanced feedback for radiology residents. Compared to humans, GPT-3.5 also performs worse in distinguishing between actual and synthetic comments. These insights could guide the development of more sophisticated algorithms to produce higher-quality feedback, supporting faculty development.
    mergedFrom:
      - source: pubmed
        pmid: "39984362"
        doi: 10.1067/j.cpradiol.2025.02.002
      - source: scopus
        doi: 10.1067/j.cpradiol.2025.02.002
        scopusId: SCOPUS_ID:85219020663
    finalDecision: include
  - title: "AI-powered standardised patients: evaluating ChatGPT-4o's impact on clinical case management in intern physicians."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Uses standardized patients, indicating assessment in simulated settings"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: ChatGPT-4o used as virtual standardized patient for intern physicians in a simulated environment; assessment conducted entirely in simulated settings, not workplace-based."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted in a simulated standardized-patient environment rather than workplace-based settings."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1186/s12909-025-06877-6
    pmid: "39979969"
    authors: "&#xd6;nc&#xfc; S, Torun F, &#xdc;lk&#xfc; H"
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial Intelligence is currently being applied in healthcare for diagnosis, decision-making and education. ChatGPT-4o, with its advanced language and problem-solving capabilities, offers an innovative alternative as a virtual standardised patient in clinical training. Intern physicians are expected to develop clinical case management skills such as problem-solving, clinical reasoning and crisis management. In this study, ChatGPT-4o's served as virtual standardised patient and medical interns as physicians on clinical case management. This study aimed to evaluate intern physicians' competencies in clinical case management; problem-solving, clinical reasoning, crisis management and explore the impact and potential of ChatGPT-4o as a viable tool for assessing these competencies.

      METHODS: This study used a simultaneous triangulation design, integrating quantitative and qualitative data. Conducted at Aydın Adnan Menderes University, with 21 sixth-year medical students, ChatGPT-4o simulated realistic patient interactions requiring competencies in clinical case management; problem-solving, clinical reasoning, crisis management. Data were gathered through self-assessment survey, semi-structured interviews, observations of the students and ChatGPT-4o during the process. Analyses included Pearson correlation, Chi-square, and Kruskal-Wallis tests, with content analysis conducted on qualitative data using MAXQDA software for coding.

      RESULTS: According to the findings, observation and self-assessment survey scores of intern physicians' clinical case management skills were positively correlated. There was a significant gap between participants' self-assessment and actual performance, indicating discrepancies in self-perceived versus real clinical competence. Participants reported feeling inadequate in their problem-solving and clinical reasoning competencies and experienced time pressure. They were satisfied with the Artificial Intelligence-powered standardised patient process and were willing to continue similar practices. Participants engaged with a uniform patient experience. Although participants were satisfied, the application process was sometimes negatively affected due to disconnection problems and language processing challenges.

      CONCLUSIONS: ChatGPT-4o successfully simulated patient interactions, providing a controlled environment without risking harm to real patients for practicing clinical case management. Although some of the technological challenges limited effectiveness, it was useful, cost-effective and accessible. It is thought that intern physicians will be better supported in acquiring clinical management skills through varied clinical scenarios using this method.

      CLINICAL TRIAL NUMBER: Not applicable.
    mergedFrom:
      - source: pubmed
        pmid: "39979969"
        doi: 10.1186/s12909-025-06877-6
    finalDecision: exclude
  - title: Benchmarking Vision Capabilities of Large Language Models in Surgical Examination Questions.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on surgical examination questions, standardized exam-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study benchmarks LLM vision capabilities on surgical licensing exam questions (GMLE and USMLE)."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Surgical licensing exam benchmarking is not conducted in a workplace-based clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.1016/j.jsurg.2025.103442
    pmid: "39923296"
    authors: Bereuter J, Geissler M, Klimova A, Steiner R, Pfeiffer K, Kolbinger F, Wiest I, Muti H, Kather J
    year: "2025"
    abstract: |-
      OBJECTIVE: Recent studies investigated the potential of large language models (LLMs) for clinical decision making and answering exam questions based on text input. Recent developments of LLMs have extended these models with vision capabilities. These image processing LLMs are called vision-language models (VLMs). However, there is limited investigation on the applicability of VLMs and their capabilities of answering exam questions with image content. Therefore, the aim of this study was to examine the performance of publicly accessible LLMs in 2 different surgical question sets consisting of text and image questions.

      DESIGN: Original text and image exam questions from 2 different surgical question subsets from the German Medical Licensing Examination (GMLE) and United States Medical Licensing Examination (USMLE) were collected and answered by publicly available LLMs (GPT-4, Claude-3 Sonnet, Gemini-1.5). LLM outputs were benchmarked for their accuracy in answering text and image questions. Additionally, the LLMs' performance was compared to students' performance based on their average historical performance (AHP) in these exams. Moreover, variations of LLM performance were analyzed in relation to question difficulty and respective image type.

      RESULTS: Overall, all LLMs achieved scores equivalent to passing grades (≥60%) on surgical text questions across both datasets. On image-based questions, only GPT-4 exceeded the score required to pass, significantly outperforming Claude-3 and Gemini-1.5 (GPT: 78% vs. Claude-3: 58% vs. Gemini-1.5: 57.3%; p < 0.001). Additionally, GPT-4 outperformed students on both text (GPT: 83.7% vs. AHP students: 67.8%; p < 0.001) and image questions (GPT: 78% vs. AHP students: 67.4%; p < 0.001).

      CONCLUSION: GPT-4 demonstrated substantial capabilities in answering surgical text and image exam questions. Therefore, it holds considerable potential for the use in surgical decision making and education of students and trainee surgeons.
    mergedFrom:
      - source: pubmed
        pmid: "39923296"
        doi: 10.1016/j.jsurg.2025.103442
    finalDecision: exclude
  - title: "AI versus human-generated multiple-choice questions for medical education: a cohort study in a high-stakes examination."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates AI-generated MCQs for a licensing examination (PEEM), not workplace-based assessment"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares AI- vs human-generated MCQs for an exam, not workplace-based assessment or feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.1186/s12909-025-06796-6
    pmid: "39923067"
    authors: Law A, So J, Lui C, Choi Y, Cheung K, Kei-Ching Hung K, Graham C
    year: "2025"
    abstract: |-
      BACKGROUND: The creation of high-quality multiple-choice questions (MCQs) is essential for medical education assessments but is resource-intensive and time-consuming when done by human experts. Large language models (LLMs) like ChatGPT-4o offer a promising alternative, but their efficacy remains unclear, particularly in high-stakes exams.

      OBJECTIVE: This study aimed to evaluate the quality and psychometric properties of ChatGPT-4o-generated MCQs compared to human-created MCQs in a high-stakes medical licensing exam.

      METHODS: A prospective cohort study was conducted among medical doctors preparing for the Primary Examination on Emergency Medicine (PEEM) organised by the Hong Kong College of Emergency Medicine in August 2024. Participants attempted two sets of 100 MCQs-one AI-generated and one human-generated. Expert reviewers assessed MCQs for factual correctness, relevance, difficulty, alignment with Bloom's taxonomy (remember, understand, apply and analyse), and item writing flaws. Psychometric analyses were performed, including difficulty and discrimination indices and KR-20 reliability. Candidate performance and time efficiency were also evaluated.

      RESULTS: Among 24 participants, AI-generated MCQs were easier (mean difficulty index = 0.78 ± 0.22 vs. 0.69 ± 0.23, p < 0.01) but showed similar discrimination indices to human MCQs (mean = 0.22 ± 0.23 vs. 0.26 ± 0.26). Agreement was moderate (ICC = 0.62, p = 0.01, 95% CI: 0.12-0.84). Expert reviews identified more factual inaccuracies (6% vs. 4%), irrelevance (6% vs. 0%), and inappropriate difficulty levels (14% vs. 1%) in AI MCQs. AI questions primarily tested lower-order cognitive skills, while human MCQs better assessed higher-order skills (χ² = 14.27, p = 0.003). AI significantly reduced time spent on question generation (24.5 vs. 96 person-hours).

      CONCLUSION: ChatGPT-4o demonstrates the potential for efficiently generating MCQs but lacks the depth needed for complex assessments. Human review remains essential to ensure quality. Combining AI efficiency with expert oversight could optimise question creation for high-stakes exams, offering a scalable model for medical education that balances time efficiency and content quality.
    mergedFrom:
      - source: pubmed
        pmid: "39923067"
        doi: 10.1186/s12909-025-06796-6
    finalDecision: exclude
  - title: "Unveiling GPT-4V's hidden challenges behind high accuracy on USMLE questions: Observational Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates GPT-4V on USMLE and radiology exam MCQs with images, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: USMLE and question-bank testing is not a workplace-based clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.2196/65146
    pmid: "39919278"
    authors: Yang Z, Yao Z, Tasmin M, Vashisht P, Jang W, Ouyang F, Wang B, McManus D, Berlowitz D, Yu H
    year: "2025"
    abstract: |-
      BACKGROUND: Recent advancements in artificial intelligence, such as GPT-3.5 Turbo (OpenAI) and GPT-4, have demonstrated significant potential by achieving good scores on text-only United States Medical Licensing Examination (USMLE) exams and effectively answering questions from physicians. However, the ability of these models to interpret medical images remains underexplored.

      OBJECTIVE: This study aimed to comprehensively evaluate the performance, interpretability, and limitations of GPT-3.5 Turbo, GPT-4, and its successor, GPT-4 Vision (GPT-4V), specifically focusing on GPT-4V's newly introduced image-understanding feature. By assessing the models on medical licensing examination questions that require image interpretation, we sought to highlight the strengths and weaknesses of GPT-4V in handling complex multimodal clinical information, thereby exposing hidden flaws and providing insights into its readiness for integration into clinical settings.

      METHODS: This cross-sectional study tested GPT-4V, GPT-4, and ChatGPT-3.5 Turbo on a total of 227 multiple-choice questions with images from USMLE Step 1 (n=19), Step 2 clinical knowledge (n=14), Step 3 (n=18), the Diagnostic Radiology Qualifying Core Exam (DRQCE) (n=26), and AMBOSS question banks (n=150). AMBOSS provided expert-written hints and question difficulty levels. GPT-4V's accuracy was compared with 2 state-of-the-art large language models, GPT-3.5 Turbo and GPT-4. The quality of the explanations was evaluated by choosing human preference between an explanation by GPT-4V (without hint), an explanation by an expert, or a tie, using 3 qualitative metrics: comprehensive explanation, question information, and image interpretation. To better understand GPT-4V's explanation ability, we modified a patient case report to resemble a typical "curbside consultation" between physicians.

      RESULTS: For questions with images, GPT-4V achieved an accuracy of 84.2%, 85.7%, 88.9%, and 73.1% in Step 1, Step 2 clinical knowledge, Step 3 of USMLE, and DRQCE, respectively. It outperformed GPT-3.5 Turbo (42.1%, 50%, 50%, 19.2%) and GPT-4 (63.2%, 64.3%, 66.7%, 26.9%). When GPT-4V answered correctly, its explanations were nearly as good as those provided by domain experts from AMBOSS. However, incorrect answers often had poor explanation quality: 18.2% (10/55) contained inaccurate text, 45.5% (25/55) had inference errors, and 76.3% (42/55) demonstrated image misunderstandings. With human expert assistance, GPT-4V reduced errors by an average of 40% (22/55). GPT-4V accuracy improved with hints, maintaining stable performance across difficulty levels, while medical student performance declined as difficulty increased. In a simulated curbside consultation scenario, GPT-4V required multiple specific prompts to interpret complex case data accurately.

      CONCLUSIONS: GPT-4V achieved high accuracy on multiple-choice questions with images, highlighting its potential in medical assessments. However, significant shortcomings were observed in the quality of explanations when questions were answered incorrectly, particularly in the interpretation of images, which could not be efficiently resolved through expert interaction. These findings reveal hidden flaws in the image interpretation capabilities of GPT-4V, underscoring the need for more comprehensive evaluations beyond multiple-choice questions before integrating GPT-4V into clinical settings.
    mergedFrom:
      - source: pubmed
        pmid: "39919278"
        doi: 10.2196/65146
    finalDecision: exclude
  - title: "Transforming medical education: leveraging large language models to enhance PBL-a proof-of-concept study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on problem-based learning (classroom-based), not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Problem-based learning context is not clearly workplace-based clinical education"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    doi: 10.1152/advan.00209.2024
    pmid: "39918742"
    authors: Arain S, Akhund S, Barakzai M, Meo S
    year: "2025"
    abstract: The alignment of learning materials with learning objectives (LOs) is critical for successfully implementing the problem-based learning (PBL) curriculum. This study investigated the capabilities of Gemini Advanced, a large language model (LLM), in creating clinical vignettes that align with LOs and comprehensive tutor guides. This study used a faculty-written clinical vignette about diabetes mellitus for third-year medical students. We submitted the LOs and the associated clinical vignette and tutor guide to the LLM to evaluate their alignment and generate new versions. Four faculty members compared both versions, using a structured questionnaire. The mean evaluation scores for original and LLM-generated versions are reported. The LLM identified new triggers for the clinical vignette to align it better with the LOs. Moreover, it restructured the tutor guide for better organization and flow and included thought-provoking questions. The medical information provided by the LLM was scientifically appropriate and accurate. The LLM-generated clinical vignette scored higher (3.0 vs. 1.25) for alignment with the LOs. However, the original version scored better for being educational level-appropriate (2.25 vs. 1.25) and adhering to PBL design (2.50 vs. 1.25). The LLM-generated tutor guide scored higher for better flow (3.0 vs. 1.25), comprehensive and relevant content (2.75 vs. 1.50), and thought-provoking questions (2.25 vs. 1.75). However, LLM-generated learning material lacked visual elements. In conclusion, this study demonstrated that Gemini could align and improve PBL learning materials. By leveraging the potential of LLMs while acknowledging their limitations, medical educators can create innovative and effective learning experiences for future physicians.NEW & NOTEWORTHY This study evaluated a large language model (LLM) (Gemini Advanced) for creating aligned problem-based learning (PBL) materials. The LLM improved the alignment of the clinical vignette with learning goals. The LLM also restructured the tutor guide and added thought-provoking questions. The LLM guide was well organized and informative, but the original vignette was considered more educational level-appropriate. Although the LLM could not generate visuals, AI can improve PBL materials, especially when combined with human expertise.
    mergedFrom:
      - source: pubmed
        pmid: "39918742"
        doi: 10.1152/advan.00209.2024
    finalDecision: exclude
  - title: ChatGPT-4 Performance on German Continuing Medical Education-Friend or Foe (Trick or Treat)? Protocol for a Randomized Controlled Trial.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Study protocol, not completed original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Protocol, not an original research article, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    doi: 10.2196/63887
    pmid: "39913914"
    authors: Burisch C, Bellary A, Breuckmann F, Ehlers J, Thal S, Sellmann T, G&#xf6;dde D
    year: "2025"
    abstract: |-
      BACKGROUND: The increasing development and spread of artificial and assistive intelligence is opening up new areas of application not only in applied medicine but also in related fields such as continuing medical education (CME), which is part of the mandatory training program for medical doctors in Germany. This study aimed to determine whether medical laypersons can successfully conduct training courses specifically for physicians with the help of a large language model (LLM) such as ChatGPT-4. This study aims to qualitatively and quantitatively investigate the impact of using artificial intelligence (AI; specifically ChatGPT) on the acquisition of credit points in German postgraduate medical education.

      OBJECTIVE: Using this approach, we wanted to test further possible applications of AI in the postgraduate medical education setting and obtain results for practical use. Depending on the results, the potential influence of LLMs such as ChatGPT-4 on CME will be discussed, for example, as part of a SWOT (strengths, weaknesses, opportunities, threats) analysis.

      METHODS: We designed a randomized controlled trial, in which adult high school students attempt to solve CME tests across six medical specialties in three study arms in total with 18 CME training courses per study arm under different interventional conditions with varying amounts of permitted use of ChatGPT-4. Sample size calculation was performed including guess probability (20% correct answers, SD=40%; confidence level of 1-α=.95/α=.05; test power of 1-β=.95; P<.05). The study was registered at open scientific framework.

      RESULTS: As of October 2024, the acquisition of data and students to participate in the trial is ongoing. Upon analysis of our acquired data, we predict our findings to be ready for publication as soon as early 2025.

      CONCLUSIONS: We aim to prove that the advances in AI, especially LLMs such as ChatGPT-4 have considerable effects on medical laypersons' ability to successfully pass CME tests. The implications that this holds on how the concept of continuous medical education requires reevaluation are yet to be contemplated.

      TRIAL REGISTRATION: OSF Registries 10.17605/OSF.IO/MZNUF; https://osf.io/mznuf.

      INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): PRR1-10.2196/63887.
    mergedFrom:
      - source: pubmed
        pmid: "39913914"
        doi: 10.2196/63887
    finalDecision: exclude
  - title: "Enhancing reflective practice with ChatGPT: A new approach to assignment design."
    reviews:
      - decision: uncertain
        comment: ChatGPT for reflective practice; reflection is core to WBA and may involve assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Uses ChatGPT for designing reflective practice assignments, not for assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on assignment design support, not assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1080/0142159X.2025.2461534
    pmid: "39912565"
    authors: Samuel A, Soh M, Jung E
    year: "2025"
    abstract: "What was the educational challenge? Reflective practice (RP) fosters professional growth but is often hindered by unclear purpose and minimal guidance. Designing comprehensive RP assignments is challenging because it takes time and not all faculty possess the skills to generate effective assignments. This innovation addressed two challenges: (1) creating clear, scaffolded reflective assignments using the transparent assessment framework (TAF) and (2) reducing faculty workload in assignment design. What was the solution? ChatGPT 4o was used to design RP assignments in health professions education (HPE). The TAF guided the design of the assessment. How was the solution implemented? A four-step process was followed to facilitate the design of the assignment. ChatGPT 4o was prompted to design the assignment and refined for the course. What lessons were learned that are relevant to a wider global audience? A pilot study in three graduate-level HPE courses showed ChatGPT-assisted assignments improved clarity, structure, and student performance while decreasing faculty preparation time. What are the next steps? We plan to expand this research to obtain student feedback on the effectiveness of the redesigned assignment and to explore the effectiveness of AI-generated reflective assignments with medical students who may require more structured guidance and contextualized prompts."
    mergedFrom:
      - source: pubmed
        pmid: "39912565"
        doi: 10.1080/0142159X.2025.2461534
    finalDecision: exclude
  - title: Benchmarking LLM chatbots' oncological knowledge with the Turkish Society of Medical Oncology's annual board examination questions.
    reviews:
      - decision: uncertain
        comment: LLM chatbots on oncology board exam; abstract needed to determine WBA relevance
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks LLMs on oncology board examination questions; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1186/s12885-025-13596-0
    pmid: "39905358"
    authors: Erdat E, Kavak E
    year: "2025"
    abstract: |-
      BACKGROUND: Large language models (LLMs) have shown promise in various medical applications, including clinical decision-making and education. In oncology, the increasing complexity of patient care and the vast volume of medical literature require efficient tools to assist practitioners. However, the use of LLMs in oncology education and knowledge assessment remains underexplored. This study aims to evaluate and compare the oncological knowledge of four LLMs using standardized board examination questions.

      METHODS: We assessed the performance of four LLMs-Claude 3.5 Sonnet (Anthropic), ChatGPT 4o (OpenAI), Llama-3 (Meta), and Gemini 1.5 (Google)-using the Turkish Society of Medical Oncology's annual board examination questions from 2016 to 2024. A total of 790 valid multiple-choice questions covering various oncology topics were included. Each model was tested on its ability to answer these questions in Turkish. Performance was analyzed based on the number of correct answers, with statistical comparisons made using chi-square tests and one-way ANOVA.

      RESULTS: Claude 3.5 Sonnet outperformed the other models, passing all eight exams with an average score of 77.6%. ChatGPT 4o passed seven out of eight exams, with an average score of 67.8%. Llama-3 and Gemini 1.5 showed lower performance, passing four and three exams respectively, with average scores below 50%. Significant differences were observed among the models' performances (F = 17.39, p < 0.001). Claude 3.5 and ChatGPT 4.0 demonstrated higher accuracy across most oncology topics. A decline in performance in recent years, particularly in the 2024 exam, suggests limitations due to outdated training data.

      CONCLUSIONS: Significant differences in oncological knowledge were observed among the four LLMs, with Claude 3.5 Sonnet and ChatGPT 4o demonstrating superior performance. These findings suggest that advanced LLMs have the potential to serve as valuable tools in oncology education and decision support. However, regular updates and enhancements are necessary to maintain their relevance and accuracy, especially to incorporate the latest medical advancements.
    mergedFrom:
      - source: pubmed
        pmid: "39905358"
        doi: 10.1186/s12885-025-13596-0
    finalDecision: exclude
  - title: "Impact of artificial intelligence on the academic performance and test anxiety of pharmacy students in objective structured clinical examination: a randomized controlled trial."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted in OSCE (simulated environment), not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is in OSCE (simulated setting) without a clear workplace-based component."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    doi: 10.1007/s11096-025-01876-5
    pmid: "39903358"
    authors: Ali M, Rehman S, Cheema E
    year: "2025"
    abstract: |-
      BACKGROUND: The rapid advancement of generative artificial intelligence (AI) in recent years has led to its increased application across various fields including education. One area where AI can significantly impact is clinical education, particularly in the preparation and execution of objective structured clinical examinations (OSCEs).

      AIM: This study aimed to evaluate the impact of AI-generated study material and feedback on the academic performance and level of anxiety of pharmacy students in formative OSCE.

      METHOD: This was a 4-week (June-July 2024) randomized controlled study. Students of 6th semester PharmD program were randomized into either an intervention or control group. The intervention group received intervention which comprised a comprehensive training session on how to use AI tools (ChatGPT, Gemini and Perplexity) for generating study materials with personalized feedback, in addition to usual OSCE instructions. The control group only received the usual OSCE instructions. In addition, all students completed the test anxiety inventory (TAI) questionnaire before the OSCE.

      RESULTS: Eighty-eight (40 male, 48 female) out of 92 (96%) students attended the OSCE and completed the TAI questionnaire. Each group had 44 (50%) students. The mean OSCE mark was 13.26 (± 5.05) out of 30. No significant difference was found between the intervention [12.98 (± 5.15)] and control [13.54 (± 5.00)] groups regarding mean OSCE marks (p = 0.550). Similarly, no significant difference was found between the groups regarding the total TAI score (p = 0.917).

      CONCLUSION: While the use of AI tools did not improve the academic performance of students or reduce test-related anxiety, they did not negatively impact these outcomes either. Future research should investigate the long-term effects of AI-based interventions on educational outcomes.
    mergedFrom:
      - source: pubmed
        pmid: "39903358"
        doi: 10.1007/s11096-025-01876-5
    finalDecision: exclude
  - title: "Accuracy, satisfaction, and impact of custom GPT in acquiring clinical knowledge: Potential for AI-assisted medical education."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; evaluates custom GPT as a clinical knowledge acquisition tool for medical education"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Studies AI-assisted clinical knowledge acquisition and test scores, not workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.1080/0142159X.2025.2458808
    pmid: "39893644"
    authors: Pu J, Hong J, Yu Q, Yu P, Tian J, He Y, Huang H, Yuan Q, Tao L, Peng Z
    year: "2025"
    abstract: |-
      BACKGROUND: Recent advancements in artificial intelligence (AI) have enabled the customization of large language models to address specific domains such as medical education. This study investigates the practical performance of a custom GPT model in enhancing clinical knowledge acquisition for medical students and physicians.

      METHODS: A custom GPT was developed by incorporating the latest readily available teaching resources. Its accuracy in providing clinical knowledge was evaluated using a set of clinical questions, and responses were compared against established medical guidelines. Satisfaction was assessed through surveys involving medical students and physicians at different stages and from various types of hospitals. The impact of the custom GPT was further evaluated by comparing its role in facilitating clinical knowledge acquisition with traditional learning methods.

      RESULTS: The custom GPT demonstrated higher accuracy (83.6%) compared to general AI models (65.5%, 69.1%) and was comparable to a professionally developed AI (Glass Health, 83.6%). Residents reported the highest satisfaction compared to clerks and physicians, citing improved learning independence, motivation, and confidence (p < 0.05). Physicians, especially those from teaching hospitals, showed greater eagerness to develop a custom GPT compared to clerks and residents (p < 0.05). The impact analysis revealed that residents using the custom GPT achieved better test scores compared to those using traditional resources (p < 0.05), though fewer perfect scores were obtained.

      CONCLUSIONS: The custom GPT demonstrates significant promise as an innovative tool for advancing medical education, particularly for residents. Its capability to deliver accurate, tailored information complements traditional teaching methods, aiding educators in promoting personalized and consistent training. However, it is essential for both learners and educators to remain critical in evaluating AI-generated information. With continued development and thoughtful integration, AI tools like custom GPTs have the potential to significantly enhance the quality and accessibility of medical education.
    mergedFrom:
      - source: pubmed
        pmid: "39893644"
        doi: 10.1080/0142159X.2025.2458808
    finalDecision: exclude
  - title: "Empowering medical students with AI writing co-pilots: design and validation of AI self-assessment toolkit."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; focuses on academic writing self-assessment toolkit for medical students, not clinical/workplace assessment"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on academic writing self-assessment rather than clinical/workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.1186/s12909-025-06753-3
    pmid: "39891148"
    authors: Khojasteh L, Kafipour R, Pakdel F, Mukundan J
    year: "2025"
    abstract: |-
      BACKGROUND AND OBJECTIVES: Assessing and improving academic writing skills is a crucial component of higher education. To support students in this endeavor, a comprehensive self-assessment toolkit was developed to provide personalized feedback and guide their writing improvement. The current study aimed to rigorously evaluate the validity and reliability of this academic writing self-assessment toolkit.

      METHODS: The development and validation of the academic writing self-assessment toolkit involved several key steps. First, a thorough review of the literature was conducted to identify the essential criteria for authentic assessment. Next, an analysis of medical students' reflection papers was undertaken to gain insights into their experiences using AI-powered tools for writing feedback. Based on these initial steps, a preliminary version of the self-assessment toolkit was devised. An expert focus group discussion was then convened to refine the questions and content of the toolkit. To assess content validity, the toolkit was evaluated by a panel of 22 medical student participants. They were asked to review each item and provide feedback on the relevance and comprehensiveness of the toolkit for evaluating academic writing skills. Face validity was also examined, with the students assessing the clarity, wording, and appropriateness of the toolkit items.

      RESULTS: The content validity evaluation revealed that 95% of the toolkit items were rated as highly relevant, and 88% were deemed comprehensive in assessing key aspects of academic writing. Minor wording changes were suggested by the students to enhance clarity and interpretability. The face validity assessment found that 92% of the items were rated as unambiguous, with 90% considered appropriate and relevant for self-assessment. Feedback from the students led to the refinement of a few items to improve their clarity in the context of the Persian language. The robust reliability testing demonstrated the consistency and stability of the academic writing self-assessment toolkit in measuring students' writing skills over time.

      CONCLUSION: The comprehensive evaluation process has established the academic writing self-assessment toolkit as a robust and credible instrument for supporting students' writing improvement. The toolkit's strong psychometric properties and user-centered design make it a valuable resource for enhancing academic writing skills in higher education.
    mergedFrom:
      - source: pubmed
        pmid: "39891148"
        doi: 10.1186/s12909-025-06753-3
      - source: scopus
        doi: 10.1186/s12909-025-06753-3
        scopusId: SCOPUS_ID:85217645896
    finalDecision: exclude
  - title: Evaluating the Performance of Large Language Models (LLMs) in Answering and Analysing the Chinese Dental Licensing Examination.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Evaluates LLM performance on standardized dental licensing examination questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Licensing exam question-answering/explanation study, not workplace-based assessment or feedback on clinical observations."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.1111/eje.13073
    pmid: "39889108"
    authors: Xiong Y, Zhan Z, Zhong C, Zeng W, Guo J, Tang W, Liu C
    year: "2025"
    abstract: |-
      BACKGROUND: This study aimed to simulate diverse scenarios of students employing LLMs for CDLE examination preparation, providing a detailed evaluation of their performance in medical education.

      METHODS: A stratified random sampling strategy was implemented to select and subsequently revise 200 questions from the CDLE. Seven LLMs, recognised for their exceptional performance in the Chinese domain, were selected as test subjects. Three distinct testing scenarios were constructed: answering questions, explaining questions and adversarial testing. The evaluation metrics included accuracy, agreement rate and teaching effectiveness score. Wald χ2 tests and Kruskal-Wallis tests were employed to determine whether the differences among the LLMs across various scenarios and before and after adversarial testing were statistically significant.

      RESULTS: The majority of the tested LLMs met the passing threshold on the CDLE benchmark, with Doubao-pro 32k and Qwen2-72b (81%) achieving the highest accuracy rates. Doubao-pro 32k demonstrated the highest 98% agreement rate with the reference answers when providing explanations. Although statistically significant differences existed among various LLMs in their teaching effectiveness scores based on the Likert scale, all these models demonstrated a commendable ability to deliver comprehensible and effective instructional content. In adversarial testing, GPT-4 exhibited the smallest decline in accuracy (2%, p = 0.623), while ChatGLM-4 demonstrated the least reduction in agreement rate (14.6%, p = 0.001).

      CONCLUSIONS: LLMs trained on Chinese corpora, such as Doubao-pro 32k, demonstrated superior performance compared to GPT-4 in answering and explaining questions, with no statistically significant difference. However, during adversarial testing, all models exhibited diminished performance, with GPT-4 displaying comparatively greater robustness. Future research should further investigate the interpretability of LLM outputs and develop strategies to mitigate hallucinations generated in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "39889108"
        doi: 10.1111/eje.13073
    finalDecision: exclude
  - title: Performance of Chat Generative Pre-Trained Transformer on Personal Review of Learning in Obstetrics and Gynecology.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT performance on PROLOG board preparation questions; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted on board-preparation assessment questions only."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.14423/SMJ.0000000000001783
    pmid: "39883147"
    authors: Cohen A, Burns J, Gabra M, Gordon A, Deebel N, Terlecki R, Woodburn K
    year: "2025"
    abstract: |-
      OBJECTIVES: Chat Generative Pre-Trained Transformer (ChatGPT) is a popular natural-language processor that is able to analyze and respond to a variety of prompts, providing eloquent answers based on a collection of Internet data. ChatGPT has been considered an avenue for the education of resident physicians in the form of board preparation in the contemporary literature, where it has been applied against board study material across multiple medical specialties. The purpose of our study was to evaluate the performance of ChatGPT on the Personal Review of Learning in Obstetrics and Gynecology (PROLOG) assessments and gauge its specialty specific knowledge for educational applications.

      METHODS: PROLOG assessments were administered to ChatGPT version 3.5, and the percentage of correct responses was recorded. Questions were categorized by question stem order and used to measure ChatGPT performance. Performance was compared using descriptive statistics.

      RESULTS: There were 848 questions without visual components; ChatGPT answered 57.8% correct (N = 490). ChatGPT performed worse on higher-order questions compared with first-order questions, 56.8% vs 60.5%, respectively. There were 65 questions containing visual data, and ChatGPT answered 16.9% correctly.

      CONCLUSIONS: The passing score for the PROLOG assessments is 80%; therefore ChatGPT 3.5 did not perform satisfactorily. Given this, it is unlikely that the tested version of ChatGPT has sufficient specialty-specific knowledge or logical capability to serve as a reliable tool for trainee education.
    mergedFrom:
      - source: pubmed
        pmid: "39883147"
        doi: 10.14423/SMJ.0000000000001783
    finalDecision: exclude
  - title: "Performance of ChatGPT-3.5 and ChatGPT-4 in the Taiwan National Pharmacist Licensing Examination: Comparative Evaluation Study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on pharmacist licensing examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates LLM performance on a pharmacist licensing examination"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Licensing-exam performance study, not assessment/feedback or analysis of workplace clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.2196/56850
    pmid: "39864950"
    authors: Wang Y, Shen H, Chen T, Chiang S, Lin T
    year: "2025"
    abstract: |-
      BACKGROUND: OpenAI released versions ChatGPT-3.5 and GPT-4 between 2022 and 2023. GPT-3.5 has demonstrated proficiency in various examinations, particularly the United States Medical Licensing Examination. However, GPT-4 has more advanced capabilities.

      OBJECTIVE: This study aims to examine the efficacy of GPT-3.5 and GPT-4 within the Taiwan National Pharmacist Licensing Examination and to ascertain their utility and potential application in clinical pharmacy and education.

      METHODS: The pharmacist examination in Taiwan consists of 2 stages: basic subjects and clinical subjects. In this study, exam questions were manually fed into the GPT-3.5 and GPT-4 models, and their responses were recorded; graphic-based questions were excluded. This study encompassed three steps: (1) determining the answering accuracy of GPT-3.5 and GPT-4, (2) categorizing question types and observing differences in model performance across these categories, and (3) comparing model performance on calculation and situational questions. Microsoft Excel and R software were used for statistical analyses.

      RESULTS: GPT-4 achieved an accuracy rate of 72.9%, overshadowing GPT-3.5, which achieved 59.1% (P<.001). In the basic subjects category, GPT-4 significantly outperformed GPT-3.5 (73.4% vs 53.2%; P<.001). However, in clinical subjects, only minor differences in accuracy were observed. Specifically, GPT-4 outperformed GPT-3.5 in the calculation and situational questions.

      CONCLUSIONS: This study demonstrates that GPT-4 outperforms GPT-3.5 in the Taiwan National Pharmacist Licensing Examination, particularly in basic subjects. While GPT-4 shows potential for use in clinical practice and pharmacy education, its limitations warrant caution. Future research should focus on refining prompts, improving model stability, integrating medical databases, and designing questions that better assess student competence and minimize guessing.
    mergedFrom:
      - source: pubmed
        pmid: "39864950"
        doi: 10.2196/56850
    finalDecision: exclude
  - title: "Factors Associated With the Accuracy of Large Language Models in Basic Medical Science Examinations: Cross-Sectional Study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on basic medical science examinations, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluating LLM accuracy on basic medical science exam MCQs and associated factors; benchmarking AI, not using AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Basic science licensing-style exam accuracy study, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.2196/58898
    pmid: "39846415"
    authors: Kaewboonlert N, Poontananggul J, Pongsuwan N, Bhakdisongkhram G
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) has become widely applied across many fields, including medical education. Content validation and its answers are based on training datasets and the optimization of each model. The accuracy of large language model (LLMs) in basic medical examinations and factors related to their accuracy have also been explored.

      OBJECTIVE: We evaluated factors associated with the accuracy of LLMs (GPT-3.5, GPT-4, Google Bard, and Microsoft Bing) in answering multiple-choice questions from basic medical science examinations.

      METHODS: We used questions that were closely aligned with the content and topic distribution of Thailand's Step 1 National Medical Licensing Examination. Variables such as the difficulty index, discrimination index, and question characteristics were collected. These questions were then simultaneously input into ChatGPT (with GPT-3.5 and GPT-4), Microsoft Bing, and Google Bard, and their responses were recorded. The accuracy of these LLMs and the associated factors were analyzed using multivariable logistic regression. This analysis aimed to assess the effect of various factors on model accuracy, with results reported as odds ratios (ORs).

      RESULTS: The study revealed that GPT-4 was the top-performing model, with an overall accuracy of 89.07% (95% CI 84.76%-92.41%), significantly outperforming the others (P<.001). Microsoft Bing followed with an accuracy of 83.69% (95% CI 78.85%-87.80%), GPT-3.5 at 67.02% (95% CI 61.20%-72.48%), and Google Bard at 63.83% (95% CI 57.92%-69.44%). The multivariable logistic regression analysis showed a correlation between question difficulty and model performance, with GPT-4 demonstrating the strongest association. Interestingly, no significant correlation was found between model accuracy and question length, negative wording, clinical scenarios, or the discrimination index for most models, except for Google Bard, which showed varying correlations.

      CONCLUSIONS: The GPT-4 and Microsoft Bing models demonstrated equal and superior accuracy compared to GPT-3.5 and Google Bard in the domain of basic medical science. The accuracy of these models was significantly influenced by the item's difficulty index, indicating that the LLMs are more accurate when answering easier questions. This suggests that the more accurate models, such as GPT-4 and Bing, can be valuable tools for understanding and learning basic medical science concepts.
    mergedFrom:
      - source: pubmed
        pmid: "39846415"
        doi: 10.2196/58898
    finalDecision: exclude
  - title: Assessing readability of explanations and reliability of answers by GPT-3.5 and GPT-4 in non-traumatic spinal cord injury education.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Assessing readability of GPT-generated patient education content, not about assessment of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Title does not indicate workplace-based assessment or feedback activity."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.1080/0142159X.2024.2430365
    pmid: "39832525"
    authors: Garc&#xed;a-Rudolph A, Sanchez-Pinsach D, Wright M, Opisso E, Vidal J
    year: "2025"
    abstract: |-
      PURPOSE: Our study aimed to: i) Assess the readability of textbook explanations using established indexes; ii) Compare these with GPT-4's default explanations, ensuring similar word counts for direct comparisons; iii) Evaluate GPT-4's adaptability by simplifying high-complexity explanations; iv) Determine the reliability of GPT-3.5 and GPT-4 in providing accurate answers.

      MATERIAL AND METHODS: We utilized a textbook designed for ABPMR certification. Our analysis covered 50 multiple-choice questions, each with a detailed explanation, focusing on non-traumatic spinal cord injury (NTSCI).

      RESULTS: Our analysis revealed statistically significant differences in readability scores, with the textbook achieving 14.5 (SD = 2.5) compared to GPT-4's 17.3 (SD = 1.9), indicating that GPT-4's explanations are generally more complex (p < 0.001). Using the Flesch Reading Ease Score, 86% of GPT-4's explanations fell into the 'Very difficult' category, significantly higher than the textbook's 58% (p = 0.006). GPT-4 successfully demonstrated adaptability by reducing the mean readability score of the top-nine most complex explanations, maintaining the word count. Regarding reliability, GPT-3.5 and GPT-4 scored 84% and 96% respectively, with GPT-4 outperforming GPT-3.5 (p = 0.046).

      CONCLUSIONS: Our results confirmed GPT-4's potential in medical education by providing highly accurate yet often complex explanations for NTSCI, which were successfully simplified without losing accuracy.
    mergedFrom:
      - source: pubmed
        pmid: "39832525"
        doi: 10.1080/0142159X.2024.2430365
    finalDecision: exclude
  - title: A randomised cross-over trial assessing the impact of AI-generated individual feedback on written online assignments for medical students.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Title indicates written online assignment context rather than workplace-based clinical education"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT generates feedback on key-feature questions delivered via a mobile app as online self-study; not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Educational intervention on online assignments, not a workplace-based or clinical education assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.1080/0142159X.2025.2451870
    pmid: "39831699"
    authors: Nissen L, Rother J, Heinemann M, Reimer L, Jonas S, Raupach T
    year: "2025"
    abstract: |-
      PURPOSE: Self-testing has been proven to significantly improve not only simple learning outcomes, but also higher-order skills such as clinical reasoning in medical students. Previous studies have shown that self-testing was especially beneficial when it was presented with feedback, which leaves the question whether an immediate and personalized feedback further encourages this effect. Therefore, we hypothesised that individual feedback has a greater effect on learning outcomes, compared to generic feedback.

      MATERIALS AND METHODS: In a randomised cross-over trial, German medical students were invited to voluntarily answer daily key-feature questions via an App. For half of the items they received a generalised feedback by an expert, while the feedback on the other half was generated immediately through ChatGPT. After the intervention, the students participated in a mandatory exit exam.

      RESULTS: Those participants who used the app more frequently experienced a better learning outcome compared to those who did not use it frequently, even though this finding was only examined in a correlative nature. The individual ChatGPT generated feedback did not show a greater effect on exit exam scores compared to the expert comment (51.8 ± 22.0% vs. 55.8 ± 22.8%; p = 0.06).

      CONCLUSION: This study proves the concept of providing personalised feedback on medical questions. Despite the promising results, improved prompting and further development of the application seems necessary to strengthen the possible impact of the personalised feedback. Our study closes a research gap and holds great potential for further use not only in medicine but also in other academic fields.
    mergedFrom:
      - source: pubmed
        pmid: "39831699"
        doi: 10.1080/0142159X.2025.2451870
    finalDecision: exclude
  - title: Claude, ChatGPT, Copilot, and Gemini performance versus students in different topics of neuroscience.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Title indicates topic-based comparison, not workplace-based clinical education context"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Compares AI chatbot vs student performance on neuroscience MCQs to benchmark AI capability, not about using AI for learner assessment or WBA feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares chatbot versus student performance on neuroscience MCQs, not workplace-based assessment or feedback on clinical observations."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1152/advan.00093.2024
    pmid: "39824512"
    authors: Mavrych V, Yaqinuddin A, Bolgova O
    year: "2025"
    abstract: Despite extensive studies on large language models and their capability to respond to questions from various licensed exams, there has been limited focus on employing chatbots for specific subjects within the medical curriculum, specifically medical neuroscience. This research compared the performances of Claude 3.5 Sonnet (Anthropic), GPT-3.5 and GPT-4-1106 (OpenAI), Copilot free version (Microsoft), and Gemini 1.5 Flash (Google) versus students on multiple-choice questions (MCQs) from the medical neuroscience course database to evaluate chatbot reliability. Five successive attempts of each chatbot to answer 200 United States Medical Licensing Examination (USMLE)-style questions were evaluated based on accuracy, relevance, and comprehensiveness. MCQs were categorized into 12 categories/topics. The results indicated that, at the current level of development, selected AI-driven chatbots, on average, can accurately answer 67.2% of MCQs from the medical neuroscience course, which is 7.4% below the students' average. However, Claude and GPT-4 outperformed other chatbots, with 83% and 81.7% correct answers, which is better than the average student result. They were followed by Copilot (59.5%), GPT-3.5 (58.3%), and Gemini (53.6%). Concerning different categories, Neurocytology, Embryology, and Diencephalon were the three best topics, with average results of 78.1-86.7%, and the lowest results were for Brain stem, Special senses, and Cerebellum, with 54.4-57.7% correct answers. Our study suggested that Claude and GPT-4 are currently two of the most evolved chatbots. They exhibit proficiency in answering MCQs related to neuroscience that surpasses that of the average medical student. This breakthrough indicates a significant milestone in how AI can supplement and enhance educational tools and techniques.NEW & NOTEWORTHY This research evaluates the effectiveness of different AI-driven large language models (Claude, ChatGPT, Copilot, and Gemini) compared to medical students in answering neuroscience questions. The study offers insights into the specific areas of neuroscience in which these chatbots may excel or have limitations, providing a comprehensive analysis of chatbots' current capabilities in processing and interacting with certain topics of the basic medical sciences curriculum.
    mergedFrom:
      - source: pubmed
        pmid: "39824512"
        doi: 10.1152/advan.00093.2024
      - source: scopus
        doi: 10.1152/advan.00093.2024
        scopusId: SCOPUS_ID:105002561167
    finalDecision: exclude
  - title: "Performance Evaluation and Implications of Large Language Models in Radiology Board Exams: Prospective Comparative Analysis."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Radiology board exams, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; benchmarks LLM performance on radiology board exam MCQs."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Board-exam question answering study, not assessment/feedback in workplace-based clinical education or analysis of observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.2196/64284
    pmid: "39819381"
    authors: Wei B
    year: "2025"
    abstract: |-
      BACKGROUND: Artificial intelligence advancements have enabled large language models to significantly impact radiology education and diagnostic accuracy.

      OBJECTIVE: This study evaluates the performance of mainstream large language models, including GPT-4, Claude, Bard, Tongyi Qianwen, and Gemini Pro, in radiology board exams.

      METHODS: A comparative analysis of 150 multiple-choice questions from radiology board exams without images was conducted. Models were assessed on their accuracy for text-based questions and were categorized by cognitive levels and medical specialties using χ2 tests and ANOVA.

      RESULTS: GPT-4 achieved the highest accuracy (83.3%, 125/150), significantly outperforming all other models. Specifically, Claude achieved an accuracy of 62% (93/150; P<.001), Bard 54.7% (82/150; P<.001), Tongyi Qianwen 70.7% (106/150; P=.009), and Gemini Pro 55.3% (83/150; P<.001). The odds ratios compared to GPT-4 were 0.33 (95% CI 0.18-0.60) for Claude, 0.24 (95% CI 0.13-0.44) for Bard, and 0.25 (95% CI 0.14-0.45) for Gemini Pro. Tongyi Qianwen performed relatively well with an accuracy of 70.7% (106/150; P=0.02) and had an odds ratio of 0.48 (95% CI 0.27-0.87) compared to GPT-4. Performance varied across question types and specialties, with GPT-4 excelling in both lower-order and higher-order questions, while Claude and Bard struggled with complex diagnostic questions.

      CONCLUSIONS: GPT-4 and Tongyi Qianwen show promise in medical education and training. The study emphasizes the need for domain-specific training datasets to enhance large language models' effectiveness in specialized fields like radiology.
    mergedFrom:
      - source: pubmed
        pmid: "39819381"
        doi: 10.2196/64284
    finalDecision: exclude
  - title: Application of ChatGPT-assisted problem-based learning teaching method in clinical medical education.
    reviews:
      - decision: uncertain
        comment: ChatGPT in clinical medical education PBL; abstract may reveal assessment or WBA elements
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: ChatGPT used as a PBL teaching tool for urology interns; Mini-CEX used as an outcome measure but AI is not generating the assessment or feedback itself."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Teaching intervention study; not focused on generative AI for assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.1186/s12909-024-06321-1
    pmid: "39799356"
    authors: Hui Z, Zewu Z, Jiao H, Yu C
    year: "2025"
    abstract: |-
      INTRODUCTION: Artificial intelligence technology has a wide range of application prospects in the field of medical education. The aim of the study was to measure the effectiveness of ChatGPT-assisted problem-based learning (PBL) teaching for urology medical interns in comparison with traditional teaching.

      METHODS: A cohort of urology interns was randomly assigned to two groups; one underwent ChatGPT-assisted PBL teaching, while the other received traditional teaching over a period of two weeks. Performance was assessed using theoretical knowledge exams and Mini-Clinical Evaluation Exercises. Students' acceptance and satisfaction with the AI-assisted method were evaluated through a survey.

      RESULTS: The scores of the two groups of students who took exams three days after the course ended were significantly higher than their scores before the course. The scores of the PBL-ChatGPT assisted group were significantly higher than those of the traditional teaching group three days after the course ended. The PBL-ChatGPT group showed statistically significant improvements in medical interviewing skills, clinical judgment and overall clinical competence compared to the traditional teaching group. The students gave highly positive feedback on the PBL-ChatGPT teaching method.

      CONCLUSION: The study suggests that ChatGPT-assisted PBL teaching method can improve the results of theoretical knowledge assessment, and play an important role in improving clinical skills. However, further research is needed to examine the validity and reliability of the information provided by different chat AI systems, and its impact on a larger sample size.
    mergedFrom:
      - source: pubmed
        pmid: "39799356"
        doi: 10.1186/s12909-024-06321-1
      - source: scopus
        doi: 10.1186/s12909-024-06321-1
        scopusId: SCOPUS_ID:85215352362
    finalDecision: exclude
  - title: "Qwen-2.5 Outperforms Other Large Language Models in the Chinese National Nursing Licensing Examination: Retrospective Cross-Sectional Comparative Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates LLMs on a national nursing licensing examination, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Licensing examination performance study is not workplace-based assessment in clinical education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.2196/63731
    pmid: "39793017"
    authors: Zhu S, Hu W, Yang Z, Yan J, Zhang F
    year: "2025"
    abstract: |-
      BACKGROUND: Large language models (LLMs) have been proposed as valuable tools in medical education and practice. The Chinese National Nursing Licensing Examination (CNNLE) presents unique challenges for LLMs due to its requirement for both deep domain-specific nursing knowledge and the ability to make complex clinical decisions, which differentiates it from more general medical examinations. However, their potential application in the CNNLE remains unexplored.

      OBJECTIVE: This study aims to evaluates the accuracy of 7 LLMs including GPT-3.5, GPT-4.0, GPT-4o, Copilot, ERNIE Bot-3.5, SPARK, and Qwen-2.5 on the CNNLE, focusing on their ability to handle domain-specific nursing knowledge and clinical decision-making. We also explore whether combining their outputs using machine learning techniques can improve their overall accuracy.

      METHODS: This retrospective cross-sectional study analyzed all 1200 multiple-choice questions from the CNNLE conducted between 2019 and 2023. Seven LLMs were evaluated on these multiple-choice questions, and 9 machine learning models, including Logistic Regression, Support Vector Machine, Multilayer Perceptron, k-nearest neighbors, Random Forest, LightGBM, AdaBoost, XGBoost, and CatBoost, were used to optimize overall performance through ensemble techniques.

      RESULTS: Qwen-2.5 achieved the highest overall accuracy of 88.9%, followed by GPT-4o (80.7%), ERNIE Bot-3.5 (78.1%), GPT-4.0 (70.3%), SPARK (65.0%), and GPT-3.5 (49.5%). Qwen-2.5 demonstrated superior accuracy in the Practical Skills section compared with the Professional Practice section across most years. It also performed well in brief clinical case summaries and questions involving shared clinical scenarios. When the outputs of the 7 LLMs were combined using 9 machine learning models, XGBoost yielded the best performance, increasing accuracy to 90.8%. XGBoost also achieved an area under the curve of 0.961, sensitivity of 0.905, specificity of 0.978, F1-score of 0.901, positive predictive value of 0.901, and negative predictive value of 0.977.

      CONCLUSIONS: This study is the first to evaluate the performance of 7 LLMs on the CNNLE and that the integration of models via machine learning significantly boosted accuracy, reaching 90.8%. These findings demonstrate the transformative potential of LLMs in revolutionizing health care education and call for further research to refine their capabilities and expand their impact on examination preparation and professional training.
    mergedFrom:
      - source: pubmed
        pmid: "39793017"
        doi: 10.2196/63731
    finalDecision: exclude
  - title: "Enhancing Medical Student Engagement Through Cinematic Clinical Narratives: Multimodal Generative AI-Based Mixed Methods Study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on student engagement through cinematic narratives, not assessment or feedback"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on engagement, not assessment/feedback/analysis of clinical observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    doi: 10.2196/63865
    pmid: "39791333"
    authors: Bland T
    year: "2025"
    abstract: |-
      BACKGROUND: Medical students often struggle to engage with and retain complex pharmacology topics during their preclinical education. Traditional teaching methods can lead to passive learning and poor long-term retention of critical concepts.

      OBJECTIVE: This study aims to enhance the teaching of clinical pharmacology in medical school by using a multimodal generative artificial intelligence (genAI) approach to create compelling, cinematic clinical narratives (CCNs).

      METHODS: We transformed a standard clinical case into an engaging, interactive multimedia experience called "Shattered Slippers." This CCN used various genAI tools for content creation: GPT-4 for developing the storyline, Leonardo.ai and Stable Diffusion for generating images, Eleven Labs for creating audio narrations, and Suno for composing a theme song. The CCN integrated narrative styles and pop culture references to enhance student engagement. It was applied in teaching first-year medical students about immune system pharmacology. Student responses were assessed through the Situational Interest Survey for Multimedia and examination performance. The target audience comprised first-year medical students (n=40), with 18 responding to the Situational Interest Survey for Multimedia survey (n=18).

      RESULTS: The study revealed a marked preference for the genAI-enhanced CCNs over traditional teaching methods. Key findings include the majority of surveyed students preferring the CCN over traditional clinical cases (14/18), as well as high average scores for triggered situational interest (mean 4.58, SD 0.53), maintained interest (mean 4.40, SD 0.53), maintained-feeling interest (mean 4.38, SD 0.51), and maintained-value interest (mean 4.42, SD 0.54). Students achieved an average score of 88% on examination questions related to the CCN material, indicating successful learning and retention. Qualitative feedback highlighted increased engagement, improved recall, and appreciation for the narrative style and pop culture references.

      CONCLUSIONS: This study demonstrates the potential of using a multimodal genAI-driven approach to create CCNs in medical education. The "Shattered Slippers" case effectively enhanced student engagement and promoted knowledge retention in complex pharmacological topics. This innovative method suggests a novel direction for curriculum development that could improve learning outcomes and student satisfaction in medical education. Future research should explore the long-term retention of knowledge and the applicability of learned material in clinical settings, as well as the potential for broader implementation of this approach across various medical education contexts.
    mergedFrom:
      - source: pubmed
        pmid: "39791333"
        doi: 10.2196/63865
    finalDecision: exclude
  - title: Evaluation of LLMs accuracy and consistency in the registered dietitian exam through prompt engineering and knowledge retrieval.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; evaluates LLM performance on registered dietitian exam questions with different prompting strategies."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM prompt strategies on dietitian exam questions, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1038/s41598-024-85003-w
    pmid: "39789057"
    authors: Azimi I, Qi M, Wang L, Rahmani A, Li Y
    year: "2025"
    abstract: "Large language models (LLMs) are fundamentally transforming human-facing applications in the health and well-being domains: boosting patient engagement, accelerating clinical decision-making, and facilitating medical education. Although state-of-the-art LLMs have shown superior performance in several conversational applications, evaluations within nutrition and diet applications are still insufficient. In this paper, we propose to employ the Registered Dietitian (RD) exam to conduct a standard and comprehensive evaluation of state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing both accuracy and consistency in nutrition queries. Our evaluation includes 1050 RD exam questions encompassing several nutrition topics and proficiency levels. In addition, for the first time, we examine the impact of Zero-Shot (ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC), and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the responses. Our findings revealed that while these LLMs obtained acceptable overall performance, their results varied considerably with different prompts and question domains. GPT-4o with CoT-SC prompting outperformed the other approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency. For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both accuracy and consistency. RAP was particularly effective for GPT-4o to answer Expert level questions. Consequently, choosing the appropriate LLM and prompting technique, tailored to the proficiency level and specific domain, can mitigate errors and potential risks in diet and nutrition chatbots."
    mergedFrom:
      - source: pubmed
        pmid: "39789057"
        doi: 10.1038/s41598-024-85003-w
    finalDecision: exclude
  - title: "Creating custom GPTs for faculty development: An example using the Johari Window and Crucial Conversation frameworks for providing feedback to struggling students."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 5: No data on validity, reliability, acceptability, or educational impact; describes design and implementation of a custom GPT for faculty feedback training without presenting empirical evidence"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Descriptive educational innovation paper without original comparative outcome data meeting eligible study designs."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.1080/0142159X.2024.2445043
    pmid: "39787039"
    authors: Mehta N, Nielsen C, Zack A, Christensen T, Isaacson J
    year: "2025"
    abstract: Feedback plays a crucial role in the growth and development of trainees, particularly when addressing areas needing improvement. However, faculty members often struggle to deliver constructive feedback, particularly when discussing underperformance. A key obstacle is the lack of comfort many faculty experience in providing feedback that fosters growth. Traditional faculty development programs designed to address these challenges can be expensive and too time-intensive, for busy clinicians.. Generative AI, specifically custom GPT models simulating virtual students and coaches, offers a promising solution for faculty development in feedback training. These AI-driven tools can simulate realistic feedback scenarios using widely accepted educational frameworks and coach faculty members on best practices in delivering constructive feedback. Through interactive, low-cost, and accessible virtual simulations, faculty members can practice in a safe environment and receive immediate, tailored coaching. This approach enhances faculty confidence and competence while reducing the logistical and financial constraints of traditional faculty development programs. By providing scalable, on-demand training, custom GPT-based simulations can be seamlessly integrated into clinical environments, fostering a supportive feedback culture prioritizing trainee development. This paper describes the stepwise process of design and implementation, of a custom GPT-powered feedback training based on an accepted framework. This process can has the potential to transform faculty development in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "39787039"
        doi: 10.1080/0142159X.2024.2445043
    finalDecision: exclude
  - title: "Entrustment and EPAs for Artificial Intelligence (AI): A Framework to Safeguard the Use of AI in Health Professions Education."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Title indicates a framework/guideline, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:20.264000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:21.767000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Conceptual framework/viewpoint proposing an entrustment framework for AI in HPE; not original research, systematic review, or meta-analysis."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Conceptual framework article without original research data, systematic review, or meta-analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1097/ACM.0000000000005930
    pmid: "39761533"
    authors: Gin B, O'Sullivan P, Hauer K, Abdulnour R, Mackenzie M, Ten Cate O, Boscardin C
    year: "2025"
    abstract: "In this article, the authors propose a repurposing of the concept of entrustment to help guide the use of artificial intelligence (AI) in health professions education (HPE). Entrustment can help identify and mitigate the risks of incorporating generative AI tools with limited transparency about their accuracy, source material, and disclosure of bias into HPE practice. With AI's growing role in education-related activities, like automated medical school application screening and feedback quality and content appraisal, there is a critical need for a trust-based approach to ensure these technologies are beneficial and safe. Drawing parallels with HPE's entrustment concept, which assesses a trainee's readiness to perform clinical tasks-or entrustable professional activities-the authors propose assessing the trustworthiness of AI tools to perform an HPE-related task across 3 characteristics: ability (competence to perform tasks accurately), integrity (transparency and honesty), and benevolence (alignment with ethical principles). The authors draw on existing theories of entrustment decision-making to envision a structured way to decide on AI's role and level of engagement in HPE-related tasks, including proposing an AI-specific entrustment scale. Identifying tasks that AI could be entrusted with provides a focus around which considerations of trustworthiness and entrustment decision-making may be synthesized, making explicit the risks associated with AI use and identifying strategies to mitigate these risks. Responsible, trustworthy, and ethical use of AI requires health professions educators to develop safeguards for using it in teaching, learning, and practice-guardrails that can be operationalized via applying the entrustment concept to AI. Without such safeguards, HPE practice stands to be shaped by the oncoming wave of AI innovations tied to commercial motivations, rather than modeled after HPE principles-principles rooted in the trust and transparency that are built together with trainees and patients."
    mergedFrom:
      - source: pubmed
        pmid: "39761533"
        doi: 10.1097/ACM.0000000000005930
      - source: scopus
        doi: 10.1097/ACM.0000000000005930
        scopusId: SCOPUS_ID:85214697204
    finalDecision: exclude
  - title: "ChatGPT's Attitude, Knowledge, and Clinical Application in Geriatrics Practice and Education: Exploratory Observational Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT's own geriatrics knowledge, attitude, and clinical reasoning using validated instruments and vignettes; not about using AI to assess or provide feedback to learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates chatbot geriatrics knowledge/vignettes, not learner assessment or feedback based on workplace clinical observations."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.2196/63494
    pmid: "39752214"
    authors: Cheng H
    year: "2025"
    abstract: |-
      BACKGROUND: The increasing use of ChatGPT in clinical practice and medical education necessitates the evaluation of its reliability, particularly in geriatrics.

      OBJECTIVE: This study aimed to evaluate ChatGPT's trustworthiness in geriatrics through 3 distinct approaches: evaluating ChatGPT's geriatrics attitude, knowledge, and clinical application with 2 vignettes of geriatric syndromes (polypharmacy and falls).

      METHODS: We used the validated University of California, Los Angeles, geriatrics attitude and knowledge instruments to evaluate ChatGPT's geriatrics attitude and knowledge and compare its performance with that of medical students, residents, and geriatrics fellows from reported results in the literature. We also evaluated ChatGPT's application to 2 vignettes of geriatric syndromes (polypharmacy and falls).

      RESULTS: The mean total score on geriatrics attitude of ChatGPT was significantly lower than that of trainees (medical students, internal medicine residents, and geriatric medicine fellows; 2.7 vs 3.7 on a scale from 1-5; 1=strongly disagree; 5=strongly agree). The mean subscore on positive geriatrics attitude of ChatGPT was higher than that of the trainees (medical students, internal medicine residents, and neurologists; 4.1 vs 3.7 on a scale from 1 to 5 where a higher score means a more positive attitude toward older adults). The mean subscore on negative geriatrics attitude of ChatGPT was lower than that of the trainees and neurologists (1.8 vs 2.8 on a scale from 1 to 5 where a lower subscore means a less negative attitude toward aging). On the University of California, Los Angeles geriatrics knowledge test, ChatGPT outperformed all medical students, internal medicine residents, and geriatric medicine fellows from validated studies (14.7 vs 11.3 with a score range of -18 to +18 where +18 means that all questions were answered correctly). Regarding the polypharmacy vignette, ChatGPT not only demonstrated solid knowledge of potentially inappropriate medications but also accurately identified 7 common potentially inappropriate medications and 5 drug-drug and 3 drug-disease interactions. However, ChatGPT missed 5 drug-disease and 1 drug-drug interaction and produced 2 hallucinations. Regarding the fall vignette, ChatGPT answered 3 of 5 pretests correctly and 2 of 5 pretests partially correctly, identified 6 categories of fall risks, followed fall guidelines correctly, listed 6 key physical examinations, and recommended 6 categories of fall prevention methods.

      CONCLUSIONS: This study suggests that ChatGPT can be a valuable supplemental tool in geriatrics, offering reliable information with less age bias, robust geriatrics knowledge, and comprehensive recommendations for managing 2 common geriatric syndromes (polypharmacy and falls) that are consistent with evidence from guidelines, systematic reviews, and other types of studies. ChatGPT's potential as an educational and clinical resource could significantly benefit trainees, health care providers, and laypeople. Further research using GPT-4o, larger geriatrics question sets, and more geriatric syndromes is needed to expand and confirm these findings before adopting ChatGPT widely for geriatrics education and practice.
    mergedFrom:
      - source: pubmed
        pmid: "39752214"
        doi: 10.2196/63494
    finalDecision: exclude
  - title: Implementation and evaluation of an optimized surgical clerkship teaching model utilizing ChatGPT.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: ChatGPT used as a supplementary teaching tool in surgical clerkships; not about AI-based assessment, feedback generation, or analysis of observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates a teaching model and learner satisfaction/compliance, not AI use for assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.1186/s12909-024-06575-9
    pmid: "39731112"
    authors: Huang Y, Xu B, Wang X, Luo Y, Teng M, Weng X
    year: "2024"
    abstract: |-
      OBJECTIVE: This study aims to explore the effect of an innovative teaching model incorporating ChatGPT on medical students' learning outcomes, compliance with learning activities, and overall satisfaction with the learning process.

      METHODS: A cohort of 64 students participating in general surgery clerkships at Wenzhou People's Hospital during the 2022-2023 academic year were randomly assigned into 4 groups, each comprising 16 students. Two of these groups were designated as the study group, where ChatGPT was employed as a supplementary educational tool. The remaining 2 groups served as control groups and used traditional multimedia-based learning methods. Outcomes, including learning effectiveness, compliance, and satisfaction, were evaluated using questionnaires and tests.

      RESULTS: The study groups exhibited significantly higher levels of compliance and satisfaction compared to the control groups. Specifically, the study groups exhibited significantly greater compliance in both pre-class preparation and post-class review activities (P < 0.05). During classroom teaching, Group 1 of the study group achieved significantly higher compliance than the control groups (P < 0.0001), while Group 2 of the study group showed significantly higher compliance than Group 1 (P < 0.001). In terms of seeking feedback and assistance, both Groups 1 and 2 of the study group had significantly higher compliance compared to Group 1 of the control group (P < 0.01, P < 0.001 respectively). Overall satisfaction was significantly higher in the study groups compared to the control groups (P < 0.0001), particularly with respect to course organization (P < 0.001, P < 0.05).

      CONCLUSION: The incorporation of ChatGPT into the surgical clerkship teaching model substantially enhances learner compliance and satisfaction, offering notable advantages in educational effectiveness.
    mergedFrom:
      - source: pubmed
        pmid: "39731112"
        doi: 10.1186/s12909-024-06575-9
    finalDecision: exclude
  - title: "Performance of ChatGPT-4o on the Japanese Medical Licensing Examination: Evalution of Accuracy in Text-Only and Image-Based Questions."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Tests ChatGPT-4o on licensing exam, not about assessment or feedback of human learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting (licensing examination context)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.2196/63129
    pmid: "39718557"
    authors: Miyazaki Y, Hata M, Omori H, Hirashima A, Nakagawa Y, Eto M, Takahashi S, Ikeda M
    year: "2024"
    abstract: This study evaluated the performance of ChatGPT with GPT-4 Omni (GPT-4o) on the 118th Japanese Medical Licensing Examination. The study focused on both text-only and image-based questions. The model demonstrated a high level of accuracy overall, with no significant difference in performance between text-only and image-based questions. Common errors included clinical judgment mistakes and prioritization issues, underscoring the need for further improvement in the integration of artificial intelligence into medical education and practice.
    mergedFrom:
      - source: pubmed
        pmid: "39718557"
        doi: 10.2196/63129
    finalDecision: exclude
  - title: "Evaluating Bard Gemini Pro and GPT-4 Vision Against Student Performance in Medical Visual Question Answering: Comparative Case Study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Visual question answering benchmark, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates LLM performance on image-based medical licensing exam questions from AMBOSS platform; not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Uses licensing examination questions, not workplace-based clinical education assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.2196/57592
    pmid: "39714199"
    authors: Roos J, Martin R, Kaczmarczyk R
    year: "2024"
    abstract: |-
      BACKGROUND: The rapid development of large language models (LLMs) such as OpenAI's ChatGPT has significantly impacted medical research and education. These models have shown potential in fields ranging from radiological imaging interpretation to medical licensing examination assistance. Recently, LLMs have been enhanced with image recognition capabilities.

      OBJECTIVE: This study aims to critically examine the effectiveness of these LLMs in medical diagnostics and training by assessing their accuracy and utility in answering image-based questions from medical licensing examinations.

      METHODS: This study analyzed 1070 image-based multiple-choice questions from the AMBOSS learning platform, divided into 605 in English and 465 in German. Customized prompts in both languages directed the models to interpret medical images and provide the most likely diagnosis. Student performance data were obtained from AMBOSS, including metrics such as the "student passed mean" and "majority vote." Statistical analysis was conducted using Python (Python Software Foundation), with key libraries for data manipulation and visualization.

      RESULTS: GPT-4 1106 Vision Preview (OpenAI) outperformed Bard Gemini Pro (Google), correctly answering 56.9% (609/1070) of questions compared to Bard's 44.6% (477/1070), a statistically significant difference (χ2₁=32.1, P<.001). However, GPT-4 1106 left 16.1% (172/1070) of questions unanswered, significantly higher than Bard's 4.1% (44/1070; χ2₁=83.1, P<.001). When considering only answered questions, GPT-4 1106's accuracy increased to 67.8% (609/898), surpassing both Bard (477/1026, 46.5%; χ2₁=87.7, P<.001) and the student passed mean of 63% (674/1070, SE 1.48%; χ2₁=4.8, P=.03). Language-specific analysis revealed both models performed better in German than English, with GPT-4 1106 showing greater accuracy in German (282/465, 60.65% vs 327/605, 54.1%; χ2₁=4.4, P=.04) and Bard Gemini Pro exhibiting a similar trend (255/465, 54.8% vs 222/605, 36.7%; χ2₁=34.3, P<.001). The student majority vote achieved an overall accuracy of 94.5% (1011/1070), significantly outperforming both artificial intelligence models (GPT-4 1106: χ2₁=408.5, P<.001; Bard Gemini Pro: χ2₁=626.6, P<.001).

      CONCLUSIONS: Our study shows that GPT-4 1106 Vision Preview and Bard Gemini Pro have potential in medical visual question-answering tasks and to serve as a support for students. However, their performance varies depending on the language used, with a preference for German. They also have limitations in responding to non-English content. The accuracy rates, particularly when compared to student responses, highlight the potential of these models in medical education, yet the need for further optimization and understanding of their limitations in diverse linguistic contexts remains critical.
    mergedFrom:
      - source: pubmed
        pmid: "39714199"
        doi: 10.2196/57592
    finalDecision: exclude
  - title: Performance of ChatGPT in the Portuguese National Residency Access Examination.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT versions on the Portuguese residency access exam; does not use generative AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Residency-access exam performance study, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.20344/amp.22506
    pmid: "39704711"
    authors: Ferraz-Costa G, Grin&#xe9; M, Oliveira-Santos M, Teixeira R
    year: "2025"
    abstract: ChatGPT, a language model developed by OpenAI, has been tested in several medical board examinations. This study aims to evaluate the performance of ChatGPT on the Portuguese National Residency Access Examination, a mandatory test for medical residency in Portugal. The study specifically compares the capabilities of ChatGPT versions 3.5 and 4o across five examination editions from 2019 to 2023. A total of 750 multiple-choice questions were submitted to both versions, and their answers were evaluated against the official responses. The findings revealed that ChatGPT 4o significantly outperformed ChatGPT 3.5, with a median examination score of 127 compared to 106 (p = 0.048). Notably, ChatGPT 4o achieved scores within the top 1% in two examination editions and exceeded the median performance of human candidates in all editions. Additionally, ChatGPT 4o's scores were high enough to qualify for any specialty. In conclusion, ChatGPT 4o can be a valuable tool for medical education and decision-making, but human oversight remains essential to ensure safe and accurate clinical practice.
    mergedFrom:
      - source: pubmed
        pmid: "39704711"
        doi: 10.20344/amp.22506
    finalDecision: exclude
  - title: "ChatGPT efficacy for answering musculoskeletal anatomy questions: a study evaluating quality and consistency between raters and timepoints: correspondence."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Correspondence, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Correspondence article, not an original research article, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    doi: 10.1007/s00276-024-03549-w
    pmid: "39680156"
    authors: Al Salkhadi M, Al Salkhadi A
    year: "2024"
    abstract: "The article \"ChatGPT Efficacy for Answering Musculoskeletal Anatomy Questions: A Study Evaluating Quality and Consistency between Raters and Timepoints\" assesses the performance of ChatGPT 3.5 in answering musculoskeletal anatomy questions, highlighting variability in response quality and reproducibility. We raise several points that may add further insights into the study's findings. While ChatGPT and other Large Language Models (LLMs) show promise in medical education, several areas require further exploration. We emphasize the importance of using larger question sets and diverse formats, such as multiple-choice questions (MCQs), where ChatGPT has demonstrated more consistent performance in prior studies. Additionally, improvements in artificial intelligence (AI) models and the incorporation of updated anatomical databases could enhance response accuracy. The study also identifies ChatGPT's lack of anatomical specificity as a limitation, which may be addressed by training AI models on specialized anatomy datasets. In conclusion, while ChatGPT is not yet a fully reliable standalone resource, it might serve as a complementary tool when integrated with traditional methods. Further research is needed to optimize AI for anatomy education."
    mergedFrom:
      - source: pubmed
        pmid: "39680156"
        doi: 10.1007/s00276-024-03549-w
    finalDecision: exclude
  - title: "ChatGPT versus expert feedback on clinical reasoning questions and their effect on learning: a randomized controlled trial."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT-generated feedback on formative clinical reasoning tests for first-year medical students in a preclinical educational setting (spaced repetition on UTI cases); not workplace-based or clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Study is in a classroom/medical-school setting and not a workplace-based clinical education context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1093/postmj/qgae170
    pmid: "39656920"
    authors: "&#xc7;i&#xe7;ek F, &#xdc;lker M, &#xd6;zer M, K&#x131;yak Y"
    year: "2025"
    abstract: |-
      PURPOSE: To evaluate the effectiveness of ChatGPT-generated feedback compared to expert-written feedback in improving clinical reasoning skills among first-year medical students.

      METHODS: This is a randomized controlled trial conducted at a single medical school and involved 129 first-year medical students who were randomly assigned to two groups. Both groups completed three formative tests with feedback on urinary tract infections (UTIs; uncomplicated, complicated, pyelonephritis) over five consecutive days as a spaced repetition, receiving either expert-written feedback (control, n = 65) or ChatGPT-generated feedback (experiment, n = 64). Clinical reasoning skills were assessed using Key-Features Questions (KFQs) immediately after the intervention and 10 days later. Students' critical approach to artificial intelligence (AI) was also measured before and after disclosing the AI involvement in feedback generation.

      RESULTS: There was no significant difference between the mean scores of the control (immediate: 78.5 ± 20.6 delayed: 78.0 ± 21.2) and experiment (immediate: 74.7 ± 15.1, delayed: 76.0 ± 14.5) groups in overall performance on Key-Features Questions (out of 120 points) immediately (P = .26) or after 10 days (P = .57), with small effect sizes. However, the control group outperformed the ChatGPT group in complicated urinary tract infection cases (P < .001). The experiment group showed a significantly higher critical approach to AI after disclosing, with medium-large effect sizes.

      CONCLUSIONS: ChatGPT-generated feedback can be an effective alternative to expert feedback in improving clinical reasoning skills in medical students, particularly in resource-constrained settings with limited expert availability. However, AI-generated feedback may lack the nuance needed for more complex cases, emphasizing the need for expert review. Additionally, exposure to the drawbacks in AI-generated feedback can enhance students' critical approach towards AI-generated educational content.
    mergedFrom:
      - source: pubmed
        pmid: "39656920"
        doi: 10.1093/postmj/qgae170
    finalDecision: exclude
  - title: "QUEST-AI: A System for Question Generation, Verification, and Refinement using AI for USMLE-Style Exams."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records. Study focuses on LLM-based generation of USMLE-style exam questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on generating and validating USMLE-style exam items rather than assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.1142/9789819807024_0005
    pmid: "39670361"
    authors: Bedi S, Fleming S, Chiang C, Morse K, Kumar A, Patel B, Jindal J, Davenport C, Yamaguchi C, Shah N
    year: "2025"
    abstract: The United States Medical Licensing Examination (USMLE) is a critical step in assessing the competence of future physicians, yet the process of creating exam questions and study materials is both time-consuming and costly. While Large Language Models (LLMs), such as OpenAI's GPT-4, have demonstrated proficiency in answering medical exam questions, their potential in generating such questions remains underexplored. This study presents QUEST-AI, a novel system that utilizes LLMs to (1) generate USMLE-style questions, (2) identify and flag incorrect questions, and (3) correct errors in the flagged questions. We evaluated this system's output by constructing a test set of 50 LLM-generated questions mixed with 50 human-generated questions and conducting a two-part assessment with three physicians and two medical students. The assessors attempted to distinguish between LLM and human-generated questions and evaluated the validity of the LLM-generated content. A majority of exam questions generated by QUEST-AI were deemed valid by a panel of three clinicians, with strong correlations between performance on LLM-generated and human-generated questions. This pioneering application of LLMs in medical education could significantly increase the ease and efficiency of developing USMLE-style medical exam content, offering a cost-effective and accessible alternative for exam preparation.
    mergedFrom:
      - source: pubmed
        pmid: "39670361"
        doi: 10.1142/9789819807024_0005
    finalDecision: exclude
  - title: Comparing Large Language Model and Human Reader Accuracy with New England Journal of Medicine Image Challenge Case Image Inputs.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Comparing LLM vs human accuracy on image challenges, not about assessment or feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    doi: 10.1148/radiol.241668
    pmid: "39656125"
    authors: Suh P, Shim W, Suh C, Heo H, Park K, Kim P, Choi S, Ahn Y, Park S, Park H, Oh N, Han M, Cho S, Woo C, Park H
    year: "2024"
    abstract: "Background Application of multimodal large language models (LLMs) with both textual and visual capabilities has been steadily increasing, but their ability to interpret radiologic images is still doubted. Purpose To evaluate the accuracy of LLMs and compare it with that of human readers with varying levels of experience and to assess the factors affecting LLM accuracy in answering New England Journal of Medicine Image Challenge cases. Materials and Methods Radiologic images of cases from October 13, 2005, to April 18, 2024, were retrospectively reviewed. Using text and image inputs, LLMs (Open AI's GPT-4 Turbo with Vision [GPT-4V] and GPT-4 Omni [GPT-4o], Google's DeepMind Gemini 1.5 Pro, and Anthropic's Claude 3) provided answers. Human readers (seven junior faculty radiologists, two clinicians, one in-training radiologist, and one medical student), blinded to the published answers, also answered. LLM accuracy with and without image inputs and short (cases from 2005 to 2015) versus long text inputs (from 2016 to 2024) was evaluated in subgroup analysis to determine the effect of these factors. Factor analysis was assessed using multivariable logistic regression. Accuracy was compared with generalized estimating equations, with multiple comparisons adjusted by using Bonferroni correction. Results A total of 272 cases were included. GPT-4o achieved the highest overall accuracy among LLMs (59.6%; 162 of 272), outperforming a medical student (47.1%; 128 of 272; P < .001) but not junior faculty (80.9%; 220 of 272; P < .001) or the in-training radiologist (70.2%; 191 of 272; P = .003). GPT-4o exhibited similar accuracy regardless of image inputs (without images vs with images, 54.0% [147 of 272] vs 59.6% [162 of 272], respectively; P = .59). Human reader accuracy was unaffected by text length, whereas LLMs demonstrated higher accuracy with long text inputs (all P < .001). Text input length affected LLM accuracy (odds ratio range, 3.2 [95% CI: 1.9, 5.5] to 6.6 [95% CI: 3.7, 12.0]). Conclusion LLMs demonstrated substantial accuracy with text and image inputs, outperforming a medical student. However, their accuracy decreased with shorter text lengths, regardless of image input. © RSNA, 2024 Supplemental material is available for this article."
    mergedFrom:
      - source: pubmed
        pmid: "39656125"
        doi: 10.1148/radiol.241668
    finalDecision: exclude
  - title: "Artificial Intelligence as a Discriminator of Competence in Urological Training: Are We There?"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted in a simulated exam environment (mock board certification examination with MCQs and oral OSCE), not workplace-based assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted in mock written/oral examination settings (including OSCE), without a workplace-based component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1097/JU.0000000000004357
    pmid: "39653020"
    authors: Touma N, Patel R, Skinner T, Leveridge M
    year: "2025"
    abstract: |-
      PURPOSE: Assessments in medical education play a central role in evaluating trainees' progress and eventual competence. Generative artificial intelligence is finding an increasing role in clinical care and medical education. The objective of this study was to evaluate the ability of the large language model ChatGPT to generate examination questions that are discriminating in the evaluation of graduating urology residents.

      MATERIALS AND METHODS: Graduating urology residents representing all Canadian training programs gather yearly for a mock examination that simulates their upcoming board certification examination. The examination consists of a written multiple-choice question (MCQ) examination and an oral objective structured clinical examination. In 2023, ChatGPT Version 4 was used to generate 20 MCQs that were added to the written component. ChatGPT was asked to use Campbell-Walsh Urology, AUA, and Canadian Urological Association guidelines as resources. Psychometric analysis of the ChatGPT MCQs was conducted. The MCQs were also researched by 3 faculty for face validity and to ascertain whether they came from a valid source.

      RESULTS: The mean score of the 35 examination takers on the ChatGPT MCQs was 60.7% vs 61.1% for the overall examination. Twenty-five of ChatGPT MCQs showed a discrimination index > 0.3, the threshold for questions that properly discriminate between high and low examination performers. Twenty-five percent of ChatGPT MCQs showed a point biserial > 0.2, which is considered a high correlation with overall performance on the examination. The assessment by faculty found that ChatGPT MCQs often provided incomplete information in the stem, provided multiple potentially correct answers, and were sometimes not rooted in the literature. Thirty-five percent of the MCQs generated by ChatGPT provided wrong answers to stems.

      CONCLUSIONS: Despite what seems to be similar performance on ChatGPT MCQs and the overall examination, ChatGPT MCQs tend not to be highly discriminating. Poorly phrased questions with potential for artificial intelligence hallucinations are ever present. Careful vetting for quality of ChatGPT questions should be undertaken before their use on assessments in urology training examinations.
    mergedFrom:
      - source: pubmed
        pmid: "39653020"
        doi: 10.1097/JU.0000000000004357
    finalDecision: exclude
  - title: "The performance of ChatGPT versus neurosurgery residents in neurosurgical board examination-like questions: a systematic review and meta-analysis."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: SR/MA benchmarking ChatGPT vs residents on neurosurgery board exam questions; not about assessment/feedback of clinical performance or analysis of observation records in workplace settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Review focuses on board exam-like questions rather than workplace-based clinical education settings."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1007/s10143-024-03144-y
    pmid: "39643792"
    authors: Bongco E, Cua S, Hernandez M, Pascual J, Khu K
    year: "2024"
    abstract: |-
      OBJECTIVE: Large language models and ChatGPT have been used in different fields of medical education. This study aimed to review the literature on the performance of ChatGPT in neurosurgery board examination-like questions compared to neurosurgery residents.

      METHODS: A literature search was performed following PRISMA guidelines, covering the time period of ChatGPT's inception (November 2022) until October 25, 2024. Two reviewers screened for eligible studies, selecting those that used ChatGPT to answer neurosurgery board examination-like questions and compared the results with neurosurgery residents' scores. Risk of bias was assessed using JBI critical appraisal tool. Overall effect sizes and 95% confidence intervals were determined using a fixed-effects model with alpha at 0.05.

      RESULTS: After screening, six studies were selected for qualitative and quantitative analysis. Accuracy of ChatGPT ranged from 50.4 to 78.8%, compared to residents' accuracy of 58.3 to 73.7%. Risk of bias was low in 4 out of 6 studies reviewed; the rest had moderate risk. There was an overall trend favoring neurosurgery residents versus ChatGPT (p < 0.00001), with high heterogeneity (I2 = 96). These findings were similar on sub-group analysis of studies that used the Self-assessment in Neurosurgery (SANS) examination questions. However, on sensitivity analysis, removal of the highest weighted study skewed the results toward better performance of ChatGPT.

      CONCLUSION: Our meta-analysis showed that neurosurgery residents performed better than ChatGPT in answering neurosurgery board examination-like questions, although reviewed studies had high heterogeneity. Further improvement is necessary before it can become a useful and reliable supplementary tool in the delivery of neurosurgical education.
    mergedFrom:
      - source: pubmed
        pmid: "39643792"
        doi: 10.1007/s10143-024-03144-y
    finalDecision: exclude
  - title: Comparing performances of french orthopaedic surgery residents with the artificial intelligence ChatGPT-4/4o in the French diploma exams of orthopaedic and trauma surgery.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares LLM performance with residents on a standardized specialty certification exam (DES), not about AI-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:51.586000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Comparative performance on diploma exam questions, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:58.822000+00:00
    doi: 10.1016/j.otsr.2024.104080
    pmid: "39643080"
    authors: Maraqa N, Samargandi R, Poichotte A, Berhouet J, Benhenneda R
    year: "2025"
    abstract: |-
      INTRODUCTION: This study evaluates the performance of ChatGPT, particularly its versions 4 and 4o, in answering questions from the French orthopedic and trauma surgery exam (Diplôme d'Études Spécialisées, DES), compared to the results of French orthopedic surgery residents. Previous research has examined ChatGPT's capabilities across various medical specialties and exams, with mixed results, especially in the interpretation of complex radiological images.

      HYPOTHESIS: ChatGPT version 4o was capable of achieving a score equal to or higher (not lower) than that of residents for the DES exam.

      METHODS: The response capabilities of the ChatGPT model, versions 4 and 4o, were evaluated and compared to the results of residents for 250 questions taken from the DES exams from 2020 to 2024. A secondary analysis focused on the differences in the AI's performance based on the type of data being analyzed (text or images) and the topic of the questions.

      RESULTS: The score achieved by ChatGPT-4o was equivalent to that of residents over the past five years: 74.8% for ChatGPT-4o vs. 70.8% for residents (p = 0.32). The accuracy rate of ChatGPT was significantly higher in its latest version 4o compared to version 4 (58.8%, p = 0.0001). Secondary subgroup analysis revealed a performance deficiency of the AI in analyzing graphical images (success rates of 48% and 65% for ChatGPT-4 and 4o, respectively). ChatGPT-4o showed superior performance to version 4 when the topics involved the spine, pediatrics, and lower limb.

      CONCLUSION: The performance of ChatGPT-4o is equivalent to that of French students in answering questions from the DES in orthopedic and trauma surgery. Significant progress has been observed between versions 4 and 4o. The analysis of questions involving iconography remains a notable challenge for the current versions of ChatGPT, with a tendency for the AI to perform less effectively compared to questions requiring only text analysis.

      LEVEL OF EVIDENCE: IV; Retrospective Observational Study.
    mergedFrom:
      - source: pubmed
        pmid: "39643080"
        doi: 10.1016/j.otsr.2024.104080
    finalDecision: exclude
  - title: Large Language Models with Vision on Diagnostic Radiology Board Exam Style Questions.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Board exam style questions, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates multimodal LLM performance on radiology board-style exam questions"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLMs on radiology board-style exam questions rather than workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.1016/j.acra.2024.11.028
    pmid: "39632215"
    authors: Sun S, Chen K, Anavim S, Phillipi M, Yeh L, Huynh K, Cortes G, Tran J, Tran M, Yaghmai V, Houshyar R
    year: "2025"
    abstract: |-
      RATIONALE AND OBJECTIVES: The expansion of large language models to process images offers new avenues for application in radiology. This study aims to assess the multimodal capabilities of contemporary large language models, which allow analysis of image inputs in addition to textual data, on radiology board-style examination questions with images.

      MATERIALS AND METHODS: 280 questions were retrospectively selected from the AuntMinnie public test bank. The test questions were converted into three formats of prompts; (1) Multimodal, (2) Image-only, and (3) Text-only input. Three models, GPT-4V, Gemini 1.5 Pro, and Claude 3.5 Sonnet, were evaluated using these prompts. The Cochran Q test and pairwise McNemar test were used to compare performances between prompt formats and models.

      RESULTS: No difference was found for the performance in terms of % correct answers between the text, image, and multimodal prompt formats for GPT-4V (54%, 52%, and 57%, respectively; p = .31) and Gemini 1.5 Pro (53%, 54%, and 57%, respectively; p = .53). For Claude 3.5 Sonnet, the image input (48%) significantly underperformed compared to the text input (63%, p < .001) and the multimodal input (66%, p < .001), but no difference was found between the text and multimodal inputs (p = .29). Claude significantly outperformed GPT and Gemini in the text and multimodal formats (p < .01).

      CONCLUSION: Vision-capable large language models cannot effectively use images to increase performance on radiology board-style examination questions. When using textual data alone, Claude 3.5 Sonnet outperforms GPT-4V and Gemini 1.5 Pro, highlighting the advancements in the field and its potential for use in further research.
    mergedFrom:
      - source: pubmed
        pmid: "39632215"
        doi: 10.1016/j.acra.2024.11.028
    finalDecision: exclude
  - title: "Performance Comparison of Junior Residents and ChatGPT in the Objective Structured Clinical Examination (OSCE) for Medical History Taking and Documentation of Medical Records: Development and Usability Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is explicitly in OSCE simulation context"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments. Study uses OSCE with standardized patients in simulated clinical scenarios."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is performed in simulated standardized-patient OSCE scenarios only."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.2196/59902
    pmid: "39622713"
    authors: Huang T, Hsieh P, Chang Y
    year: "2024"
    abstract: |-
      BACKGROUND: This study explores the cutting-edge abilities of large language models (LLMs) such as ChatGPT in medical history taking and medical record documentation, with a focus on their practical effectiveness in clinical settings-an area vital for the progress of medical artificial intelligence.

      OBJECTIVE: Our aim was to assess the capability of ChatGPT versions 3.5 and 4.0 in performing medical history taking and medical record documentation in simulated clinical environments. The study compared the performance of nonmedical individuals using ChatGPT with that of junior medical residents.

      METHODS: A simulation involving standardized patients was designed to mimic authentic medical history-taking interactions. Five nonmedical participants used ChatGPT versions 3.5 and 4.0 to conduct medical histories and document medical records, mirroring the tasks performed by 5 junior residents in identical scenarios. A total of 10 diverse scenarios were examined.

      RESULTS: Evaluation of the medical documentation created by laypersons with ChatGPT assistance and those created by junior residents was conducted by 2 senior emergency physicians using audio recordings and the final medical records. The assessment used the Objective Structured Clinical Examination benchmarks in Taiwan as a reference. ChatGPT-4.0 exhibited substantial enhancements over its predecessor and met or exceeded the performance of human counterparts in terms of both checklist and global assessment scores. Although the overall quality of human consultations remained higher, ChatGPT-4.0's proficiency in medical documentation was notably promising.

      CONCLUSIONS: The performance of ChatGPT 4.0 was on par with that of human participants in Objective Structured Clinical Examination evaluations, signifying its potential in medical history and medical record documentation. Despite this, the superiority of human consultations in terms of quality was evident. The study underscores both the promise and the current limitations of LLMs in the realm of clinical practice.
    mergedFrom:
      - source: pubmed
        pmid: "39622713"
        doi: 10.2196/59902
    finalDecision: exclude
  - title: "Evaluating AI Competence in Specialized Medicine: Comparative Analysis of ChatGPT and Neurologists in a Neurology Specialist Examination in Spain."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on specialist examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Compares ChatGPT performance with neurologists on a specialist MCQ examination, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Specialist exam benchmarking is not conducted in a workplace-based or clinical education assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.2196/56762
    pmid: "39622707"
    authors: Ros-Arlanz&#xf3;n P, Perez-Sempere A
    year: "2024"
    abstract: |-
      BACKGROUND: With the rapid advancement of artificial intelligence (AI) in various fields, evaluating its application in specialized medical contexts becomes crucial. ChatGPT, a large language model developed by OpenAI, has shown potential in diverse applications, including medicine.

      OBJECTIVE: This study aims to compare the performance of ChatGPT with that of attending neurologists in a real neurology specialist examination conducted in the Valencian Community, Spain, assessing the AI's capabilities and limitations in medical knowledge.

      METHODS: We conducted a comparative analysis using the 2022 neurology specialist examination results from 120 neurologists and responses generated by ChatGPT versions 3.5 and 4. The examination consisted of 80 multiple-choice questions, with a focus on clinical neurology and health legislation. Questions were classified according to Bloom's Taxonomy. Statistical analysis of performance, including the κ coefficient for response consistency, was performed.

      RESULTS: Human participants exhibited a median score of 5.91 (IQR: 4.93-6.76), with 32 neurologists failing to pass. ChatGPT-3.5 ranked 116th out of 122, answering 54.5% of questions correctly (score 3.94). ChatGPT-4 showed marked improvement, ranking 17th with 81.8% of correct answers (score 7.57), surpassing several human specialists. No significant variations were observed in the performance on lower-order questions versus higher-order questions. Additionally, ChatGPT-4 demonstrated increased interrater reliability, as reflected by a higher κ coefficient of 0.73, compared to ChatGPT-3.5's coefficient of 0.69.

      CONCLUSIONS: This study underscores the evolving capabilities of AI in medical knowledge assessment, particularly in specialized fields. ChatGPT-4's performance, outperforming the median score of human participants in a rigorous neurology examination, represents a significant milestone in AI development, suggesting its potential as an effective tool in specialized medical education and assessment.
    mergedFrom:
      - source: pubmed
        pmid: "39622707"
        doi: 10.2196/56762
      - source: scopus
        doi: 10.2196/56762
        scopusId: SCOPUS_ID:105004850007
    finalDecision: exclude
  - title: Using artificial intelligence to provide a 'flipped assessment' approach to medical education learning opportunities.
    reviews:
      - decision: uncertain
        comment: AI in assessment for medical education; abstract needed to determine if generative AI and WBA-related
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study focuses on students using GenAI to create practice exam questions in a workshop/classroom setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Examines student use of GenAI to create practice exam questions, not assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1080/0142159X.2024.2434101
    pmid: "39616548"
    authors: Birks S, Gray J, Darling-Pomranz C
    year: "2025"
    abstract: |-
      PURPOSE: Generative AI can potentially streamline the creation of practice exam questions. This study sought to evaluate medical students' confidence using generative AI for this purpose, and overall attitudes towards its use.

      MATERIALS AND METHODS: The study used a mixed-methods approach with a pre-post intervention design. 68 medical and physician associate students were recruited to attend a workshop where they were shown how to use Google Bard (now Gemini) to write exam questions before being encouraged to do this themselves with guidance. A survey was completed before and after. Seven students also participated in a follow-up focus group.

      RESULTS: The results showed an increase in participants' confidence in using AI to write practice exam questions (p < 0.001) after the workshop. Qualitative feedback highlighted pros and cons of using generative AI to write exam questions, alongside some concerns about its implementation. Students noted other positive uses in the curriculum and expressed a desire for institutional clarity on appropriate AI use.

      CONCLUSIONS: While increased confidence is positive, rigorous evaluation of AI-generated question quality is needed to confirm accuracy. Teaching students to use generative AI to create and critique practice questions represents a means of encouraging appropriate AI use.
    mergedFrom:
      - source: pubmed
        pmid: "39616548"
        doi: 10.1080/0142159X.2024.2434101
    finalDecision: exclude
  - title: "Enhancements in artificial intelligence for medical examinations: A leap from ChatGPT 3.5 to ChatGPT 4.0 in the FRCS trauma & orthopaedics examination."
    reviews:
      - decision: uncertain
        comment: ChatGPT on FRCS exam; abstract needed to determine WBA relevance
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; benchmarks ChatGPT versions on FRCS Trauma & Orthopaedics written exam questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates AI on examination questions, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.1016/j.surge.2024.11.008
    pmid: "39613651"
    authors: Khan A, Sarraf K, Simpson A
    year: "2025"
    abstract: |-
      INTRODUCTION: ChatGPT is a sophisticated AI model capable of generating human-like text based on the input it receives. ChatGPT 3.5 showed an inability to pass the FRCS (Tr&Orth) examination due to a lack of higher-order judgement in previous studies. Enhancements in ChatGPT 4.0 warrant an evaluation of its performance.

      METHODOLOGY: Questions from the UK-based December 2022 In-Training examination were input into ChatGPT 3.5 and 4.0. Methodology from a prior study was replicated to maintain consistency, allowing for a direct comparison between the two model versions. The performance threshold remained at 65.8 %, aligning with the November 2022 sitting of Section 1 of the FRCS (Tr&Orth).

      RESULTS: ChatGPT 4.0 achieved a passing score (73.9 %), indicating an improvement in its ability to analyse clinical information and make decisions reflective of a competent trauma and orthopaedic consultant. Compared to ChatGPT 4.0, version 3.5 scored 38.1 % lower, which represents a significant difference (p < 0.0001; Chi-square). The breakdown by subspecialty further demonstrated version 4.0's enhanced understanding and application in complex clinical scenarios. ChatGPT 4.0 had a significantly significant improvement in answering image-based questions (p = 0.0069) compared to its predecessor.

      CONCLUSION: ChatGPT 4.0's success in passing Section One of the FRCS (Tr&Orth) examination highlights the rapid evolution of AI technologies and their potential applications in healthcare and education.
    mergedFrom:
      - source: pubmed
        pmid: "39613651"
        doi: 10.1016/j.surge.2024.11.008
    finalDecision: exclude
  - title: "Large language models improve clinical decision making of medical students through patient simulation and structured feedback: a randomized controlled trial."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Patient simulation-based setting only."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: AI-simulated patient interactions with structured feedback for preclinical medical students (semester 4); assessment conducted entirely in simulated environment, not workplace-based."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment and feedback are conducted in simulated patient interactions without a workplace-based component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1186/s12909-024-06399-7
    pmid: "39609823"
    authors: Br&#xfc;gge E, Ricchizzi S, Arenbeck M, Keller M, Schur L, Stummer W, Holling M, Lu M, Darici D
    year: "2024"
    abstract: |-
      BACKGROUND: Clinical decision-making (CDM) refers to physicians' ability to gather, evaluate, and interpret relevant diagnostic information. An integral component of CDM is the medical history conversation, traditionally practiced on real or simulated patients. In this study, we explored the potential of using Large Language Models (LLM) to simulate patient-doctor interactions and provide structured feedback.

      METHODS: We developed AI prompts to simulate patients with different symptoms, engaging in realistic medical history conversations. In our double-blind randomized design, the control group participated in simulated medical history conversations with AI patients (control group), while the intervention group, in addition to simulated conversations, also received AI-generated feedback on their performances (feedback group). We examined the influence of feedback based on their CDM performance, which was evaluated by two raters (ICC = 0.924) using the Clinical Reasoning Indicator - History Taking Inventory (CRI-HTI). The data was analyzed using an ANOVA for repeated measures.

      RESULTS: Our final sample included 21 medical students (agemean = 22.10 years, semestermean = 4, 14 females). At baseline, the feedback group (mean = 3.28 ± 0.09 [standard deviation]) and the control group (3.21 ± 0.08) achieved similar CRI-HTI scores, indicating successful randomization. After only four training sessions, the feedback group (3.60 ± 0.13) outperformed the control group (3.02 ± 0.12), F (1,18) = 4.44, p = .049 with a strong effect size, partial η2 = 0.198. Specifically, the feedback group showed improvements in the subdomains of CDM of creating context (p = .046) and securing information (p = .018), while their ability to focus questions did not improve significantly (p = .265).

      CONCLUSION: The results suggest that AI-simulated medical history conversations can support CDM training, especially when combined with structured feedback. Such training format may serve as a cost-effective supplement to existing training methods, better preparing students for real medical history conversations.
    mergedFrom:
      - source: pubmed
        pmid: "39609823"
        doi: 10.1186/s12909-024-06399-7
    finalDecision: exclude
  - title: Evaluating the Utility of ChatGPT in Diagnosing and Managing Maxillofacial Trauma.
    reviews:
      - decision: uncertain
        comment: ChatGPT in clinical diagnosis; abstract may reveal assessment or educational components
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments; uses clinical vignettes to compare ChatGPT vs residents, not real workplace-based patient encounters."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares ChatGPT and residents on trauma vignettes for diagnosis/management, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1097/SCS.0000000000010931
    pmid: "39607329"
    authors: Rothchild E, Baker C, Smith I, Tanna N, Ricci J
    year: "2025"
    abstract: Maxillofacial trauma is a significant concern in emergency departments (EDs) due to its high prevalence and the complexity of its management. However, many ED physicians lack specialized training and confidence in handling these cases, leading to a high rate of facial trauma referrals and increased stress on consult services. Recent advancements in artificial intelligence, particularly in large language models such as ChatGPT, have shown potential in aiding clinical decision-making. This study specifically examines the efficacy of ChatGPT in diagnosing and managing maxillofacial trauma. Ten clinical vignettes describing common facial trauma scenarios were presented to a group of plastic surgery residents from a tertiary care center and to ChatGPT. The chatbot and residents were asked to provide their diagnosis, ED management, and definitive management for each scenario. Responses were scored by attending plastic surgeons who were blinded to the response source. The study included 13 resident and ChatGPT responses. The mean total scores were similar between residents and ChatGPT (23.23 versus 22.77, P > 0.05). ChatGPT outperformed residents in diagnostic accuracy (9.85 versus 8.54, P < 0.001) but underperformed in definitive management (8.35 versus 6.35, P < 0.001). There was no significant difference in ED management scores between ChatGPT and the residents. ChatGPT demonstrated high accuracy in diagnosing maxillofacial trauma. However, its ability to suggest appropriate ED management and definitive treatment plans was limited. These findings suggest that while ChatGPT may serve as a valuable diagnostic tool in ED settings, further advancements are necessary before it can reliably contribute to treatment planning in emergent maxillofacial clinical scenarios.
    mergedFrom:
      - source: pubmed
        pmid: "39607329"
        doi: 10.1097/SCS.0000000000010931
    finalDecision: exclude
  - title: "Keeping humans in the loop efficiently by generating question templates instead of questions using AI: Validity evidence on Hybrid AIG."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Uses ChatGPT to generate MCQ item templates for written exams, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Study is about MCQ item generation/validity, not workplace-based clinical education assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.1080/0142159X.2024.2430360
    pmid: "39604119"
    authors: K&#x131;yak Y, Emekli E, Co&#x15f;kun &, Budako&#x11f;lu I
    year: "2025"
    abstract: |-
      BACKGROUND: Manually creating multiple-choice questions (MCQ) is inefficient. Automatic item generation (AIG) offers a scalable solution, with two main approaches: template-based and non-template-based (AI-driven). Template-based AIG ensures accuracy but requires significant expert input to develop templates. In contrast, AI-driven AIG can generate questions quickly but with inaccuracies. The Hybrid AIG combines the strengths of both methods. However, neither have MCQs been generated using the Hybrid AIG approach nor has any validity evidence been provided.

      METHODS: We generated MCQs using the Hybrid AIG approach and investigated the validity evidence of these questions by determining whether experts could identify the correct answers. We used a custom ChatGPT to develop an item template, which were then fed into Gazitor, a template-based AIG (non-AI) software. A panel of medical doctors identified the answers.

      RESULTS: Of 105 decisions, 101 (96.2%) matched the software's correct answer. In all MCQs (100%), the experts reached a consensus on the correct answer. The evidence corresponds to the 'Relations to Other Variables' in Messick's validity framework.

      CONCLUSIONS: The Hybrid AIG approach can enhance the efficiency of MCQ generation while maintaining accuracy. It mitigates concerns about hallucinations while benefiting from AI.
    mergedFrom:
      - source: pubmed
        pmid: "39604119"
        doi: 10.1080/0142159X.2024.2430360
    finalDecision: exclude
  - title: "Performance of ChatGPT and Bard on the medical licensing examinations varies across different cultures: a comparison study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM performance on medical licensing exams across cultures (USMLE, PLAB, HKMLE, NMLE); not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Medical licensing exam performance comparison, not workplace-based assessment/feedback analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1186/s12909-024-06309-x
    pmid: "39593041"
    authors: Chen Y, Huang X, Yang F, Lin H, Lin H, Zheng Z, Liang Q, Zhang J, Li X
    year: "2024"
    abstract: |-
      BACKGROUND: This study aimed to evaluate the performance of GPT-3.5, GPT-4, GPT-4o and Google Bard on the United States Medical Licensing Examination (USMLE), the Professional and Linguistic Assessments Board (PLAB), the Hong Kong Medical Licensing Examination (HKMLE) and the National Medical Licensing Examination (NMLE).

      METHODS: This study was conducted in June 2023. Four LLMs (Large Language Models) (GPT-3.5, GPT-4, GPT-4o and Google Bard) were applied to four medical standardized tests (USMLE, PLAB, HKMLE and NMLE). All questions are multiple-choice questions and were sourced from the question banks of these examinations.

      RESULTS: In USMLE step 1, step 2CK and Step 3, there are accuracy rates of 91.5%, 94.2% and 92.7% provided from GPT-4o, 93.2%, 95.0% and 92.0% provided from GPT-4, 65.6%, 71.6% and 68.5% provided from GPT-3.5, and 64.3%, 55.6%, 58.1% from Google Bard, respectively. In PLAB, HKMLE and NMLE, GPT-4o scored 93.3%, 91.7% and 84.9%, GPT-4 scored 86.7%, 89.6% and 69.8%, GPT-3.5 scored 80.0%, 68.1% and 60.4%, and Google Bard scored 54.2%, 71.7% and 61.3%. There was significant difference in the accuracy rates of four LLMs in the four medical licensing examinations.

      CONCLUSION: GPT-4o performed better in the medical licensing examinations than other three LLMs. The performance of the four models in the NMLE examination needs further improvement.

      CLINICAL TRIAL NUMBER: Not applicable.
    mergedFrom:
      - source: pubmed
        pmid: "39593041"
        doi: 10.1186/s12909-024-06309-x
    finalDecision: exclude
  - title: Evaluation of a Large Language Model on the American Academy of Pediatrics' PREP Emergency Medicine Question Bank.
    reviews:
      - decision: uncertain
        comment: LLM on emergency medicine question bank; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study benchmarks ChatGPT performance on PEM board certification practice questions; no WBA component."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Assessment is based on board-style question bank performance, not a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.1097/PEC.0000000000003271
    pmid: "39591396"
    authors: Ramgopal S, Varma S, Gorski J, Kester K, Shieh A, Suresh S
    year: "2024"
    abstract: |-
      BACKGROUND: Large language models (LLMs), including ChatGPT (Chat Generative Pretrained Transformer), a popular, publicly available LLM, represent an important innovation in the application of artificial intelligence. These systems generate relevant content by identifying patterns in large text datasets based on user input across various topics. We sought to evaluate the performance of ChatGPT in practice test questions designed to assess knowledge competency for pediatric emergency medicine (PEM).

      METHODS: We evaluated the performance of ChatGPT for PEM board certification using a popular question bank used for board certification in PEM published between 2022 and 2024. Clinicians assessed performance of ChatGPT by inputting prompts and recording the software's responses, asking each question over 3 separate iterations. We calculated correct answer percentages (defined as correct in at least 2/3 iterations) and assessed for agreement between the iterations using Fleiss' κ.

      RESULTS: We included 215 questions over the 3 study years. ChatGPT responded correctly to 161 of PREP EM questions over 3 years (74.5%; 95% confidence interval, 68.5%-80.5%), which was similar within each study year (75.0%, 71.8%, and 77.8% for study years 2022, 2023, and 2024, respectively). Among correct responses, most were answered correctly on all 3 iterations (137/161, 85.1%). Performance varied by topic, with the highest scores in research and medical specialties and lower in procedures and toxicology. Fleiss' κ across the 3 iterations was 0.71, indicating substantial agreement.

      CONCLUSION: ChatGPT provided correct answers to PEM responses in three-quarters of cases, over the recommended minimum of 65% provided by the question publisher for passing. Responses by ChatGPT included detailed explanations, suggesting potential use for medical education. We identified limitations in specific topics and image interpretation. These results demonstrate opportunities for LLMs to enhance both the education and clinical practice of PEM.
    mergedFrom:
      - source: pubmed
        pmid: "39591396"
        doi: 10.1097/PEC.0000000000003271
    finalDecision: exclude
  - title: "Artificial intelligence and pain medicine education: Benefits and pitfalls for the medical trainee."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses on general AI, not clearly generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Reviews AI benefits/pitfalls for pain medicine trainees with a pilot QI project of LLMs answering MCQs; not in a workplace-based or clinical education assessment context."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Broad trainee education and MCQ-focused study without workplace-based assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1111/papr.13428
    pmid: "39588809"
    authors: Glicksman M, Wang S, Yellapragada S, Robinson C, Orhurhu V, Emerick T
    year: "2025"
    abstract: |-
      OBJECTIVES: Artificial intelligence (AI) represents an exciting and evolving technology that is increasingly being utilized across pain medicine. Large language models (LLMs) are one type of AI that has become particularly popular. Currently, there is a paucity of literature analyzing the impact that AI may have on trainee education. As such, we sought to assess the benefits and pitfalls that AI may have on pain medicine trainee education. Given the rapidly increasing popularity of LLMs, we particularly assessed how these LLMs may promote and hinder trainee education through a pilot quality improvement project.

      MATERIALS AND METHODS: A comprehensive search of the existing literature regarding AI within medicine was performed to identify its potential benefits and pitfalls within pain medicine. The pilot project was approved by UPMC Quality Improvement Review Committee (#4547). Three of the most commonly utilized LLMs at the initiation of this pilot study - ChatGPT Plus, Google Bard, and Bing AI - were asked a series of multiple choice questions to evaluate their ability to assist in learner education within pain medicine.

      RESULTS: Potential benefits of AI within pain medicine trainee education include ease of use, imaging interpretation, procedural/surgical skills training, learner assessment, personalized learning experiences, ability to summarize vast amounts of knowledge, and preparation for the future of pain medicine. Potential pitfalls include discrepancies between AI devices and associated cost-differences, correlating radiographic findings to clinical significance, interpersonal/communication skills, educational disparities, bias/plagiarism/cheating concerns, lack of incorporation of private domain literature, and absence of training specifically for pain medicine education. Regarding the quality improvement project, ChatGPT Plus answered the highest percentage of all questions correctly (16/17). Lowest correctness scores by LLMs were in answering first-order questions, with Google Bard and Bing AI answering 4/9 and 3/9 first-order questions correctly, respectively. Qualitative evaluation of these LLM-provided explanations in answering second- and third-order questions revealed some reasoning inconsistencies (e.g., providing flawed information in selecting the correct answer).

      CONCLUSIONS: AI represents a continually evolving and promising modality to assist trainees pursuing a career in pain medicine. Still, limitations currently exist that may hinder their independent use in this setting. Future research exploring how AI may overcome these challenges is thus required. Until then, AI should be utilized as supplementary tool within pain medicine trainee education and with caution.
    mergedFrom:
      - source: pubmed
        pmid: "39588809"
        doi: 10.1111/papr.13428
    finalDecision: exclude
  - title: "Using large language models (ChatGPT, Copilot, PaLM, Bard, and Gemini) in Gross Anatomy course: Comparative analysis."
    reviews:
      - decision: uncertain
        comment: LLMs in anatomy course; may involve assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Gross anatomy course context is classroom-based, not workplace-based clinical education."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Tests LLMs on Gross Anatomy course MCQs for preclinical medical students, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Gross anatomy course MCQs/scenario generation are not workplace-based clinical education assessments."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.1002/ca.24244
    pmid: "39573871"
    authors: Mavrych V, Ganguly P, Bolgova O
    year: "2025"
    abstract: The increasing application of generative artificial intelligence large language models (LLMs) in various fields, including medical education, raises questions about their accuracy. The primary aim of our study was to undertake a detailed comparative analysis of the proficiencies and accuracies of six different LLMs (ChatGPT-4, ChatGPT-3.5-turbo, ChatGPT-3.5, Copilot, PaLM, Bard, and Gemini) in responding to medical multiple-choice questions (MCQs), and in generating clinical scenarios and MCQs for upper limb topics in a Gross Anatomy course for medical students. Selected chatbots were tested, answering 50 USMLE-style MCQs. The questions were randomly selected from the Gross Anatomy course exam database for medical students and reviewed by three independent experts. The results of five successive attempts to answer each set of questions by the chatbots were evaluated in terms of accuracy, relevance, and comprehensiveness. The best result was provided by ChatGPT-4, which answered 60.5% ± 1.9% of questions accurately, then Copilot (42.0% ± 0.0%) and ChatGPT-3.5 (41.0% ± 5.3%), followed by ChatGPT-3.5-turbo (38.5% ± 5.7%). Google PaLM 2 (34.5% ± 4.4%) and Bard (33.5% ± 3.0%) gave the poorest results. The overall performance of GPT-4 was statistically superior (p < 0.05) to those of Copilot, GPT-3.5, GPT-Turbo, PaLM2, and Bard by 18.6%, 19.5%, 22%, 26%, and 27%, respectively. Each chatbot was then asked to generate a clinical scenario for each of the three randomly selected topics-anatomical snuffbox, supracondylar fracture of the humerus, and the cubital fossa-and three related anatomical MCQs with five options each, and to indicate the correct answers. Two independent experts analyzed and graded 216 records received (0-5 scale). The best results were recorded for ChatGPT-4, then for Gemini, ChatGPT-3.5, and ChatGPT-3.5-turbo, Copilot, followed by Google PaLM 2; Copilot had the lowest grade. Technological progress notwithstanding, LLMs have yet to mature sufficiently to take over the role of teacher or facilitator completely within a Gross Anatomy course; however, they can be valuable tools for medical educators.
    mergedFrom:
      - source: pubmed
        pmid: "39573871"
        doi: 10.1002/ca.24244
    finalDecision: exclude
  - title: "Enhancing puncture skills training with generative AI and digital technologies: a parallel cohort study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: ChatGPT used as a training aid for puncture skills; not about AI-based assessment, feedback generation, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Skills training intervention does not evaluate generative AI for assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.1186/s12909-024-06217-0
    pmid: "39563290"
    authors: Ji Z, Jiang Y, Sun H, Qiu B, Chen Y, Li M, Fan J, Wang J
    year: "2024"
    abstract: |-
      BACKGROUND: Traditional puncture skills training for refresher doctors faces limitations in effectiveness and efficiency. This study explored the application of generative AI (ChatGPT), templates, and digital imaging to enhance puncture skills training.

      METHODS: 90 refresher doctors were enrolled sequentially into 3 groups: traditional training; template and digital imaging training; and ChatGPT, template and digital imaging training. Outcomes included theoretical knowledge, technical skills, and trainee satisfaction measured at baseline, post-training, and 3-month follow-up.

      RESULTS: The ChatGPT group increased theoretical knowledge scores by 17-21% over traditional training at post-training (81.6 ± 4.56 vs. 69.6 ± 4.58, p < 0.001) and follow-up (86.5 ± 4.08 vs. 71.3 ± 4.83, p < 0.001). It also outperformed template training by 4-5% at post-training (81.6 ± 4.56 vs. 78.5 ± 4.65, p = 0.032) and follow-up (86.5 ± 4.08 vs. 82.7 ± 4.68, p = 0.004). For technical skills, the ChatGPT (4.0 ± 0.32) and template (4.0 ± 0.18) groups showed similar scores at post-training, outperforming traditional training (3.6 ± 0.50) by 11% (p < 0.001). At follow-up, ChatGPT (4.0 ± 0.18) and template (4.0 ± 0.32) still exceeded traditional training (3.8 ± 0.43) by 5% (p = 0.071, p = 0.026). Learning curve analysis revealed fastest knowledge (slope 13.02) and skill (slope 0.62) acquisition for ChatGPT group over template (slope 11.28, 0.38) and traditional (slope 5.17, 0.53). ChatGPT responses showed 100% relevance, 50% completeness, 60% accuracy, with 15.9 s response time. For training satisfaction, ChatGPT group had highest scores (4.2 ± 0.73), over template (3.8 ± 0.68) and traditional groups (2.6 ± 0.94) (p < 0.01).

      CONCLUSION: Integrating AI, templates and digital imaging significantly improved puncture knowledge and skills over traditional training. Combining technological innovations and AI shows promise for streamlining complex medical competency mastery.
    mergedFrom:
      - source: pubmed
        pmid: "39563290"
        doi: 10.1186/s12909-024-06217-0
    finalDecision: exclude
  - title: Comparing the performance of ChatGPT-3.5-Turbo, ChatGPT-4, and Google Bard with Iranian students in pre-internship comprehensive exams.
    reviews:
      - decision: uncertain
        comment: LLMs compared with students on pre-internship exams; abstract may reveal WBA elements
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; benchmarks LLM performance on written pre-internship comprehensive exams."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares LLM scores on comprehensive exams, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.1038/s41598-024-79335-w
    pmid: "39557926"
    authors: Zare S, Vafaeian S, Amini M, Farhadi K, Vali M, Golestani A
    year: "2024"
    abstract: This study aims to measure the performance of different AI-language models in three sets of pre-internship medical exams and to compare their performance with Iranian medical students. Three sets of Persian pre-internship exams were used, along with their English translation (six sets in total). In late September 2023, we sent requests to ChatGPT-3.5-Turbo-0613, GPT-4-0613, and Google Bard in both Persian and English languages (excluding questions with any visual content) with each query in a new session and reviewed their responses. GPT models produced responses at varying levels of randomness. In both Persian and English tests, GPT-4 ranked first and obtained the highest score in all exams and different levels of randomness. While Google Bard scored below average on the Persian exams (still in an acceptable range), ChatGPT-3.5 failed all exams. There was a significant difference between the Large Language Models (LLMs) in Persian exams. While GPT-4 yielded the best scores on the English exams, the distinction between all LLMs and students was not statistically significant. The GPT-4 model outperformed students and other LLMs in medical exams, highlighting its potential application in the medical field. However, more research is needed to fully understand and address the limitations of using these models.
    mergedFrom:
      - source: pubmed
        pmid: "39557926"
        doi: 10.1038/s41598-024-79335-w
    finalDecision: exclude
  - title: Leveraging large language models to construct feedback from medical multiple-choice Questions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Uses LLMs to generate content-based feedback for Progress Test Medizin (a formative MCQ exam); not about assessment, feedback, or analysis of clinical observation records in workplace-based settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1038/s41598-024-79245-x
    pmid: "39537899"
    authors: Tomova M, Rosell&#xf3; Atanet I, Sehy V, Sieg M, M&#xe4;rz M, M&#xe4;der P
    year: "2024"
    abstract: Exams like the formative Progress Test Medizin can enhance their effectiveness by offering feedback beyond numerical scores. Content-based feedback, which encompasses relevant information from exam questions, can be valuable for students by offering them insight into their performance on the current exam, as well as serving as study aids and tools for revision. Our goal was to utilize Large Language Models (LLMs) in preparing content-based feedback for the Progress Test Medizin and evaluate their effectiveness in this task. We utilize two popular LLMs and conduct a comparative assessment by performing textual similarity on the generated outputs. Furthermore, we study via a survey how medical practitioners and medical educators assess the capabilities of LLMs and perceive the usage of LLMs for the task of generating content-based feedback for PTM exams. Our findings show that both examined LLMs performed similarly. Both have their own advantages and disadvantages. Our survey results indicate that one LLM produces slightly better outputs; however, this comes at a cost since it is a paid service, while the other is free to use. Overall, medical practitioners and educators who participated in the survey find the generated feedback relevant and useful, and they are open to using LLMs for such tasks in the future. We conclude that while the content-based feedback generated by the LLM may not be perfect, it nevertheless can be considered a valuable addition to the numerical feedback currently provided.
    mergedFrom:
      - source: pubmed
        pmid: "39537899"
        doi: 10.1038/s41598-024-79245-x
    finalDecision: exclude
  - title: "Large language models in pathology: A comparative study of ChatGPT and Bard with pathology trainees on multiple-choice questions."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Compares LLM performance vs pathology trainees on MCQs to benchmark AI knowledge, not about using AI for learner assessment or WBA feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Study is LLM performance on pathology multiple-choice questions, not WBA-related assessment feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1016/j.anndiagpath.2024.152392
    pmid: "39515029"
    authors: Du W, Jin X, Harris J, Brunetti A, Johnson E, Leung O, Li X, Walle S, Yu Q, Zhou X, Bian F, McKenzie K, Kanathanavanich M, Ozcelik Y, El-Sharkawy F, Koga S
    year: "2024"
    abstract: Large language models (LLMs), such as ChatGPT and Bard, have shown potential in various medical applications. This study aimed to evaluate the performance of LLMs, specifically ChatGPT and Bard, in pathology by comparing their performance with those of pathology trainees, and to assess the consistency of their responses. We selected 150 multiple-choice questions from 15 subspecialties, excluding those with images. Both ChatGPT and Bard were tested on these questions across three separate sessions between June 2023 and January 2024, and their responses were compared with those of 16 pathology trainees (8 junior and 8 senior) from two hospitals. Questions were categorized into easy, intermediate, and difficult based on trainee performance. Consistency and variability in LLM responses were analyzed across three evaluation sessions. ChatGPT significantly outperformed Bard and trainees, achieving an average total score of 82.2% compared to Bard's 49.5%, junior trainees' 45.1%, and senior trainees' 56.0%. ChatGPT's performance was notably stronger in difficult questions (63.4%-68.3%) compared to Bard (31.7%-34.1%) and trainees (4.9%-48.8%). For easy questions, ChatGPT (83.1%-91.5%) and trainees (73.7%-100.0%) showed similar high scores. Consistency analysis revealed that ChatGPT showed a high consistency rate of 80%-85% across three tests, whereas Bard exhibited greater variability with consistency rates of 54%-61%. While LLMs show significant promise in pathology education and practice, continued development and human oversight are crucial for reliable clinical application.
    mergedFrom:
      - source: pubmed
        pmid: "39515029"
        doi: 10.1016/j.anndiagpath.2024.152392
    finalDecision: exclude
  - title: "Bias Sensitivity in Diagnostic Decision-Making: Comparing ChatGPT with Residents."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares ChatGPT diagnostic accuracy with residents on experimental clinical vignettes to study cognitive bias, not about using generative AI for assessment, feedback, or analysis of observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Examines diagnostic bias sensitivity, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.1007/s11606-024-09177-9
    pmid: "39511117"
    authors: Schmidt H, Rotgans J, Mamede S
    year: "2025"
    abstract: |-
      BACKGROUND: Diagnostic errors, often due to biases in clinical reasoning, significantly affect patient care. While artificial intelligence chatbots like ChatGPT could help mitigate such biases, their potential susceptibility to biases is unknown.

      METHODS: This study evaluated diagnostic accuracy of ChatGPT against the performance of 265 medical residents in five previously published experiments aimed at inducing bias. The residents worked in several major teaching hospitals in the Netherlands. The biases studied were case-intrinsic (presence of salient distracting findings in the patient history, effects of disruptive patient behaviors) and situational (prior availability of a look-alike patient). ChatGPT's accuracy in identifying the most-likely diagnosis was measured.

      RESULTS: Diagnostic accuracy of residents and ChatGPT was equivalent. For clinical cases involving case-intrinsic bias, both ChatGPT and the residents exhibited a decline in diagnostic accuracy. Residents' accuracy decreased on average 12%, while the accuracy of ChatGPT 4.0 decreased 21%. Accuracy of ChatGPT 3.5 decreased 9%. These findings suggest that, like human diagnosticians, ChatGPT is sensitive to bias when the biasing information is part of the patient history. When the biasing information was extrinsic to the case in the form of the prior availability of a look-alike case, residents' accuracy decreased by 15%. By contrast, ChatGPT's performance was not affected by the biasing information. Chi-square goodness-of-fit tests corroborated these outcomes.

      CONCLUSIONS: It seems that, while ChatGPT is not sensitive to bias when biasing information is situational, it is sensitive to bias when the biasing information is part of the patient's disease history. Its utility in diagnostic support has potential, but caution is advised. Future research should enhance AI's bias detection and mitigation to make it truly useful for diagnostic support.
    mergedFrom:
      - source: pubmed
        pmid: "39511117"
        doi: 10.1007/s11606-024-09177-9
    finalDecision: exclude
  - title: "ChatGPT-4 Omni Performance in USMLE Disciplines and Clinical Skills: Comparative Analysis."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on USMLE exam questions, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Evaluates LLM accuracy on standardized USMLE exam questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates USMLE question accuracy rather than assessment/feedback or analysis of workplace clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.2196/63430
    pmid: "39504445"
    authors: Bicknell B, Butler D, Whalen S, Ricks J, Dixon C, Clark A, Spaedy O, Skelton A, Edupuganti N, Dzubinski L, Tate H, Dyess G, Lindeman B, Lehmann L
    year: "2024"
    abstract: |-
      BACKGROUND: Recent studies, including those by the National Board of Medical Examiners, have highlighted the remarkable capabilities of recent large language models (LLMs) such as ChatGPT in passing the United States Medical Licensing Examination (USMLE). However, there is a gap in detailed analysis of LLM performance in specific medical content areas, thus limiting an assessment of their potential utility in medical education.

      OBJECTIVE: This study aimed to assess and compare the accuracy of successive ChatGPT versions (GPT-3.5, GPT-4, and GPT-4 Omni) in USMLE disciplines, clinical clerkships, and the clinical skills of diagnostics and management.

      METHODS: This study used 750 clinical vignette-based multiple-choice questions to characterize the performance of successive ChatGPT versions (ChatGPT 3.5 [GPT-3.5], ChatGPT 4 [GPT-4], and ChatGPT 4 Omni [GPT-4o]) across USMLE disciplines, clinical clerkships, and in clinical skills (diagnostics and management). Accuracy was assessed using a standardized protocol, with statistical analyses conducted to compare the models' performances.

      RESULTS: GPT-4o achieved the highest accuracy across 750 multiple-choice questions at 90.4%, outperforming GPT-4 and GPT-3.5, which scored 81.1% and 60.0%, respectively. GPT-4o's highest performances were in social sciences (95.5%), behavioral and neuroscience (94.2%), and pharmacology (93.2%). In clinical skills, GPT-4o's diagnostic accuracy was 92.7% and management accuracy was 88.8%, significantly higher than its predecessors. Notably, both GPT-4o and GPT-4 significantly outperformed the medical student average accuracy of 59.3% (95% CI 58.3-60.3).

      CONCLUSIONS: GPT-4o's performance in USMLE disciplines, clinical clerkships, and clinical skills indicates substantial improvements over its predecessors, suggesting significant potential for the use of this technology as an educational aid for medical students. These findings underscore the need for careful consideration when integrating LLMs into medical education, emphasizing the importance of structured curricula to guide their appropriate use and the need for ongoing critical analyses to ensure their reliability and effectiveness.
    mergedFrom:
      - source: pubmed
        pmid: "39504445"
        doi: 10.2196/63430
    finalDecision: exclude
  - title: Performance of Chatgpt in ophthalmology exam; human versus AI.
    reviews:
      - decision: uncertain
        comment: ChatGPT on ophthalmology exam; abstract needed to determine WBA relevance
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT performance on ophthalmology resident training exam questions against resident performance; not about using AI for assessment or feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares AI versus residents on exam questions, not workplace-based assessment feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.1007/s10792-024-03353-w
    pmid: "39503920"
    authors: Balci A, Yazar Z, Ozturk B, Altan C
    year: "2024"
    abstract: |-
      PURPOSE: This cross-sectional study focuses on evaluating the success rate of ChatGPT in answering questions from the 'Resident Training Development Exam' and comparing these results with the performance of the ophthalmology residents.

      METHODS: The 75 exam questions, across nine sections and three difficulty levels, were presented to ChatGPT. The responses and explanations were recorded. The readability and complexity of the explanations were analyzed and The Flesch Reading Ease (FRE) score (0-100) was recorded using the program named Readable. Residents were categorized into four groups based on their seniority. The overall and seniority-specific success rates of the residents were compared separately with ChatGPT.

      RESULTS: Out of 69 questions, ChatGPT answered 37 correctly (53.62%). The highest success was in Lens and Cataract (77.77%), and the lowest in Pediatric Ophthalmology and Strabismus (0.00%). Of 789 residents, overall accuracy was 50.37%. Seniority-specific accuracy rates were 43.49%, 51.30%, 54.91%, and 60.05% for 1st to 4th-year residents. ChatGPT ranked 292nd among residents. Difficulty-wise, 11 questions were easy, 44 moderate, and 14 difficult. ChatGPT's accuracy for each level was 63.63%, 54.54%, and 42.85%, respectively. The average FRE score of responses generated by ChatGPT was found to be 27.56 ± 12.40.

      CONCLUSION: ChatGPT correctly answered 53.6% of questions in an exam for residents. ChatGPT has a lower success rate on average than a 3rd year resident. The readability of responses provided by ChatGPT is low, and they are difficult to understand. As difficulty increases, ChatGPT's success decreases. Predictably, these results will change with more information loaded into ChatGPT.
    mergedFrom:
      - source: pubmed
        pmid: "39503920"
        doi: 10.1007/s10792-024-03353-w
    finalDecision: exclude
  - title: "Large language models (LLMs) in radiology exams for medical students: Performance and consequences."
    reviews:
      - decision: uncertain
        comment: LLMs on radiology exams for medical students; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Benchmarks LLM performance on radiology MCQs for medical students in an academic exam setting, not workplace-based or clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Medical student radiology exam study, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1055/a-2437-2067
    pmid: "39496293"
    authors: Gotta J, Le Hong Q, Koch V, Gruenewald L, Geyer T, Martin S, Scholtz J, Booz C, Pinto Dos Santos D, Mahmoudi S, Eichler K, Gruber-Rouh T, Hammerstingl R, Biciusca T, Juergens L, H&#xf6;hne E, Mader C, Vogl T, Reschke P
    year: "2025"
    abstract: "The evolving field of medical education is being shaped by technological advancements, including the integration of Large Language Models (LLMs) like ChatGPT. These models could be invaluable resources for medical students, by simplifying complex concepts and enhancing interactive learning by providing personalized support. LLMs have shown impressive performance in professional examinations, even without specific domain training, making them particularly relevant in the medical field. This study aims to assess the performance of LLMs in radiology examinations for medical students, thereby shedding light on their current capabilities and implications.This study was conducted using 151 multiple-choice questions, which were used for radiology exams for medical students. The questions were categorized by type and topic and were then processed using OpenAI's GPT-3.5 and GPT- 4 via their API, or manually put into Perplexity AI with GPT-3.5 and Bing. LLM performance was evaluated overall, by question type and by topic.GPT-3.5 achieved a 67.6% overall accuracy on all 151 questions, while GPT-4 outperformed it significantly with an 88.1% overall accuracy (p<0.001). GPT-4 demonstrated superior performance in both lower-order and higher-order questions compared to GPT-3.5, Perplexity AI, and medical students, with GPT-4 particularly excelling in higher-order questions. All GPT models would have successfully passed the radiology exam for medical students at our university.In conclusion, our study highlights the potential of LLMs as accessible knowledge resources for medical students. GPT-4 performed well on lower-order as well as higher-order questions, making ChatGPT-4 a potentially very useful tool for reviewing radiology exam questions. Radiologists should be aware of ChatGPT's limitations, including its tendency to confidently provide incorrect responses. · ChatGPT demonstrated remarkable performance, achieving a passing grade on a radiology examination for medical students that did not include image questions.. · GPT-4 exhibits significantly improved performance compared to its predecessors GPT-3.5 and Perplexity AI with 88% of questions answered correctly.. · Radiologists as well as medical students should be aware of ChatGPT's limitations, including its tendency to confidently provide incorrect responses.. · Gotta J, Le Hong QA, Koch V et al. Large language models (LLMs) in radiology exams for medical students: Performance and consequences. Rofo 2025; 197: 1057-1067."
    mergedFrom:
      - source: pubmed
        pmid: "39496293"
        doi: 10.1055/a-2437-2067
    finalDecision: exclude
  - title: "Evaluating the Effectiveness of advanced large language models in medical Knowledge: A Comparative study using Japanese national medical examination."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on national medical examination, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates LLM accuracy on Japanese national medical licensing examination questions"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares LLM accuracy on licensing exam questions, not workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.1016/j.ijmedinf.2024.105673
    pmid: "39471700"
    authors: Liu M, Okuhara T, Dai Z, Huang W, Gu L, Okada H, Furukawa E, Kiuchi T
    year: "2025"
    abstract: |-
      UNLABELLED: Study aims and objectives. This study aims to evaluate the accuracy of medical knowledge in the most advanced LLMs (GPT-4o, GPT-4, Gemini 1.5 Pro, and Claude 3 Opus) as of 2024. It is the first to evaluate these LLMs using a non-English medical licensing exam. The insights from this study will guide educators, policymakers, and technical experts in the effective use of AI in medical education and clinical diagnosis.

      METHOD: Authors inputted 790 questions from Japanese National Medical Examination into the chat windows of the LLMs to obtain responses. Two authors independently assessed the correctness. Authors analyzed the overall accuracy rates of the LLMs and compared their performance on image and non-image questions, questions of varying difficulty levels, general and clinical questions, and questions from different medical specialties. Additionally, authors examined the correlation between the number of publications and LLMs' performance in different medical specialties.

      RESULTS: GPT-4o achieved highest accuracy rate of 89.2% and outperformed the other LLMs in overall performance and each specific category. All four LLMs performed better on non-image questions than image questions, with a 10% accuracy gap. They also performed better on easy questions compared to normal and difficult ones. GPT-4o achieved a 95.0% accuracy rate on easy questions, marking it as an effective knowledge source for medical education. Four LLMs performed worst on "Gastroenterology and Hepatology" specialty. There was a positive correlation between the number of publications and LLM performance in different specialties.

      CONCLUSIONS: GPT-4o achieved an overall accuracy rate close to 90%, with 95.0% on easy questions, significantly outperforming the other LLMs. This indicates GPT-4o's potential as a knowledge source for easy questions. Image-based questions and question difficulty significantly impact LLM accuracy. "Gastroenterology and Hepatology" is the specialty with the lowest performance. The LLMs' performance across medical specialties correlates positively with the number of related publications.
    mergedFrom:
      - source: pubmed
        pmid: "39471700"
        doi: 10.1016/j.ijmedinf.2024.105673
    finalDecision: exclude
  - title: "Performance of ChatGPT in medical licensing examinations in countries worldwide: A systematic review and meta-analysis protocol."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Study protocol, not completed original research or systematic review"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Protocol, not an original research article, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    doi: 10.1371/journal.pone.0312771
    pmid: "39466750"
    authors: Liu M, Okuhara T, Chang X, Okada H, Kiuchi T
    year: "2024"
    abstract: |-
      INTRODUCTION: In November 2022, the online artificial intelligence (AI) chatbot ChatGPT was released to the public, and swiftly garnered global attention because of its ability to provide detailed answers to complex queries. In medical field, ChatGPT has shown great potential to be used in medical education and has excelled in many English-language medical licensing examinations. However, due to the variability of medical licensing examinations in different countries, and ChatGPT's particular proficiency in English, the previous literatures showed that ChatGPT is unable to pass medical licensing examinations from non-English-speaking countries or those not administered in English. To the best of our knowledge, this is the first study to review whether ChatGPT can demonstrate consistent accuracy across diverse medical licensing examinations and be used in medical education across countries.

      OBJECTIVE: In this study protocol, we aimed to analyze and review the differences in performance of ChatGPT in medical exams in various language environments and countries, as well as its potential in medical education.

      METHODS AND ANALYSIS: A systematic review and meta-analysis was conducted using PubMed, Web of Science, and Scopus to collect papers testing the performance of ChatGPT in medical licensing examinations. We imported all the collected literatures into Rayyan and screened the literatures based on the selection criteria and exclusion criteria. The risk of bias and quality of included studies was assessed by using Mixed Methods Appraisal Tool (MMAT). Data from included studies was extracted into an Excel spreadsheet. All of the above processes were completed by two reviewers independently. A third reviewer was consulted in cases of disagreement. Finally, we provided both quantitative and qualitative analysis of the findings from the included studies.

      TRIAL REGISTRATION: PROSPERO registration number: CRD42024506687.
    mergedFrom:
      - source: pubmed
        pmid: "39466750"
        doi: 10.1371/journal.pone.0312771
    finalDecision: exclude
  - title: Performance Assessment of GPT 4.0 on the Japanese Medical Licensing Examination.
    reviews:
      - decision: uncertain
        comment: GPT-4 on licensing exam; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates GPT-4 accuracy on Japanese medical licensing exam questions; not about using AI for assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates GPT-4 on licensing exam questions rather than workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1007/s11596-024-2932-9
    pmid: "39460888"
    authors: Wang H, Zhou H, Zhang J, Xie Y, Yang J, Xue M, Yan Z, Li W, Zhang X, Wu Y, Chen X, Liu P, Lu L, Ye Z
    year: "2024"
    abstract: |-
      OBJECTIVE: To evaluate the accuracy and parsing ability of GPT 4.0 for Japanese medical practitioner qualification examinations in a multidimensional way to investigate its response accuracy and comprehensiveness to medical knowledge.

      METHODS: We evaluated the performance of the GPT 4.0 on Japanese Medical Licensing Examination (JMLE) questions (2021-2023). Questions are categorized by difficulty and type, with distinctions between general and clinical parts, as well as between single-choice (MCQ1) and multiple-choice (MCQ2) questions. Difficulty levels were determined on the basis of correct rates provided by the JMLE Preparatory School. The accuracy and quality of the GPT 4.0 responses were analyzed via an improved Global Qualily Scale (GQS) scores, considering both the chosen options and the accompanying analysis. Descriptive statistics and Pearson Chi-square tests were used to examine performance across exam years, question difficulty, type, and choice. GPT 4.0 ability was evaluated via the GQS, with comparisons made via the Mann-Whitney U or Kruskal-Wallis test.

      RESULTS: The correct response rate and parsing ability of the GPT4.0 to the JMLE questions reached the qualification level (80.4%). In terms of the accuracy of the GPT4.0 response to the JMLE, we found significant differences in accuracy across both difficulty levels and option types. According to the GQS scores for the GPT 4.0 responses to all the JMLE questions, the performance of the questionnaire varied according to year and choice type.

      CONCLUSION: GTP4.0 performs well in providing basic support in medical education and medical research, but it also needs to input a large amount of medical-related data to train its model and improve the accuracy of its medical knowledge output. Further integration of ChatGPT with the medical field could open new opportunities for medicine.
    mergedFrom:
      - source: pubmed
        pmid: "39460888"
        doi: 10.1007/s11596-024-2932-9
    finalDecision: exclude
  - title: Performance of ChatGPT and Dental Students on Concepts of Periodontal Surgery.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT on periodontal surgery MCQs vs dental students; not about assessment, feedback, or analysis of clinical observation records in workplace-based education"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education assessment setting; focuses on exam-style knowledge testing."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1111/eje.13047
    pmid: "39446672"
    authors: Li C, Zhang J, Abdul-Masih J, Zhang S, Yang J
    year: "2025"
    abstract: |-
      INTRODUCTION: As a large language model, chat generative pretrained transformer (ChatGPT) has provided a valuable tool for various medical scenarios with its interactive dialogue-based interface. However, there is a lack of studies on ChatGPT's effectiveness in handling dental tasks. This study aimed to compare the knowledge and comprehension abilities of ChatGPT-3.5/4 with that of dental students about periodontal surgery.

      MATERIALS AND METHODS: A total of 134 dental students participated in this study. We designed a questionnaire consisting of four questions about the inclination for ChatGPT, 25 multiple-choice, and one open-ended question. As the comparison of ChatGPT-3.5 and 4, the question about the inclination was removed, and the rest was the same. The response time of ChatGPT-3.5 and 4 as well as the comparison of ChatGPT-3.5 and 4' performances with dental students were measured. Regarding students' feedback on the open-ended question, we also compared the outcomes of ChatGPT-4' and teacher's review.

      RESULTS: On average, ChatGPT-3.5 and 4 required 3.63 ± 1.18 s (95% confidence interval [CI], 3.14, 4.11) and 12.49 ± 7.29 s (95% CI, 9.48, 15.50), respectively (p < 0.001) for each multiple-choice question. For these 25 questions, the accuracy was 21.51 ± 2.72, 14 and 20 for students, ChatGPT-3.5 and 4, respectively. Furthermore, the outcomes of ChatGPT-4's review were consistent with that of teacher's review.

      CONCLUSIONS: For dental examinations related to periodontal surgery, ChatGPT's accuracy was not yet comparable to that of the students. Nevertheless, ChatGPT shows promise in assisting students with the curriculum and helping practitioners with clinical letters and reviews of students' textual descriptions.
    mergedFrom:
      - source: pubmed
        pmid: "39446672"
        doi: 10.1111/eje.13047
    finalDecision: exclude
  - title: ChatGPT as a prospective undergraduate and medical school student.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Tests ChatGPT's performance as a student on exams, not about assessment or feedback of human learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1371/journal.pone.0308157
    pmid: "39441779"
    authors: Giunti M, Garavaglia F, Giuntini R, Sergioli G, Pinna S
    year: "2024"
    abstract: "This article reports the results of an experiment conducted with ChatGPT to see how its performance compares to human performance on tests that require specific knowledge and skills, such as university admission tests. We chose a general undergraduate admission test and two tests for admission to biomedical programs: the Scholastic Assessment Test (SAT), the Cambridge BioMedical Admission Test (BMAT), and the Italian Medical School Admission Test (IMSAT). In particular, we looked closely at the difference in performance between ChatGPT-4 and its predecessor, ChatGPT-3.5, to assess its evolution. The performance of ChatGPT-4 showed a significant improvement over ChatGPT-3.5 and, compared to real students, was on average within the top 10% in the SAT test, while the score in the IMSAT test granted admission to the two highest ranked Italian medical schools. In addition to the performance analysis, we provide a qualitative analysis of incorrect answers and a classification of three different types of logical and computational errors made by ChatGPT-4, which reveal important weaknesses of the model. This provides insight into the skills needed to use these models effectively despite their weaknesses, and also suggests possible applications of our analysis in the field of education."
    mergedFrom:
      - source: pubmed
        pmid: "39441779"
        doi: 10.1371/journal.pone.0308157
    finalDecision: exclude
  - title: Medical imaging and radiation science students' use of artificial intelligence for learning and assessment.
    reviews:
      - decision: uncertain
        comment: AI for learning and assessment; may involve generative AI in assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Qualitative study of radiography students' general use of AI for learning; not specifically about using generative AI for workplace-based assessment/feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Discusses general AI use by students without a specific generative AI/LLM intervention tied to WBA outcomes."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.1016/j.radi.2024.10.006
    pmid: "39427502"
    authors: Lewis S, Bhyat F, Casmod Y, Gani A, Gumede L, Hajat A, Hazell L, Kammies C, Mahlaola T, Mokoena L, Vermeulen L
    year: "2024"
    abstract: |-
      INTRODUCTION: Artificial intelligence has permeated all aspects of our existence, and medical imaging has shown the burgeoning use of artificial intelligence in clinical environments. However, there are limited empirical studies on radiography students' use of artificial intelligence for learning and assessment. Therefore, this study aimed to gain an understanding of this phenomenon.

      METHODS: The study used a qualitative explorative and descriptive research design. Data was obtained through five focus group interviews with purposively sampled undergraduate medical imaging and radiation science students at a single higher education institution in South Africa. Verbatim transcripts of the audio-recorded interviews were analysed thematically.

      RESULTS: Three themes and related subthemes were developed: 1) understanding artificial intelligence, 2) experiences with the use of artificial intelligence with the subthemes of the use of artificial intelligence in theoretical and clinical learning and challenges of using artificial intelligence, and 3) incorporation of artificial intelligence in undergraduate medical imaging and radiation sciences education with the subthemes of student education, ethical considerations and responsible use and curriculum integration of artificial intelligence in relation to learning and assessment.

      CONCLUSION: Participants used artificial intelligence for learning and assessment by generating ideas to enhance academic writing, as a learning tool, finding literature, language translation and for enhanced efficiency. Simulation-based artificial intelligence supports students' clinical learning, and artificial intelligence within the clinical departments assists with improved patient outcomes. However, participants expressed concerns about the reliability and ethical implications of artificial intelligence-generated information. To address these concerns, participants suggested integrating artificial intelligence into medical imaging and radiation sciences education, where educators need to educate students on the responsible use of artificial intelligence in learning and consider artificial intelligence in assessments.

      IMPLICATIONS FOR PRACTICE: The study findings contribute to understanding medical imaging and radiation sciences students' use of artificial intelligence and may be used to develop evidence-based strategies for integrating artificial intelligence into the curriculum to enhance medical imaging and radiation sciences education and support students.
    mergedFrom:
      - source: pubmed
        pmid: "39427502"
        doi: 10.1016/j.radi.2024.10.006
    finalDecision: exclude
  - title: Legal aspects of generative artificial intelligence and large language models in examinations and theses.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Legal aspects article, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting (focuses on examinations/theses legal aspects)."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    doi: 10.3205/zma001702
    pmid: "39415812"
    authors: M&#xe4;rz M, Himmelbauer M, Boldt K, Oksche A
    year: "2024"
    abstract: "The high performance of generative artificial intelligence (AI) and large language models (LLM) in examination contexts has triggered an intense debate about their applications, effects and risks. What legal aspects need to be considered when using LLM in teaching and assessment? What possibilities do language models offer? Statutes and laws are used to assess the use of LLM: - University statutes, state higher education laws, licensing regulations for doctors - Copyright Act (UrhG) - General Data Protection Regulation (DGPR) - AI Regulation (EU AI Act) LLM and AI offer opportunities but require clear university frameworks. These should define legitimate uses and areas where use is prohibited. Cheating and plagiarism violate good scientific practice and copyright laws. Cheating is difficult to detect. Plagiarism by AI is possible. Users of the products are responsible. LLM are effective tools for generating exam questions. Nevertheless, careful review is necessary as even apparently high-quality products may contain errors. However, the risk of copyright infringement with AI-generated exam questions is low, as copyright law allows up to 15% of protected works to be used for teaching and exams. The grading of exam content is subject to higher education laws and regulations and the GDPR. Exclusively computer-based assessment without human review is not permitted. For high-risk applications in education, the EU's AI Regulation will apply in the future. When dealing with LLM in assessments, evaluation criteria for existing assessments can be adapted, as can assessment programmes, e.g. to reduce the motivation to cheat. LLM can also become the subject of the examination themselves. Teachers should undergo further training in AI and consider LLM as an addition."
    mergedFrom:
      - source: pubmed
        pmid: "39415812"
        doi: 10.3205/zma001702
    finalDecision: exclude
  - title: Comparative Analysis of the Response Accuracies of Large Language Models in the Korean National Dental Hygienist Examination Across Korean and English Questions.
    reviews:
      - decision: uncertain
        comment: LLMs on dental hygienist exam; health professional assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on a national licensing exam; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted on licensing-exam question sets only."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.1111/idh.12848
    pmid: "39415339"
    authors: Song E, Lee S
    year: "2025"
    abstract: |-
      INTRODUCTION: Large language models such as Gemini, GPT-3.5, and GPT-4 have demonstrated significant potential in the medical field. Their performance in medical licensing examinations globally has highlighted their capabilities in understanding and processing specialized medical knowledge. This study aimed to evaluate and compare the performance of Gemini, GPT-3.5, and GPT-4 in the Korean National Dental Hygienist Examination. The accuracy of answering the examination questions in both Korean and English was assessed.

      METHODS: This study used a dataset comprising questions from the Korean National Dental Hygienist Examination over 5 years (2019-2023). A two-way analysis of variance (ANOVA) test was employed to investigate the impacts of model type and language on the accuracy of the responses. Questions were input into each model under standardized conditions, and responses were classified as correct or incorrect based on predefined criteria.

      RESULTS: GPT-4 consistently outperformed the other models, achieving the highest accuracy rates across both language versions annually. In particular, it showed superior performance in English, suggesting advancements in its training algorithms for language processing. However, all models demonstrated variable accuracies in subjects with localized characteristics, such as health and medical law.

      CONCLUSIONS: These findings indicate that GPT-4 holds significant promise for application in medical education and standardized testing, especially in English. However, the variability in performance across different subjects and languages underscores the need for ongoing improvements and the inclusion of more diverse and localized training datasets to enhance the models' effectiveness in multilingual and multicultural contexts.
    mergedFrom:
      - source: pubmed
        pmid: "39415339"
        doi: 10.1111/idh.12848
    finalDecision: exclude
  - title: "Enhancing clinical reasoning skills for medical students: a qualitative comparison of LLM-powered social robotic versus computer-based virtual patients within rheumatology."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted in simulated environment (virtual patients/robotic simulation), not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Focuses on virtual patient simulation rather than workplace-based assessment context."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    doi: 10.1007/s00296-024-05731-0
    pmid: "39412574"
    authors: Borg A, Jobs B, Huss V, Gentline C, Espinosa F, Ruiz M, Edelbring S, Georg C, Skantze G, Parodis I
    year: "2024"
    abstract: "Virtual patients (VPs) are increasingly used in medical education to train clinical reasoning (CR) skills. However, optimal VP design for enhancing interactivity and authenticity remains unclear. Novel interactive modalities, such as large language model (LLM)-enhanced social robotic VPs might increase interactivity and authenticity in CR skill practice. To evaluate medical students' perceptions of CR training using an LLM-enhanced social robotic VP platform compared with a conventional computer-based VP platform. A qualitative study involved 23 third-year medical students from Karolinska Institutet, who completed VP cases on an LLM-enhanced social robotic platform and a computer-based semi-linear platform. In-depth interviews assessed students' self-perceived acquirement of CR skills using the two platforms. Thematic analysis was employed to identify themes and sub-themes. Three main themes were identified: authenticity, VP application, and strengths and limitations. Students found the social robotic platform more authentic and engaging. It enabled highly interactive communication and expressed emotions, collectively offering a realistic experience. It facilitated active learning, hypothesis generation, and adaptive thinking. Limitations included lack of physical examination options and, occasionally, mechanical dialogue. The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform. Despite some limitations, it shows promise in training CR skills, communication, and adaptive thinking. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations."
    mergedFrom:
      - source: pubmed
        pmid: "39412574"
        doi: 10.1007/s00296-024-05731-0
    finalDecision: exclude
  - title: Comparing Scoring Consistency of Large Language Models with Faculty for Formative Assessments in Medical Education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study involves pre-clinical students' critical appraisal assignments, not workplace-based assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Formative scoring of pre-clinical assignments occurs in classroom/pre-clinical education, not workplace-based clinical training."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.1007/s11606-024-09050-9
    pmid: "39402411"
    authors: Sreedhar R, Chang L, Gangopadhyaya A, Shiels P, Loza J, Chi E, Gabel E, Park Y
    year: "2025"
    abstract: |-
      BACKGROUND: The Liaison Committee on Medical Education requires that medical students receive individualized feedback on their self-directed learning skills. Pre-clinical students are asked to complete multiple spaced critical appraisal assignments. However, the individual feedback requires significant faculty time. As large language models (LLMs) can score and generate feedback, we explored their use in grading formative assessments through validity and feasibility lenses.

      OBJECTIVE: To explore the consistency and feasibility of using an LLM to assess and provide feedback for formative assessments in undergraduate medical education.

      DESIGN AND PARTICIPANTS: This was a cross-sectional study of pre-clinical students' critical appraisal assignments at University of Illinois College of Medicine (UICOM) during the 2022-2023 academic year.

      INTERVENTION: An initial sample of ten assignments was used to develop a prompt. For each student entry, the de-identified assignment and prompt were provided to ChatGPT 3.5, and its scoring was compared to the existing faculty grade.

      MAIN MEASURES: Differences in scoring of individual items between ChatGPT and faculty were assessed. Scoring consistency using inter-rater reliability (IRR) was calculated as percent exact agreement. Chi-squared test was used to determine if there were significant differences in scores. Psychometric characteristics including internal-consistency reliability, area under precision-recall curve (AUCPR), and cost were studied.

      KEY RESULTS: In this cross-sectional study, 111 pre-clinical students' faculty graded assignments were compared with that of ChatGPT and the scoring of individual items was comparable. The overall agreement between ChatGPT and faculty was 67% (OR = 2.53, P < 0.001); mean AUCPR was 0.69 (range 0.61-0.76). Internal-consistency reliability of ChatGPT was 0.64 and its use resulted in a fivefold reduction in faculty time, and potential savings of 150 faculty hours.

      CONCLUSIONS: This study of psychometric characteristics of ChatGPT demonstrates the potential role for LLMs to assist faculty in assessing and providing feedback for formative assignments.
    mergedFrom:
      - source: pubmed
        pmid: "39402411"
        doi: 10.1007/s11606-024-09050-9
    finalDecision: exclude
  - title: "The future of AI clinicians: assessing the modern standard of chatbots and their approach to diagnostic uncertainty."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Does not indicate medical or health-professional learners as participants"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates AI chatbot performance on extracted Progress Test questions compared to residents"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates chatbot answers on progress-test questions rather than workplace-based assessment or feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.1186/s12909-024-06115-5
    pmid: "39394122"
    authors: Huang R, Benour A, Kemppainen J, Leung F
    year: "2024"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) chatbots have demonstrated proficiency in structured knowledge assessments; however, there is limited research on their performance in scenarios involving diagnostic uncertainty, which requires careful interpretation and complex decision-making. This study aims to evaluate the efficacy of AI chatbots, GPT-4o and Claude-3, in addressing medical scenarios characterized by diagnostic uncertainty relative to Family Medicine residents.

      METHODS: Questions with diagnostic uncertainty were extracted from the Progress Tests administered by the Department of Family and Community Medicine at the University of Toronto between 2022 and 2023. Diagnostic uncertainty questions were defined as those presenting clinical scenarios where symptoms, clinical findings, and patient histories do not converge on a definitive diagnosis, necessitating nuanced diagnostic reasoning and differential diagnosis. These questions were administered to a cohort of 320 Family Medicine residents in their first (PGY-1) and second (PGY-2) postgraduate years and inputted into GPT-4o and Claude-3. Errors were categorized into statistical, information, and logical errors. Statistical analyses were conducted using a binomial generalized estimating equation model, paired t-tests, and chi-squared tests.

      RESULTS: Compared to the residents, both chatbots scored lower on diagnostic uncertainty questions (p < 0.01). PGY-1 residents achieved a correctness rate of 61.1% (95% CI: 58.4-63.7), and PGY-2 residents achieved 63.3% (95% CI: 60.7-66.1). In contrast, Claude-3 correctly answered 57.7% (n = 52/90) of questions, and GPT-4o correctly answered 53.3% (n = 48/90). Claude-3 had a longer mean response time (24.0 s, 95% CI: 21.0-32.5 vs. 12.4 s, 95% CI: 9.3-15.3; p < 0.01) and produced longer answers (2001 characters, 95% CI: 1845-2212 vs. 1596 characters, 95% CI: 1395-1705; p < 0.01) compared to GPT-4o. Most errors by GPT-4o were logical errors (62.5%).

      CONCLUSIONS: While AI chatbots like GPT-4o and Claude-3 demonstrate potential in handling structured medical knowledge, their performance in scenarios involving diagnostic uncertainty remains suboptimal compared to human residents.
    mergedFrom:
      - source: pubmed
        pmid: "39394122"
        doi: 10.1186/s12909-024-06115-5
    finalDecision: exclude
  - title: Evaluating the use of large language models to provide clinical recommendations in the Emergency Department.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on clinical recommendations, not learner assessment/feedback or observation-record analysis"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluating LLMs for clinical decision-making recommendations (admission, radiology, antibiotics), not about assessment/feedback of learners. Also criterion 6: participants are not learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Study evaluates ED recommendation tasks on patient notes, not medical/health-professional learners as participants."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.1038/s41467-024-52415-1
    pmid: "39379357"
    authors: Williams C, Miao B, Kornblith A, Butte A
    year: "2024"
    abstract: The release of GPT-4 and other large language models (LLMs) has the potential to transform healthcare. However, existing research evaluating LLM performance on real-world clinical notes is limited. Here, we conduct a highly-powered study to determine whether LLMs can provide clinical recommendations for three tasks (admission status, radiological investigation(s) request status, and antibiotic prescription status) using clinical notes from the Emergency Department. We randomly selected 10,000 Emergency Department visits to evaluate the accuracy of zero-shot, GPT-3.5-turbo- and GPT-4-turbo-generated clinical recommendations across four different prompting strategies. We found that both GPT-4-turbo and GPT-3.5-turbo performed poorly compared to a resident physician, with accuracy scores 8% and 24%, respectively, lower than physician on average. Both LLMs tended to be overly cautious in its recommendations, with high sensitivity at the cost of specificity. Our findings demonstrate that, while early evaluations of the clinical use of LLMs are promising, LLM performance must be significantly improved before their deployment as decision support systems for clinical recommendations and other complex tasks.
    mergedFrom:
      - source: pubmed
        pmid: "39379357"
        doi: 10.1038/s41467-024-52415-1
      - source: scopus
        doi: 10.1038/s41467-024-52415-1
        scopusId: SCOPUS_ID:85206023306
    finalDecision: exclude
  - title: "Assessment of ChatGPT-4 in Family Medicine Board Examinations Using Advanced AI Learning and Analytical Methods: Observational Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT-4 on ABFM board exam questions; benchmarks AI exam performance, not about assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Examines ChatGPT performance on board-style exam questions, not workplace-based assessment or feedback from clinical observation data."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.2196/56128
    pmid: "39378442"
    authors: Goodings A, Kajitani S, Chhor A, Albakri A, Pastrak M, Kodancha M, Ives R, Lee Y, Kajitani K
    year: "2024"
    abstract: |-
      BACKGROUND: This research explores the capabilities of ChatGPT-4 in passing the American Board of Family Medicine (ABFM) Certification Examination. Addressing a gap in existing literature, where earlier artificial intelligence (AI) models showed limitations in medical board examinations, this study evaluates the enhanced features and potential of ChatGPT-4, especially in document analysis and information synthesis.

      OBJECTIVE: The primary goal is to assess whether ChatGPT-4, when provided with extensive preparation resources and when using sophisticated data analysis, can achieve a score equal to or above the passing threshold for the Family Medicine Board Examinations.

      METHODS: In this study, ChatGPT-4 was embedded in a specialized subenvironment, "AI Family Medicine Board Exam Taker," designed to closely mimic the conditions of the ABFM Certification Examination. This subenvironment enabled the AI to access and analyze a range of relevant study materials, including a primary medical textbook and supplementary web-based resources. The AI was presented with a series of ABFM-type examination questions, reflecting the breadth and complexity typical of the examination. Emphasis was placed on assessing the AI's ability to interpret and respond to these questions accurately, leveraging its advanced data processing and analysis capabilities within this controlled subenvironment.

      RESULTS: In our study, ChatGPT-4's performance was quantitatively assessed on 300 practice ABFM examination questions. The AI achieved a correct response rate of 88.67% (95% CI 85.08%-92.25%) for the Custom Robot version and 87.33% (95% CI 83.57%-91.10%) for the Regular version. Statistical analysis, including the McNemar test (P=.45), indicated no significant difference in accuracy between the 2 versions. In addition, the chi-square test for error-type distribution (P=.32) revealed no significant variation in the pattern of errors across versions. These results highlight ChatGPT-4's capacity for high-level performance and consistency in responding to complex medical examination questions under controlled conditions.

      CONCLUSIONS: The study demonstrates that ChatGPT-4, particularly when equipped with specialized preparation and when operating in a tailored subenvironment, shows promising potential in handling the intricacies of medical board examinations. While its performance is comparable with the expected standards for passing the ABFM Certification Examination, further enhancements in AI technology and tailored training methods could push these capabilities to new heights. This exploration opens avenues for integrating AI tools such as ChatGPT-4 in medical education and assessment, emphasizing the importance of continuous advancement and specialized training in medical applications of AI.
    mergedFrom:
      - source: pubmed
        pmid: "39378442"
        doi: 10.2196/56128
    finalDecision: exclude
  - title: "Understanding and training for the impact of large language models and artificial intelligence in healthcare practice: a narrative review."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Narrative review, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article, systematic review, meta-analysis, or preprint"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    doi: 10.1186/s12909-024-06048-z
    pmid: "39375721"
    authors: McCoy L, Ci Ng F, Sauer C, Yap Legaspi K, Jain B, Gallifant J, McClurkin M, Hammond A, Goode D, Gichoya J, Celi L
    year: "2024"
    abstract: Reports of Large Language Models (LLMs) passing board examinations have spurred medical enthusiasm for their clinical integration. Through a narrative review, we reflect upon the skill shifts necessary for clinicians to succeed in an LLM-enabled world, achieving benefits while minimizing risks. We suggest how medical education must evolve to prepare clinicians capable of navigating human-AI systems.
    mergedFrom:
      - source: pubmed
        pmid: "39375721"
        doi: 10.1186/s12909-024-06048-z
    finalDecision: exclude
  - title: "Accuracy of large language models in answering ophthalmology board-style questions: A meta-analysis."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Board-style questions are standardized written examinations, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:20.264000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Board-style question answering is not a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:21.767000+00:00
    doi: 10.1016/j.apjo.2024.100106
    pmid: "39374807"
    authors: Wu J, Nishida T, Liu T
    year: "2024"
    abstract: |-
      PURPOSE: To evaluate the accuracy of large language models (LLMs) in answering ophthalmology board-style questions.

      DESIGN: Meta-analysis.

      METHODS: Literature search was conducted using PubMed and Embase in March 2024. We included full-length articles and research letters published in English that reported the accuracy of LLMs in answering ophthalmology board-style questions. Data on LLM performance, including the number of questions submitted and correct responses generated, were extracted for each question set from individual studies. Pooled accuracy was calculated using a random-effects model. Subgroup analyses were performed based on the LLMs used and specific ophthalmology topics assessed.

      RESULTS: Among the 14 studies retrieved, 13 (93 %) tested LLMs on multiple ophthalmology topics. ChatGPT-3.5, ChatGPT-4, Bard, and Bing Chat were assessed in 12 (86 %), 11 (79 %), 4 (29 %), and 4 (29 %) studies, respectively. The overall pooled accuracy of LLMs was 0.65 (95 % CI: 0.61-0.69). Among the different LLMs, ChatGPT-4 achieved the highest pooled accuracy at 0.74 (95 % CI: 0.73-0.79), while ChatGPT-3.5 recorded the lowest at 0.52 (95 % CI: 0.51-0.54). LLMs performed best in "pathology" (0.78 [95 % CI: 0.70-0.86]) and worst in "fundamentals and principles of ophthalmology" (0.52 [95 % CI: 0.48-0.56]).

      CONCLUSIONS: The overall accuracy of LLMs in answering ophthalmology board-style questions was acceptable but not exceptional, with ChatGPT-4 and Bing Chat being top-performing models. Performance varied significantly based on specific ophthalmology topics tested. Inconsistent performances are of concern, highlighting the need for future studies to include ophthalmology board-style questions with images to more comprehensively examine the competency of LLMs.
    mergedFrom:
      - source: pubmed
        pmid: "39374807"
        doi: 10.1016/j.apjo.2024.100106
    finalDecision: exclude
  - title: "Performance of ChatGPT on Nursing Licensure Examinations in the United States and China: Cross-Sectional Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarking ChatGPT accuracy on nursing licensure exam MCQs; not about using AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Nursing licensure MCQ performance study, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.2196/52746
    pmid: "39363539"
    authors: Wu Z, Gan W, Xue Z, Ni Z, Zheng X, Zhang Y
    year: "2024"
    abstract: |-
      BACKGROUND: The creation of large language models (LLMs) such as ChatGPT is an important step in the development of artificial intelligence, which shows great potential in medical education due to its powerful language understanding and generative capabilities. The purpose of this study was to quantitatively evaluate and comprehensively analyze ChatGPT's performance in handling questions for the National Nursing Licensure Examination (NNLE) in China and the United States, including the National Council Licensure Examination for Registered Nurses (NCLEX-RN) and the NNLE.

      OBJECTIVE: This study aims to examine how well LLMs respond to the NCLEX-RN and the NNLE multiple-choice questions (MCQs) in various language inputs. To evaluate whether LLMs can be used as multilingual learning assistance for nursing, and to assess whether they possess a repository of professional knowledge applicable to clinical nursing practice.

      METHODS: First, we compiled 150 NCLEX-RN Practical MCQs, 240 NNLE Theoretical MCQs, and 240 NNLE Practical MCQs. Then, the translation function of ChatGPT 3.5 was used to translate NCLEX-RN questions from English to Chinese and NNLE questions from Chinese to English. Finally, the original version and the translated version of the MCQs were inputted into ChatGPT 4.0, ChatGPT 3.5, and Google Bard. Different LLMs were compared according to the accuracy rate, and the differences between different language inputs were compared.

      RESULTS: The accuracy rates of ChatGPT 4.0 for NCLEX-RN practical questions and Chinese-translated NCLEX-RN practical questions were 88.7% (133/150) and 79.3% (119/150), respectively. Despite the statistical significance of the difference (P=.03), the correct rate was generally satisfactory. Around 71.9% (169/235) of NNLE Theoretical MCQs and 69.1% (161/233) of NNLE Practical MCQs were correctly answered by ChatGPT 4.0. The accuracy of ChatGPT 4.0 in processing NNLE Theoretical MCQs and NNLE Practical MCQs translated into English was 71.5% (168/235; P=.92) and 67.8% (158/233; P=.77), respectively, and there was no statistically significant difference between the results of text input in different languages. ChatGPT 3.5 (NCLEX-RN P=.003, NNLE Theoretical P<.001, NNLE Practical P=.12) and Google Bard (NCLEX-RN P<.001, NNLE Theoretical P<.001, NNLE Practical P<.001) had lower accuracy rates for nursing-related MCQs than ChatGPT 4.0 in English input. English accuracy was higher when compared with ChatGPT 3.5's Chinese input, and the difference was statistically significant (NCLEX-RN P=.02, NNLE Practical P=.02). Whether submitted in Chinese or English, the MCQs from the NCLEX-RN and NNLE demonstrated that ChatGPT 4.0 had the highest number of unique correct responses and the lowest number of unique incorrect responses among the 3 LLMs.

      CONCLUSIONS: This study, focusing on 618 nursing MCQs including NCLEX-RN and NNLE exams, found that ChatGPT 4.0 outperformed ChatGPT 3.5 and Google Bard in accuracy. It excelled in processing English and Chinese inputs, underscoring its potential as a valuable tool in nursing education and clinical decision-making.
    mergedFrom:
      - source: pubmed
        pmid: "39363539"
        doi: 10.2196/52746
    finalDecision: exclude
  - title: Fine-Tuning Large Language Models to Enhance Programmatic Assessment in Graduate Medical Education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses BERT and FastText for classification of narrative feedback into ACGME subcompetencies; these are conventional NLP/encoder models, not generative AI"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses non-generative models (BERT/SciBERT/FastText) for classification rather than generative AI/LLMs."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.46374/VolXXVI_Issue3_Moore
    pmid: "39354917"
    authors: Booth G, Hauert T, Mynes M, Hodgson J, Slama E, Goldman A, Moore J
    year: "2024"
    abstract: |-
      BACKGROUND: Natural language processing is a collection of techniques designed to empower computer systems to comprehend and/or produce human language. The purpose of this investigation was to train several large language models (LLMs) to explore the tradeoff between model complexity and performance while classifying narrative feedback on trainees into the Accreditation Council for Graduate Medical Education subcompetencies. We hypothesized that classification accuracy would increase with model complexity.

      METHODS: The authors fine-tuned several transformer-based LLMs (Bidirectional Encoder Representations from Transformers [BERT]-base, BERT-medium, BERT-small, BERT-mini, BERT-tiny, and SciBERT) to predict Accreditation Council for Graduate Medical Education subcompetencies on a curated dataset of 10 218 feedback comments. Performance was compared with the authors' previous work, which trained a FastText model on the same dataset. Performance metrics included F1 score for global model performance and area under the receiver operating characteristic curve for each competency.

      RESULTS: No models were superior to FastText. Only BERT-tiny performed worse than FastText. The smallest model with comparable performance to FastText, BERT-mini, was 94% smaller. Area under the receiver operating characteristic curve for each competency was similar on BERT-mini and FastText with the exceptions of Patient Care 7 (Situational Awareness and Crisis Management) and Systems-Based Practice.

      DISCUSSION: Transformer-based LLMs were fine-tuned to understand anesthesiology graduate medical education language. Complex LLMs did not outperform FastText. However, equivalent performance was achieved with a model that was 94% smaller, which may allow model deployment on personal devices to enhance speed and data privacy. This work advances our understanding of best practices when integrating LLMs into graduate medical education.
    mergedFrom:
      - source: pubmed
        pmid: "39354917"
        doi: 10.46374/VolXXVI_Issue3_Moore
    finalDecision: exclude
  - title: Evaluation of GPT Large Language Model Performance on RSNA 2023 Case of the Day Questions.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM answering RSNA radiology questions, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates GPT-4V accuracy on radiology case questions; about AI diagnostic performance, not about assessment/feedback/analysis of clinical observation records for learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Performance on radiology case questions is not workplace-based clinical education assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.1148/radiol.240609
    pmid: "39352277"
    authors: Mukherjee P, Hou B, Suri A, Zhuang Y, Parnell C, Lee N, Stroie O, Jain R, Wang K, Sharma K, Summers R
    year: "2024"
    abstract: 'Background GPT-4V (GPT-4 with vision, ChatGPT; OpenAI) has shown impressive performance in several medical assessments. However, few studies have assessed its performance in interpreting radiologic images. Purpose To assess and compare the accuracy of GPT-4V in assessing radiologic cases with both images and textual context to that of radiologists and residents, to assess if GPT-4V assistance improves human accuracy, and to assess and compare the accuracy of GPT-4V with that of image-only or text-only inputs. Materials and Methods Seventy-two Case of the Day questions at the RSNA 2023 Annual Meeting were curated in this observer study. Answers from GPT-4V were obtained between November 26 and December 10, 2023, with the following inputs for each question: image only, text only, and both text and images. Five radiologists and three residents also answered the questions in an "open book" setting. For the artificial intelligence (AI)-assisted portion, the radiologists and residents were provided with the outputs of GPT-4V. The accuracy of radiologists and residents, both with and without AI assistance, was analyzed using a mixed-effects linear model. The accuracies of GPT-4V with different input combinations were compared by using the McNemar test. P < .05 was considered to indicate a significant difference. Results The accuracy of GPT-4V was 43% (31 of 72; 95% CI: 32, 55). Radiologists and residents did not significantly outperform GPT-4V in either imaging-dependent (59% and 56% vs 39%; P = .31 and .52, respectively) or imaging-independent (76% and 63% vs 70%; both P = .99) cases. With access to GPT-4V responses, there was no evidence of improvement in the average accuracy of the readers. The accuracy obtained by GPT-4V with text-only and image-only inputs was 50% (35 of 70; 95% CI: 39, 61) and 38% (26 of 69; 95% CI: 27, 49), respectively. Conclusion The radiologists and residents did not significantly outperform GPT-4V. Assistance from GPT-4V did not help human raters. GPT-4V relied on the textual context for its outputs. © RSNA, 2024 Supplemental material is available for this article. See also the editorial by Katz in this issue.'
    mergedFrom:
      - source: pubmed
        pmid: "39352277"
        doi: 10.1148/radiol.240609
    finalDecision: exclude
  - title: Suitability of GPT-4o as an evaluator of cardiopulmonary resuscitation skills examinations.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: GPT-4o scoring CPR skills test videos from a national medical licensing examination; assessment conducted in a standardized examination setting, not workplace-based."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Skills-exam evaluation study not situated in workplace-based or clinical education settings."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1016/j.resuscitation.2024.110404
    pmid: "39343124"
    authors: Wang L, Mao Y, Wang L, Sun Y, Song J, Zhang Y
    year: "2024"
    abstract: |-
      AIM: To assess the accuracy and reliability of GPT-4o for scoring examinees' performance on cardiopulmonary resuscitation (CPR) skills tests.

      METHODS: This study included six experts certified to supervise the national medical licensing examination (three junior and three senior) who reviewed the CPR skills test videos across 103 examinees. All videos reviewed by the experts were subjected to automated assessment by GPT-4o. Both the experts and GPT-4o scored the videos across four sections: patient assessment, chest compressions, rescue breathing, and repeated operations. The experts subsequently rated GPT-4o's reliability on a 5-point Likert scale (1, completely unreliable; 5, completely reliable). GPT-4o's accuracy was evaluated using the intraclass correlation coefficient (for the first three sections) and Fleiss' Kappa (for the last section) to assess the agreement between its scores vs. those of the experts.

      RESULTS: The mean accuracy scores for the patient assessment, chest compressions, rescue breathing, and repeated operation sections were 0.65, 0.58, 0.60, and 0.31, respectively, when comparing the GPT-4o's vs. junior experts' scores and 0.75, 0.65, 0.72, and 0.41, respectively, when comparing the GPT-4o's vs. senior experts' scores. For reliability, the median Likert scale scores were 4.00 (interquartile range [IQR] = 3.66-4.33, mean [standard deviation] = 3.95 [0.55]) and 4.33 (4.00-4.67, 4.29 [0.50]) for the junior and senior experts, respectively.

      CONCLUSIONS: GPT-4o demonstrated a level of accuracy that was similar to that of senior experts in examining CPR skills examination videos. The results demonstrate the potential for deploying this large language model in medical examination settings.
    mergedFrom:
      - source: pubmed
        pmid: "39343124"
        doi: 10.1016/j.resuscitation.2024.110404
    finalDecision: exclude
  - title: LLM-based automatic short answer grading in undergraduate medical education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on short-answer grading, not assessment/feedback from clinical observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study focuses on automated grading of written exam answers in undergraduate medical education courses."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Conducted in undergraduate course assessment context rather than a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1186/s12909-024-06026-5
    pmid: "39334087"
    authors: Gr&#xe9;visse C
    year: "2024"
    abstract: |-
      BACKGROUND: Multiple choice questions are heavily used in medical education assessments, but rely on recognition instead of knowledge recall. However, grading open questions is a time-intensive task for teachers. Automatic short answer grading (ASAG) has tried to fill this gap, and with the recent advent of Large Language Models (LLM), this branch has seen a new momentum.

      METHODS: We graded 2288 student answers from 12 undergraduate medical education courses in 3 languages using GPT-4 and Gemini 1.0 Pro.

      RESULTS: GPT-4 proposed significantly lower grades than the human evaluator, but reached low rates of false positives. The grades of Gemini 1.0 Pro were not significantly different from the teachers'. Both LLMs reached a moderate agreement with human grades, and a high precision for GPT-4 among answers considered fully correct. A consistent grading behavior could be determined for high-quality keys. A weak correlation was found wrt. the length or language of student answers. There is a risk of bias if the LLM knows the human grade a priori.

      CONCLUSIONS: LLM-based ASAG applied to medical education still requires human oversight, but time can be spared on the edge cases, allowing teachers to focus on the middle ones. For Bachelor-level medical education questions, the training knowledge of LLMs seems to be sufficient, fine-tuning is thus not necessary.
    mergedFrom:
      - source: pubmed
        pmid: "39334087"
        doi: 10.1186/s12909-024-06026-5
    finalDecision: exclude
  - title: ChatGPT and trainee performances in the management of maxillofacial patients.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares ChatGPT diagnostic/treatment accuracy to trainees on clinical cases; evaluates AI clinical performance rather than using AI for assessment or feedback of learners"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates AI clinical case management performance, not WBA assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.1016/j.jormas.2024.102090
    pmid: "39332706"
    authors: Peters M, Le Clercq M, Yanni A, Vanden Eynden X, Martin L, Vanden Haute N, Tancredi S, De Passe C, Boutremans E, Lechien J, Dequanter D
    year: "2025"
    abstract: |-
      INTRODUCTION: ChatGPT is an artificial intelligence based large language model with the ability to generate human-like response to text input, its performance has already been the subject of several studies in different fields. The aim of this study was to evaluate the performance of ChatGPT in the management of maxillofacial clinical cases.

      MATERIALS AND METHODS: A total of 38 clinical cases consulting at the Stomatology-Maxillofacial Surgery Department were prospectively recruited and presented to ChatGPT, which was interrogated for diagnosis, differential diagnosis, management and treatment. The performance of trainees and ChatGPT was compared by three blinded board-certified maxillofacial surgeons using the AIPI score.

      RESULTS: The average total AIPI score assigned to the practitioners was 18.71 and 16.39 to ChatGPT, significantly lower (p < 0.001). According to the experts, ChatGPT was significantly less effective for diagnosis and treatment (p < 0.001). Following two of the three experts, ChatGPT was significantly less effective in considering patient data (p = 0.001) and suggesting additional examinations (p < 0.0001). The primary diagnosis proposed by ChatGPT was judged by the experts as not plausible and /or incomplete in 2.63 % to 18 % of the cases, the additional examinations were associated with inadequate examinations in 2.63 %, to 21.05 % of the cases and proposed an association of pertinent, but incomplete therapeutic findings in 18.42 % to 47.37 % of the cases, while the therapeutic findings were considered pertinent, necessary and inadequate in 18.42 % of cases.

      CONCLUSIONS: ChatGPT appears less efficient in diagnosis, the selection of the most adequate additional examination and the proposition of pertinent and necessary therapeutic approaches.
    mergedFrom:
      - source: pubmed
        pmid: "39332706"
        doi: 10.1016/j.jormas.2024.102090
    finalDecision: exclude
  - title: "AI in obstetrics: Evaluating residents' capabilities and interaction strategies with ChatGPT."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates residents' interaction with ChatGPT during summative exam-style clinical evaluations (not workplace-based assessment); focuses on AI literacy and prompt skills rather than AI-based assessment of clinical performance."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates resident prompting of ChatGPT rather than generative-AI use for assessment, feedback, or analysis of observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1016/j.ejogrb.2024.09.008
    pmid: "39326228"
    authors: Desseauve D, Lescar R, de la Fourniere B, Ceccaldi P, Dziadzko M
    year: "2024"
    abstract: "In line with the digital transformation trend in medical training, students may resort to artificial intelligence (AI) for learning. This study assessed the interaction between obstetrics residents and ChatGPT during clinically oriented summative evaluations related to acute hepatic steatosis of pregnancy, and their self-reported competencies in information technology (IT) and AI. The participants in this semi-qualitative observational study were 14 obstetrics residents from two university hospitals. Students' queries were categorized into three distinct types: third-party enquiries; search-engine-style queries; and GPT-centric prompts. Responses were compared against a standardized answer produced by ChatGPT with a Delphi-developed expert prompt. Data analysis employed descriptive statistics and correlation analysis to explore the relationship between AI/IT skills and response accuracy. The study participants showed moderate IT proficiency but low AI proficiency. Interaction with ChatGPT regarding clinical signs of acute hepatic steatosis gravidarum revealed a preference for third-party questioning, resulting in only 21% accurate responses due to misinterpretation of medical acronyms. No correlation was found between AI response accuracy and the residents' self-assessed IT or AI skills, with most expressing dissatisfaction with their AI training. This study underlines the discrepancy between perceived and actual AI proficiency, highlighted by clinically inaccurate yet plausible AI responses - a manifestation of the 'stochastic parrot' phenomenon. These findings advocate for the inclusion of structured AI literacy programmes in medical education, focusing on prompt engineering. These academic skills are essential to exploit AI's potential in obstetrics and gynaecology. The ultimate aim is to optimize patient care in AI-augmented health care, and prevent misleading and unsafe knowledge acquisition."
    mergedFrom:
      - source: pubmed
        pmid: "39326228"
        doi: 10.1016/j.ejogrb.2024.09.008
    finalDecision: exclude
  - title: Is ChatGPT 3.5 smarter than Otolaryngology trainees? A comparison study of board style exam questions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; evaluates LLM exam-taking performance against trainees on board-style questions, not AI-assisted workplace-based assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares ChatGPT with trainees on board-style exam questions rather than workplace-based assessment or feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1371/journal.pone.0306233
    pmid: "39325705"
    authors: Patel J, Robinson P, Illing E, Anthony B
    year: "2024"
    abstract: |-
      OBJECTIVES: This study compares the performance of the artificial intelligence (AI) platform Chat Generative Pre-Trained Transformer (ChatGPT) to Otolaryngology trainees on board-style exam questions.

      METHODS: We administered a set of 30 Otolaryngology board-style questions to medical students (MS) and Otolaryngology residents (OR). 31 MSs and 17 ORs completed the questionnaire. The same test was administered to ChatGPT version 3.5, five times. Comparisons of performance were achieved using a one-way ANOVA with Tukey Post Hoc test, along with a regression analysis to explore the relationship between education level and performance.

      RESULTS: The average scores increased each year from MS1 to PGY5. A one-way ANOVA revealed that ChatGPT outperformed trainee years MS1, MS2, and MS3 (p = <0.001, 0.003, and 0.019, respectively). PGY4 and PGY5 otolaryngology residents outperformed ChatGPT (p = 0.033 and 0.002, respectively). For years MS4, PGY1, PGY2, and PGY3 there was no statistical difference between trainee scores and ChatGPT (p = .104, .996, and 1.000, respectively).

      CONCLUSION: ChatGPT can outperform lower-level medical trainees on Otolaryngology board-style exam but still lacks the ability to outperform higher-level trainees. These questions primarily test rote memorization of medical facts; in contrast, the art of practicing medicine is predicated on the synthesis of complex presentations of disease and multilayered application of knowledge of the healing process. Given that upper-level trainees outperform ChatGPT, it is unlikely that ChatGPT, in its current form will provide significant clinical utility over an Otolaryngologist.
    mergedFrom:
      - source: pubmed
        pmid: "39325705"
        doi: 10.1371/journal.pone.0306233
    finalDecision: exclude
  - title: The Comparative Performance of Large Language Models on the Hand Surgery Self-Assessment Examination.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT and Bing AI performance on ASSH Self-Assessment Exam questions; not about using AI for assessment or feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam-question performance study without workplace-based assessment feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.1177/15589447241279460
    pmid: "39324769"
    authors: Chen C, Sobol K, Hickey C, Raphael J
    year: "2026"
    abstract: |-
      BACKGROUND: Generative artificial intelligence (AI) models have emerged as capable of producing human-like responses and have showcased their potential in general medical specialties. This study explores the performance of AI systems on the American Society for Surgery of the Hand (ASSH) Self-Assessment Exams (SAE).

      METHODS: ChatGPT 4.0 and Bing AI were evaluated on a set of multiple-choice questions drawn from the ASSH SAE online question bank spanning 5 years (2019-2023). Each system was evaluated with 999 questions. Images and video links were inserted into question prompts to allow for complete AI interpretation. The performance of both systems was standardized using the May 2023 version of ChatGPT 4.0 and Microsoft Bing AI, both of which had web browsing and image capabilities.

      RESULTS: ChatGPT 4.0 scored an average of 66.5% on the ASSH questions. Bing AI scored higher, with an average of 75.3%. Bing AI outperformed ChatGPT 4.0 by an average of 8.8%. As a benchmark, a minimum passing score of 50% was required for continuing medical education credit. Both ChatGPT 4.0 and Bing AI had poorer performance on video-type and image-type questions on analysis of variance testing. Responses from both models contained elements from sources such as PubMed, Journal of Hand Surgery, and American Academy of Orthopedic Surgeons.

      CONCLUSIONS: ChatGPT 4.0 with browsing and Bing AI can both be anticipated to achieve passing scores on the ASSH SAE. Generative AI, with its ability to provide logical responses and literature citations, presents a convincing argument for use as an interactive learning aid and educational tool.
    mergedFrom:
      - source: pubmed
        pmid: "39324769"
        doi: 10.1177/15589447241279460
    finalDecision: exclude
  - title: "Enhancing Medical Interview Skills Through AI-Simulated Patient Interactions: Nonrandomized Controlled Trial."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: AI-simulated patient interactions are a simulated environment, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments (simulated patient interactions)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.2196/58753
    pmid: "39312284"
    authors: Yamamoto A, Koda M, Ogawa H, Miyoshi T, Maeda Y, Otsuka F, Ino H
    year: "2024"
    abstract: |-
      BACKGROUND: Medical interviewing is a critical skill in clinical practice, yet opportunities for practical training are limited in Japanese medical schools, necessitating urgent measures. Given advancements in artificial intelligence (AI) technology, its application in the medical field is expanding. However, reports on its application in medical interviews in medical education are scarce.

      OBJECTIVE: This study aimed to investigate whether medical students' interview skills could be improved by engaging with AI-simulated patients using large language models, including the provision of feedback.

      METHODS: This nonrandomized controlled trial was conducted with fourth-year medical students in Japan. A simulation program using large language models was provided to 35 students in the intervention group in 2023, while 110 students from 2022 who did not participate in the intervention were selected as the control group. The primary outcome was the score on the Pre-Clinical Clerkship Objective Structured Clinical Examination (pre-CC OSCE), a national standardized clinical skills examination, in medical interviewing. Secondary outcomes included surveys such as the Simulation-Based Training Quality Assurance Tool (SBT-QA10), administered at the start and end of the study.

      RESULTS: The AI intervention group showed significantly higher scores on medical interviews than the control group (AI group vs control group: mean 28.1, SD 1.6 vs 27.1, SD 2.2; P=.01). There was a trend of inverse correlation between the SBT-QA10 and pre-CC OSCE scores (regression coefficient -2.0 to -2.1). No significant safety concerns were observed.

      CONCLUSIONS: Education through medical interviews using AI-simulated patients has demonstrated safety and a certain level of educational effectiveness. However, at present, the educational effects of this platform on nonverbal communication skills are limited, suggesting that it should be used as a supplementary tool to traditional simulation education.
    mergedFrom:
      - source: pubmed
        pmid: "39312284"
        doi: 10.2196/58753
    finalDecision: exclude
  - title: "Apple Vision Pro: A Paradigm Shift in Medical Technology."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About Apple Vision Pro hardware, not generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.7759/cureus.69608
    pmid: "39308843"
    authors: Shanbhag N, Bin Sumaida A, Al Shamisi K, Balaraj K
    year: "2024"
    abstract: The introduction of Apple Vision Pro (AVP) marks a significant milestone in the intersection of technology and healthcare, offering unique capabilities in mixed reality, which Apple terms "spatial computing." This narrative review aims to explore the various applications of AVP in medical technology, emphasizing its impact on patient care, clinical practices, medical education, and future directions. The review synthesizes findings from multiple studies and articles published between January 2023 and May 2024, highlighting AVP's potential to enhance visualization in diagnostic imaging and surgical planning, assist visually impaired patients, and revolutionize medical education through immersive learning environments. Despite its promise, challenges remain in integrating AVP into existing healthcare systems and understanding its long-term impact on patient outcomes. As research continues, AVP is poised to play a pivotal role in the future of medicine, offering a transformative tool for healthcare professionals.
    mergedFrom:
      - source: pubmed
        pmid: "39308843"
        doi: 10.7759/cureus.69608
    finalDecision: exclude
  - title: Evaluating Performance of ChatGPT on MKSAP Cardiology Board Review Questions.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Tests ChatGPT on board review questions, not about assessment or feedback of human learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting (board review question context)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1016/j.ijcard.2024.132576
    pmid: "39306288"
    authors: Milutinovic S, Petrovic M, Begosh-Mayne D, Lopez-Mattei J, Chazal R, Wood M, Escarcega R
    year: "2024"
    abstract: Chat Generative Pretrained Transformer (ChatGPT) is a natural language processing tool created by OpenAI. Much of the discussion regarding artificial intelligence (AI) in medicine is the ability of the language to enhance medical practice, improve efficiency and decrease errors. The objective of this study was to analyze the ability of ChatGPT to answer board-style cardiovascular medicine questions by using the Medical Knowledge Self-Assessment Program (MKSAP).The study evaluated the performance of ChatGPT (versions 3.5 and 4), alongside internal medicine residents and internal medicine and cardiology attendings, in answering 98 multiple-choice questions (MCQs) from the Cardiovascular Medicine Chapter of MKSAP. ChatGPT-4 demonstrated an accuracy of 74.5 %, comparable to internal medicine (IM) intern (63.3 %), senior resident (63.3 %), internal medicine attending physician (62.2 %), and ChatGPT-3.5 (64.3 %) but significantly lower than cardiology attending physician (85.7 %). Subcategory analysis revealed no statistical difference between ChatGPT and physicians, except in valvular heart disease where cardiology attending outperformed ChatGPT (p = 0.031) for version 3.5, and for heart failure (p = 0.046) where ChatGPT-4 outperformed senior resident. While ChatGPT shows promise in certain subcategories, in order to establish AI as a reliable educational tool for medical professionals, performance of ChatGPT will likely need to surpass the accuracy of instructors, ideally achieving the near-perfect score on posed questions.
    mergedFrom:
      - source: pubmed
        pmid: "39306288"
        doi: 10.1016/j.ijcard.2024.132576
    finalDecision: exclude
  - title: "Class in Session: Analysis of GPT-4-created Plastic Surgery In-service Examination Questions."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not workplace-based assessment; GPT-4 used to generate standardized in-service training exam questions for plastic surgery residency, not WBA"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLM-generated board-style questions rather than WBA assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1097/GOX.0000000000006185
    pmid: "39301304"
    authors: Najafali D, Galbraith L, Camacho J, Stoffel V, Herzog I, Moss C, Taiberg S, Knoedler L
    year: "2024"
    abstract: |-
      BACKGROUND: The Plastic Surgery In-Service Training Examination (PSITE) remains a critical milestone in residency training. Successful preparation requires extensive studying during an individual's residency. This study focuses on the capacity of Generative Pre-trained Transformer 4 (GPT-4) to generate PSITE practice questions.

      METHODS: GPT-4 was prompted to generate multiple choice questions for each PSITE section and provide answer choices with detailed rationale. Question composition via readability metrics were analyzed, along with quality. Descriptive statistics compared GPT-4 and the 2022 PSITE.

      RESULTS: The overall median Flesch-Kincaid reading ease for GPT-4-generated questions was 43.90 (versus 50.35 PSITE, P = 0.036). GPT-4 provided questions that contained significantly fewer mean sentences (1 versus 4), words (16 versus 56), and percentage of complex words (3 versus 13) than 2022 PSITE questions (P < 0.001). When evaluating GPT-4 generated questions for each examination section, the highest median Flesch-Kincaid reading ease was on the core surgical principles section (median: 63.30, interquartile range [54.45-68.28]) and the lowest was on the craniomaxillofacial section (median: 36.25, interquartile range [12.57-58.40]). Most readability metrics were higher for the 2022 PSITE compared with GPT-4 generated questions. Overall question quality was poor for the chatbot.

      CONCLUSIONS: Our study found that GPT-4 can be adapted to generate practice questions for the 2022 PSITE, but its questions are of poor quality. The program can offer general explanations for both the correct and incorrect answer options but was observed to generate false information and poor-quality explanations. Although trainees should navigate with caution as the technology develops, GPT-4 has the potential to serve as an effective educational adjunct under the supervision of trained plastic surgeons.
    mergedFrom:
      - source: pubmed
        pmid: "39301304"
        doi: 10.1097/GOX.0000000000006185
      - source: scopus
        doi: 10.1097/GOX.0000000000006185
        scopusId: SCOPUS_ID:85205027423
    finalDecision: exclude
  - title: "Encouragement vs. liability: How prompt engineering influences ChatGPT-4's radiology exam performance."
    reviews:
      - decision: uncertain
        comment: ChatGPT-4 on radiology exam; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates prompt engineering effects on LLM accuracy on radiology in-training exam questions; not about assessment/feedback of learners in clinical settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: In-training exam performance study without a workplace-based assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1016/j.clinimag.2024.110276
    pmid: "39288636"
    authors: Nguyen D, MacKenzie A, Kim Y
    year: "2024"
    abstract: Large Language Models (LLM) like ChatGPT-4 hold significant promise in medical application, especially in the field of radiology. While previous studies have shown the promise of ChatGTP-4 in textual-based scenarios, its performance on image-based response remains suboptimal. This study investigates the impact of prompt engineering on ChatGPT-4's accuracy on the 2022 American College of Radiology In Training Test Questions for Diagnostic Radiology Residents that include textual and visual-based questions. Four personas were created, each with unique prompts, and evaluated using ChatGPT-4. Results indicate that encouraging prompts and those disclaiming responsibility led to higher overall accuracy (number of questions answered correctly) compared to other personas. Personas that threaten the LLM with legal action or mounting clinical responsibility were not only found to score less, but also refrain of answering questions at a higher rate. These findings highlight the importance of prompt context in optimizing LLM responses and the need for further research to integrate AI responsibly into medical practice.
    mergedFrom:
      - source: pubmed
        pmid: "39288636"
        doi: 10.1016/j.clinimag.2024.110276
    finalDecision: exclude
  - title: "Socratic Artificial Intelligence Learning (SAIL): The Role of a Virtual Voice Assistant in Learning Orthopedic Knowledge."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Custom NLP-based voice assistant for studying board prep questions via spaced repetition; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on AI-assisted studying with practice questions, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.1016/j.jsurg.2024.08.006
    pmid: "39288509"
    authors: Li T, Slocum S, Sahoo A, Ochuba A, Kolakowski L, Henn Iii R, Johnson A, LaPorte D
    year: "2024"
    abstract: |-
      OBJECTIVE: We hypothesized that learning through multiple sensory modalities would improve knowledge recall and recognition in orthopedic surgery residents and medical students.

      DESIGN: We developed a virtual study assistant, named Socratic Artificial Intelligence Learning (SAIL), based on a custom-built natural language processing algorithm. SAIL draws from practice questions approved by the American Board of Orthopaedic Surgery and quizzes users through a conversational, voice-enabled Web interface. We performed a randomized controlled study using a within-subjects, repeated measures design.

      SETTING: Participants first took a pretest to assess their baseline knowledge. They then underwent 10 days of spaced repetition training with practice questions using 3 modalities: oral response, typed response, and multiple-choice. Recall and recognition of the practiced knowledge were assessed via a post-test administered on the first day, first week, and 2 months after the training period.

      PARTICIPANTS: Twenty-four volunteers, who were medical students and orthopedic surgery residents at multiple US medical institutions.

      RESULTS: The oral, typed, and multiple-choice modalities produced similar recall and recognition rates. Although participants preferred using the traditional multiple-choice modality to study for standardized examinations, many were interested in supplementing their study routine with SAIL and believe that SAIL may improve their performance on written and oral examinations.

      CONCLUSIONS: SAIL is not inferior to the multiple-choice modality for learning orthopedic core knowledge. These results indicate that SAIL can be used to supplement traditional study methods. COMPETENCIES: medical knowledge; practice-based learning and improvement.
    mergedFrom:
      - source: pubmed
        pmid: "39288509"
        doi: 10.1016/j.jsurg.2024.08.006
    finalDecision: exclude
  - title: "Performance of ChatGPT-3.5 and GPT-4 in national licensing examinations for medicine, pharmacy, dentistry, and nursing: a systematic review and meta-analysis."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on national licensing examinations, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Systematic review benchmarking ChatGPT accuracy on national licensing exams; not about using AI for assessment/feedback of learners in clinical settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Systematic review of licensing exam question performance, not workplace-based assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.1186/s12909-024-05944-8
    pmid: "39285377"
    authors: Jin H, Lee H, Kim E
    year: "2024"
    abstract: |-
      BACKGROUND: ChatGPT, a recently developed artificial intelligence (AI) chatbot, has demonstrated improved performance in examinations in the medical field. However, thus far, an overall evaluation of the potential of ChatGPT models (ChatGPT-3.5 and GPT-4) in a variety of national health licensing examinations is lacking. This study aimed to provide a comprehensive assessment of the ChatGPT models' performance in national licensing examinations for medical, pharmacy, dentistry, and nursing research through a meta-analysis.

      METHODS: Following the PRISMA protocol, full-text articles from MEDLINE/PubMed, EMBASE, ERIC, Cochrane Library, Web of Science, and key journals were reviewed from the time of ChatGPT's introduction to February 27, 2024. Studies were eligible if they evaluated the performance of a ChatGPT model (ChatGPT-3.5 or GPT-4); related to national licensing examinations in the fields of medicine, pharmacy, dentistry, or nursing; involved multiple-choice questions; and provided data that enabled the calculation of effect size. Two reviewers independently completed data extraction, coding, and quality assessment. The JBI Critical Appraisal Tools were used to assess the quality of the selected articles. Overall effect size and 95% confidence intervals [CIs] were calculated using a random-effects model.

      RESULTS: A total of 23 studies were considered for this review, which evaluated the accuracy of four types of national licensing examinations. The selected articles were in the fields of medicine (n = 17), pharmacy (n = 3), nursing (n = 2), and dentistry (n = 1). They reported varying accuracy levels, ranging from 36 to 77% for ChatGPT-3.5 and 64.4-100% for GPT-4. The overall effect size for the percentage of accuracy was 70.1% (95% CI, 65-74.8%), which was statistically significant (p < 0.001). Subgroup analyses revealed that GPT-4 demonstrated significantly higher accuracy in providing correct responses than its earlier version, ChatGPT-3.5. Additionally, in the context of health licensing examinations, the ChatGPT models exhibited greater proficiency in the following order: pharmacy, medicine, dentistry, and nursing. However, the lack of a broader set of questions, including open-ended and scenario-based questions, and significant heterogeneity were limitations of this meta-analysis.

      CONCLUSIONS: This study sheds light on the accuracy of ChatGPT models in four national health licensing examinations across various countries and provides a practical basis and theoretical support for future research. Further studies are needed to explore their utilization in medical and health education by including a broader and more diverse range of questions, along with more advanced versions of AI chatbots.
    mergedFrom:
      - source: pubmed
        pmid: "39285377"
        doi: 10.1186/s12909-024-05944-8
    finalDecision: exclude
  - title: "Artificial Intelligence in Orthopaedics: Performance of ChatGPT on Text and Image Questions on a Complete AAOS Orthopaedic In-Training Examination (OITE)."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT-4 performance on orthopaedic in-training exam questions; not about using AI for assessment or feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Tests ChatGPT on in-training exam questions rather than workplace-based assessment feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.1016/j.jsurg.2024.08.002
    pmid: "39284250"
    authors: Hayes D, Foster B, Makar G, Manzar S, Ozdag Y, Shultz M, Klena J, Grandizio L
    year: "2024"
    abstract: |-
      OBJECTIVE: Artificial intelligence (AI) is capable of answering complex medical examination questions, offering the potential to revolutionize medical education and healthcare delivery. In this study we aimed to assess ChatGPT, a model that has demonstrated exceptional performance on standardized exams. Specifically, our focus was on evaluating ChatGPT's performance on the complete 2019 Orthopaedic In-Training Examination (OITE), including questions with an image component. Furthermore, we explored difference in performance when questions varied by text only or text with an associated image, including whether the image was described using AI or a trained orthopaedist.

      DESIGN AND SETTING: Questions from the 2019 OITE were input into ChatGPT version 4.0 (GPT-4) using 3 response variants. As the capacity to input or interpret images is not publicly available in ChatGPT at the time of this study, questions with an image component were described and added to the OITE question using descriptions generated by Microsoft Azure AI Vision Studio or authors of the study.

      RESULTS: ChatGPT performed equally on OITE questions with or without imaging components, with an average correct answer choice of 49% and 48% across all 3 input methods. Performance dropped by 6% when using image descriptions generated by AI. When using single answer multiple-choice input methods, ChatGPT performed nearly double the rate of random guessing, answering 49% of questions correctly. The performance of ChatGPT was worse than all resident classes on the 2019 exam, scoring 4% lower than PGY-1 residents.

      DISCUSSION: ChatGT performed below all resident classes on the 2019 OITE. Performance on text only questions and questions with images was nearly equal if the image was described by a trained orthopaedic specialist but decreased when using an AI generated description. Recognizing the performance abilities of AI software may provide insight into the current and future applications of this technology into medical education.
    mergedFrom:
      - source: pubmed
        pmid: "39284250"
        doi: 10.1016/j.jsurg.2024.08.002
    finalDecision: exclude
  - title: "Performance of ChatGPT in the In-Training Examination for Anesthesiology and Pain Medicine Residents in South Korea: Observational Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT versions on Korean anesthesiology in-training exam questions; not about using AI for assessment or feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: In-training exam performance comparison, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.2196/56859
    pmid: "39284182"
    authors: Yoon S, Oh S, Lim B, Lee H
    year: "2024"
    abstract: |-
      BACKGROUND: ChatGPT has been tested in health care, including the US Medical Licensing Examination and specialty exams, showing near-passing results. Its performance in the field of anesthesiology has been assessed using English board examination questions; however, its effectiveness in Korea remains unexplored.

      OBJECTIVE: This study investigated the problem-solving performance of ChatGPT in the fields of anesthesiology and pain medicine in the Korean language context, highlighted advancements in artificial intelligence (AI), and explored its potential applications in medical education.

      METHODS: We investigated the performance (number of correct answers/number of questions) of GPT-4, GPT-3.5, and CLOVA X in the fields of anesthesiology and pain medicine, using in-training examinations that have been administered to Korean anesthesiology residents over the past 5 years, with an annual composition of 100 questions. Questions containing images, diagrams, or photographs were excluded from the analysis. Furthermore, to assess the performance differences of the GPT across different languages, we conducted a comparative analysis of the GPT-4's problem-solving proficiency using both the original Korean texts and their English translations.

      RESULTS: A total of 398 questions were analyzed. GPT-4 (67.8%) demonstrated a significantly better overall performance than GPT-3.5 (37.2%) and CLOVA-X (36.7%). However, GPT-3.5 and CLOVA X did not show significant differences in their overall performance. Additionally, the GPT-4 showed superior performance on questions translated into English, indicating a language processing discrepancy (English: 75.4% vs Korean: 67.8%; difference 7.5%; 95% CI 3.1%-11.9%; P=.001).

      CONCLUSIONS: This study underscores the potential of AI tools, such as ChatGPT, in medical education and practice but emphasizes the need for cautious application and further refinement, especially in non-English medical contexts. The findings suggest that although AI advancements are promising, they require careful evaluation and development to ensure acceptable performance across diverse linguistic and professional settings.
    mergedFrom:
      - source: pubmed
        pmid: "39284182"
        doi: 10.2196/56859
    finalDecision: exclude
  - title: "Gemini AI vs. ChatGPT: A comprehensive examination alongside ophthalmology residents in medical knowledge."
    reviews:
      - decision: uncertain
        comment: AI vs residents on medical knowledge; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks AI models against ophthalmology residents on board exam questions; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1007/s00417-024-06625-4
    pmid: "39277830"
    authors: Bahir D, Zur O, Attal L, Nujeidat Z, Knaanie A, Pikkel J, Mimouni M, Plopsky G
    year: "2025"
    abstract: |-
      INTRODUCTION: The rapid advancement of artificial intelligence (AI), particularly in large language models like ChatGPT and Google's Gemini AI, marks a transformative era in technological innovation. This study explores the potential of AI in ophthalmology, focusing on the capabilities of ChatGPT and Gemini AI. While these models hold promise for medical education and clinical support, their integration requires comprehensive evaluation. This research aims to bridge a gap in the literature by comparing Gemini AI and ChatGPT, assessing their performance against ophthalmology residents using a dataset derived from ophthalmology board exams.

      METHODS: A dataset comprising 600 questions across 12 subspecialties was curated from Israeli ophthalmology residency exams, encompassing text and image-based formats. Four AI models - ChatGPT-3.5, ChatGPT-4, Gemini, and Gemini Advanced - underwent testing on this dataset. The study includes a comparative analysis with Israeli ophthalmology residents, employing specific metrics for performance assessment.

      RESULTS: Gemini Advanced demonstrated superior performance with a 66% accuracy rate. Notably, ChatGPT-4 exhibited improvement at 62%, Gemini at 58%, and ChatGPT-3.5 served as the reference at 46%. Comparative analysis with residents offered insights into AI models' performance relative to human-level medical knowledge. Further analysis delved into yearly performance trends, topic-specific variations, and the impact of images on chatbot accuracy.

      CONCLUSION: The study unveils nuanced AI model capabilities in ophthalmology, emphasizing domain-specific variations. The superior performance of Gemini Advanced superior performance indicates significant advancements, while ChatGPT-4's improvement is noteworthy. Both Gemini and ChatGPT-3.5 demonstrated commendable performance. The comparative analysis underscores AI's evolving role as a supplementary tool in medical education. This research contributes vital insights into AI effectiveness in ophthalmology, highlighting areas for refinement. As AI models evolve, targeted improvements can enhance adaptability across subspecialties, making them valuable tools for medical professionals and enriching patient care.

      KEY MESSAGES: What is known AI breakthroughs, like ChatGPT and Google's Gemini AI, are reshaping healthcare. In ophthalmology, AI integration has overhauled clinical workflows, particularly in analyzing images for diseases like diabetic retinopathy and glaucoma. What is new This study presents a pioneering comparison between Gemini AI and ChatGPT, evaluating their performance against ophthalmology residents using a meticulously curated dataset derived from real-world ophthalmology board exams. Notably, Gemini Advanced demonstrates superior performance, showcasing substantial advancements, while the evolution of ChatGPT-4 also merits attention. Both models exhibit commendable capabilities. These findings offer crucial insights into the efficacy of AI in ophthalmology, shedding light on areas ripe for further enhancement and optimization.
    mergedFrom:
      - source: pubmed
        pmid: "39277830"
        doi: 10.1007/s00417-024-06625-4
    finalDecision: exclude
  - title: "How I Do It: Using ChatGPT to Create Clinical Scenarios for Surgical Learning and Oral Boards Preparation."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: 'How I Do It' article type indicates instructional commentary, not eligible study design."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 5: Descriptive 'How I Do It' piece about using ChatGPT to simulate oral boards; no validity, reliability, acceptability, or educational impact data presented."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Describes simulated oral boards scenarios without a workplace-based assessment component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.1016/j.jsurg.2024.07.030
    pmid: "39276707"
    authors: Wilson T, Lee K
    year: "2024"
    abstract: |-
      OBJECTIVE: To evaluate the use of the large language model ChatGPT to simulate an oral surgical boards examination environment.

      DESIGN: ChatGPT was asked to create oral surgical boards questions based on a series of clinical scenarios.

      RESULTS: ChatGPT created clinically relevant oral board-type questions. ChatGPT provided pertinent follow-up questions after the user's response as would occur in an oral examination as well as feedback regarding the user's response.

      CONCLUSIONS: Chat GPT can simulate an oral boards-style examination of a surgical trainee with a reasonable degree of clinical detail and immediate feedback. It may be a useful as a curricular tool and for self-education and board preparation.
    mergedFrom:
      - source: pubmed
        pmid: "39276707"
        doi: 10.1016/j.jsurg.2024.07.030
    finalDecision: exclude
  - title: "An Exploratory Analysis of ChatGPT Compared to Human Performance With the Anesthesiology Oral Board Examination: Initial Insights and Implications."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on board examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Compares ChatGPT with anesthesiology fellows on standardized oral board examination (ABA SOE), which is a simulated exam environment, not workplace-based assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Oral board examination setup is simulated and lacks a workplace-based assessment component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.1213/ANE.0000000000006875
    pmid: "39269908"
    authors: Blacker S, Chen F, Winecoff D, Antonio B, Arora H, Hierlmeier B, Kacmar R, Passannante A, Plunkett A, Zvara D, Cobb B, Doyal A, Rosenkrans D, Brown K, Gonzalez M, Hood C, Pham T, Lele A, Hall L, Ali A, Isaak R
    year: "2025"
    abstract: |-
      BACKGROUND: Chat Generative Pre-Trained Transformer (ChatGPT) has been tested and has passed various high-level examinations. However, it has not been tested on an examination such as the American Board of Anesthesiology (ABA) Standardized Oral Examination (SOE). The SOE is designed to assess higher-level competencies, such as judgment, organization, adaptability to unexpected clinical changes, and presentation of information.

      METHODS: Four anesthesiology fellows were examined on 2 sample ABA SOEs. Their answers were compared to those produced by the same questions asked to ChatGPT. All human and ChatGPT responses were transcribed, randomized by module, and then reproduced as complete examinations, using a commercially available software-based human voice replicator. Eight ABA applied examiners listened to and scored the topic and modules from 1 of the 4 versions of each of the 2 sample examinations. The ABA did not provide any support or collaboration with any authors.

      RESULTS: The anesthesiology fellow's answers were found to have a better median score than ChatGPT, for the module topics scores ( P = .03). However, there was no significant difference in the median overall global module scores between the human and ChatGPT responses ( P = .17). The examiners were able to identify the ChatGPT-generated answers for 23 of 24 modules (95.83%), with only 1 ChatGPT response perceived as from a human. In contrast, the examiners thought the human (fellow) responses were artificial intelligence (AI)-generated in 10 of 24 modules (41.67%). Examiner comments explained that ChatGPT generated relevant content, but were lengthy answers, which at times did not focus on the specific scenario priorities. There were no comments from the examiners regarding Chat GPT fact "hallucinations."

      CONCLUSIONS: ChatGPT generated SOE answers with comparable module ratings to anesthesiology fellows, as graded by 8 ABA oral board examiners. However, the ChatGPT answers were deemed subjectively inferior due to the length of responses and lack of focus. Future curation and training of an AI database, like ChatGPT, could produce answers more in line with ideal ABA SOE answers. This could lead to higher performance and an anesthesiology-specific trained AI useful for training and examination preparation.
    mergedFrom:
      - source: pubmed
        pmid: "39269908"
        doi: 10.1213/ANE.0000000000006875
    finalDecision: exclude
  - title: Analysis of Responses of GPT-4 V to the Japanese National Clinical Engineer Licensing Examination.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Tests GPT-4V on a licensing exam, not about assessment or feedback of human learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting (licensing examination context)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1007/s10916-024-02103-w
    pmid: "39259341"
    authors: Ishida K, Arisaka N, Fujii K
    year: "2024"
    abstract: Chat Generative Pretrained Transformer (ChatGPT; OpenAI) is a state-of-the-art large language model that can simulate human-like conversations based on user input. We evaluated the performance of GPT-4 V in the Japanese National Clinical Engineer Licensing Examination using 2,155 questions from 2012 to 2023. The average correct answer rate for all questions was 86.0%. In particular, clinical medicine, basic medicine, medical materials, biological properties, and mechanical engineering achieved a correct response rate of ≥ 90%. Conversely, medical device safety management, electrical and electronic engineering, and extracorporeal circulation obtained low correct answer rates ranging from 64.8% to 76.5%. The correct answer rates for questions that included figures/tables, required numerical calculation, figure/table ∩ calculation, and knowledge of Japanese Industrial Standards were 55.2%, 85.8%, 64.2% and 31.0%, respectively. The reason for the low correct answer rates is that ChatGPT lacked recognition of the images and knowledge of standards and laws. This study concludes that careful attention is required when using ChatGPT because several of its explanations lack the correct description.
    mergedFrom:
      - source: pubmed
        pmid: "39259341"
        doi: 10.1007/s10916-024-02103-w
    finalDecision: exclude
  - title: Prompt engineering to increase GPT3.5's performance on the Plastic Surgery In-Service Exams.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Tests prompt engineering strategies for LLM performance on standardized in-service exam questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on exam item answering performance, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.1016/j.bjps.2024.09.001
    pmid: "39255523"
    authors: Nahass G, Chin S, Scharf I, Kazmouz S, Kaplan N, Chiu R, Yang K, Bou Zeid N, Corcoran J, Alkureishi L
    year: "2024"
    abstract: This study assesses ChatGPT's (GPT-3.5) performance on the 2021 ASPS Plastic Surgery In-Service Examination using prompt modifications and Retrieval Augmented Generation (RAG). ChatGPT was instructed to act as a "resident," "attending," or "medical student," and RAG utilized a curated vector database for context. Results showed no significant improvement, with the "resident" prompt yielding the highest accuracy at 54%, and RAG failing to enhance performance, with accuracy remaining at 54.3%. Despite appropriate reasoning when correct, ChatGPT's overall performance fell in the 10th percentile, indicating the need for fine-tuning and more sophisticated approaches to improve AI's utility in complex medical tasks.
    mergedFrom:
      - source: pubmed
        pmid: "39255523"
        doi: 10.1016/j.bjps.2024.09.001
    finalDecision: exclude
  - title: Performance of GPT-4 with Vision on Text- and Image-based ACR Diagnostic Radiology In-Training Examination Questions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates GPT-4V performance on radiology in-training exam questions; not about using AI for assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Tests GPT-4V on in-training exam questions rather than assessment/feedback tied to workplace clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1148/radiol.240153
    pmid: "39225605"
    authors: Hayden N, Gilbert S, Poisson L, Griffith B, Klochko C
    year: "2024"
    abstract: Background Recent advancements, including image processing capabilities, present new potential applications of large language models such as ChatGPT (OpenAI), a generative pretrained transformer, in radiology. However, baseline performance of ChatGPT in radiology-related tasks is understudied. Purpose To evaluate the performance of GPT-4 with vision (GPT-4V) on radiology in-training examination questions, including those with images, to gauge the model's baseline knowledge in radiology. Materials and Methods In this prospective study, conducted between September 2023 and March 2024, the September 2023 release of GPT-4V was assessed using 386 retired questions (189 image-based and 197 text-only questions) from the American College of Radiology Diagnostic Radiology In-Training Examinations. Nine question pairs were identified as duplicates; only the first instance of each duplicate was considered in ChatGPT's assessment. A subanalysis assessed the impact of different zero-shot prompts on performance. Statistical analysis included χ2 tests of independence to ascertain whether the performance of GPT-4V varied between question types or subspecialty. The McNemar test was used to evaluate performance differences between the prompts, with Benjamin-Hochberg adjustment of the P values conducted to control the false discovery rate (FDR). A P value threshold of less than.05 denoted statistical significance. Results GPT-4V correctly answered 246 (65.3%) of the 377 unique questions, with significantly higher accuracy on text-only questions (81.5%, 159 of 195) than on image-based questions (47.8%, 87 of 182) (χ2 test, P < .001). Subanalysis revealed differences between prompts on text-based questions, where chain-of-thought prompting outperformed long instruction by 6.1% (McNemar, P = .02; FDR = 0.063), basic prompting by 6.8% (P = .009, FDR = 0.044), and the original prompting style by 8.9% (P = .001, FDR = 0.014). No differences were observed between prompts on image-based questions with P values of .27 to >.99. Conclusion While GPT-4V demonstrated a level of competence in text-based questions, it showed deficits interpreting radiologic images. © RSNA, 2024 See also the editorial by Deng in this issue.
    mergedFrom:
      - source: pubmed
        pmid: "39225605"
        doi: 10.1148/radiol.240153
    finalDecision: exclude
  - title: Performance Evaluation of the Generative Pre-trained Transformer (GPT-4) on the Family Medicine In-Training Examination.
    reviews:
      - decision: uncertain
        comment: GPT-4 on in-training exam; may have assessment relevance
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; evaluates LLM performance on in-training examination questions compared to residents."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam-performance study on family medicine in-training questions, not assessment/feedback or analysis of clinical observation records in WBA contexts."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.3122/jabfm.2023.230433R1
    pmid: "39214695"
    authors: Wang T, Mainous A, Stelter K, O'Neill T, Newton W
    year: "2024"
    abstract: |-
      OBJECTIVE: In this study, we sought to comprehensively evaluate GPT-4 (Generative Pre-trained Transformer)'s performance on the 2022 American Board of Family Medicine's (ABFM) In-Training Examination (ITE), compared with its predecessor, GPT-3.5, and the national family residents' performance on the same examination.

      METHODS: We utilized both quantitative and qualitative analyses. First, a quantitative analysis was employed to evaluate the model's performance metrics using zero-shot prompt (where only examination questions were provided without any additional information). After this, qualitative analysis was executed to understand the nature of the model's responses, the depth of its medical knowledge, and its ability to comprehend contextual or new information through chain-of-thoughts prompts (interactive conversation) with the model.

      RESULTS: This study demonstrated that GPT-4 made significant improvement in accuracy compared with GPT-3.5 over a 4-month interval between their respective release dates. The correct percentage with zero-shot prompt increased from 56% to 84%, which translates to a scaled score growth from 280 to 690, a 410-point increase. Most notably, further chain-of-thought investigation revealed GPT-4's ability to integrate new information and make self-correction when needed.

      CONCLUSIONS: In this study, GPT-4 has demonstrated notably high accuracy, as well as rapid reading and learning capabilities. These results are consistent with previous research indicating GPT-4's significant potential to assist in clinical decision making. Furthermore, the study highlights the essential role of physicians' critical thinking and lifelong learning skills, particularly evident through the analysis of GPT-4's incorrect responses. This emphasizes the indispensable human element in effectively implementing and using AI technologies in medical settings.
    mergedFrom:
      - source: pubmed
        pmid: "39214695"
        doi: 10.3122/jabfm.2023.230433R1
    finalDecision: exclude
  - title: ChatGPT Performs Worse on USMLE-Style Ethics Questions Compared to Medical Knowledge Questions.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: USMLE-style exam questions, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT accuracy on USMLE-style ethics vs knowledge questions; not about assessment or feedback of learner performance."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: USMLE-style question answering benchmark is not workplace-based assessment or clinical observation-record feedback/analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1055/a-2405-0138
    pmid: "39209308"
    authors: Danehy T, Hecht J, Kentis S, Schechter C, Jariwala S
    year: "2024"
    abstract: |-
      OBJECTIVES:  The main objective of this study is to evaluate the ability of the Large Language Model Chat Generative Pre-Trained Transformer (ChatGPT) to accurately answer the United States Medical Licensing Examination (USMLE) board-style medical ethics questions compared to medical knowledge-based questions. This study has the additional objectives of comparing the overall accuracy of GPT-3.5 to GPT-4 and assessing the variability of responses given by each version.

      METHODS:  Using AMBOSS, a third-party USMLE Step Exam test prep service, we selected one group of 27 medical ethics questions and a second group of 27 medical knowledge questions matched on question difficulty for medical students. We ran 30 trials asking these questions on GPT-3.5 and GPT-4 and recorded the output. A random-effects linear probability regression model evaluated accuracy and a Shannon entropy calculation evaluated response variation.

      RESULTS:  Both versions of ChatGPT demonstrated worse performance on medical ethics questions compared to medical knowledge questions. GPT-4 performed 18% points (p < 0.05) worse on medical ethics questions compared to medical knowledge questions and GPT-3.5 performed 7% points (p = 0.41) worse. GPT-4 outperformed GPT-3.5 by 22% points (p < 0.001) on medical ethics and 33% points (p < 0.001) on medical knowledge. GPT-4 also exhibited an overall lower Shannon entropy for medical ethics and medical knowledge questions (0.21 and 0.11, respectively) than GPT-3.5 (0.59 and 0.55, respectively) which indicates lower variability in response.

      CONCLUSION:  Both versions of ChatGPT performed more poorly on medical ethics questions compared to medical knowledge questions. GPT-4 significantly outperformed GPT-3.5 on overall accuracy and exhibited a significantly lower response variability in answer choices. This underscores the need for ongoing assessment of ChatGPT versions for medical education.
    mergedFrom:
      - source: pubmed
        pmid: "39209308"
        doi: 10.1055/a-2405-0138
    finalDecision: exclude
  - title: ChatGPT, Bard, and Bing Chat Are Large Language Processing Models That Answered Orthopaedic In-Training Examination Questions With Similar Accuracy to First-Year Orthopaedic Surgery Residents.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Tests LLM performance on standardized orthopaedic in-training exam questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam question-answering performance study, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.1016/j.arthro.2024.08.023
    pmid: "39209078"
    authors: Guerra G, Hofmann H, Le J, Wong A, Fathi A, Mayfield C, Petrigliano F, Liu J
    year: "2025"
    abstract: |-
      PURPOSE: To assess ChatGPT's, Bard's, and Bing Chat's ability to generate accurate orthopaedic diagnoses or corresponding treatments by comparing their performance on the Orthopaedic In-Training Examination (OITE) with that of orthopaedic trainees.

      METHODS: OITE question sets from 2021 and 2022 were compiled to form a large set of 420 questions. ChatGPT (GPT-3.5), Bard, and Bing Chat were instructed to select one of the provided responses to each question. The accuracy of composite questions was recorded and comparatively analyzed to human cohorts including medical students and orthopaedic residents, stratified by postgraduate year (PGY).

      RESULTS: ChatGPT correctly answered 46.3% of composite questions whereas Bing Chat correctly answered 52.4% of questions and Bard correctly answered 51.4% of questions on the OITE. When image-associated questions were excluded, ChatGPT's, Bing Chat's, and Bard's overall accuracies improved to 49.1%, 53.5%, and 56.8%, respectively. Medical students correctly answered 30.8%, and PGY-1, -2, -3, -4, and -5 orthopaedic residents correctly answered 53.1%, 60.4%, 66.6%, 70.0%, and 71.9%, respectively.

      CONCLUSIONS: ChatGPT, Bard, and Bing Chat are artificial intelligence (AI) models that answered OITE questions with accuracy similar to that of first-year orthopaedic surgery residents. ChatGPT, Bard, and Bing Chat achieved this result without using images or other supplementary media that human test takers are provided.

      CLINICAL RELEVANCE: Our comparative performance analysis of AI models on orthopaedic board-style questions highlights ChatGPT's, Bing Chat's, and Bard's clinical knowledge and proficiency. Our analysis establishes a baseline of AI model proficiency in the field of orthopaedics and provides a comparative marker for future, more advanced deep learning models. Although in its elementary phase, future AI models' orthopaedic knowledge may provide clinical support and serve as an educational tool.
    mergedFrom:
      - source: pubmed
        pmid: "39209078"
        doi: 10.1016/j.arthro.2024.08.023
    finalDecision: exclude
  - title: Performance of Language Models on the Family Medicine In-Training Exam.
    reviews:
      - decision: uncertain
        comment: LLMs on in-training exam; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment/feedback of learners; evaluates LLM performance on the family medicine in-training exam questions"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: In-training exam performance study without WBA assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.22454/FamMed.2024.233738
    pmid: "39207788"
    authors: Hanna R, Smith L, Mhaskar R, Hanna K
    year: "2024"
    abstract: |-
      BACKGROUND AND OBJECTIVES: Artificial intelligence (AI), such as ChatGPT and Bard, has gained popularity as a tool in medical education. The use of AI in family medicine has not yet been assessed. The objective of this study is to compare the performance of three large language models (LLMs; ChatGPT 3.5, ChatGPT 4.0, and Google Bard) on the family medicine in-training exam (ITE).

      METHODS: The 193 multiple-choice questions of the 2022 ITE, written by the American Board of Family Medicine, were inputted in ChatGPT 3.5, ChatGPT 4.0, and Bard. The LLMs' performance was then scored and scaled.

      RESULTS: ChatGPT 4.0 scored 167/193 (86.5%) with a scaled score of 730 out of 800. According to the Bayesian score predictor, ChatGPT 4.0 has a 100% chance of passing the family medicine board exam. ChatGPT 3.5 scored 66.3%, translating to a scaled score of 400 and an 88% chance of passing the family medicine board exam. Bard scored 64.2%, with a scaled score of 380 and an 85% chance of passing the boards. Compared to the national average of postgraduate year 3 residents, only ChatGPT 4.0 surpassed the residents' mean of 68.4%.

      CONCLUSIONS: ChatGPT 4.0 was the only LLM that outperformed the family medicine postgraduate year 3 residents' national averages on the 2022 ITE, providing robust explanations and demonstrating its potential use in delivering background information on common medical concepts that appear on board exams.
    mergedFrom:
      - source: pubmed
        pmid: "39207788"
        doi: 10.22454/FamMed.2024.233738
    finalDecision: exclude
  - title: Utility of large language models for creating clinical assessment items.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study evaluates GPT-assisted creation of clinical assessment items (practical exam cases), not AI use in workplace-based assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Studies AI-assisted creation of assessment items, not AI for assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1080/0142159X.2024.2382860
    pmid: "39186054"
    authors: Lam G, Shammoon Y, Coulson A, Lalloo F, Maini A, Amin A, Brown C, Sam A
    year: "2025"
    abstract: |-
      PURPOSE: To compare student performance, examiner perceptions and cost of GPT-assisted (generative pretrained transformer-assisted) clinical and professional skills assessment (CPSAs) items against items created using standard methods.

      METHODS: We conducted a prospective, controlled, double-blinded comparison of CPSA items developed using GPT-assistance with those created through standard methods. Two sets of six practical cases were developed for a formative assessment sat by final year medical students. One clinical case in each set was created with GPT-assistance. Students were assigned to one of the two sets.

      RESULTS: The results of 239 participants were analysed in the study. There was no statistically significant difference in item difficulty, or discriminative ability between GPT-assisted and standard items. One hundred percent (n = 15) of respondents to an examiner feedback questionnaire felt GPT-assisted cases were appropriately difficult and realistic. GPT-assistance resulted in significant labour cost savings, with a mean reduction of 57% (880 GBP) in labour cost per case when compared to standard case drafting methods.

      CONCLUSIONS: GPT-assistance can create CPSA items of comparable quality with significantly less cost when compared to standard methods. Future studies could evaluate GPT's ability to create CPSA material in other areas of clinical practice, aiming to validate the generalisability of these findings.
    mergedFrom:
      - source: pubmed
        pmid: "39186054"
        doi: 10.1080/0142159X.2024.2382860
    finalDecision: exclude
  - title: "Enhancing radiology training with GPT-4: Pilot analysis of automated feedback in trainee preliminary reports."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: include
        comment: Generative AI (GPT-4) providing automated feedback on radiology trainee preliminary reports from real clinical call shifts, with data on accuracy, sensitivity, and resident satisfaction. Directly relevant to workplace-based assessment/feedback.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
      - decision: include
        comment: GPT-4 used to generate feedback on radiology residents' preliminary reports from independent call, identifying missed diagnoses. Workplace-based setting (real clinical reports). Presents accuracy data (79.2% prediction accuracy) and acceptability data (satisfaction 3.50/5, 71.4% favored combined feedback).
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:52.271000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:00.184000+00:00
    doi: 10.1067/j.cpradiol.2024.08.003
    pmid: "39179466"
    authors: Bala W, Li H, Moon J, Trivedi H, Gichoya J, Balthazar P
    year: "2025"
    abstract: |-
      RATIONALE AND OBJECTIVES: Radiology residents often receive limited feedback on preliminary reports issued during independent call. This study aimed to determine if Large Language Models (LLMs) can supplement traditional feedback by identifying missed diagnoses in radiology residents' preliminary reports.

      MATERIALS & METHODS: A randomly selected subset of 500 (250 train/250 validation) paired preliminary and final reports between 12/17/2022 and 5/22/2023 were extracted and de-identified from our institutional database. The prompts and report text were input into the GPT-4 language model via the GPT-4 API (gpt-4-0314 model version). Iterative prompt tuning was used on a subset of the training/validation sets to direct the model to identify important findings in the final report that were absent in preliminary reports. For testing, a subset of 10 reports with confirmed diagnostic errors were randomly selected. Fourteen residents with on-call experience assessed the LLM-generated discrepancies and completed a survey on their experience using a 5-point Likert scale.

      RESULTS: The model identified 24 unique missed diagnoses across 10 test reports with i% model prediction accuracy as rated by 14 residents. Five additional diagnoses were identified by users, resulting in a model sensitivity of 79.2 %. Post-evaluation surveys showed a mean satisfaction rating of 3.50 and perceived accuracy rating of 3.64 out of 5 for LLM-generated feedback. Most respondents (71.4 %) favored a combination of LLM-generated and traditional feedback.

      CONCLUSION: This pilot study on the use of LLM-generated feedback for radiology resident preliminary reports demonstrated notable accuracy in identifying missed diagnoses and was positively received, highlighting LLMs' potential role in supplementing conventional feedback methods.
    mergedFrom:
      - source: pubmed
        pmid: "39179466"
        doi: 10.1067/j.cpradiol.2024.08.003
    finalDecision: include
  - title: "Integrating ChatGPT in Orthopedic Education for Medical Undergraduates: Randomized Controlled Trial."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT as a study aid for undergraduates measured by exam performance; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Educational intervention is classroom-style undergraduate MCQ learning, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.2196/57037
    pmid: "39163598"
    authors: Gan W, Ouyang J, Li H, Xue Z, Zhang Y, Dong Q, Huang J, Zheng X, Zhang Y
    year: "2024"
    abstract: |-
      BACKGROUND: ChatGPT is a natural language processing model developed by OpenAI, which can be iteratively updated and optimized to accommodate the changing and complex requirements of human verbal communication.

      OBJECTIVE: The study aimed to evaluate ChatGPT's accuracy in answering orthopedics-related multiple-choice questions (MCQs) and assess its short-term effects as a learning aid through a randomized controlled trial. In addition, long-term effects on student performance in other subjects were measured using final examination results.

      METHODS: We first evaluated ChatGPT's accuracy in answering MCQs pertaining to orthopedics across various question formats. Then, 129 undergraduate medical students participated in a randomized controlled study in which the ChatGPT group used ChatGPT as a learning tool, while the control group was prohibited from using artificial intelligence software to support learning. Following a 2-week intervention, the 2 groups' understanding of orthopedics was assessed by an orthopedics test, and variations in the 2 groups' performance in other disciplines were noted through a follow-up at the end of the semester.

      RESULTS: ChatGPT-4.0 answered 1051 orthopedics-related MCQs with a 70.60% (742/1051) accuracy rate, including 71.8% (237/330) accuracy for A1 MCQs, 73.7% (330/448) accuracy for A2 MCQs, 70.2% (92/131) accuracy for A3/4 MCQs, and 58.5% (83/142) accuracy for case analysis MCQs. As of April 7, 2023, a total of 129 individuals participated in the experiment. However, 19 individuals withdrew from the experiment at various phases; thus, as of July 1, 2023, a total of 110 individuals accomplished the trial and completed all follow-up work. After we intervened in the learning style of the students in the short term, the ChatGPT group answered more questions correctly than the control group (ChatGPT group: mean 141.20, SD 26.68; control group: mean 130.80, SD 25.56; P=.04) in the orthopedics test, particularly on A1 (ChatGPT group: mean 46.57, SD 8.52; control group: mean 42.18, SD 9.43; P=.01), A2 (ChatGPT group: mean 60.59, SD 10.58; control group: mean 56.66, SD 9.91; P=.047), and A3/4 MCQs (ChatGPT group: mean 19.57, SD 5.48; control group: mean 16.46, SD 4.58; P=.002). At the end of the semester, we found that the ChatGPT group performed better on final examinations in surgery (ChatGPT group: mean 76.54, SD 9.79; control group: mean 72.54, SD 8.11; P=.02) and obstetrics and gynecology (ChatGPT group: mean 75.98, SD 8.94; control group: mean 72.54, SD 8.66; P=.04) than the control group.

      CONCLUSIONS: ChatGPT answers orthopedics-related MCQs accurately, and students using it excel in both short-term and long-term assessments. Our findings strongly support ChatGPT's integration into medical education, enhancing contemporary instructional methods.

      TRIAL REGISTRATION: Chinese Clinical Trial Registry Chictr2300071774; https://www.chictr.org.cn/hvshowproject.html ?id=225740&v=1.0.
    mergedFrom:
      - source: pubmed
        pmid: "39163598"
        doi: 10.2196/57037
    finalDecision: exclude
  - title: "GPT-4o’s competency in answering the simulated written European Board of Interventional Radiology exam compared to a medical student and experts in Germany and its ability to generate exam items on interventional radiology: a descriptive study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Tests GPT-4o's performance on board exam and item generation, not about assessment or feedback of human learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments (simulated written exam)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.3352/jeehp.2024.21.21
    pmid: "39161266"
    authors: Ebel S, Ehrengut C, Denecke T, G&#xf6;&#xdf;mann H, Beeskow A
    year: "2024"
    abstract: |-
      PURPOSE: This study aimed to determine whether ChatGPT-4o, a generative artificial intelligence (AI) platform, was able to pass a simulated written European Board of Interventional Radiology (EBIR) exam and whether GPT-4o can be used to train medical students and interventional radiologists of different levels of expertise by generating exam items on interventional radiology.

      METHODS: GPT-4o was asked to answer 370 simulated exam items of the Cardiovascular and Interventional Radiology Society of Europe (CIRSE) for EBIR preparation (CIRSE Prep). Subsequently, GPT-4o was requested to generate exam items on interventional radiology topics at levels of difficulty suitable for medical students and the EBIR exam. Those generated items were answered by 4 participants, including a medical student, a resident, a consultant, and an EBIR holder. The correctly answered items were counted. One investigator checked the answers and items generated by GPT-4o for correctness and relevance. This work was done from April to July 2024.

      RESULTS: GPT-4o correctly answered 248 of the 370 CIRSE Prep items (67.0%). For 50 CIRSE Prep items, the medical student answered 46.0%, the resident 42.0%, the consultant 50.0%, and the EBIR holder 74.0% correctly. All participants answered 82.0% to 92.0% of the 50 GPT-4o generated items at the student level correctly. For the 50 GPT-4o items at the EBIR level, the medical student answered 32.0%, the resident 44.0%, the consultant 48.0%, and the EBIR holder 66.0% correctly. All participants could pass the GPT-4o-generated items for the student level; while the EBIR holder could pass the GPT-4o-generated items for the EBIR level. Two items (0.3%) out of 150 generated by the GPT-4o were assessed as implausible.

      CONCLUSION: GPT-4o could pass the simulated written EBIR exam and create exam items of varying difficulty to train medical students and interventional radiologists.
    mergedFrom:
      - source: pubmed
        pmid: "39161266"
        doi: 10.3352/jeehp.2024.21.21
    finalDecision: exclude
  - title: "From Revisions to Insights: Converting Radiology Report Revisions into Actionable Educational Feedback Using Generative AI Models."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: include
        comment: GPT-4 Turbo analyzes discrepancies between trainee preliminary and attending finalized radiology reports to generate structured educational feedback. Workplace-based context (radiology reporting), with validity data (sensitivity, interrater reliability, F1 scores, reproducibility). Directly applies generative AI to feedback on clinical documentation.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
      - decision: include
        comment: "Uses GPT-4 Turbo to analyze radiology report revisions (preliminary vs finalized reports) to identify discrepancies, classify their severity and type, and generate educational teaching points for radiology trainees. Setting is radiology residency training (workplace-based clinical education). Presents validity data: discrepancy detection accuracy correlated with expert radiologists (r=0.778), classification agreement (weighted F1 0.66 for severity, 0.64 for type), reproducibility analysis (ICC 0.690), and teaching point relevance rated by experts (84.5% relevant). Although synthetic/open-source datasets were used due to institutional constraints, the tool is designed for workplace-based radiology education feedback."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:56.143000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:04.368000+00:00
    doi: 10.1007/s10278-024-01233-4
    pmid: "39160366"
    authors: Lyo S, Mohan S, Hassankhani A, Noor A, Dako F, Cook T
    year: "2025"
    abstract: Expert feedback on trainees' preliminary reports is crucial for radiologic training, but real-time feedback can be challenging due to non-contemporaneous, remote reading and increasing imaging volumes. Trainee report revisions contain valuable educational feedback, but synthesizing data from raw revisions is challenging. Generative AI models can potentially analyze these revisions and provide structured, actionable feedback. This study used the OpenAI GPT-4 Turbo API to analyze paired synthesized and open-source analogs of preliminary and finalized reports, identify discrepancies, categorize their severity and type, and suggest review topics. Expert radiologists reviewed the output by grading discrepancies, evaluating the severity and category accuracy, and suggested review topic relevance. The reproducibility of discrepancy detection and maximal discrepancy severity was also examined. The model exhibited high sensitivity, detecting significantly more discrepancies than radiologists (W = 19.0, p < 0.001) with a strong positive correlation (r = 0.778, p < 0.001). Interrater reliability for severity and type were fair (Fleiss' kappa = 0.346 and 0.340, respectively; weighted kappa = 0.622 for severity). The LLM achieved a weighted F1 score of 0.66 for severity and 0.64 for type. Generated teaching points were considered relevant in ~ 85% of cases, and relevance correlated with the maximal discrepancy severity (Spearman ρ = 0.76, p < 0.001). The reproducibility was moderate to good (ICC (2,1) = 0.690) for the number of discrepancies and substantial for maximal discrepancy severity (Fleiss' kappa = 0.718; weighted kappa = 0.94). Generative AI models can effectively identify discrepancies in report revisions and generate relevant educational feedback, offering promise for enhancing radiology training.
    mergedFrom:
      - source: pubmed
        pmid: "39160366"
        doi: 10.1007/s10278-024-01233-4
    finalDecision: include
  - title: "Large Language Models in Otolaryngology Residency Admissions: A Random Sampling Analysis."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on residency admissions, not assessment or feedback"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on residency admissions rather than workplace-based clinical education assessment"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    doi: 10.1002/lary.31705
    pmid: "39157995"
    authors: Halagur A, Balakrishnan K, Ayoub N
    year: "2025"
    abstract: |-
      OBJECTIVES: To investigate potential demographic bias in artificial intelligence (AI)-based simulations of otolaryngology, residency selection committee (RSC) members tasked with selecting one applicant among candidates with varied racial, gender, and sexual orientations.

      METHODS: This study employed random sampling of simulated RSC member decisions using a novel Application Programming Interface (API) to virtually connect to OpenAI's Generative Pre-Trained Transformers (GPT-4 and GPT-4o). Simulated RSC members with diverse demographics were tasked with ranking to match 1 applicant among 10 with varied racial, gender, and sexual orientations. All applicants had identical qualifications; only demographics of the applicants and RSC members were varied for each simulation. Each RSC simulation ran 1000 times. Chi-square tests analyzed differences across categorical variables. GPT-4o simulations additionally requested a rationale for each decision.

      RESULTS: Simulated RSCs consistently showed racial, gender, and sexual orientation bias. Most applicant pairwise comparisons showed statistical significance (p < 0.05). White and Black RSCs exhibited greatest preference for applicants sharing their own demographic characteristics, favoring White and Black female applicants, respectively, over others (all pairwise p < 0.001). Asian male applicants consistently received lowest selection rates. Male RSCs favored White male and female applicants, while female RSCs preferred LGBTQIA+, White and Black female applicants (all p < 0.05). High socioeconomic status (SES) RSCs favored White female and LGBTQIA+ applicants, while low SES RSCs favored Black female and LGBTQIA+ applicants over others (all p < 0.001). Results from the newest iteration of the LLM, ChatGPT-4o, indicated evolved selection preferences favoring Black female and LGBTQIA+ applicants across all RSCs, with the rationale of prioritizing inclusivity given in >95% of such decisions.

      CONCLUSION: Utilizing publicly available LLMs to aid in otolaryngology residency selection may introduce significant racial, gender, and sexual orientation bias. Potential for significant and evolving LLM bias should be appreciated and minimized to promote a diverse and representative field of future otolaryngologists in alignment with current workforce data.

      LEVEL OF EVIDENCE: NA Laryngoscope, 135:87-93, 2025.
    mergedFrom:
      - source: pubmed
        pmid: "39157995"
        doi: 10.1002/lary.31705
    finalDecision: exclude
  - title: "A Language Model-Powered Simulated Patient With Automated Feedback for History Taking: Prospective Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment/training appears entirely simulation-based (simulated patient)."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in a simulated environment. Students perform history taking with a GPT-powered chatbot simulated patient, not in real clinical encounters."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Feedback is generated within simulated patient interactions without a workplace-based component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.2196/59213
    pmid: "39150749"
    authors: Holderried F, Stegemann-Philipps C, Herrmann-Werner A, Festl-Wietek T, Holderried M, Eickhoff C, Mahling M
    year: "2024"
    abstract: |-
      BACKGROUND: Although history taking is fundamental for diagnosing medical conditions, teaching and providing feedback on the skill can be challenging due to resource constraints. Virtual simulated patients and web-based chatbots have thus emerged as educational tools, with recent advancements in artificial intelligence (AI) such as large language models (LLMs) enhancing their realism and potential to provide feedback.

      OBJECTIVE: In our study, we aimed to evaluate the effectiveness of a Generative Pretrained Transformer (GPT) 4 model to provide structured feedback on medical students' performance in history taking with a simulated patient.

      METHODS: We conducted a prospective study involving medical students performing history taking with a GPT-powered chatbot. To that end, we designed a chatbot to simulate patients' responses and provide immediate feedback on the comprehensiveness of the students' history taking. Students' interactions with the chatbot were analyzed, and feedback from the chatbot was compared with feedback from a human rater. We measured interrater reliability and performed a descriptive analysis to assess the quality of feedback.

      RESULTS: Most of the study's participants were in their third year of medical school. A total of 1894 question-answer pairs from 106 conversations were included in our analysis. GPT-4's role-play and responses were medically plausible in more than 99% of cases. Interrater reliability between GPT-4 and the human rater showed "almost perfect" agreement (Cohen κ=0.832). Less agreement (κ<0.6) detected for 8 out of 45 feedback categories highlighted topics about which the model's assessments were overly specific or diverged from human judgement.

      CONCLUSIONS: The GPT model was effective in providing structured feedback on history-taking dialogs provided by medical students. Although we unraveled some limitations regarding the specificity of feedback for certain feedback categories, the overall high agreement with human raters suggests that LLMs can be a valuable tool for medical education. Our findings, thus, advocate the careful integration of AI-driven feedback mechanisms in medical training and highlight important aspects when LLMs are used in that context.
    mergedFrom:
      - source: pubmed
        pmid: "39150749"
        doi: 10.2196/59213
    finalDecision: exclude
  - title: "AI chatbots show promise but limitations on UK medical exam questions: a comparative performance study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks multiple LLMs on simulated UK medical board exam questions; not about using AI for assessment or feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is based on simulated board-style exam questions rather than workplace-based clinical assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.1038/s41598-024-68996-2
    pmid: "39143077"
    authors: Sadeq M, Ghorab R, Ashry M, Abozaid A, Banihani H, Salem M, Aisheh M, Abuzahra S, Mourid M, Assker M, Ayyad M, Moawad M
    year: "2024"
    abstract: Large language models (LLMs) like ChatGPT have potential applications in medical education such as helping students study for their licensing exams by discussing unclear questions with them. However, they require evaluation on these complex tasks. The purpose of this study was to evaluate how well publicly accessible LLMs performed on simulated UK medical board exam questions. 423 board-style questions from 9 UK exams (MRCS, MRCP, etc.) were answered by seven LLMs (ChatGPT-3.5, ChatGPT-4, Bard, Perplexity, Claude, Bing, Claude Instant). There were 406 multiple-choice, 13 true/false, and 4 "choose N" questions covering topics in surgery, pediatrics, and other disciplines. The accuracy of the output was graded. Statistics were used to analyze differences among LLMs. Leaked questions were excluded from the primary analysis. ChatGPT 4.0 scored (78.2%), Bing (67.2%), Claude (64.4%), and Claude Instant (62.9%). Perplexity scored the lowest (56.1%). Scores differed significantly between LLMs overall (p < 0.001) and in pairwise comparisons. All LLMs scored higher on multiple-choice vs true/false or "choose N" questions. LLMs demonstrated limitations in answering certain questions, indicating refinements needed before primary reliance in medical education. However, their expanding capabilities suggest a potential to improve training if thoughtfully implemented. Further research should explore specialty specific LLMs and optimal integration into medical curricula.
    mergedFrom:
      - source: pubmed
        pmid: "39143077"
        doi: 10.1038/s41598-024-68996-2
    finalDecision: exclude
  - title: "Influence of Model Evolution and System Roles on ChatGPT's Performance in Chinese Medical Licensing Exams: Comparative Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study benchmarks ChatGPT on Chinese National Medical Licensing Examination questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Licensing exam benchmarking is not a workplace-based clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.2196/52784
    pmid: "39140269"
    authors: Ming S, Guo Q, Cheng W, Lei B
    year: "2024"
    abstract: |-
      BACKGROUND: With the increasing application of large language models like ChatGPT in various industries, its potential in the medical domain, especially in standardized examinations, has become a focal point of research.

      OBJECTIVE: The aim of this study is to assess the clinical performance of ChatGPT, focusing on its accuracy and reliability in the Chinese National Medical Licensing Examination (CNMLE).

      METHODS: The CNMLE 2022 question set, consisting of 500 single-answer multiple choices questions, were reclassified into 15 medical subspecialties. Each question was tested 8 to 12 times in Chinese on the OpenAI platform from April 24 to May 15, 2023. Three key factors were considered: the version of GPT-3.5 and 4.0, the prompt's designation of system roles tailored to medical subspecialties, and repetition for coherence. A passing accuracy threshold was established as 60%. The χ2 tests and κ values were employed to evaluate the model's accuracy and consistency.

      RESULTS: GPT-4.0 achieved a passing accuracy of 72.7%, which was significantly higher than that of GPT-3.5 (54%; P<.001). The variability rate of repeated responses from GPT-4.0 was lower than that of GPT-3.5 (9% vs 19.5%; P<.001). However, both models showed relatively good response coherence, with κ values of 0.778 and 0.610, respectively. System roles numerically increased accuracy for both GPT-4.0 (0.3%-3.7%) and GPT-3.5 (1.3%-4.5%), and reduced variability by 1.7% and 1.8%, respectively (P>.05). In subgroup analysis, ChatGPT achieved comparable accuracy among different question types (P>.05). GPT-4.0 surpassed the accuracy threshold in 14 of 15 subspecialties, while GPT-3.5 did so in 7 of 15 on the first response.

      CONCLUSIONS: GPT-4.0 passed the CNMLE and outperformed GPT-3.5 in key areas such as accuracy, consistency, and medical subspecialty expertise. Adding a system role insignificantly enhanced the model's reliability and answer coherence. GPT-4.0 showed promising potential in medical education and clinical practice, meriting further study.
    mergedFrom:
      - source: pubmed
        pmid: "39140269"
        doi: 10.2196/52784
    finalDecision: exclude
  - title: "Educational Utility of Clinical Vignettes Generated in Japanese by ChatGPT-4: Mixed Methods Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates quality of AI-generated clinical vignettes for educational use; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Assesses AI-generated vignette quality for education, not workplace-based assessment/feedback or analysis of observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.2196/59133
    pmid: "39137031"
    authors: Takahashi H, Shikino K, Kondo T, Komori A, Yamada Y, Saita M, Naito T
    year: "2024"
    abstract: |-
      BACKGROUND: Evaluating the accuracy and educational utility of artificial intelligence-generated medical cases, especially those produced by large language models such as ChatGPT-4 (developed by OpenAI), is crucial yet underexplored.

      OBJECTIVE: This study aimed to assess the educational utility of ChatGPT-4-generated clinical vignettes and their applicability in educational settings.

      METHODS: Using a convergent mixed methods design, a web-based survey was conducted from January 8 to 28, 2024, to evaluate 18 medical cases generated by ChatGPT-4 in Japanese. In the survey, 6 main question items were used to evaluate the quality of the generated clinical vignettes and their educational utility, which are information quality, information accuracy, educational usefulness, clinical match, terminology accuracy (TA), and diagnosis difficulty. Feedback was solicited from physicians specializing in general internal medicine or general medicine and experienced in medical education. Chi-square and Mann-Whitney U tests were performed to identify differences among cases, and linear regression was used to examine trends associated with physicians' experience. Thematic analysis of qualitative feedback was performed to identify areas for improvement and confirm the educational utility of the cases.

      RESULTS: Of the 73 invited participants, 71 (97%) responded. The respondents, primarily male (64/71, 90%), spanned a broad range of practice years (from 1976 to 2017) and represented diverse hospital sizes throughout Japan. The majority deemed the information quality (mean 0.77, 95% CI 0.75-0.79) and information accuracy (mean 0.68, 95% CI 0.65-0.71) to be satisfactory, with these responses being based on binary data. The average scores assigned were 3.55 (95% CI 3.49-3.60) for educational usefulness, 3.70 (95% CI 3.65-3.75) for clinical match, 3.49 (95% CI 3.44-3.55) for TA, and 2.34 (95% CI 2.28-2.40) for diagnosis difficulty, based on a 5-point Likert scale. Statistical analysis showed significant variability in content quality and relevance across the cases (P<.001 after Bonferroni correction). Participants suggested improvements in generating physical findings, using natural language, and enhancing medical TA. The thematic analysis highlighted the need for clearer documentation, clinical information consistency, content relevance, and patient-centered case presentations.

      CONCLUSIONS: ChatGPT-4-generated medical cases written in Japanese possess considerable potential as resources in medical education, with recognized adequacy in quality and accuracy. Nevertheless, there is a notable need for enhancements in the precision and realism of case details. This study emphasizes ChatGPT-4's value as an adjunctive educational tool in the medical field, requiring expert oversight for optimal application.
    mergedFrom:
      - source: pubmed
        pmid: "39137031"
        doi: 10.2196/59133
    finalDecision: exclude
  - title: "CLINICAL REASONING AND ARTIFICIAL INTELLIGENCE: CAN AI REALLY THINK?"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/editorial on AI and clinical reasoning; not original research, systematic review, or meta-analysis."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/opinion article, not an eligible empirical study/review type."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    pmid: "39135584"
    authors: Schwartzstein R
    year: "2024"
    abstract: "Artificial intelligence (AI) in the form of ChatGPT has rapidly attracted attention from physicians and medical educators. While it holds great promise for more routine medical tasks, may broaden one's differential diagnosis, and may be able to assist in the evaluation of images, such as radiographs and electrocardiograms, the technology is largely based on advanced algorithms akin to pattern recognition. One of the key questions raised in concert with these advances is: What does the growth of artificial intelligence mean for medical education, particularly the development of critical thinking and clinical reasoning? In this commentary, we will explore the elements of cognitive theory that underlie the ways in which physicians are taught to reason through a diagnostic case and compare hypothetico-deductive reasoning, often employing illness scripts, with inductive reasoning, which is based on a deeper understanding of mechanisms of health and disease. Issues of cognitive bias and their impact on diagnostic error will be examined. The constructs of routine and adaptive expertise will also be delineated. The application of artificial intelligence to diagnostic problem solving, along with concerns about racial and gender bias, will be delineated. Using several case examples, we will demonstrate the limitations of this technology and its potential pitfalls and outline the direction medical education may need to take in the years to come."
    mergedFrom:
      - source: pubmed
        pmid: "39135584"
    finalDecision: exclude
  - title: "Simulation training in mammography with AI-generated images: a multireader study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: Simulation training with AI-generated images, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Training is simulation-based only."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    doi: 10.1007/s00330-024-11005-x
    pmid: "39134745"
    authors: Rangarajan K, Manivannan V, Singh H, Gupta A, Maheshwari H, Gogoi R, Gogoi D, Das R, Hari S, Vyas S, Sharma R, Pandey S, Seenu V, Banerjee S, Namboodiri V, Arora C
    year: "2025"
    abstract: |-
      OBJECTIVES: The interpretation of mammograms requires many years of training and experience. Currently, training in mammography, like the rest of diagnostic radiology, is through institutional libraries, books, and experience accumulated over time. We explore whether artificial Intelligence (AI)-generated images can help in simulation education and result in measurable improvement in performance of residents in training.

      METHODS: We developed a generative adversarial network (GAN) that was capable of generating mammography images with varying characteristics, such as size and density, and created a tool with which a user could control these characteristics. The tool allowed the user (a radiology resident) to realistically insert cancers within different regions of the mammogram. We then provided this tool to residents in training. Residents were randomized into a practice group and a non-practice group, and the difference in performance before and after practice with such a tool (in comparison to no intervention in the non-practice group) was assessed.

      RESULTS: Fifty residents participated in the study, 27 underwent simulation training, and 23 did not. There was a significant improvement in the sensitivity (7.43 percent, significant at p-value = 0.03), negative predictive value (5.05 percent, significant at p-value = 0.008) and accuracy (6.49 percent, significant at p-value = 0.01) among residents in the detection of cancer on mammograms after simulation training.

      CONCLUSION: Our study shows the value of simulation training in diagnostic radiology and explores the potential of generative AI to enable such simulation training.

      CLINICAL RELEVANCE STATEMENT: Using generative artificial intelligence, simulation training modules can be developed that can help residents in training by providing them with a visual impression of a variety of different cases.

      KEY POINTS: Generative networks can produce diagnostic imaging with specific characteristics, potentially useful for training residents. Training with generating images improved residents' mammographic diagnostic abilities. Development of a game-like interface that exploits these networks can result in improvement in performance over a short training period.
    mergedFrom:
      - source: pubmed
        pmid: "39134745"
        doi: 10.1007/s00330-024-11005-x
    finalDecision: exclude
  - title: "Assessing the Ability of a Large Language Model to Score Free-Text Medical Student Clinical Notes: Quantitative Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environment. Standardized patient encounters in a lab setting, not workplace-based clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment was conducted in a standardized-patient simulated setting rather than a workplace-based clinical environment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.2196/56342
    pmid: "39118469"
    authors: Burke H, Hoang A, Lopreiato J, King H, Hemmer P, Montgomery M, Gagarin V
    year: "2024"
    abstract: |-
      BACKGROUND: Teaching medical students the skills required to acquire, interpret, apply, and communicate clinical information is an integral part of medical education. A crucial aspect of this process involves providing students with feedback regarding the quality of their free-text clinical notes.

      OBJECTIVE: The goal of this study was to assess the ability of ChatGPT 3.5, a large language model, to score medical students' free-text history and physical notes.

      METHODS: This is a single-institution, retrospective study. Standardized patients learned a prespecified clinical case and, acting as the patient, interacted with medical students. Each student wrote a free-text history and physical note of their interaction. The students' notes were scored independently by the standardized patients and ChatGPT using a prespecified scoring rubric that consisted of 85 case elements. The measure of accuracy was percent correct.

      RESULTS: The study population consisted of 168 first-year medical students. There was a total of 14,280 scores. The ChatGPT incorrect scoring rate was 1.0%, and the standardized patient incorrect scoring rate was 7.2%. The ChatGPT error rate was 86%, lower than the standardized patient error rate. The ChatGPT mean incorrect scoring rate of 12 (SD 11) was significantly lower than the standardized patient mean incorrect scoring rate of 85 (SD 74; P=.002).

      CONCLUSIONS: ChatGPT demonstrated a significantly lower error rate compared to standardized patients. This is the first study to assess the ability of a generative pretrained transformer (GPT) program to score medical students' standardized patient-based free-text clinical notes. It is expected that, in the near future, large language models will provide real-time feedback to practicing physicians regarding their free-text notes. GPT artificial intelligence programs represent an important advance in medical education and medical practice.
    mergedFrom:
      - source: pubmed
        pmid: "39118469"
        doi: 10.2196/56342
      - source: scopus
        doi: 10.2196/56342
        scopusId: SCOPUS_ID:85203660250
    finalDecision: exclude
  - title: Evaluating the competency of ChatGPT in MRCP Part 1 and a systematic literature review of its capabilities in postgraduate medical assessments.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT performance on MRCP Part 1 exam questions with systematic review of LLM performance on UK postgraduate exams; not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam-performance and review study, not workplace-based assessment or feedback analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1371/journal.pone.0307372
    pmid: "39083455"
    authors: Vij O, Calver H, Myall N, Dey M, Kouranloo K
    year: "2024"
    abstract: |-
      OBJECTIVES: As a large language model (LLM) trained on a large data set, ChatGPT can perform a wide array of tasks without additional training. We evaluated the performance of ChatGPT on postgraduate UK medical examinations through a systematic literature review of ChatGPT's performance in UK postgraduate medical assessments and its performance on Member of Royal College of Physicians (MRCP) Part 1 examination.

      METHODS: Medline, Embase and Cochrane databases were searched. Articles discussing the performance of ChatGPT in UK postgraduate medical examinations were included in the systematic review. Information was extracted on exam performance including percentage scores and pass/fail rates. MRCP UK Part 1 sample paper questions were inserted into ChatGPT-3.5 and -4 four times each and the scores marked against the correct answers provided.

      RESULTS: 12 studies were ultimately included in the systematic literature review. ChatGPT-3.5 scored 66.4% and ChatGPT-4 scored 84.8% on MRCP Part 1 sample paper, which is 4.4% and 22.8% above the historical pass mark respectively. Both ChatGPT-3.5 and -4 performance was significantly above the historical pass mark for MRCP Part 1, indicating they would likely pass this examination. ChatGPT-3.5 failed eight out of nine postgraduate exams it performed with an average percentage of 5.0% below the pass mark. ChatGPT-4 passed nine out of eleven postgraduate exams it performed with an average percentage of 13.56% above the pass mark. ChatGPT-4 performance was significantly better than ChatGPT-3.5 in all examinations that both models were tested on.

      CONCLUSION: ChatGPT-4 performed at above passing level for the majority of UK postgraduate medical examinations it was tested on. ChatGPT is prone to hallucinations, fabrications and reduced explanation accuracy which could limit its potential as a learning tool. The potential for these errors is an inherent part of LLMs and may always be a limitation for medical applications of ChatGPT.
    mergedFrom:
      - source: pubmed
        pmid: "39083455"
        doi: 10.1371/journal.pone.0307372
    finalDecision: exclude
  - title: "Artificial intelligence: Reducing inconsistency in the surgical residency application review process."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About AI for residency application review, not about assessment/feedback of learners in clinical settings"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    doi: 10.1016/j.amjsurg.2024.115816
    pmid: "39079879"
    authors: Markow M, Jebbia M, Hasjim B, Nahmias J, Grigorian A, Burruss S, Schubl S, Vilchez V, Fairbairn K, Chau A, Keshava H, Yamamoto M, Smith B, Swentek L
    year: "2024"
    abstract: The incorporation of artificial intelligence (AI) into the general surgery residency recruitment process holds great promise for overcoming limitations inherent to traditional application review methods. This study assesses the consistency of AI, particularly ChatGPT, in evaluating medical student performance evaluation (MSPE) letters in comparison to experienced human reviewers. While the results suggest that ChatGPT demonstrates greater consistency in grading than human reviewers, AI still has its limitations. This underscores the necessity for careful refinement and consideration in its implementation. While AI presents opportunities to enhance residency selection procedures, further research is imperative to fully grasp its capabilities and implications.
    mergedFrom:
      - source: pubmed
        pmid: "39079879"
        doi: 10.1016/j.amjsurg.2024.115816
    finalDecision: exclude
  - title: Evaluating prompt engineering on GPT-3.5's performance in USMLE-style medical calculations and clinical scenarios generated by GPT-4.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluating GPT performance on USMLE-style exam questions, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates prompt engineering effects on GPT-3.5 answering USMLE-style questions, an academic exam benchmarking study, not workplace-based or clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: USMLE-style exam question study, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1038/s41598-024-66933-x
    pmid: "39069520"
    authors: Patel D, Raut G, Zimlichman E, Cheetirala S, Nadkarni G, Glicksberg B, Apakama D, Bell E, Freeman R, Timsina P, Klang E
    year: "2024"
    abstract: "This study was designed to assess how different prompt engineering techniques, specifically direct prompts, Chain of Thought (CoT), and a modified CoT approach, influence the ability of GPT-3.5 to answer clinical and calculation-based medical questions, particularly those styled like the USMLE Step 1 exams. To achieve this, we analyzed the responses of GPT-3.5 to two distinct sets of questions: a batch of 1000 questions generated by GPT-4, and another set comprising 95 real USMLE Step 1 questions. These questions spanned a range of medical calculations and clinical scenarios across various fields and difficulty levels. Our analysis revealed that there were no significant differences in the accuracy of GPT-3.5's responses when using direct prompts, CoT, or modified CoT methods. For instance, in the USMLE sample, the success rates were 61.7% for direct prompts, 62.8% for CoT, and 57.4% for modified CoT, with a p-value of 0.734. Similar trends were observed in the responses to GPT-4 generated questions, both clinical and calculation-based, with p-values above 0.05 indicating no significant difference between the prompt types. The conclusion drawn from this study is that the use of CoT prompt engineering does not significantly alter GPT-3.5's effectiveness in handling medical calculations or clinical scenario questions styled like those in USMLE exams. This finding is crucial as it suggests that performance of ChatGPT remains consistent regardless of whether a CoT technique is used instead of direct prompts. This consistency could be instrumental in simplifying the integration of AI tools like ChatGPT into medical education, enabling healthcare professionals to utilize these tools with ease, without the necessity for complex prompt engineering."
    mergedFrom:
      - source: pubmed
        pmid: "39069520"
        doi: 10.1038/s41598-024-66933-x
    finalDecision: exclude
  - title: "Performance of ChatGPT Across Different Versions in Medical Licensing Examinations Worldwide: Systematic Review and Meta-Analysis."
    reviews:
      - decision: uncertain
        comment: Systematic review of ChatGPT on licensing exams; abstract needed to assess WBA relevance
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Systematic review and meta-analysis of ChatGPT performance on medical licensing exams worldwide; benchmarking AI on exams, not about using AI for assessment or feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Systematic review of licensing-exam performance, not workplace-based assessment feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.2196/60807
    pmid: "39052324"
    authors: Liu M, Okuhara T, Chang X, Shirabe R, Nishiie Y, Okada H, Kiuchi T
    year: "2024"
    abstract: |-
      BACKGROUND: Over the past 2 years, researchers have used various medical licensing examinations to test whether ChatGPT (OpenAI) possesses accurate medical knowledge. The performance of each version of ChatGPT on the medical licensing examination in multiple environments showed remarkable differences. At this stage, there is still a lack of a comprehensive understanding of the variability in ChatGPT's performance on different medical licensing examinations.

      OBJECTIVE: In this study, we reviewed all studies on ChatGPT performance in medical licensing examinations up to March 2024. This review aims to contribute to the evolving discourse on artificial intelligence (AI) in medical education by providing a comprehensive analysis of the performance of ChatGPT in various environments. The insights gained from this systematic review will guide educators, policymakers, and technical experts to effectively and judiciously use AI in medical education.

      METHODS: We searched the literature published between January 1, 2022, and March 29, 2024, by searching query strings in Web of Science, PubMed, and Scopus. Two authors screened the literature according to the inclusion and exclusion criteria, extracted data, and independently assessed the quality of the literature concerning Quality Assessment of Diagnostic Accuracy Studies-2. We conducted both qualitative and quantitative analyses.

      RESULTS: A total of 45 studies on the performance of different versions of ChatGPT in medical licensing examinations were included in this study. GPT-4 achieved an overall accuracy rate of 81% (95% CI 78-84; P<.01), significantly surpassing the 58% (95% CI 53-63; P<.01) accuracy rate of GPT-3.5. GPT-4 passed the medical examinations in 26 of 29 cases, outperforming the average scores of medical students in 13 of 17 cases. Translating the examination questions into English improved GPT-3.5's performance but did not affect GPT-4. GPT-3.5 showed no difference in performance between examinations from English-speaking and non-English-speaking countries (P=.72), but GPT-4 performed better on examinations from English-speaking countries significantly (P=.02). Any type of prompt could significantly improve GPT-3.5's (P=.03) and GPT-4's (P<.01) performance. GPT-3.5 performed better on short-text questions than on long-text questions. The difficulty of the questions affected the performance of GPT-3.5 and GPT-4. In image-based multiple-choice questions (MCQs), ChatGPT's accuracy rate ranges from 13.1% to 100%. ChatGPT performed significantly worse on open-ended questions than on MCQs.

      CONCLUSIONS: GPT-4 demonstrates considerable potential for future use in medical education. However, due to its insufficient accuracy, inconsistent performance, and the challenges posed by differing medical policies and knowledge across countries, GPT-4 is not yet suitable for use in medical education.

      TRIAL REGISTRATION: PROSPERO CRD42024506687; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=506687.
    mergedFrom:
      - source: pubmed
        pmid: "39052324"
        doi: 10.2196/60807
    finalDecision: exclude
  - title: Comparative performance of artificial ıntelligence models in physical medicine and rehabilitation board-level questions.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: AI performance on PM&R board-level questions, standardized exam-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment/feedback of learners; evaluates AI model performance on PMR board-style exam questions"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Board-question answering study without workplace-based assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1590/1806-9282.20240241
    pmid: "39045939"
    authors: Menek&#x15f;eo&#x11f;lu A, &#x130;&#x15f; E
    year: "2024"
    abstract: |-
      OBJECTI&#x307;VES: The aim of this study was to compare the performance of artificial intelligence models ChatGPT-3.5, ChatGPT-4, and Google Bard in answering Physical Medicine and Rehabilitation board-style questions, assessing their capabilities in medical education and potential clinical applications.

      METHODS: A comparative cross-sectional study was conducted using the PMR100, an example question set for the American Board of Physical Medicine and Rehabilitation Part I exam, focusing on artificial intelligence models' ability to answer and categorize questions by difficulty. The study evaluated the artificial intelligence models and analyzed them for accuracy, reliability, and alignment with difficulty levels determined by physiatrists.

      RESULTS: ChatGPT-4 led with a 74% success rate, followed by Bard at 66%, and ChatGPT-3.5 at 63.8%. Bard showed remarkable answer consistency, altering responses in only 1% of cases. The difficulty assessment by ChatGPT models closely matched that of physiatrists. The study highlighted nuanced differences in artificial intelligence models' performance across various Physical Medicine and Rehabilitation subfields.

      CONCLUSION: The study illustrates the potential of artificial intelligence in medical education and clinical settings, with ChatGPT-4 showing a slight edge in performance. It emphasizes the importance of artificial intelligence as a supportive tool for physiatrists, despite the need for careful oversight of artificial intelligence-generated responses to ensure patient safety.
    mergedFrom:
      - source: pubmed
        pmid: "39045939"
        doi: 10.1590/1806-9282.20240241
    finalDecision: exclude
  - title: "Assessing ChatGPT's Competency in Addressing Interdisciplinary Inquiries on Chatbot Uses in Sports Rehabilitation: Simulation Study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: Simulation study assessing ChatGPT's knowledge responses, not workplace-based assessment of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    doi: 10.2196/51157
    pmid: "39042885"
    authors: McBee J, Han D, Liu L, Ma L, Adjeroh D, Xu D, Hu G
    year: "2024"
    abstract: |-
      BACKGROUND: ChatGPT showcases exceptional conversational capabilities and extensive cross-disciplinary knowledge. In addition, it can perform multiple roles in a single chat session. This unique multirole-playing feature positions ChatGPT as a promising tool for exploring interdisciplinary subjects.

      OBJECTIVE: The aim of this study was to evaluate ChatGPT's competency in addressing interdisciplinary inquiries based on a case study exploring the opportunities and challenges of chatbot uses in sports rehabilitation.

      METHODS: We developed a model termed PanelGPT to assess ChatGPT's competency in addressing interdisciplinary topics through simulated panel discussions. Taking chatbot uses in sports rehabilitation as an example of an interdisciplinary topic, we prompted ChatGPT through PanelGPT to role-play a physiotherapist, psychologist, nutritionist, artificial intelligence expert, and athlete in a simulated panel discussion. During the simulation, we posed questions to the panel while ChatGPT acted as both the panelists for responses and the moderator for steering the discussion. We performed the simulation using ChatGPT-4 and evaluated the responses by referring to the literature and our human expertise.

      RESULTS: By tackling questions related to chatbot uses in sports rehabilitation with respect to patient education, physiotherapy, physiology, nutrition, and ethical considerations, responses from the ChatGPT-simulated panel discussion reasonably pointed to various benefits such as 24/7 support, personalized advice, automated tracking, and reminders. ChatGPT also correctly emphasized the importance of patient education, and identified challenges such as limited interaction modes, inaccuracies in emotion-related advice, assurance of data privacy and security, transparency in data handling, and fairness in model training. It also stressed that chatbots are to assist as a copilot, not to replace human health care professionals in the rehabilitation process.

      CONCLUSIONS: ChatGPT exhibits strong competency in addressing interdisciplinary inquiry by simulating multiple experts from complementary backgrounds, with significant implications in assisting medical education.
    mergedFrom:
      - source: pubmed
        pmid: "39042885"
        doi: 10.2196/51157
    finalDecision: exclude
  - title: "Appraisal of ChatGPT's Aptitude for Medical Education: Comparative Analysis With Third-Year Medical Students in a Pulmonology Examination."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Pulmonology examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Study compares ChatGPT performance with third-year medical students on a pulmonology examination."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Pulmonology written examination in undergraduate students is not workplace-based clinical education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.2196/52818
    pmid: "39042876"
    authors: Cherif H, Moussa C, Missaoui A, Salouage I, Mokaddem S, Dhahri B
    year: "2024"
    abstract: |-
      BACKGROUND: The rapid evolution of ChatGPT has generated substantial interest and led to extensive discussions in both public and academic domains, particularly in the context of medical education.

      OBJECTIVE: This study aimed to evaluate ChatGPT's performance in a pulmonology examination through a comparative analysis with that of third-year medical students.

      METHODS: In this cross-sectional study, we conducted a comparative analysis with 2 distinct groups. The first group comprised 244 third-year medical students who had previously taken our institution's 2020 pulmonology examination, which was conducted in French. The second group involved ChatGPT-3.5 in 2 separate sets of conversations: without contextualization (V1) and with contextualization (V2). In both V1 and V2, ChatGPT received the same set of questions administered to the students.

      RESULTS: V1 demonstrated exceptional proficiency in radiology, microbiology, and thoracic surgery, surpassing the majority of medical students in these domains. However, it faced challenges in pathology, pharmacology, and clinical pneumology. In contrast, V2 consistently delivered more accurate responses across various question categories, regardless of the specialization. ChatGPT exhibited suboptimal performance in multiple choice questions compared to medical students. V2 excelled in responding to structured open-ended questions. Both ChatGPT conversations, particularly V2, outperformed students in addressing questions of low and intermediate difficulty. Interestingly, students showcased enhanced proficiency when confronted with highly challenging questions. V1 fell short of passing the examination. Conversely, V2 successfully achieved examination success, outperforming 139 (62.1%) medical students.

      CONCLUSIONS: While ChatGPT has access to a comprehensive web-based data set, its performance closely mirrors that of an average medical student. Outcomes are influenced by question format, item complexity, and contextual nuances. The model faces challenges in medical contexts requiring information synthesis, advanced analytical aptitude, and clinical judgment, as well as in non-English language assessments and when confronted with data outside mainstream internet sources.
    mergedFrom:
      - source: pubmed
        pmid: "39042876"
        doi: 10.2196/52818
    finalDecision: exclude
  - title: "Effects of interacting with a large language model compared with a human coach on the clinical diagnostic process and outcomes among fourth-year medical students: study protocol for a prospective, randomised experiment using patient vignettes."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Study protocol, not original research with results"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Study protocol only, not an eligible completed original study/systematic review/meta-analysis/preprint."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.1136/bmjopen-2024-087469
    pmid: "39025818"
    authors: K&#xe4;mmer J, Hautz W, Krummrey G, Sauter T, Penders D, Birrenbach T, Bienefeld N
    year: "2024"
    abstract: |-
      INTRODUCTION: Versatile large language models (LLMs) have the potential to augment diagnostic decision-making by assisting diagnosticians, thanks to their ability to engage in open-ended, natural conversations and their comprehensive knowledge access. Yet the novelty of LLMs in diagnostic decision-making introduces uncertainties regarding their impact. Clinicians unfamiliar with the use of LLMs in their professional context may rely on general attitudes towards LLMs more broadly, potentially hindering thoughtful use and critical evaluation of their input, leading to either over-reliance and lack of critical thinking or an unwillingness to use LLMs as diagnostic aids. To address these concerns, this study examines the influence on the diagnostic process and outcomes of interacting with an LLM compared with a human coach, and of prior training vs no training for interacting with either of these 'coaches'. Our findings aim to illuminate the potential benefits and risks of employing artificial intelligence (AI) in diagnostic decision-making.

      METHODS AND ANALYSIS: We are conducting a prospective, randomised experiment with N=158 fourth-year medical students from Charité Medical School, Berlin, Germany. Participants are asked to diagnose patient vignettes after being assigned to either a human coach or ChatGPT and after either training or no training (both between-subject factors). We are specifically collecting data on the effects of using either of these 'coaches' and of additional training on information search, number of hypotheses entertained, diagnostic accuracy and confidence. Statistical methods will include linear mixed effects models. Exploratory analyses of the interaction patterns and attitudes towards AI will also generate more generalisable knowledge about the role of AI in medicine.

      ETHICS AND DISSEMINATION: The Bern Cantonal Ethics Committee considered the study exempt from full ethical review (BASEC No: Req-2023-01396). All methods will be conducted in accordance with relevant guidelines and regulations. Participation is voluntary and informed consent will be obtained. Results will be published in peer-reviewed scientific medical journals. Authorship will be determined according to the International Committee of Medical Journal Editors guidelines.
    mergedFrom:
      - source: pubmed
        pmid: "39025818"
        doi: 10.1136/bmjopen-2024-087469
    finalDecision: exclude
  - title: Development and evaluation of a large language model of ophthalmology in Chinese.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Develops and evaluates a Chinese ophthalmic LLM on board exams, evidence-based questions, and clinical vignettes; not about assessment, feedback, or analysis of clinical observation records in workplace-based settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1136/bjo-2023-324526
    pmid: "39019566"
    authors: Zheng C, Ye H, Guo J, Yang J, Fei P, Yuan Y, Huang D, Huang Y, Peng J, Xie X, Xie M, Zhao P, Chen L, Zhang M
    year: "2024"
    abstract: |-
      BACKGROUND: Large language models (LLMs), such as ChatGPT, have considerable implications for various medical applications. However, ChatGPT's training primarily draws from English-centric internet data and is not tailored explicitly to the medical domain. Thus, an ophthalmic LLM in Chinese is clinically essential for both healthcare providers and patients in mainland China.

      METHODS: We developed an LLM of ophthalmology (MOPH) using Chinese corpora and evaluated its performance in three clinical scenarios: ophthalmic board exams in Chinese, answering evidence-based medicine-oriented ophthalmic questions and diagnostic accuracy for clinical vignettes. Additionally, we compared MOPH's performance to that of human doctors.

      RESULTS: In the ophthalmic exam, MOPH's average score closely aligned with the mean score of trainees (64.7 (range 62-68) vs 66.2 (range 50-92), p=0.817), but achieving a score above 60 in all seven mock exams. In answering ophthalmic questions, MOPH demonstrated an adherence of 83.3% (25/30) of responses following Chinese guidelines (Likert scale 4-5). Only 6.7% (2/30, Likert scale 1-2) and 10% (3/30, Likert scale 3) of responses were rated as 'poor or very poor' or 'potentially misinterpretable inaccuracies' by reviewers. In diagnostic accuracy, although the rate of correct diagnosis by ophthalmologists was superior to that by MOPH (96.1% vs 81.1%, p>0.05), the difference was not statistically significant.

      CONCLUSION: This study demonstrated the promising performance of MOPH, a Chinese-specific ophthalmic LLM, in diverse clinical scenarios. MOPH has potential real-world applications in Chinese-language ophthalmology settings.
    mergedFrom:
      - source: pubmed
        pmid: "39019566"
        doi: 10.1136/bjo-2023-324526
    finalDecision: exclude
  - title: Large Language Models as Tools to Generate Radiology Board-Style Multiple-Choice Questions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records in workplace-based settings; focuses on LLM-generated radiology board-style MCQs for exam preparation."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on generating radiology board-style MCQs rather than workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1016/j.acra.2024.06.046
    pmid: "39013736"
    authors: Mistry N, Saeed H, Rafique S, Le T, Obaid H, Adams S
    year: "2024"
    abstract: |-
      RATIONALE AND OBJECTIVES: To determine the potential of large language models (LLMs) to be used as tools by radiology educators to create radiology board-style multiple choice questions (MCQs), answers, and rationales.

      METHODS: Two LLMs (Llama 2 and GPT-4) were used to develop 104 MCQs based on the American Board of Radiology exam blueprint. Two board-certified radiologists assessed each MCQ using a 10-point Likert scale across five criteria-clarity, relevance, suitability for a board exam based on level of difficulty, quality of distractors, and adequacy of rationale. For comparison, MCQs from prior American College of Radiology (ACR) Diagnostic Radiology In-Training (DXIT) exams were also assessed using these criteria, with radiologists blinded to the question source.

      RESULTS: Mean scores (±standard deviation) for clarity, relevance, suitability, quality of distractors, and adequacy of rationale were 8.7 (±1.4), 9.2 (±1.3), 9.0 (±1.2), 8.4 (±1.9), and 7.2 (±2.2), respectively, for Llama 2; 9.9 (±0.4), 9.9 (±0.5), 9.9 (±0.4), 9.8 (±0.5), and 9.9 (±0.3), respectively, for GPT-4; and 9.9 (±0.3), 9.9 (±0.2), 9.9 (±0.2), 9.9 (±0.4), and 9.8 (±0.6), respectively, for ACR DXIT items (p < 0.001 for Llama 2 vs. ACR DXIT across all criteria; no statistically significant difference for GPT-4 vs. ACR DXIT). The accuracy of model-generated answers was 69% for Llama 2 and 100% for GPT-4.

      CONCLUSION: A state-of-the art LLM such as GPT-4 may be used to develop radiology board-style MCQs and rationales to enhance exam preparation materials and expand exam banks, and may allow radiology educators to further use MCQs as teaching and learning tools.
    mergedFrom:
      - source: pubmed
        pmid: "39013736"
        doi: 10.1016/j.acra.2024.06.046
    finalDecision: exclude
  - title: "ChatGPT as an item calibration tool: Psychometric insights in a high-stakes examination."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Uses ChatGPT for psychometric calibration of standardized national residency selection exam items."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Item calibration for a high-stakes exam, not workplace-based assessment/feedback or analysis of observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.1080/0142159X.2024.2376205
    pmid: "39012024"
    authors: Pereira D, Mour&#xe3;o F, Ribeiro J, Costa P, Guimar&#xe3;es S, P&#xea;go J
    year: "2025"
    abstract: |-
      INTRODUCTION: ChatGPT has attracted a lot of interest worldwide for its versatility in a range of natural language tasks, including in the education and evaluation industry. It can automate time- and labor-intensive tasks with clear economic and efficiency gains.

      METHODS: This study evaluated the potential of ChatGPT to automate psychometric analysis of test questions from the 2020 Portuguese National Residency Selection Exam (PNA). ChatGPT was queried 100 times with the 150 MCQ from the exam. Using ChatGPT's responses, difficulty indices were calculated for each question based on the proportion of correct answers. The predicted difficulty levels were compared to the actual difficulty levels of the 2020 exam MCQ's using methods from classical test theory.

      RESULTS: ChatGPT's predicted item difficulty indices positively correlated with the actual item difficulties (r (148) = -0.372, p < .001), suggesting a general consistency between the real and the predicted values. There was also a moderate significant negative correlation between the difficulty index predicted by ChatGPT and the number of challenges (r (148) = -0.302, p < .001), highlighting ChatGPT's potential for identifying less problematic questions.

      CONCLUSION: These findings unveiled ChatGPT's potential as a tool for assessment development, proving its capability to predict the psychometric characteristics of high-stakes test items in automated item calibration without pre-testing in real-life scenarios.
    mergedFrom:
      - source: pubmed
        pmid: "39012024"
        doi: 10.1080/0142159X.2024.2376205
    finalDecision: exclude
  - title: "Evaluating the application of ChatGPT in China's residency training education: An exploratory study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Study evaluates ChatGPT on residency exam questions and generated virtual patient cases with resident satisfaction surveys; not about assessment, feedback, or analysis of clinical observation records in workplace-based settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1080/0142159X.2024.2377808
    pmid: "38994848"
    authors: Shang L, Li R, Xue M, Guo Q, Hou Y
    year: "2025"
    abstract: |-
      OBJECTIVE: The purpose of this study was to assess the utility of information generated by ChatGPT for residency education in China.

      METHODS: We designed a three-step survey to evaluate the performance of ChatGPT in China's residency training education including residency final examination questions, patient cases, and resident satisfaction scores. First, 204 questions from the residency final exam were input into ChatGPT's interface to obtain the percentage of correct answers. Next, ChatGPT was asked to generate 20 clinical cases, which were subsequently evaluated by three instructors using a pre-designed Likert scale with 5 points. The quality of the cases was assessed based on criteria including clarity, relevance, logicality, credibility, and comprehensiveness. Finally, interaction sessions between 31 third-year residents and ChatGPT were conducted. Residents' perceptions of ChatGPT's feedback were assessed using a Likert scale, focusing on aspects such as ease of use, accuracy and completeness of responses, and its effectiveness in enhancing understanding of medical knowledge.

      RESULTS: Our results showed ChatGPT-3.5 correctly answered 45.1% of exam questions. In the virtual patient cases, ChatGPT received mean ratings of 4.57 ± 0.50, 4.68 ± 0.47, 4.77 ± 0.46, 4.60 ± 0.53, and 3.95 ± 0.59 points for clarity, relevance, logicality, credibility, and comprehensiveness from clinical instructors, respectively. Among training residents, ChatGPT scored 4.48 ± 0.70, 4.00 ± 0.82 and 4.61 ± 0.50 points for ease of use, accuracy and completeness, and usefulness, respectively.

      CONCLUSION: Our findings demonstrate ChatGPT's immense potential for personalized Chinese medical education.
    mergedFrom:
      - source: pubmed
        pmid: "38994848"
        doi: 10.1080/0142159X.2024.2377808
    finalDecision: exclude
  - title: The conversational AI "ChatGPT" outperforms medical students on a physiology university examination.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: University physiology examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: University examination context, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    doi: 10.1152/advan.00181.2023
    pmid: "38991037"
    authors: Soulage C, Van Coppenolle F, Guebre-Egziabher F
    year: "2024"
    abstract: Artificial intelligence (AI) has gained massive interest with the public release of the conversational AI "ChatGPT," but it also has become a matter of concern for academia as it can easily be misused. We performed a quantitative evaluation of the performance of ChatGPT on a medical physiology university examination. Forty-one answers were obtained with ChatGPT and compared to the results of 24 students. The results of ChatGPT were significantly better than those of the students; the median (IQR) score was 75% (66-84%) for the AI compared to 56% (43-65%) for students (P < 0.001). The exam success rate was 100% for ChatGPT, whereas 29% (n = 7) of students failed. ChatGPT could promote plagiarism and intellectual laziness among students and could represent a new and easy way to cheat, especially when evaluations are performed online. Considering that these powerful AI tools are now freely available, scholars should take great care to construct assessments that really evaluate student reflection skills and prevent AI-assisted cheating.NEW & NOTEWORTHY The release of the conversational artificial intelligence (AI) ChatGPT has become a matter of concern for academia as it can easily be misused by students for cheating purposes. We performed a quantitative evaluation of the performance of ChatGPT on a medical physiology university examination and observed that ChatGPT outperforms medical students obtaining significantly better grades. Scholars should therefore take great care to construct assessments crafted to really evaluate the student reflection skills and prevent AI-assisted cheating.
    mergedFrom:
      - source: pubmed
        pmid: "38991037"
        doi: 10.1152/advan.00181.2023
    finalDecision: exclude
  - title: "Performance of GPT-3.5 and GPT-4 on standardized urology knowledge assessment items in the United States: a descriptive study."
    reviews:
      - decision: uncertain
        comment: GPT on urology assessment items; abstract needed to determine WBA relevance
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment/feedback of learners; evaluates GPT-3.5 and GPT-4 performance on standardized urology board exam-style items"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Tests LLM performance on exam items, not assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.3352/jeehp.2024.21.17
    pmid: "38977032"
    authors: Yudovich M, Makarova E, Hague C, Raman J
    year: "2024"
    abstract: |-
      PURPOSE: This study aimed to evaluate the performance of Chat Generative Pre-Trained Transformer (ChatGPT) with respect to standardized urology multiple-choice items in the United States.

      METHODS: In total, 700 multiple-choice urology board exam-style items were submitted to GPT-3.5 and GPT-4, and responses were recorded. Items were categorized based on topic and question complexity (recall, interpretation, and problem-solving). The accuracy of GPT-3.5 and GPT-4 was compared across item types in February 2024.

      RESULTS: GPT-4 answered 44.4% of items correctly compared to 30.9% for GPT-3.5 (P>0.0001). GPT-4 (vs. GPT-3.5) had higher accuracy with urologic oncology (43.8% vs. 33.9%, P=0.03), sexual medicine (44.3% vs. 27.8%, P=0.046), and pediatric urology (47.1% vs. 27.1%, P=0.012) items. Endourology (38.0% vs. 25.7%, P=0.15), reconstruction and trauma (29.0% vs. 21.0%, P=0.41), and neurourology (49.0% vs. 33.3%, P=0.11) items did not show significant differences in performance across versions. GPT-4 also outperformed GPT-3.5 with respect to recall (45.9% vs. 27.4%, P<0.00001), interpretation (45.6% vs. 31.5%, P=0.0005), and problem-solving (41.8% vs. 34.5%, P=0.56) type items. This difference was not significant for the higher-complexity items.

      CONCLUSION: s: ChatGPT performs relatively poorly on standardized multiple-choice urology board exam-style items, with GPT-4 outperforming GPT-3.5. The accuracy was below the proposed minimum passing standards for the American Board of Urology's Continuing Urologic Certification knowledge reinforcement activity (60%). As artificial intelligence progresses in complexity, ChatGPT may become more capable and accurate with respect to board examination items. For now, its responses should be scrutinized.
    mergedFrom:
      - source: pubmed
        pmid: "38977032"
        doi: 10.3352/jeehp.2024.21.17
    finalDecision: exclude
  - title: Comparing customized ChatGPT and pathology residents in histopathologic description and diagnosis of common diseases.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares AI diagnostic performance against pathology residents on histopathological images; not about assessment, feedback, or analysis of observation records for learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Image-based pathology benchmarking study without a workplace-based clinical education context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.1016/j.anndiagpath.2024.152359
    pmid: "38972166"
    authors: Apornvirat S, Thinpanja W, Damrongkiet K, Benjakul N, Laohawetwanit T
    year: "2024"
    abstract: This study aimed to evaluate and analyze the performance of a customized Chat Generative Pre-Trained Transformer (ChatGPT), known as GPT, against pathology residents in providing microscopic descriptions and diagnosing diseases from histopathological images. A dataset of representative photomicrographs from 70 diseases across 14 organ systems was analyzed by a customized version of ChatGPT-4 (GPT-4) and pathology residents. Two pathologists independently evaluated the microscopic descriptions and diagnoses using a predefined scoring system (0-4 for microscopic descriptions and 0-2 for pathological diagnoses), with higher scores indicating greater accuracy. Microscopic descriptions that received perfect scores, which included all relevant keywords and findings, were then presented to the standard version of ChatGPT to assess its diagnostic capabilities based on these descriptions. GPT-4 showed consistency in microscopic description and diagnosis scores across five rounds, accomplishing median scores of 50 % and 48.6 %, respectively. However, its performance was still inferior to junior and senior pathology residents (73.9 % and 93.9 % description scores and 63.9 % and 87.9 % diagnosis scores, respectively). When analyzing classic ChatGPT's understanding of microscopic descriptions provided by residents, it correctly diagnosed 35 (87.5 %) of cases from junior residents and 44 (68.8 %) from senior residents, given that the initial descriptions consisted of keywords and relevant findings. While GPT-4 can accurately interpret some histopathological images, its overall performance is currently inferior to that of pathology residents. However, ChatGPT's ability to accurately interpret and diagnose diseases from the descriptions provided by residents suggests that this technology could serve as a valuable support tool in pathology diagnostics.
    mergedFrom:
      - source: pubmed
        pmid: "38972166"
        doi: 10.1016/j.anndiagpath.2024.152359
    finalDecision: exclude
  - title: Using Natural Language Processing to Visualize Narrative Feedback in a Medical Student Performance Dashboard.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Traditional NLP for visualization, not generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses on conventional NLP, not generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    doi: 10.1097/ACM.0000000000005800
    pmid: "38967963"
    authors: Maimone C, Dolan B, Green M, Sanguino S, O'Brien C
    year: "2024"
    abstract: |-
      PROBLEM: Clinical competency committees rely on narrative feedback for important insight into learner performance, but reviewing comments can be time-consuming. Techniques such as natural language processing (NLP) could create efficiencies in narrative feedback review. In this study, the authors explored whether using NLP to create a visual dashboard of narrative feedback to preclerkship medical students would improve the competency review efficiency.

      APPROACH: Preclerkship competency review data collected at the Northwestern University Feinberg School of Medicine from 2014 to 2021 were used to identify relevant features of narrative data associated with review outcome (ready or not ready) and draft visual summary reports of the findings. A user needs analysis was held with experienced reviewers to better understand work processes in December 2019. Dashboards were designed based on this input to help reviewers efficiently navigate large amounts of narrative data. The dashboards displayed the model's prediction of the review outcome along with visualizations of how narratives in a student's portfolio compared with previous students' narratives. Excerpts of the most relevant comments were also provided. Six faculty reviewers who comprised the competency committee in spring 2023 were surveyed on the dashboard's utility.

      OUTCOMES: Reviewers found the predictive component of the dashboard most useful. Only 1 of 6 reviewers (17%) agreed that the dashboard improved process efficiency. However, 3 (50%) thought the visuals made them more confident in decisions about competence, and 3 (50%) thought they would use the visual summaries for future reviews. The outcomes highlight limitations of visualizing and summarizing narrative feedback in a comprehensive assessment system.

      NEXT STEPS: Future work will explore how to optimize the dashboards to meet reviewer needs. Ongoing advancements in large language models may facilitate these efforts. Opportunities to collaborate with other institutions to apply the model to an external context will also be sought.
    mergedFrom:
      - source: pubmed
        pmid: "38967963"
        doi: 10.1097/ACM.0000000000005800
      - source: scopus
        doi: 10.1097/ACM.0000000000005800
        scopusId: SCOPUS_ID:85198468541
    finalDecision: exclude
  - title: Evaluation of artificial intelligence-generated drug therapy communication skill competencies in medical education.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluating AI-generated communication skill competencies and case scenarios for medical education; not in a workplace-based clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates AI-generated communication competencies/cases, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.1111/bcp.16144
    pmid: "38953544"
    authors: Sridharan K, Sequeira R
    year: "2025"
    abstract: |-
      AIMS: This study compared three artificial intelligence (AI) platforms' potential to identify drug therapy communication competencies expected of a graduating medical doctor.

      METHODS: We presented three AI platforms, namely, Poe Assistant©, ChatGPT© and Google Bard©, with structured queries to generate communication skill competencies and case scenarios appropriate for graduating medical doctors. These case scenarios comprised 15 prototypical medical conditions that required drug prescriptions. Two authors independently evaluated the AI-enhanced clinical encounters, which integrated a diverse range of information to create patient-centred care plans. Through a consensus-based approach using a checklist, the communication components generated for each scenario were assessed. The instructions and warnings provided for each case scenario were evaluated by referencing the British National Formulary.

      RESULTS: AI platforms demonstrated overlap in competency domains generated, albeit with variations in wording. The domains of knowledge (basic and clinical pharmacology, prescribing, communication and drug safety) were unanimously recognized by all platforms. A broad consensus among Poe Assistant© and ChatGPT© on drug therapy-related communication issues specific to each case scenario was evident. The consensus primarily encompassed salutation, generic drug prescribed, treatment goals and follow-up schedules. Differences were observed in patient instruction clarity, listed side effects, warnings and patient empowerment. Google Bard did not provide guidance on patient communication issues.

      CONCLUSIONS: AI platforms recognized competencies with variations in how these were stated. Poe Assistant© and ChatGPT© exhibited alignment of communication issues. However, significant discrepancies were observed in specific skill components, indicating the necessity of human intervention to critically evaluate AI-generated outputs.
    mergedFrom:
      - source: pubmed
        pmid: "38953544"
        doi: 10.1111/bcp.16144
    finalDecision: exclude
  - title: Can ChatGPT generate practice question explanations for medical students, a new faculty teaching tool?
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT generating MCQ explanations for pre-clerkship curriculum; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on pre-clerkship MCQ learning support, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.1080/0142159X.2024.2363486
    pmid: "38900675"
    authors: Tong L, Wang J, Rapaka S, Garg P
    year: "2025"
    abstract: |-
      INTRODUCTION: Multiple-choice questions (MCQs) are frequently used for formative assessment in medical school but often lack sufficient answer explanations given time-restraints of faculty. Chat Generated Pre-trained Transformer (ChatGPT) has emerged as a potential student learning aid and faculty teaching tool. This study aims to evaluate ChatGPT's performance in answering and providing explanations for MCQs.

      METHOD: Ninety-four faculty-generated MCQs were collected from the pre-clerkship curriculum at a US medical school. ChatGPT's accuracy in answering MCQ's were tracked on first attempt without an answer prompt (Pass 1) and after being given a prompt for the correct answer (Pass 2). Explanations provided by ChatGPT were compared with faculty-generated explanations, and a 3-point evaluation scale was used to assess accuracy and thoroughness compared to faculty-generated answers.

      RESULTS: On first attempt, ChatGPT demonstrated a 75% accuracy in correctly answering faculty-generated MCQs. Among correctly answered questions, 66.4% of ChatGPT's explanations matched faculty explanations, and 89.1% captured some key aspects without providing inaccurate information. The amount of inaccurately generated explanations increases significantly if the questions was not answered correctly on the first pass (2.7% if correct on first pass vs. 34.6% if incorrect on first pass, p < 0.001).

      CONCLUSION: ChatGPT shows promise in assisting faculty and students with explanations for practice MCQ's but should be used with caution. Faculty should review explanations and supplement to ensure coverage of learning objectives. Students can benefit from ChatGPT for immediate feedback through explanations if ChatGPT answers the question correctly on the first try. If the question is answered incorrectly students should remain cautious of the explanation and seek clarification from instructors.
    mergedFrom:
      - source: pubmed
        pmid: "38900675"
        doi: 10.1080/0142159X.2024.2363486
    finalDecision: exclude
  - title: Exploring the Use of Natural Language Processing to Understand Emotions of Trainees and Faculty Regarding Entrustable Professional Activity Assessments.
    reviews:
      - decision: uncertain
        comment: NLP on EPA assessments; NLP could involve LLMs, EPAs are WBA
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP; title does not indicate generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP (validated sentiment lexicon-based word frequency analysis), not generative AI/LLMs"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP sentiment analysis, not generative AI or LLMs."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.4300/JGME-D-23-00526.1
    pmid: "38882409"
    authors: Johnson D, Chopra S, Bilgic E
    year: "2024"
    abstract: Background In medical education, artificial intelligence techniques such as natural language processing (NLP) are starting to be used to capture and analyze emotions through written text. Objective To explore the application of NLP techniques to understand resident and faculty emotions related to entrustable professional activity (EPA) assessments. Methods Open-ended text data from a survey on emotions toward EPA assessments were analyzed. Respondents were residents and faculty from pediatrics (Peds), general surgery (GS), and emergency medicine (EM), recruited for a larger emotions study in 2023. Participants wrote about their emotions related to receiving/completing EPA assessments. We analyzed the frequency of words rated as positive via a validated sentiment lexicon used in NLP studies. Specifically, we were interested if the count of positive words varied as a function of group membership (faculty, resident), specialty (Peds, GS, EM), gender (man, woman, nonbinary), or visible minority status (yes, no, omit). Results A total of 66 text responses (30 faculty, 36 residents) contained text data useful for sentiment analysis. We analyzed the difference in the count of words categorized as positive across group, specialty, gender, and being a visible minority. Specialty was the only category revealing significant differences via a bootstrapped Poisson regression model with GS responses containing fewer positive words than EM responses. Conclusions By analyzing text data to understand emotions of residents and faculty through an NLP approach, we identified differences in EPA assessment-related emotions of residents versus faculty, and differences across specialties.
    mergedFrom:
      - source: pubmed
        pmid: "38882409"
        doi: 10.4300/JGME-D-23-00526.1
    finalDecision: exclude
  - title: In-depth analysis of ChatGPT's performance based on specific signaling words and phrases in the question stem of 2377 USMLE step 1 style questions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates ChatGPT accuracy on USMLE Step 1 standardized exam questions"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Examines ChatGPT test-taking on USMLE questions, not workplace-based assessment, feedback generation, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.1038/s41598-024-63997-7
    pmid: "38866891"
    authors: Knoedler L, Knoedler S, Hoch C, Prantl L, Frank K, Soiderer L, Cotofana S, Dorafshar A, Schenck T, Vollbach F, Sofo G, Alfertshofer M
    year: "2024"
    abstract: ChatGPT has garnered attention as a multifaceted AI chatbot with potential applications in medicine. Despite intriguing preliminary findings in areas such as clinical management and patient education, there remains a substantial knowledge gap in comprehensively understanding the chances and limitations of ChatGPT's capabilities, especially in medical test-taking and education. A total of n = 2,729 USMLE Step 1 practice questions were extracted from the Amboss question bank. After excluding 352 image-based questions, a total of 2,377 text-based questions were further categorized and entered manually into ChatGPT, and its responses were recorded. ChatGPT's overall performance was analyzed based on question difficulty, category, and content with regards to specific signal words and phrases. ChatGPT achieved an overall accuracy rate of 55.8% in a total number of n = 2,377 USMLE Step 1 preparation questions obtained from the Amboss online question bank. It demonstrated a significant inverse correlation between question difficulty and performance with rs = -0.306; p < 0.001, maintaining comparable accuracy to the human user peer group across different levels of question difficulty. Notably, ChatGPT outperformed in serology-related questions (61.1% vs. 53.8%; p = 0.005) but struggled with ECG-related content (42.9% vs. 55.6%; p = 0.021). ChatGPT achieved statistically significant worse performances in pathophysiology-related question stems. (Signal phrase = "what is the most likely/probable cause"). ChatGPT performed consistent across various question categories and difficulty levels. These findings emphasize the need for further investigations to explore the potential and limitations of ChatGPT in medical examination and education.
    mergedFrom:
      - source: pubmed
        pmid: "38866891"
        doi: 10.1038/s41598-024-63997-7
    finalDecision: exclude
  - title: "ChatGPT prompts for generating multiple-choice questions in medical education and evidence on their validity: a literature review."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Title indicates general medical education test-item generation, not workplace-based clinical education."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; literature review on ChatGPT-generated MCQs for medical examinations, not WBA"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on generating medical exam MCQs rather than workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.1093/postmj/qgae065
    pmid: "38840505"
    authors: K&#x131;yak Y, Emekli E
    year: "2024"
    abstract: ChatGPT's role in creating multiple-choice questions (MCQs) is growing but the validity of these artificial-intelligence-generated questions is unclear. This literature review was conducted to address the urgent need for understanding the application of ChatGPT in generating MCQs for medical education. Following the database search and screening of 1920 studies, we found 23 relevant studies. We extracted the prompts for MCQ generation and assessed the validity evidence of MCQs. The findings showed that prompts varied, including referencing specific exam styles and adopting specific personas, which align with recommended prompt engineering tactics. The validity evidence covered various domains, showing mixed accuracy rates, with some studies indicating comparable quality to human-written questions, and others highlighting differences in difficulty and discrimination levels, alongside a significant reduction in question creation time. Despite its efficiency, we highlight the necessity of careful review and suggest a need for further research to optimize the use of ChatGPT in question generation. Main messages  Ensure high-quality outputs by utilizing well-designed prompts; medical educators should prioritize the use of detailed, clear ChatGPT prompts when generating MCQs. Avoid using ChatGPT-generated MCQs directly in examinations without thorough review to prevent inaccuracies and ensure relevance. Leverage ChatGPT's potential to streamline the test development process, enhancing efficiency without compromising quality.
    mergedFrom:
      - source: pubmed
        pmid: "38840505"
        doi: 10.1093/postmj/qgae065
    finalDecision: exclude
  - title: A Comparative Analysis of ChatGPT, ChatGPT-4, and Google Bard Performances at the Advanced Burn Life Support Exam.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on burn life support exam, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarking LLMs on Advanced Burn Life Support exam questions; not about using AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Burn life support exam question performance study, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.1093/jbcr/irae044
    pmid: "38833383"
    authors: Alessandri-Bonetti M, Liu H, Donovan J, Ziembicki J, Egro F
    year: "2024"
    abstract: Artificial intelligence and large language models (LLMs) have recently gained attention as promising tools in various healthcare domains, offering potential benefits in clinical decision-making, medical education, and research. The Advanced Burn Life Support (ABLS) program is a didactic initiative endorsed by the American Burn Association, aiming to provide knowledge on the immediate care of severely burn patients. The aim of the study was to compare the performance of three LLMs (ChatGPT-3.5, ChatGPT-4, and Google Bard) on the ABLS exam. The ABLS exam consists of 50 questions with 5 multiple-choice answers. The passing threshold is 80% of correct answers. The 3 LLMs were queried with the 50 questions included in the latest version of the ABLS exam, on July 18th, 2023. ChatGPT-3.5 scored 86% (43 out of 50), ChatGPT-4 scored 90% (45 out of 50), and Bard scored 70% (35 out of 50). No difference was measured between ChatGPT-3.5 and ChatGPT-4 (P = .538) and between ChatGPT-3.5 and Bard (P = .054), despite the borderline P-value. ChatGPT-4 performed significantly better than Bard (P = .012). Out of the 50 questions, 78% (n = 39) were direct questions, while 12% (n = 11) were presented as clinical scenarios. No difference in the rate of wrong answers was found based on the type of question for the 3 LLMs. ChatGPT-3.5 and ChatGPT-4 demonstrated high accuracy at the ABLS exam and outperformed Google Bard. However, the potential multiple applications of LLMs in emergency burn and trauma care necessitate appropriate surveillance and most likely should represent a tool to complement human cognition.
    mergedFrom:
      - source: pubmed
        pmid: "38833383"
        doi: 10.1093/jbcr/irae044
    finalDecision: exclude
  - title: The effects of supplemental instruction derived from peer leaders on student outcomes in undergraduate human anatomy.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on peer-led supplemental instruction in anatomy"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    doi: 10.1002/ase.2464
    pmid: "38825716"
    authors: Wilson R, Holman P, Dragan M, MacPherson R, Beaudette S
    year: "2024"
    abstract: Supplemental instruction (SI) confers student success, as represented by grades, knowledge retention, and student engagement. However, studies often report professional, not undergraduate, program findings. To measure these effects, students studying human anatomy at a university in Ontario, Canada, attended structured (peer-assisted) or unstructured (nonpeer-assisted) SI sessions and completed a pre-/post-survey. Fifty-eight learners (39 systems (SYS) and 19 musculoskeletal (MSK) anatomy) completed both surveys and had responses analyzed. Both cohorts, maintained initial perceptions across pre-/post-analyses (MSK p = 0.1376 and SYS p = 0.3521). Resource usage was similar across both cohorts with discrepancies in skeletal model and textbook use. No MSK learner ranked any lab resources as "not at all useful." MSK learners felt more prepared to write a graded assessment (p = 0.0269), whereas SYS learners did not (p = 0.0680). Stratification of learners in MSK and SYS revealed learners spending between 30 and 60 min in SI sessions during the study period had the highest grades compared to students who spent less than 30 (p = 0.0286) or more than 60 (p = 0.0286) min attending SI sessions, respectively. Most learners in MSK (89.4%) and SYS (66%) concluded that they preferred structured over unstructured SI. Sentiment/thematic analysis using a generative AI-driven large language model revealed learners held positive perceptions of SI, emphasizing structured learning, resources, personalized learning, and support offered as the most prevalent themes surrounding SI. Ultimately, this study provides evidence that supports SI for improving student outcomes related to perceived preparedness for completing assessments and preferred teaching/learning styles in undergraduate human anatomy.
    mergedFrom:
      - source: pubmed
        pmid: "38825716"
        doi: 10.1002/ase.2464
    finalDecision: exclude
  - title: Performance of generative pre-trained transformers (GPTs) in Certification Examination of the College of Family Physicians of Canada.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks GPT-3.5 and GPT-4 on CFPC certification exam questions; does not use generative AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Certification-exam question-answering study, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1136/fmch-2023-002626
    pmid: "38806403"
    authors: Mousavi M, Shafiee S, Harley J, Cheung J, Abbasgholizadeh Rahimi S
    year: "2024"
    abstract: |-
      INTRODUCTION: The application of large language models such as generative pre-trained transformers (GPTs) has been promising in medical education, and its performance has been tested for different medical exams. This study aims to assess the performance of GPTs in responding to a set of sample questions of short-answer management problems (SAMPs) from the certification exam of the College of Family Physicians of Canada (CFPC).

      METHOD: Between August 8th and 25th, 2023, we used GPT-3.5 and GPT-4 in five rounds to answer a sample of 77 SAMPs questions from the CFPC website. Two independent certified family physician reviewers scored AI-generated responses twice: first, according to the CFPC answer key (ie, CFPC score), and second, based on their knowledge and other references (ie, Reviews' score). An ordinal logistic generalised estimating equations (GEE) model was applied to analyse repeated measures across the five rounds.

      RESULT: According to the CFPC answer key, 607 (73.6%) lines of answers by GPT-3.5 and 691 (81%) by GPT-4 were deemed accurate. Reviewer's scoring suggested that about 84% of the lines of answers provided by GPT-3.5 and 93% of GPT-4 were correct. The GEE analysis confirmed that over five rounds, the likelihood of achieving a higher CFPC Score Percentage for GPT-4 was 2.31 times more than GPT-3.5 (OR: 2.31; 95% CI: 1.53 to 3.47; p<0.001). Similarly, the Reviewers' Score percentage for responses provided by GPT-4 over 5 rounds were 2.23 times more likely to exceed those of GPT-3.5 (OR: 2.23; 95% CI: 1.22 to 4.06; p=0.009). Running the GPTs after a one week interval, regeneration of the prompt or using or not using the prompt did not significantly change the CFPC score percentage.

      CONCLUSION: In our study, we used GPT-3.5 and GPT-4 to answer complex, open-ended sample questions of the CFPC exam and showed that more than 70% of the answers were accurate, and GPT-4 outperformed GPT-3.5 in responding to the questions. Large language models such as GPTs seem promising for assisting candidates of the CFPC exam by providing potential answers. However, their use for family medicine education and exam preparation needs further studies.
    mergedFrom:
      - source: pubmed
        pmid: "38806403"
        doi: 10.1136/fmch-2023-002626
    finalDecision: exclude
  - title: "ChatGPT as a teaching tool: Preparing pathology residents for board examination with AI-generated digestive system pathology tests."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Board examination preparation context, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: AI-generated MCQs for pathology board exam preparation; not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Board-exam test generation study, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1093/ajcp/aqae062
    pmid: "38795049"
    authors: Laohawetwanit T, Apornvirat S, Kantasiripitak C
    year: "2024"
    abstract: |-
      OBJECTIVES: To evaluate the effectiveness of ChatGPT 4 in generating multiple-choice questions (MCQs) with explanations for pathology board examinations, specifically for digestive system pathology.

      METHODS: The customized ChatGPT 4 model was developed for MCQ and explanation generation. Expert pathologists evaluated content accuracy and relevance. These MCQs were then administered to pathology residents, followed by an analysis focusing on question difficulty, accuracy, item discrimination, and internal consistency.

      RESULTS: The customized ChatGPT 4 generated 80 MCQs covering various gastrointestinal and hepatobiliary topics. While the MCQs demonstrated moderate to high agreement in evaluation parameters such as content accuracy, clinical relevance, and overall quality, there were issues in cognitive level and distractor quality. The explanations were generally acceptable. Involving 9 residents with a median experience of 1 year, the average score was 57.4 (71.8%). Pairwise comparisons revealed a significant difference in performance between each year group (P < .01). The test analysis showed moderate difficulty, effective item discrimination (index = 0.15), and good internal consistency (Cronbach's α = 0.74).

      CONCLUSIONS: ChatGPT 4 demonstrated significant potential as a supplementary educational tool in medical education, especially in generating MCQs with explanations similar to those seen in board examinations. While artificial intelligence-generated content was of high quality, it necessitated refinement and expert review.
    mergedFrom:
      - source: pubmed
        pmid: "38795049"
        doi: 10.1093/ajcp/aqae062
    finalDecision: exclude
  - title: Performance of ChatGPT on American Board of Surgery In-Training Examination Preparation Questions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates ChatGPT on SCORE self-assessment MCQs, not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: In-training exam preparation question performance is not workplace-based assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.1016/j.jss.2024.04.060
    pmid: "38788470"
    authors: Tran C, Chang J, Sherman S, De Andrade J
    year: "2024"
    abstract: |-
      INTRODUCTION: Chat Generative Pretrained Transformer (ChatGPT) is a large language model capable of generating human-like text. This study sought to evaluate ChatGPT's performance on Surgical Council on Resident Education (SCORE) self-assessment questions.

      METHODS: General surgery multiple choice questions were randomly selected from the SCORE question bank. ChatGPT (GPT-3.5, April-May 2023) evaluated questions and responses were recorded.

      RESULTS: ChatGPT correctly answered 123 of 200 questions (62%). ChatGPT scored lowest on biliary (2/8 questions correct, 25%), surgical critical care (3/10, 30%), general abdomen (1/3, 33%), and pancreas (1/3, 33%) topics. ChatGPT scored higher on biostatistics (4/4 correct, 100%), fluid/electrolytes/acid-base (4/4, 100%), and small intestine (8/9, 89%) questions. ChatGPT answered questions with thorough and structured support for its answers. It scored 56% on ethics questions and provided coherent explanations regarding end-of-life discussions, communication with coworkers and patients, and informed consent. For many questions answered incorrectly, ChatGPT provided cogent, yet factually incorrect descriptions, including anatomy and steps of operations. In two instances, it gave a correct explanation but chose the wrong answer. It did not answer two questions, stating it needed additional information to determine the next best step in treatment.

      CONCLUSIONS: ChatGPT answered 62% of SCORE questions correctly. It performed better at questions requiring standard recall but struggled with higher-level questions that required complex clinical decision making, despite providing detailed responses behind its rationale. Due to its mediocre performance on this question set and sometimes confidently-worded, yet factually inaccurate responses, caution should be used when interpreting ChatGPT's answers to general surgery questions.
    mergedFrom:
      - source: pubmed
        pmid: "38788470"
        doi: 10.1016/j.jss.2024.04.060
    finalDecision: exclude
  - title: The Performance of ChatGPT-4V in Interpreting Images and Tables in the Japanese Medical Licensing Exam.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Study evaluates ChatGPT-4V performance on a national medical licensing examination (JMLE). This is a standardized exam-based evaluation with no workplace-based or clinical education setting component."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:53.576000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Conducted in a licensing exam context, not a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:01.509000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: National licensing exam, not workplace-based."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.2196/54283
    pmid: "38787024"
    authors: Takagi S, Koda M, Watari T
    year: "2024"
    mergedFrom:
      - source: pubmed
        pmid: "38787024"
        doi: 10.2196/54283
    finalDecision: exclude
  - title: "Crafting medical MCQs with generative AI: A how-to guide on leveraging ChatGPT."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: 'How-to guide' indicates this is not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article, systematic review, meta-analysis, or preprint (how-to guide)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.3205/zma001675
    pmid: "38779693"
    authors: Stadler M, Horrer A, Fischer M
    year: "2024"
    abstract: As medical educators grapple with the consistent demand for high-quality assessments, the integration of artificial intelligence presents a novel solution. This how-to article delves into the mechanics of employing ChatGPT for generating Multiple Choice Questions (MCQs) within the medical curriculum. Focusing on the intricacies of prompt engineering, we elucidate the steps and considerations imperative for achieving targeted, high-fidelity results. The article presents varying outcomes based on different prompt structures, highlighting the AI's adaptability in producing questions of distinct complexities. While emphasizing the transformative potential of ChatGPT, we also spotlight challenges, including the AI's occasional "hallucination", underscoring the importance of rigorous review. This guide aims to furnish educators with the know-how to integrate AI into their assessment creation process, heralding a new era in medical education tools.
    mergedFrom:
      - source: pubmed
        pmid: "38779693"
        doi: 10.3205/zma001675
    finalDecision: exclude
  - title: "Enhancing clinical skills in pediatric trainees: a comparative study of ChatGPT-assisted and traditional teaching methods."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on teaching methods rather than assessment/feedback or analysis of clinical observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: include
        comment: Generative AI (ChatGPT) used in training pediatric trainees with Mini-CEX as the assessment tool in a clinical education setting; presents data on educational impact (Mini-CEX scores, trainee satisfaction). Meets all PCC criteria.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about using generative AI for assessment, feedback generation, or analysis of observation records. ChatGPT was used as a teaching/learning tool (generating case vignettes, orienting students to clinical scenarios), while assessments (Mini-CEX, theoretical exams) were conducted entirely by human evaluators. The study measures educational impact of ChatGPT-assisted instruction but does not use AI for assessment or feedback purposes."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 03:36:32.323000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: ChatGPT is used as a teaching aid, but the study does not evaluate AI-based assessment/feedback or analysis of clinical observation records/documentation."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 03:36:33.873000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: ChatGPT used as teaching/learning tool, not for assessment or feedback. Mini-CEX was conducted by human evaluators."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1186/s12909-024-05565-1
    pmid: "38778332"
    authors: Ba H, Zhang L, Yi Z
    year: "2024"
    abstract: |-
      BACKGROUND: As artificial intelligence (AI) increasingly integrates into medical education, its specific impact on the development of clinical skills among pediatric trainees needs detailed investigation. Pediatric training presents unique challenges which AI tools like ChatGPT may be well-suited to address.

      OBJECTIVE: This study evaluates the effectiveness of ChatGPT-assisted instruction versus traditional teaching methods on pediatric trainees' clinical skills performance.

      METHODS: A cohort of pediatric trainees (n = 77) was randomly assigned to two groups; one underwent ChatGPT-assisted training, while the other received conventional instruction over a period of two weeks. Performance was assessed using theoretical knowledge exams and Mini-Clinical Evaluation Exercises (Mini-CEX), with particular attention to professional conduct, clinical judgment, patient communication, and overall clinical skills. Trainees' acceptance and satisfaction with the AI-assisted method were evaluated through a structured survey.

      RESULTS: Both groups performed similarly in theoretical exams, indicating no significant difference (p > 0.05). However, the ChatGPT-assisted group showed a statistically significant improvement in Mini-CEX scores (p < 0.05), particularly in patient communication and clinical judgment. The AI-teaching approach received positive feedback from the majority of trainees, highlighting the perceived benefits in interactive learning and skill acquisition.

      CONCLUSION: ChatGPT-assisted instruction did not affect theoretical knowledge acquisition but did enhance practical clinical skills among pediatric trainees. The positive reception of the AI-based method suggests that it has the potential to complement and augment traditional training approaches in pediatric education. These promising results warrant further exploration into the broader applications of AI in medical education scenarios.
    mergedFrom:
      - source: pubmed
        pmid: "38778332"
        doi: 10.1186/s12909-024-05565-1
      - source: scopus
        doi: 10.1186/s12909-024-05565-1
        scopusId: SCOPUS_ID:85193741423
    finalDecision: exclude
  - title: "AI Versus MD: Evaluating the surgical decision-making accuracy of ChatGPT-4."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT-4 surgical decision-making against residents/attendings on clinical scenarios; not about using AI for assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares AI and clinicians on scenario decision-making, not assessment/feedback processes based on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1016/j.surg.2024.04.003
    pmid: "38769038"
    authors: Palenzuela D, Mullen J, Phitayakorn R
    year: "2024"
    abstract: |-
      BACKGROUND: ChatGPT-4 is a large language model with possible applications to surgery education The aim of this study was to investigate the accuracy of ChatGPT-4's surgical decision-making compared with general surgery residents and attending surgeons.

      METHODS: Five clinical scenarios were created from actual patient data based on common general surgery diagnoses. Scripts were developed to sequentially provide clinical information and ask decision-making questions. Responses to the prompts were scored based on a standardized rubric for a total of 50 points. Each clinical scenario was run through Chat GPT-4 and sent electronically to all general surgery residents and attendings at a single institution. Scores were compared using Wilcoxon rank sum tests.

      RESULTS: On average, ChatGPT-4 scored 39.6 points (79.2%, standard deviation ± 0.89 points). A total of five junior residents, 12 senior residents, and five attendings completed the clinical scenarios (resident response rate = 15.9%; attending response rate = 13.8%). On average, the junior residents scored a total of 33.4 (66.8%, standard deviation ± 3.29), senior residents 38.0 (76.0%, standard deviation ± 4.75), and attendings 38.8 (77.6%, standard deviation ± 5.45). ChatGPT-4 scored significantly better than junior residents (P = .009) but was not significantly different from senior residents or attendings. ChatGPT-4 was significantly better than junior residents at identifying the correct operation to perform (P = .0182) and recommending additional workup for postoperative complications (P = .012).

      CONCLUSION: ChatGPT-4 performed superior to junior residents and equivalent to senior residents and attendings when faced with surgical patient scenarios. Large language models, such as ChatGPT, may have the potential to be an educational resource for junior residents to develop surgical decision-making skills.
    mergedFrom:
      - source: pubmed
        pmid: "38769038"
        doi: 10.1016/j.surg.2024.04.003
    finalDecision: exclude
  - title: "Artificial intelligence in dermatology: GPT-3.5-Turbo demonstrates strengths and limitations in residency exams."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on residency exams, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. This letter to the editor tests GPT-3.5-Turbo on dermatology residency exam questions (DermQBank). No workplace-based assessment or learner feedback."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17T07:34:40.555Z
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates ChatGPT on residency exam question-bank performance, not a workplace-based or clinical education assessment setting."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17T07:34:43.172Z
      - decision: exclude
        comment: "Exclusion criterion 4: Residency exam question-bank performance, not workplace-based."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.5070/D330163300
    pmid: "38762869"
    authors: Haynes D, Lewis W, Jariwala N
    year: "2024"
    mergedFrom:
      - source: pubmed
        pmid: "38762869"
        doi: 10.5070/D330163300
    finalDecision: exclude
  - title: "The Surgical Clerkship in the COVID Era: A Natural Language Processing and Thematic Analysis."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Uses traditional NLP and thematic analysis, not generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses NLP/thematic analysis without clear generative AI/LLM focus"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
    doi: 10.1016/j.jss.2024.04.016
    pmid: "38759331"
    authors: Howell T, Ladowski J, Nash A, Rhodin K, Tracy E, Migaly J, Bloom D, Vatsaas C
    year: "2024"
    abstract: |-
      INTRODUCTION: Responses to COVID-19 within medical education prompted significant changes to the surgical clerkship. We analyzed the changes in medical student end of course feedback before and after the COVID-19 outbreak.

      METHODS: Postclerkship surveys from 2017 to 2022 were analyzed including both Likert scale data and free text, excluding the COVID outbreak year 2019-2020. Likert scale questions were compared between pre-COVID (2017-2019) and COVID-era cohorts (2020-2022) with the Mann-Whitney U-test. Free-text comments were analyzed using both thematic analysis and natural language processing including sentiment, word and phrase frequency, and topic modeling.

      RESULTS: Of the 483 medical students surveyed from 2017 to 2022, 297 responded (61% response rate) to the included end of clerkship surveys. Most medical students rated the clerkship above average or excellent with no significant difference between the pre-COVID and COVID-era cohorts (70.4% Versus 64.8%, P = 0.35). Perception of grading expectations did significantly differ, 51% of pre-COVID students reported clerkship grading standards were almost always clear compared to 27.5% of COVID-era students (P = 0.01). Pre-COVID cohorts more frequently mentioned learning and feedback while COVID-era cohorts more frequently mentioned case, attending, and expectation. Natural language processing topic modeling and formal thematic analysis identified similar themes: team, time, autonomy, and expectations.

      CONCLUSIONS: COVID-19 presented many challenges to undergraduate medical education. Despite many changes, there was no significant difference in clerkship satisfaction ratings. Unexpectedly, the greater freedom and autonomy of asynchronous lectures and choice of cases became a highlight of the new curriculum. Future research should investigate if there are similar associations nationally with a multi-institutional study.
    mergedFrom:
      - source: pubmed
        pmid: "38759331"
        doi: 10.1016/j.jss.2024.04.016
    finalDecision: exclude
  - title: "Expert assessment of ChatGPT's ability to generate illness scripts: an evaluative study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; focuses on AI-generated educational content (illness scripts) rather than assessment or feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates illness-script generation for teaching, not assessment/feedback or analysis of workplace clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.1186/s12909-024-05534-8
    pmid: "38750546"
    authors: Yanagita Y, Yokokawa D, Fukuzawa F, Uchida S, Uehara T, Ikusaka M
    year: "2024"
    abstract: |-
      BACKGROUND: An illness script is a specific script format geared to represent patient-oriented clinical knowledge organized around enabling conditions, faults (i.e., pathophysiological process), and consequences. Generative artificial intelligence (AI) stands out as an educational aid in continuing medical education. The effortless creation of a typical illness script by generative AI could help the comprehension of key features of diseases and increase diagnostic accuracy. No systematic summary of specific examples of illness scripts has been reported since illness scripts are unique to each physician.

      OBJECTIVE: This study investigated whether generative AI can generate illness scripts.

      METHODS: We utilized ChatGPT-4, a generative AI, to create illness scripts for 184 diseases based on the diseases and conditions integral to the National Model Core Curriculum in Japan for undergraduate medical education (2022 revised edition) and primary care specialist training in Japan. Three physicians applied a three-tier grading scale: "A" denotes that the content of each disease's illness script proves sufficient for training medical students, "B" denotes that it is partially lacking but acceptable, and "C" denotes that it is deficient in multiple respects.

      RESULTS: By leveraging ChatGPT-4, we successfully generated each component of the illness script for 184 diseases without any omission. The illness scripts received "A," "B," and "C" ratings of 56.0% (103/184), 28.3% (52/184), and 15.8% (29/184), respectively.

      CONCLUSION: Useful illness scripts were seamlessly and instantaneously created using ChatGPT-4 by employing prompts appropriate for medical students. The technology-driven illness script is a valuable tool for introducing medical students to key features of diseases.
    mergedFrom:
      - source: pubmed
        pmid: "38750546"
        doi: 10.1186/s12909-024-05534-8
    finalDecision: exclude
  - title: "Surgeons vs ChatGPT: Assessment and Feedback Performance Based on Real Surgical Scenarios."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: include
        comment: Generative AI (ChatGPT) providing feedback on real surgical scenarios, assessed by residents and experts for utility and quality. Relevant to workplace-based assessment/feedback with validity evidence.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Feedback was evaluated on written surgical scenarios (simulated cases), not workplace-based clinical assessments."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
      - decision: include
        comment: Uses ChatGPT 4.0 to identify surgical errors and provide feedback on real laparoscopic cholecystectomy scenarios derived from recorded workplace procedures. Involves surgical residents as assessors of feedback quality. Presents data on feedback quality, clinical usefulness rates, and error detection rates. Applies generative AI to assessment and feedback generation based on real clinical/workplace surgical performance.
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:54.870000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:03.048000+00:00
    doi: 10.1016/j.jsurg.2024.03.012
    pmid: "38749814"
    authors: Jarry Trujillo C, Vela Ulloa J, Escalona Vivas G, Grasset Escobar E, Villagr&#xe1;n Guti&#xe9;rrez I, Achurra Tirado P, Varas Cohen J
    year: "2024"
    abstract: |-
      INTRODUCTION: Artificial intelligence tools are being progressively integrated into medicine and surgical education. Large language models, such as ChatGPT, could provide relevant feedback aimed at improving surgical skills. The purpose of this study is to assess ChatGPT´s ability to provide feedback based on surgical scenarios.

      METHODS: Surgical situations were transformed into texts using a neutral narrative. Texts were evaluated by ChatGPT 4.0 and 3 surgeons (A, B, C) after a brief instruction was delivered: identify errors and provide feedback accordingly. Surgical residents were provided with each of the situations and feedback obtained during the first stage, as written by each surgeon and ChatGPT, and were asked to assess the utility of feedback (FCUR) and its quality (FQ). As control measurement, an Education-Expert (EE) and a Clinical-Expert (CE) were asked to assess FCUR and FQ.

      RESULTS: Regarding residents' evaluations, 96.43% of times, outputs provided by ChatGPT were considered useful, comparable to what surgeons' B and C obtained. Assessing FQ, ChatGPT and all surgeons received similar scores. Regarding EE's assessment, ChatGPT obtained a significantly higher FQ score when compared to surgeons A and B (p = 0.019; p = 0.033) with a median score of 8 vs. 7 and 7.5, respectively; and no difference respect surgeon C (score of 8; p = 0.2). Regarding CE´s assessment, surgeon B obtained the highest FQ score while ChatGPT received scores comparable to that of surgeons A and C. When participants were asked to identify the source of the feedback, residents, CE, and EE perceived ChatGPT's outputs as human-provided in 33.9%, 28.5%, and 14.3% of cases, respectively.

      CONCLUSION: When given brief written surgical situations, ChatGPT was able to identify errors with a detection rate comparable to that of experienced surgeons and to generate feedback that was considered useful for skill improvement in a surgical context performing as well as surgical instructors across assessments made by general surgery residents, an experienced surgeon, and a nonsurgeon feedback expert.
    mergedFrom:
      - source: pubmed
        pmid: "38749814"
        doi: 10.1016/j.jsurg.2024.03.012
    finalDecision: include
  - title: "ChatGPT's performance in dentistry and allergyimmunology assessments: a comparative study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on standardized assessments, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Tests ChatGPT on dentistry and allergy/immunology exam questions; benchmarks AI on licensing exam MCQs, not about assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT on licensing/self-assessment MCQs rather than workplace-based assessment or feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.61872/sdj-2024-06-01
    pmid: "38726506"
    authors: Fuchs A, Trachsel T, Weiger R, Eggmann F
    year: "2023"
    abstract: Large language models (LLMs) such as ChatGPT have potential applications in healthcare, including dentistry. Priming, the practice of providing LLMs with initial, relevant information, is an approach to improve their output quality. This study aimed to evaluate the performance of ChatGPT 3 and ChatGPT 4 on self-assessment questions for dentistry, through the Swiss Federal Licensing Examination in Dental Medicine (SFLEDM), and allergy and clinical immunology, through the European Examination in Allergy and Clinical Immunology (EEAACI). The second objective was to assess the impact of priming on ChatGPT's performance. The SFLEDM and EEAACI multiple-choice questions from the University of Bern's Institute for Medical Education platform were administered to both ChatGPT versions, with and without priming. Performance was analyzed based on correct responses. The statistical analysis included Wilcoxon rank sum tests (alpha=0.05). The average accuracy rates in the SFLEDM and EEAACI assessments were 63.3% and 79.3%, respectively. Both ChatGPT versions performed better on EEAACI than SFLEDM, with ChatGPT 4 outperforming ChatGPT 3 across all tests. ChatGPT 3's performance exhibited a significant improvement with priming for both EEAACI (p=0.017) and SFLEDM (p=0.024) assessments. For ChatGPT 4, the priming effect was significant only in the SFLEDM assessment (p=0.038). The performance disparity between SFLEDM and EEAACI assessments underscores ChatGPT's varying proficiency across different medical domains, likely tied to the nature and amount of training data available in each field. Priming can be a tool for enhancing output, especially in earlier LLMs. Advancements from ChatGPT 3 to 4 highlight the rapid developments in LLM technology. Yet, their use in critical fields such as healthcare must remain cautious owing to LLMs' inherent limitations and risks.
    mergedFrom:
      - source: pubmed
        pmid: "38726506"
        doi: 10.61872/sdj-2024-06-01
    finalDecision: exclude
  - title: "The Revival of Essay-Type Questions in Medical Education: Harnessing Artificial Intelligence and Machine Learning."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. Uses ChatGPT to grade formative essay examinations in a classroom/dental materials course setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Essay grading in classroom education, not assessment/feedback tied to clinical observation records in workplace settings."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.29271/jcpsp.2024.05.595
    pmid: "38720222"
    authors: Shamim M, Zaidi S, Rehman A
    year: "2024"
    abstract: |-
      OBJECTIVE: To analyse and compare the assessment and grading of human-written and machine-written formative essays.

      STUDY DESIGN: Quasi-experimental, qualitative cross-sectional study. Place and Duration of the Study: Department of Science of Dental Materials, Hamdard College of Medicine & Dentistry, Hamdard University, Karachi, from February to April 2023.

      METHODOLOGY: Ten short formative essays of final-year dental students were manually assessed and graded. These essays were then graded using ChatGPT version 3.5. The chatbot responses and prompts were recorded and matched with manually graded essays. Qualitative analysis of the chatbot responses was then performed.

      RESULTS: Four different prompts were given to the artificial intelligence (AI) driven platform of ChatGPT to grade the summative essays. These were the chatbot's initial responses without grading, the chatbot's response to grading against criteria, the chatbot's response to criteria-wise grading, and the chatbot's response to questions for the difference in grading. Based on the results, four innovative ways of using AI and machine learning (ML) have been proposed for medical educators: Automated grading, content analysis, plagiarism detection, and formative assessment. ChatGPT provided a comprehensive report with feedback on writing skills, as opposed to manual grading of essays.

      CONCLUSION: The chatbot's responses were fascinating and thought-provoking. AI and ML technologies can potentially supplement human grading in the assessment of essays. Medical educators need to embrace AI and ML technology to enhance the standards and quality of medical education, particularly when assessing long and short essay-type questions. Further empirical research and evaluation are needed to confirm their effectiveness.

      KEY WORDS: Machine learning, Artificial intelligence, Essays, ChatGPT, Formative assessment.
    mergedFrom:
      - source: pubmed
        pmid: "38720222"
        doi: 10.29271/jcpsp.2024.05.595
    finalDecision: exclude
  - title: "Medical knowledge of ChatGPT in public health, infectious diseases, COVID-19 pandemic, and vaccines: multiple choice questions examination based performance."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT performance on MCQ examinations, standardized exam-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Evaluates ChatGPT's own medical knowledge via MCQs, not about using AI for learner assessment or feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT performance on MCQs rather than workplace-based assessment, feedback, or analysis of observation documentation."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.3389/fpubh.2024.1360597
    pmid: "38711764"
    authors: Meo S, Alotaibi M, Meo M, Meo M, Hamid M
    year: "2024"
    abstract: |-
      BACKGROUND: At the beginning of the year 2023, the Chatbot Generative Pre-Trained Transformer (ChatGPT) gained remarkable attention from the public. There is a great discussion about ChatGPT and its knowledge in medical sciences, however, literature is lacking to evaluate the ChatGPT knowledge level in public health. Therefore, this study investigates the knowledge of ChatGPT in public health, infectious diseases, the COVID-19 pandemic, and its vaccines.

      METHODS: Multiple Choice Questions (MCQs) bank was established. The question's contents were reviewed and confirmed that the questions were appropriate to the contents. The MCQs were based on the case scenario, with four sub-stems, with a single correct answer. From the MCQs bank, 60 MCQs we selected, 30 MCQs were from public health, and infectious diseases topics, 17 MCQs were from the COVID-19 pandemic, and 13 MCQs were on COVID-19 vaccines. Each MCQ was manually entered, and tasks were given to determine the knowledge level of ChatGPT on MCQs.

      RESULTS: Out of a total of 60 MCQs in public health, infectious diseases, the COVID-19 pandemic, and vaccines, ChatGPT attempted all the MCQs and obtained 17/30 (56.66%) marks in public health, infectious diseases, 15/17 (88.23%) in COVID-19, and 12/13 (92.30%) marks in COVID-19 vaccines MCQs, with an overall score of 44/60 (73.33%). The observed results of the correct answers in each section were significantly higher (p = 0.001). The ChatGPT obtained satisfactory grades in all three domains of public health, infectious diseases, and COVID-19 pandemic-allied examination.

      CONCLUSION: ChatGPT has satisfactory knowledge of public health, infectious diseases, the COVID-19 pandemic, and its vaccines. In future, ChatGPT may assist medical educators, academicians, and healthcare professionals in providing a better understanding of public health, infectious diseases, the COVID-19 pandemic, and vaccines.
    mergedFrom:
      - source: pubmed
        pmid: "38711764"
        doi: 10.3389/fpubh.2024.1360597
    finalDecision: exclude
  - title: Performance of ChatGPT on ACC/SCAI Interventional Cardiology Certification Simulation Exam.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is explicitly a simulation exam"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. This study evaluates ChatGPT 4.0 performance on a certification simulation exam (60 MCQs from ACC/SCAI CathSAP platform). Purely a board exam question performance study with no workplace-based assessment component."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17T07:34:39.189Z
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is a certification simulation exam (non-workplace setting), not workplace-based clinical assessment."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17T07:34:41.875Z
      - decision: exclude
        comment: "Exclusion criterion 4: Certification exam MCQs (CathSAP), not workplace-based. 'Simulation exam' here means practice exam, not clinical simulation (OSCE); Cr.3 is not appropriate."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1016/j.jcin.2024.03.012
    pmid: "38703151"
    authors: Alexandrou M, Mahtani A, Rempakos A, Mutlu D, Al Ogaili A, Gill G, Sharma A, Prasad A, Mastrodemos O, Sandoval Y, Brilakis E
    year: "2024"
    mergedFrom:
      - source: pubmed
        pmid: "38703151"
        doi: 10.1016/j.jcin.2024.03.012
    finalDecision: exclude
  - title: "Exploring the Performance of ChatGPT-4 in the Taiwan Audiologist Qualification Examination: Preliminary Observational Study Highlighting the Potential of AI Chatbots in Hearing Care."
    reviews:
      - decision: uncertain
        comment: ChatGPT-4 on audiologist qualification exam; health professional assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT-4 on audiologist qualification exam; not about using AI for assessment or feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Qualification-exam performance study, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.2196/55595
    pmid: "38693697"
    authors: Wang S, Mo C, Chen Y, Dai X, Wang H, Shen X
    year: "2024"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) chatbots, such as ChatGPT-4, have shown immense potential for application across various aspects of medicine, including medical education, clinical practice, and research.

      OBJECTIVE: This study aimed to evaluate the performance of ChatGPT-4 in the 2023 Taiwan Audiologist Qualification Examination, thereby preliminarily exploring the potential utility of AI chatbots in the fields of audiology and hearing care services.

      METHODS: ChatGPT-4 was tasked to provide answers and reasoning for the 2023 Taiwan Audiologist Qualification Examination. The examination encompassed six subjects: (1) basic auditory science, (2) behavioral audiology, (3) electrophysiological audiology, (4) principles and practice of hearing devices, (5) health and rehabilitation of the auditory and balance systems, and (6) auditory and speech communication disorders (including professional ethics). Each subject included 50 multiple-choice questions, with the exception of behavioral audiology, which had 49 questions, amounting to a total of 299 questions.

      RESULTS: The correct answer rates across the 6 subjects were as follows: 88% for basic auditory science, 63% for behavioral audiology, 58% for electrophysiological audiology, 72% for principles and practice of hearing devices, 80% for health and rehabilitation of the auditory and balance systems, and 86% for auditory and speech communication disorders (including professional ethics). The overall accuracy rate for the 299 questions was 75%, which surpasses the examination's passing criteria of an average 60% accuracy rate across all subjects. A comprehensive review of ChatGPT-4's responses indicated that incorrect answers were predominantly due to information errors.

      CONCLUSIONS: ChatGPT-4 demonstrated a robust performance in the Taiwan Audiologist Qualification Examination, showcasing effective logical reasoning skills. Our results suggest that with enhanced information accuracy, ChatGPT-4's performance could be further improved. This study indicates significant potential for the application of AI chatbots in audiology and hearing care services.
    mergedFrom:
      - source: pubmed
        pmid: "38693697"
        doi: 10.2196/55595
    finalDecision: exclude
  - title: "Exploring the Performance of ChatGPT Versions 3.5, 4, and 4 With Vision in the Chilean Medical Licensing Examination: Observational Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT versions on Chilean medical licensing exam (EUNACOM) MCQs; not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Licensing exam performance study, not assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.2196/55048
    pmid: "38686550"
    authors: Rojas M, Rojas M, Burgess V, Toro-P&#xe9;rez J, Salehi S
    year: "2024"
    abstract: |-
      BACKGROUND: The deployment of OpenAI's ChatGPT-3.5 and its subsequent versions, ChatGPT-4 and ChatGPT-4 With Vision (4V; also known as "GPT-4 Turbo With Vision"), has notably influenced the medical field. Having demonstrated remarkable performance in medical examinations globally, these models show potential for educational applications. However, their effectiveness in non-English contexts, particularly in Chile's medical licensing examinations-a critical step for medical practitioners in Chile-is less explored. This gap highlights the need to evaluate ChatGPT's adaptability to diverse linguistic and cultural contexts.

      OBJECTIVE: This study aims to evaluate the performance of ChatGPT versions 3.5, 4, and 4V in the EUNACOM (Examen Único Nacional de Conocimientos de Medicina), a major medical examination in Chile.

      METHODS: Three official practice drills (540 questions) from the University of Chile, mirroring the EUNACOM's structure and difficulty, were used to test ChatGPT versions 3.5, 4, and 4V. The 3 ChatGPT versions were provided 3 attempts for each drill. Responses to questions during each attempt were systematically categorized and analyzed to assess their accuracy rate.

      RESULTS: All versions of ChatGPT passed the EUNACOM drills. Specifically, versions 4 and 4V outperformed version 3.5, achieving average accuracy rates of 79.32% and 78.83%, respectively, compared to 57.53% for version 3.5 (P<.001). Version 4V, however, did not outperform version 4 (P=.73), despite the additional visual capabilities. We also evaluated ChatGPT's performance in different medical areas of the EUNACOM and found that versions 4 and 4V consistently outperformed version 3.5. Across the different medical areas, version 3.5 displayed the highest accuracy in psychiatry (69.84%), while versions 4 and 4V achieved the highest accuracy in surgery (90.00% and 86.11%, respectively). Versions 3.5 and 4 had the lowest performance in internal medicine (52.74% and 75.62%, respectively), while version 4V had the lowest performance in public health (74.07%).

      CONCLUSIONS: This study reveals ChatGPT's ability to pass the EUNACOM, with distinct proficiencies across versions 3.5, 4, and 4V. Notably, advancements in artificial intelligence (AI) have not significantly led to enhancements in performance on image-based questions. The variations in proficiency across medical fields suggest the need for more nuanced AI training. Additionally, the study underscores the importance of exploring innovative approaches to using AI to augment human cognition and enhance the learning process. Such advancements have the potential to significantly influence medical education, fostering not only knowledge acquisition but also the development of critical thinking and problem-solving skills among health care professionals.
    mergedFrom:
      - source: pubmed
        pmid: "38686550"
        doi: 10.2196/55048
    finalDecision: exclude
  - title: Comparison of ChatGPT version 3.5 & 4 for utility in respiratory medicine education using clinical case scenarios.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT performance on clinical case scenarios for respiratory medicine education and decision support; not about assessment, feedback, or analysis of clinical observation records in workplace settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1016/j.resmer.2024.101091
    pmid: "38657295"
    authors: Balasanjeevi G, Surapaneni K
    year: "2024"
    abstract: Integration of ChatGPT in Respiratory medicine presents a promising avenue for enhancing clinical practice and pedagogical approaches. This study compares the performance of ChatGPT version 3.5 and 4 in respiratory medicine, emphasizing its potential in clinical decision support and medical education using clinical cases. Results indicate moderate performance highlighting limitations in handling complex case scenarios. Compared to ChatGPT 3.5, version 4 showed greater promise as a pedagogical tool, providing interactive learning experiences. While serving as a preliminary decision support tool clinically, caution is advised, stressing the need for ongoing validation. Future research should refine its clinical capabilities for optimal integration into medical education and practice.
    mergedFrom:
      - source: pubmed
        pmid: "38657295"
        doi: 10.1016/j.resmer.2024.101091
    finalDecision: exclude
  - title: "Performance of GPT-4 on the American College of Radiology In-training Examination: Evaluating Accuracy, Model Drift, and Fine-tuning."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks GPT-4 performance on radiology in-training exam questions; does not use generative AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam-performance study; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1016/j.acra.2024.04.006
    pmid: "38653599"
    authors: Payne D, Purohit K, Borrero W, Chung K, Hao M, Mpoy M, Jin M, Prasanna P, Hill V
    year: "2024"
    abstract: |-
      RATIONALE AND OBJECTIVES: In our study, we evaluate GPT-4's performance on the American College of Radiology (ACR) 2022 Diagnostic Radiology In-Training Examination (DXIT). We perform multiple experiments across time points to assess for model drift, as well as after fine-tuning to assess for differences in accuracy.

      MATERIALS AND METHODS: Questions were sequentially input into GPT-4 with a standardized prompt. Each answer was recorded and overall accuracy was calculated, as was logic-adjusted accuracy, and accuracy on image-based questions. This experiment was repeated several months later to assess for model drift, then again after the performance of fine-tuning to assess for changes in GPT's performance.

      RESULTS: GPT-4 achieved 58.5% overall accuracy, lower than the PGY-3 average (61.9%) but higher than the PGY-2 average (52.8%). Adjusted accuracy was 52.8%. GPT-4 showed significantly higher (p = 0.012) confidence for correct answers (87.1%) compared to incorrect (84.0%). Performance on image-based questions was significantly poorer (p < 0.001) at 45.4% compared to text-only questions (80.0%), with adjusted accuracy for image-based questions of 36.4%. When the questions were repeated, GPT-4 chose a different answer 25.5% of the time and there was no change in accuracy. Fine-tuning did not improve accuracy.

      CONCLUSION: GPT-4 performed between PGY-2 and PGY-3 levels on the 2022 DXIT, significantly poorer on image-based questions, and with large variability in answer choices across time points. Exploratory experiments in fine-tuning did not improve performance. This study underscores the potential and risks of using minimally-prompted general AI models in interpreting radiologic images as a diagnostic tool. Implementers of general AI radiology systems should exercise caution given the possibility of spurious yet confident responses.
    mergedFrom:
      - source: pubmed
        pmid: "38653599"
        doi: 10.1016/j.acra.2024.04.006
    finalDecision: exclude
  - title: "Learning with AI Language Models: Guidelines for the Development and Scoring of Medical Questions for Higher Education."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Guideline-oriented article type rather than original research/systematic review/meta-analysis/preprint."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 5: No data on validity, reliability, acceptability, or educational impact; proposes theoretical frameworks and scoring rubrics without empirical evidence"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Guidance/opinion-style article, not original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1007/s10916-024-02069-9
    pmid: "38652327"
    authors: Moulin T
    year: "2024"
    abstract: In medical and biomedical education, traditional teaching methods often struggle to engage students and promote critical thinking. The use of AI language models has the potential to transform teaching and learning practices by offering an innovative, active learning approach that promotes intellectual curiosity and deeper understanding. To effectively integrate AI language models into biomedical education, it is essential for educators to understand the benefits and limitations of these tools and how they can be employed to achieve high-level learning outcomes.This article explores the use of AI language models in biomedical education, focusing on their application in both classroom teaching and learning assignments. Using the SOLO taxonomy as a framework, I discuss strategies for designing questions that challenge students to exercise critical thinking and problem-solving skills, even when assisted by AI models. Additionally, I propose a scoring rubric for evaluating student performance when collaborating with AI language models, ensuring a comprehensive assessment of their learning outcomes.AI language models offer a promising opportunity for enhancing student engagement and promoting active learning in the biomedical field. Understanding the potential use of these technologies allows educators to create learning experiences that are fit for their students' needs, encouraging intellectual curiosity and a deeper understanding of complex subjects. The application of these tools will be fundamental to provide more effective and engaging learning experiences for students in the future.
    mergedFrom:
      - source: pubmed
        pmid: "38652327"
        doi: 10.1007/s10916-024-02069-9
    finalDecision: exclude
  - title: "Artificial intelligence and medical education: application in classroom instruction and student assessment using a pharmacology & therapeutics case study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Classroom instruction and student assessment, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Explicitly classroom-based context, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    doi: 10.1186/s12909-024-05365-7
    pmid: "38649959"
    authors: Sridharan K, Sequeira R
    year: "2024"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) tools are designed to create or generate content from their trained parameters using an online conversational interface. AI has opened new avenues in redefining the role boundaries of teachers and learners and has the potential to impact the teaching-learning process.

      METHODS: In this descriptive proof-of- concept cross-sectional study we have explored the application of three generative AI tools on drug treatment of hypertension theme to generate: (1) specific learning outcomes (SLOs); (2) test items (MCQs- A type and case cluster; SAQs; OSPE); (3) test standard-setting parameters for medical students.

      RESULTS: Analysis of AI-generated output showed profound homology but divergence in quality and responsiveness to refining search queries. The SLOs identified key domains of antihypertensive pharmacology and therapeutics relevant to stages of the medical program, stated with appropriate action verbs as per Bloom's taxonomy. Test items often had clinical vignettes aligned with the key domain stated in search queries. Some test items related to A-type MCQs had construction defects, multiple correct answers, and dubious appropriateness to the learner's stage. ChatGPT generated explanations for test items, this enhancing usefulness to support self-study by learners. Integrated case-cluster items had focused clinical case description vignettes, integration across disciplines, and targeted higher levels of competencies. The response of AI tools on standard-setting varied. Individual questions for each SAQ clinical scenario were mostly open-ended. The AI-generated OSPE test items were appropriate for the learner's stage and identified relevant pharmacotherapeutic issues. The model answers supplied for both SAQs and OSPEs can aid course instructors in planning classroom lessons, identifying suitable instructional methods, establishing rubrics for grading, and for learners as a study guide. Key lessons learnt for improving AI-generated test item quality are outlined.

      CONCLUSIONS: AI tools are useful adjuncts to plan instructional methods, identify themes for test blueprinting, generate test items, and guide test standard-setting appropriate to learners' stage in the medical program. However, experts need to review the content validity of AI-generated output. We expect AIs to influence the medical education landscape to empower learners, and to align competencies with curriculum implementation. AI literacy is an essential competency for health professionals.
    mergedFrom:
      - source: pubmed
        pmid: "38649959"
        doi: 10.1186/s12909-024-05365-7
    finalDecision: exclude
  - title: GPT-4/4V's performance on the Japanese National Medical Licensing Examination.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Tests GPT-4 performance on licensing exam, not about assessment or feedback of human learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting (licensing examination context)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1080/0142159X.2024.2342545
    pmid: "38648547"
    authors: Kawahara T, Sumi Y
    year: "2025"
    abstract: |-
      BACKGROUND: Recent advances in Artificial Intelligence (AI) are changing the medical world, and AI will likely replace many of the actions performed by medical professionals. The overall clinical ability of the AI has been evaluated by its ability to answer a text-based national medical examination. This study uniquely assesses the performance of Open AI's ChatGPT against all Japanese National Medical Licensing Examination (NMLE), including images, illustrations, and pictures.

      METHODS: We obtained the questions of the past six years of the NMLE (112th to 117th) from the Japanese Ministry of Health, Labour and Welfare website. We converted them to JavaScript Object Notation (JSON) format. We created an application programming interface (API) to output correct answers using GPT-4 for questions without images and GPT4-V(ision) or GPT4 console for questions with images.

      RESULTS: The percentage of image questions was 723/2400 (30.1%) over the past six years. In all years, GPT-4/4V exceeded the minimum score the examinee should score. In total, over the six years, the percentage of correct answers for basic medical knowledge questions was 665/905 (73.5%); for clinical knowledge questions, 1143/1531 (74.7%); and for image questions 497/723 (68.7%), respectively.

      CONCLUSIONS: Regarding medical knowledge, GPT-4/4V met the minimum criteria regardless of whether the questions included images, illustrations, and pictures. Our study sheds light on the potential utility of AI in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "38648547"
        doi: 10.1080/0142159X.2024.2342545
    finalDecision: exclude
  - title: Effectiveness of AI-powered Chatbots in responding to orthopaedic postgraduate exam questions-an observational study.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment/feedback of learners; evaluates AI chatbot performance on postgraduate orthopaedic MCQs"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam-question answering study without WBA assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1007/s00264-024-06182-9
    pmid: "38619565"
    authors: Vaishya R, Iyengar K, Patralekh M, Botchu R, Shirodkar K, Jain V, Vaish A, Scarlat M
    year: "2024"
    abstract: |-
      PURPOSE: This study analyses the performance and proficiency of the three Artificial Intelligence (AI) generative chatbots (ChatGPT-3.5, ChatGPT-4.0, Bard Google AI®) and in answering the Multiple Choice Questions (MCQs) of postgraduate (PG) level orthopaedic qualifying examinations.

      METHODS: A series of 120 mock Single Best Answer' (SBA) MCQs with four possible options named A, B, C and D as answers on various musculoskeletal (MSK) conditions covering Trauma and Orthopaedic curricula were compiled. A standardised text prompt was used to generate and feed ChatGPT (both 3.5 and 4.0 versions) and Google Bard programs, which were then statistically analysed.

      RESULTS: Significant differences were found between responses from Chat GPT 3.5 with Chat GPT 4.0 (Chi square = 27.2, P < 0.001) and on comparing both Chat GPT 3.5 (Chi square = 63.852, P < 0.001) with Chat GPT 4.0 (Chi square = 44.246, P < 0.001) with. Bard Google AI® had 100% efficiency and was significantly more efficient than both Chat GPT 3.5 with Chat GPT 4.0 (p < 0.0001).

      CONCLUSION: The results demonstrate the variable potential of the different AI generative chatbots (Chat GPT 3.5, Chat GPT 4.0 and Bard Google) in their ability to answer the MCQ of PG-level orthopaedic qualifying examinations. Bard Google AI® has shown superior performance than both ChatGPT versions, underlining the potential of such large language processing models in processing and applying orthopaedic subspecialty knowledge at a PG level.
    mergedFrom:
      - source: pubmed
        pmid: "38619565"
        doi: 10.1007/s00264-024-06182-9
    finalDecision: exclude
  - title: Application of ChatGPT-based blended medical teaching in clinical education of hepatobiliary surgery.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; ChatGPT is used as a teaching tool in hepatobiliary surgery education, with outcomes measured by traditional exams."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT-enhanced teaching outcomes, not assessment/feedback or analysis of clinical observation records in workplace-based assessment."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1080/0142159X.2024.2339412
    pmid: "38614458"
    authors: Wu C, Chen L, Han M, Li Z, Yang N, Yu C
    year: "2025"
    abstract: |-
      OBJECTIVE: This study evaluates the effectiveness of incorporating the Chat Generative Pre-trained Transformer (ChatGPT) into the clinical teaching of hepatobiliary surgery for undergraduate medical students.

      MATERIALS AND METHODS: A group of 61 medical undergraduates from the Affiliated Hospital of Guizhou Medical University, undergoing hepatobiliary surgery training, were randomly assigned to either an experimental group (31 students) using ChatGPT-based blended teaching or a control group (30 students) with traditional teaching methods. The evaluation metrics included final exam scores, teaching satisfaction, and teaching effectiveness ratings, analyzed using SPSS 26.0 (SPSS Inc., Chicago, IL) with t-tests and χ2 tests.

      RESULTS: The experimental group significantly outperformed the control group in final exam theoretical scores (86.44 ± 5.59 vs. 77.86 ± 4.16, p < .001) and clinical skills scores (83.84 ± 6.13 vs. 79.12 ± 4.27, p = .001). Additionally, the experimental group reported higher teaching satisfaction (17.23 ± 1.33) and self-evaluation of teaching effectiveness (9.14 ± 0.54) compared to the control group (15.38 ± 1.5 and 8.46 ± 0.70, respectively, p < .001).

      CONCLUSIONS: The integration of ChatGPT into hepatobiliary surgery education significantly enhances theoretical knowledge, clinical skills, and overall satisfaction among medical undergraduates, suggesting a beneficial impact on their educational development.
    mergedFrom:
      - source: pubmed
        pmid: "38614458"
        doi: 10.1080/0142159X.2024.2339412
    finalDecision: exclude
  - title: Performance of ChatGPT on Chinese Master's Degree Entrance Examination in Clinical Medicine.
    reviews:
      - decision: uncertain
        comment: ChatGPT on entrance exam; may relate to clinical assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Entrance examination context, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; evaluates LLM performance on standardized entrance examination questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT performance on an entrance examination, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1371/journal.pone.0301702
    pmid: "38573944"
    authors: Li K, Bu Z, Shahjalal M, He B, Zhuang Z, Li C, Liu J, Wang B, Liu Z
    year: "2024"
    abstract: |-
      BACKGROUND: ChatGPT is a large language model designed to generate responses based on a contextual understanding of user queries and requests. This study utilised the entrance examination for the Master of Clinical Medicine in Traditional Chinese Medicine to assesses the reliability and practicality of ChatGPT within the domain of medical education.

      METHODS: We selected 330 single and multiple-choice questions from the 2021 and 2022 Chinese Master of Clinical Medicine comprehensive examinations, which did not include any images or tables. To ensure the test's accuracy and authenticity, we preserved the original format of the query and alternative test texts, without any modifications or explanations.

      RESULTS: Both ChatGPT3.5 and GPT-4 attained average scores surpassing the admission threshold. Noteworthy is that ChatGPT achieved the highest score in the Medical Humanities section, boasting a correct rate of 93.75%. However, it is worth noting that ChatGPT3.5 exhibited the lowest accuracy percentage of 37.5% in the Pathology division, while GPT-4 also displayed a relatively lower correctness percentage of 60.23% in the Biochemistry section. An analysis of sub-questions revealed that ChatGPT demonstrates superior performance in handling single-choice questions but performs poorly in multiple-choice questions.

      CONCLUSION: ChatGPT exhibits a degree of medical knowledge and the capacity to aid in diagnosing and treating diseases. Nevertheless, enhancements are warranted to address its accuracy and reliability limitations. Imperatively, rigorous evaluation and oversight must accompany its utilization, accompanied by proactive measures to surmount prevailing constraints.
    mergedFrom:
      - source: pubmed
        pmid: "38573944"
        doi: 10.1371/journal.pone.0301702
    finalDecision: exclude
  - title: Performance of GPT-4 Vision on kidney pathology exam questions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; evaluates LLM performance on kidney pathology exam questions compared to nephrology trainees."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:05.662000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates GPT-4V on pathology exam questions, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:12.068000+00:00
    doi: 10.1093/ajcp/aqae030
    pmid: "38567909"
    authors: Miao J, Thongprayoon C, Cheungpasitporn W, Cornell L
    year: "2024"
    abstract: |-
      OBJECTIVES: ChatGPT (OpenAI, San Francisco, CA) has shown impressive results across various medical examinations, but its performance in kidney pathology is not yet established. This study evaluated proficiencies of GPT-4 Vision (GPT-4V), an updated version of the platform with the ability to analyze images, on kidney pathology questions and compared its responses with those of nephrology trainees.

      METHODS: Thirty-nine questions (19 text-based questions and 20 with various kidney biopsy images) designed specifically for the training of nephrology fellows were employed.

      RESULTS: GPT-4V displayed comparable accuracy rates in the first and second runs (67% and 72%, respectively, P = .50). The aggregated accuracy, however-particularly, the consistent accuracy-of GPT-4V was lower than that of trainees (72% and 67% vs 79%). Both GPT-4V and trainees displayed comparable accuracy in responding to image-based and text-only questions (55% vs 79% and 81% vs 78%, P = .11 and P = .67, respectively). The consistent accuracy in image-based, directly asked questions for GPT-4V was 29%, much lower than its 88% consistency on text-only, directly asked questions (P = .02). In contrast, trainees maintained similar accuracy in directly asked image-based and text-based questions (80% vs 77%, P = .65). Although the aggregated accuracy for correctly interpreting images was 69%, the consistent accuracy across both runs was only 39%. The accuracy of GPT-4V in answering questions with correct image interpretation was significantly higher than for questions with incorrect image interpretation (100% vs 0% and 100% vs 33% for the first and second runs, P = .001 and P = .02, respectively).

      CONCLUSIONS: The performance of GPT-4V in handling kidney pathology questions, especially those including images, is limited. There is a notable need for enhancement in GPT-4V proficiency in interpreting images.
    mergedFrom:
      - source: pubmed
        pmid: "38567909"
        doi: 10.1093/ajcp/aqae030
    finalDecision: exclude
  - title: Evaluating The Role of ChatGPT as a Study Aid in Medical Education in Surgery.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: ChatGPT as study aid in surgery education, not about assessment or feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; evaluates ChatGPT as a study aid for knowledge acquisition during surgery clerkship"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT as a study aid using quizzes, not workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.1016/j.jsurg.2024.01.014
    pmid: "38556438"
    authors: Araji T, Brooks A
    year: "2024"
    abstract: |-
      OBJECTIVE: Our aim was to assess how ChatGPT compares to Google search in assisting medical students during their surgery clerkships.

      DESIGN: We conducted a crossover study where participants were asked to complete 2 standardized assessments on different general surgery topics before and after they used either Google search or ChatGPT.

      SETTING: The study was conducted at the Perelman School of Medicine at the University of Pennsylvania (PSOM) in Philadelphia, Pennsylvania.

      PARTICIPANTS: 19 third-year medical students participated in our study.

      RESULTS: The baseline (preintervention) performance of participants on both quizzes did not differ between the Google search and ChatGPT groups (p = 0.728). Students overall performed better postintervention and the difference in test scores was statistically significant for both the Google group (p < 0.001) and the ChatGPT group (p = 0.01). The mean percent increase in test scores pre- and postintervention was higher in the Google group at 11% vs. 10% in the ChatGPT group, but this difference was not statistically significant (p = 0.87). Similarly, there was no statistically significant difference in postintervention scores on both assessments between the 2 groups (p = 0.508). Postassessment surveys revealed that all students (100%) have known about ChatGPT before, and 47% have previously used it for various purposes. On a scale of 1 to 10 with 1 being the lowest and 10 being the highest, the feasibility of ChatGPT and its usefulness in finding answers were rated as 8.4 and 6.6 on average, respectively. When asked to rate the likelihood of using ChatGPT in their surgery rotation, the answers ranged between 1 and 3 ("Unlikely" 47%), 4 to 6 ("intermediate" 26%), and 7 to 10 ("likely" 26%).

      CONCLUSION: Our results show that even though ChatGPT was comparable to Google search in finding answers pertaining to surgery questions, many students were reluctant to use ChatGPT for learning purposes during their surgery clerkship.
    mergedFrom:
      - source: pubmed
        pmid: "38556438"
        doi: 10.1016/j.jsurg.2024.01.014
    finalDecision: exclude
  - title: "Large language models for generating medical examinations: systematic review."
    reviews:
      - decision: uncertain
        comment: LLMs used for assessment generation; 'medical examinations' likely refers to written exams but scope may include WBA-related items; retain for abstract screening
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:20.264000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on generating examinations rather than workplace-based clinical education settings."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:21.767000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; systematic review on LLM-generated MCQs for medical examinations, not WBA"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Systematic review on LLM-generated medical exam questions, not workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.1186/s12909-024-05239-y
    pmid: "38553693"
    authors: Artsi Y, Sorin V, Konen E, Glicksberg B, Nadkarni G, Klang E
    year: "2024"
    abstract: |-
      BACKGROUND: Writing multiple choice questions (MCQs) for the purpose of medical exams is challenging. It requires extensive medical knowledge, time and effort from medical educators. This systematic review focuses on the application of large language models (LLMs) in generating medical MCQs.

      METHODS: The authors searched for studies published up to November 2023. Search terms focused on LLMs generated MCQs for medical examinations. Non-English, out of year range and studies not focusing on AI generated multiple-choice questions were excluded. MEDLINE was used as a search database. Risk of bias was evaluated using a tailored QUADAS-2 tool.

      RESULTS: Overall, eight studies published between April 2023 and October 2023 were included. Six studies used Chat-GPT 3.5, while two employed GPT 4. Five studies showed that LLMs can produce competent questions valid for medical exams. Three studies used LLMs to write medical questions but did not evaluate the validity of the questions. One study conducted a comparative analysis of different models. One other study compared LLM-generated questions with those written by humans. All studies presented faulty questions that were deemed inappropriate for medical exams. Some questions required additional modifications in order to qualify.

      CONCLUSIONS: LLMs can be used to write MCQs for medical examinations. However, their limitations cannot be ignored. Further study in this field is essential and more conclusive evidence is needed. Until then, LLMs may serve as a supplementary tool for writing medical examinations. 2 studies were at high risk of bias. The study followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines.
    mergedFrom:
      - source: pubmed
        pmid: "38553693"
        doi: 10.1186/s12909-024-05239-y
    finalDecision: exclude
  - title: ChatGPT Earns American Board Certification in Hand Surgery.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT performance on a hand surgery board examination; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted on board self-assessment exam questions only."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.1016/j.hansur.2024.101688
    pmid: "38552842"
    authors: Ghanem D, Nassar J, El Bachour J, Hanna T
    year: "2024"
    abstract: |-
      PURPOSE: Artificial Intelligence (AI), and specifically ChatGPT, has shown potential in healthcare, yet its performance in specialized medical examinations such as the Orthopaedic Surgery In-Training Examination and European Board Hand Surgery diploma has been inconsistent. This study aims to evaluate the capability of ChatGPT-4 to pass the American Hand Surgery Certifying Examination.

      METHODS: ChatGPT-4 was tested on the 2019 American Society for Surgery of the Hand (ASSH) Self-Assessment Exam. All 200 questions available online (https://onlinecme.assh.org) were retrieved. All media-containing questions were flagged and carefully reviewed. Eight media-containing questions were excluded as they either relied purely on videos or could not be rationalized from the presented information. Descriptive statistics were used to summarize the performance (% correct) of ChatGPT-4. The ASSH report was used to compare ChatGPT-4's performance to that of the 322 physicians who completed the 2019 ASSH self-assessment.

      RESULTS: ChatGPT-4 answered 192 questions with an overall score of 61.98%. Performance on media-containing questions was 55.56%, while on non-media questions it was 65.83%, with no statistical difference in performance based on media inclusion. Despite scoring below the average physician's performance, ChatGPT-4 outperformed in the 'vascular' section with 81.82%. Its performance was lower in the 'bone and joint' (48.54%) and 'neuromuscular' (56.25%) sections.

      CONCLUSIONS: ChatGPT-4 achieved a good overall score of 61.98%. This AI language model demonstrates significant capability in processing and answering specialized medical examination questions, albeit with room for improvement in areas requiring complex clinical judgment and nuanced interpretation. ChatGPT-4's proficiency is influenced by the structure and language of the examination, with no replacement for the depth of trained medical specialists. This study underscores the supportive role of AI in medical education and clinical decision-making while highlighting the current limitations in nuanced fields such as hand surgery.
    mergedFrom:
      - source: pubmed
        pmid: "38552842"
        doi: 10.1016/j.hansur.2024.101688
    finalDecision: exclude
  - title: "Efficacy of ChatGPT in solving attitude, ethics, and communication case scenario used for competency-based medical education in India: A case study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT solving case scenarios for competency-based education, exam/case-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates ChatGPT responses to AETCOM case scenarios in a classroom/curriculum context"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Uses case scenarios in competency-based education without a workplace-based assessment component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.4103/jehp.jehp_625_23
    scopusId: SCOPUS_ID:85187492029
    authors: Roy A.D., Das D., Mondal H.
    year: "2024"
    abstract: "BACKGROUND: Competency-based medical education (CBME) is a method of medical training that focuses on developing learners' competencies rather than simply assessing their knowledge and skills. Attitude, ethics, and communication (AETCOM) are important components of CBME, and the use of artificial intelligence (AI) tools such as ChatGPT for CBME has not been studied. Hence, we aimed to assess the capability of ChatGPT in solving AETCOM case scenarios used for CBME in India. MATERIALS AND METHODS: A total of 11 case scenarios were developed based on the AETCOM competencies. The scenarios were presented to ChatGPT, and the responses generated by ChatGPT were evaluated by three independent experts by awarding score ranging from 0 to 5. The scores were compared with a predefined score of 2.5 (50% accuracy) and 4 (80% accuracy) of a one-sample median test. Scores among the three raters were compared by the Kruskal–Wallis H test. The inter-rater reliability of the evaluations was assessed using the intraclass correlation coefficient (ICC). RESULTS: The mean score of solution provided by ChatGPT was 3.88 ± 0.47 (out of 5), indicating an accuracy of approximately 78%. The responses evaluated by three raters were similar (Kruskal–Wallis H P value 0.51), and the ICC value was 0.796, which indicates a relatively high level of agreement among the raters. CONCLUSION: ChatGPT shows moderate capability in solving AETCOM case scenarios used for CBME in India. The inter-rater reliability of the evaluations suggests that ChatGPT's responses were consistent and reliable. Further studies are needed to explore the potential of ChatGPT and other AI tools in CBME and to determine the optimal use of these tools in medical education."
    mergedFrom:
      - source: pubmed
        pmid: "38545309"
        doi: 10.4103/jehp.jehp_625_23
      - source: scopus
        doi: 10.4103/jehp.jehp_625_23
        scopusId: SCOPUS_ID:85187492029
    finalDecision: exclude
  - title: Comparing the performance of artificial intelligence learning models to medical students in solving histology and embryology multiple choice questions.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on histology and embryology MCQs, preclinical classroom-based, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses on AI learning models without clear generative AI/LLM use"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    doi: 10.1016/j.aanat.2024.152261
    pmid: "38521363"
    authors: Baj&#x10d;eti&#x107; M, Mir&#x10d;i&#x107; A, Rako&#x10d;evi&#x107; J, &#x110;okovi&#x107; D, Milutinovi&#x107; K, Zaletel I
    year: "2024"
    abstract: |-
      INTRODUCTION: The appearance of artificial intelligence language models (AI LMs) in the form of chatbots has gained a lot of popularity worldwide, potentially interfering with different aspects of education, including medical education as well. The present study aims to assess the accuracy and consistency of different AI LMs regarding the histology and embryology knowledge obtained during the 1st year of medical studies.

      METHODS: Five different chatbots (ChatGPT, Bing AI, Bard AI, Perplexity AI, and ChatSonic) were given two sets of multiple-choice questions (MCQs). AI LMs test results were compared to the same test results obtained from 1st year medical students. Chatbots were instructed to use revised Bloom's taxonomy when classifying questions depending on hierarchical cognitive domains. Simultaneously, two histology teachers independently rated the questions applying the same criteria, followed by the comparison between chatbots' and teachers' question classification. The consistency of chatbots' answers was explored by giving the chatbots the same tests two months apart.

      RESULTS: AI LMs successfully and correctly solved MCQs regarding histology and embryology material. All five chatbots showed better results than the 1st year medical students on both histology and embryology tests. Chatbots showed poor results when asked to classify the questions according to revised Bloom's cognitive taxonomy compared to teachers. There was an inverse correlation between the difficulty of questions and their correct classification by the chatbots. Retesting the chatbots after two months showed a lack of consistency concerning both MCQs answers and question classification according to revised Bloom's taxonomy learning stage.

      CONCLUSION: Despite the ability of certain chatbots to provide correct answers to the majority of diverse and heterogeneous questions, a lack of consistency in answers over time warrants their careful use as a medical education tool.
    mergedFrom:
      - source: pubmed
        pmid: "38521363"
        doi: 10.1016/j.aanat.2024.152261
    finalDecision: exclude
  - title: "Charting new AI education in gastroenterology: Cross-sectional evaluation of ChatGPT and perplexity AI in medical residency exam."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates chatbot performance on Italian national residency admission exam questions in gastroenterology; benchmarks AI on exam questions, not about assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on chatbot accuracy in residency admission exam questions rather than workplace-based assessment or feedback on clinical observations."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1016/j.dld.2024.02.019
    pmid: "38503659"
    authors: Gravina A, Pellegrino R, Palladino G, Imperio G, Ventura A, Federico A
    year: "2024"
    abstract: |-
      BACKGROUND: Conversational chatbots, fueled by large language models, spark debate over their potential in education and medical career exams. There is debate in the literature about the scientific integrity of the outputs produced by these chatbots.

      AIMS: This study evaluates ChatGPT 3.5 and Perplexity AI's cross-sectional performance in responding to questions from the 2023 Italian national residency admission exam (SSM23), comparing results and chatbots' concordance with previous years SSMs.

      METHODS: Gastroenterology-related SSM23 questions were input into ChatGPT 3.5 and Perplexity AI, evaluating their performance in correct responses and total scores. This process was repeated with questions from the three preceding years. Additionally, chatbot concordance was assessed using Cohen's method.

      RESULTS: In SSM23, ChatGPT 3.5 outperforms Perplexity AI with 94.11% correct responses, demonstrating consistency across years. Concordance weakened in 2023 (κ=0.203, P = 0.148), but ChatGPT consistently maintains a high standard compared to Perplexity AI.

      CONCLUSION: ChatGPT 3.5 and Perplexity AI exhibit promise in addressing gastroenterological queries, emphasizing potential educational roles. However, their variable performance mandates cautious use as supplementary tools alongside conventional study methods. Clear guidelines are crucial for educators to balance traditional approaches and innovative systems, enhancing educational standards.
    mergedFrom:
      - source: pubmed
        pmid: "38503659"
        doi: 10.1016/j.dld.2024.02.019
    finalDecision: exclude
  - title: Performance of generative pre-trained Transformer-4 (GPT-4) in RCOG diploma-style questions.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: GPT-4 performance on RCOG diploma exam questions, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: The fulltext provided is a survey of practicing UK GPs about their preferences for LLM applications in primary care clinical workflows. Participants are practicing physicians, not medical/health-professional learners. Additionally, the study is not about assessment or feedback in an educational setting (criterion 2/4)."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:53.576000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants were practicing GPs, not medical/health-professional learners."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:01.509000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are practicing GPs, not medical/health-professional learners."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1093/postmj/qgae038
    pmid: "38497288"
    authors: Armitage R
    year: "2024"
    mergedFrom:
      - source: pubmed
        pmid: "38497288"
        doi: 10.1093/postmj/qgae038
    finalDecision: exclude
  - title: "Using artificial intelligence to improve human performance: efficient retinal disease detection training with synthetic images."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Training with AI-generated synthetic images for disease detection, not about assessment or feedback"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on image-based training rather than assessment/feedback of clinical observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    doi: 10.1136/bjo-2023-324923
    pmid: "38485215"
    authors: Tabuchi H, Engelmann J, Maeda F, Nishikawa R, Nagasawa T, Yamauchi T, Tanabe M, Akada M, Kihara K, Nakae Y, Kiuchi Y, Bernabeu M
    year: "2024"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) in medical imaging diagnostics has huge potential, but human judgement is still indispensable. We propose an AI-aided teaching method that leverages generative AI to train students on many images while preserving patient privacy.

      METHODS: A web-based course was designed using 600 synthetic ultra-widefield (UWF) retinal images to teach students to detect disease in these images. The images were generated by stable diffusion, a large generative foundation model, which we fine-tuned with 6285 real UWF images from six categories: five retinal diseases (age-related macular degeneration, glaucoma, diabetic retinopathy, retinal detachment and retinal vein occlusion) and normal. 161 trainee orthoptists took the course. They were evaluated with two tests: one consisting of UWF images and another of standard field (SF) images, which the students had not encountered in the course. Both tests contained 120 real patient images, 20 per category. The students took both tests once before and after training, with a cool-off period in between.

      RESULTS: On average, students completed the course in 53 min, significantly improving their diagnostic accuracy. For UWF images, student accuracy increased from 43.6% to 74.1% (p<0.0001 by paired t-test), nearly matching the previously published state-of-the-art AI model's accuracy of 73.3%. For SF images, student accuracy rose from 42.7% to 68.7% (p<0.0001), surpassing the state-of-the-art AI model's 40%.

      CONCLUSION: Synthetic images can be used effectively in medical education. We also found that humans are more robust to novel situations than AI models, thus showcasing human judgement's essential role in medical diagnosis.
    mergedFrom:
      - source: pubmed
        pmid: "38485215"
        doi: 10.1136/bjo-2023-324923
    finalDecision: exclude
  - title: "ChatGPT to generate clinical vignettes for teaching and multiple-choice questions for assessment: A randomized controlled experiment."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT generating MCQs and clinical vignettes for teaching/assessment, exam-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; RCT on ChatGPT-generated clinical vignettes and MCQs for evidence-based medicine training program"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is based on generated vignettes and MCQs in a training program without a workplace-based clinical assessment component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.1080/0142159X.2024.2327477
    pmid: "38478902"
    authors: Co&#x15f;kun &, K&#x131;yak Y, Budako&#x11f;lu I
    year: "2025"
    abstract: |-
      AIM: This study aimed to evaluate the real-life performance of clinical vignettes and multiple-choice questions generated by using ChatGPT.

      METHODS: This was a randomized controlled study in an evidence-based medicine training program. We randomly assigned seventy-four medical students to two groups. The ChatGPT group received ill-defined cases generated by ChatGPT, while the control group received human-written cases. At the end of the training, they evaluated the cases by rating 10 statements using a Likert scale. They also answered 15 multiple-choice questions (MCQs) generated by ChatGPT. The case evaluations of the two groups were compared. Some psychometric characteristics (item difficulty and point-biserial correlations) of the test were also reported.

      RESULTS: None of the scores in 10 statements regarding the cases showed a significant difference between the ChatGPT group and the control group (p > .05). In the test, only six MCQs had acceptable levels (higher than 0.30) of point-biserial correlation, and five items could be considered acceptable in classroom settings.

      CONCLUSIONS: The results showed that the quality of the vignettes are comparable to those created by human authors, and some multiple-questions have acceptable psychometric characteristics. ChatGPT has potential in generating clinical vignettes for teaching and MCQs for assessment in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "38478902"
        doi: 10.1080/0142159X.2024.2327477
    finalDecision: exclude
  - title: Performance of Two Artificial Intelligence Generative Language Models on the Orthopaedic In-Training Examination.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates AI generative language model performance on the Orthopaedic In-Training Examination"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Orthopaedic in-training exam benchmarking study without workplace-based assessment or analysis of clinical observation documentation."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.3928/01477447-20240304-02
    pmid: "38466827"
    authors: Lubitz M, Latario L
    year: "2024"
    abstract: |-
      BACKGROUND: Artificial intelligence (AI) generative large language models are powerful and increasingly accessible tools with potential applications in health care education and training. The annual Orthopaedic In-Training Examination (OITE) is widely used to assess resident academic progress and preparation for the American Board of Orthopaedic Surgery Part 1 Examination.

      MATERIALS AND METHODS: Open AI's ChatGPT and Google's Bard generative language models were administered the 2022 OITE. Question stems that contained images were input without and then with a text-based description of the imaging findings.

      RESULTS: ChatGPT answered 69.1% of questions correctly. When provided with text describing accompanying media, this increased to 77.8% correct. In contrast, Bard answered 49.8% of questions correctly. This increased to 58% correct when text describing imaging in question stems was provided (P<.0001). ChatGPT was most accurate in questions within the shoulder category, with 90.9% correct. Bard performed best in the sports category, with 65.4% correct. ChatGPT performed above the published mean of Accreditation Council for Graduate Medical Education orthopedic resident test-takers (66%).

      CONCLUSION: There is significant variability in the accuracy of publicly available AI models on the OITE. AI generative language software may play numerous potential roles in the future in orthopedic education, including simulating patient presentations and clinical scenarios, customizing individual learning plans, and driving evidence-based case discussion. Further research and collaboration within the orthopedic community is required to safely adopt these tools and minimize risks associated with their use. [Orthopedics. 2024;47(3):e146-e150.].
    mergedFrom:
      - source: pubmed
        pmid: "38466827"
        doi: 10.3928/01477447-20240304-02
    finalDecision: exclude
  - title: Comparing the performance of ChatGPT GPT-4, Bard, and Llama-2 in the Taiwan Psychiatric Licensing Examination and in differential diagnosis with multi-center psychiatrists.
    reviews:
      - decision: uncertain
        comment: LLMs on psychiatric licensing exam and differential diagnosis; abstract may reveal WBA elements
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; benchmarks LLMs on psychiatric licensing exam questions and clinical vignettes for differential diagnosis."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants/comparators are AI models and experienced psychiatrists rather than medical or health-professional learners."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.1111/pcn.13656
    pmid: "38404249"
    authors: Li D, Kao Y, Tsai S, Bai Y, Yeh T, Chu C, Hsu C, Cheng S, Hsu T, Liang C, Su K
    year: "2024"
    abstract: |-
      AIM: Large language models (LLMs) have been suggested to play a role in medical education and medical practice. However, the potential of their application in the psychiatric domain has not been well-studied.

      METHOD: In the first step, we compared the performance of ChatGPT GPT-4, Bard, and Llama-2 in the 2022 Taiwan Psychiatric Licensing Examination conducted in traditional Mandarin. In the second step, we compared the scores of these three LLMs with those of 24 experienced psychiatrists in 10 advanced clinical scenario questions designed for psychiatric differential diagnosis.

      RESULT: Only GPT-4 passed the 2022 Taiwan Psychiatric Licensing Examination (scoring 69 and ≥ 60 being considered a passing grade), while Bard scored 36 and Llama-2 scored 25. GPT-4 outperformed Bard and Llama-2, especially in the areas of 'Pathophysiology & Epidemiology' (χ2 = 22.4, P < 0.001) and 'Psychopharmacology & Other therapies' (χ2 = 15.8, P < 0.001). In the differential diagnosis, the mean score of the 24 experienced psychiatrists (mean 6.1, standard deviation 1.9) was higher than that of GPT-4 (5), Bard (3), and Llama-2 (1).

      CONCLUSION: Compared to Bard and Llama-2, GPT-4 demonstrated superior abilities in identifying psychiatric symptoms and making clinical judgments. Besides, GPT-4's ability for differential diagnosis closely approached that of the experienced psychiatrists. GPT-4 revealed a promising potential as a valuable tool in psychiatric practice among the three LLMs.
    mergedFrom:
      - source: pubmed
        pmid: "38404249"
        doi: 10.1111/pcn.13656
    finalDecision: exclude
  - title: "Assessing supervisor versus trainee viewpoints of entrustment through cognitive and affective lenses: an artificial intelligence investigation of bias in feedback."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: include
        comment: Uses LLMs to analyze feedback dialogs (N=24,187) between medical students and clinical supervisors with associated entrustment ratings in a workplace-based setting. Presents data on bias in feedback and entrustment decisions.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: While the study uses pre-trained language models (Universal Sentence Encoder, BERT-PubMed) for NLP analysis of entrustment feedback narratives, these are encoder-based models used for embedding and sentiment classification tasks, not generative AI. The AI is applied as a traditional NLP tool for text analysis (theme extraction via PCA on embeddings, sentiment classification), not as generative AI for assessment, feedback generation, or analysis of observation records. The study does not use generative AI models (e.g., GPT, ChatGPT, Claude) for the core methodology."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:56.143000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:04.368000+00:00
    doi: 10.1007/s10459-024-10311-9
    pmid: "38388855"
    authors: Gin B, Ten Cate O, O'Sullivan P, Boscardin C
    year: "2024"
    abstract: "The entrustment framework redirects assessment from considering only trainees' competence to decision-making about their readiness to perform clinical tasks independently. Since trainees and supervisors both contribute to entrustment decisions, we examined the cognitive and affective factors that underly their negotiation of trust, and whether trainee demographic characteristics may bias them. Using a document analysis approach, we adapted large language models (LLMs) to examine feedback dialogs (N = 24,187, each with an associated entrustment rating) between medical student trainees and their clinical supervisors. We compared how trainees and supervisors differentially documented feedback dialogs about similar tasks by identifying qualitative themes and quantitatively assessing their correlation with entrustment ratings. Supervisors' themes predominantly reflected skills related to patient presentations, while trainees' themes were broader-including clinical performance and personal qualities. To examine affect, we trained an LLM to measure feedback sentiment. On average, trainees used more negative language (5.3% lower probability of positive sentiment, p < 0.05) compared to supervisors, while documenting higher entrustment ratings (+ 0.08 on a 1-4 scale, p < 0.05). We also found biases tied to demographic characteristics: trainees' documentation reflected more positive sentiment in the case of male trainees (+ 1.3%, p < 0.05) and of trainees underrepresented in medicine (UIM) (+ 1.3%, p < 0.05). Entrustment ratings did not appear to reflect these biases, neither when documented by trainee nor supervisor. As such, bias appeared to influence the emotive language trainees used to document entrustment more than the degree of entrustment they experienced. Mitigating these biases is nonetheless important because they may affect trainees' assimilation into their roles and formation of trusting relationships."
    mergedFrom:
      - source: pubmed
        pmid: "38388855"
        doi: 10.1007/s10459-024-10311-9
    finalDecision: include
  - title: Assessing the medical reasoning skills of GPT-4 in complex ophthalmology cases.
    reviews:
      - decision: uncertain
        comment: GPT-4 reasoning in clinical cases; may involve clinical assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records. Study evaluates GPT-4's own diagnostic reasoning ability on ophthalmology clinical challenges, not its use for assessing learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on diagnostic question-answering accuracy, not assessment/feedback on learner clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.1136/bjo-2023-325053
    pmid: "38365427"
    authors: Milad D, Antaki F, Milad J, Farah A, Khairy T, Mikhail D, Gigu&#xe8;re C, Touma S, Bernstein A, Szigiato A, Nayman T, Mullie G, Duval R
    year: "2024"
    abstract: |-
      BACKGROUND/AIMS: This study assesses the proficiency of Generative Pre-trained Transformer (GPT)-4 in answering questions about complex clinical ophthalmology cases.

      METHODS: We tested GPT-4 on 422 Journal of the American Medical Association Ophthalmology Clinical Challenges, and prompted the model to determine the diagnosis (open-ended question) and identify the next-step (multiple-choice question). We generated responses using two zero-shot prompting strategies, including zero-shot plan-and-solve+ (PS+), to improve the reasoning of the model. We compared the best-performing model to human graders in a benchmarking effort.

      RESULTS: Using PS+ prompting, GPT-4 achieved mean accuracies of 48.0% (95% CI (43.1% to 52.9%)) and 63.0% (95% CI (58.2% to 67.6%)) in diagnosis and next step, respectively. Next-step accuracy did not significantly differ by subspecialty (p=0.44). However, diagnostic accuracy in pathology and tumours was significantly higher than in uveitis (p=0.027). When the diagnosis was accurate, 75.2% (95% CI (68.6% to 80.9%)) of the next steps were correct. Conversely, when the diagnosis was incorrect, 50.2% (95% CI (43.8% to 56.6%)) of the next steps were accurate. The next step was three times more likely to be accurate when the initial diagnosis was correct (p<0.001). No significant differences were observed in diagnostic accuracy and decision-making between board-certified ophthalmologists and GPT-4. Among trainees, senior residents outperformed GPT-4 in diagnostic accuracy (p≤0.001 and 0.049) and in accuracy of next step (p=0.002 and 0.020).

      CONCLUSION: Improved prompting enhances GPT-4's performance in complex clinical situations, although it does not surpass ophthalmology trainees in our context. Specialised large language models hold promise for future assistance in medical decision-making and diagnosis.
    mergedFrom:
      - source: pubmed
        pmid: "38365427"
        doi: 10.1136/bjo-2023-325053
    finalDecision: exclude
  - title: "Performance of ChatGPT on Chinese national medical licensing examinations: a five-year examination evaluation study for physicians, pharmacists and nurses."
    reviews:
      - decision: uncertain
        comment: ChatGPT on licensing exams; abstract may reveal WBA-relevant elements
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT accuracy on Chinese national licensing examinations for physicians, pharmacists, and nurses; not about assessment or feedback of learner performance."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Licensing-exam performance study is not about workplace-based assessment or feedback from clinical observations."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1186/s12909-024-05125-7
    pmid: "38355517"
    authors: Zong H, Li J, Wu E, Wu R, Lu J, Shen B
    year: "2024"
    abstract: |-
      BACKGROUND: Large language models like ChatGPT have revolutionized the field of natural language processing with their capability to comprehend and generate textual content, showing great potential to play a role in medical education. This study aimed to quantitatively evaluate and comprehensively analysis the performance of ChatGPT on three types of national medical examinations in China, including National Medical Licensing Examination (NMLE), National Pharmacist Licensing Examination (NPLE), and National Nurse Licensing Examination (NNLE).

      METHODS: We collected questions from Chinese NMLE, NPLE and NNLE from year 2017 to 2021. In NMLE and NPLE, each exam consists of 4 units, while in NNLE, each exam consists of 2 units. The questions with figures, tables or chemical structure were manually identified and excluded by clinician. We applied direct instruction strategy via multiple prompts to force ChatGPT to generate the clear answer with the capability to distinguish between single-choice and multiple-choice questions.

      RESULTS: ChatGPT failed to pass the accuracy threshold of 0.6 in any of the three types of examinations over the five years. Specifically, in the NMLE, the highest recorded accuracy was 0.5467, which was attained in both 2018 and 2021. In the NPLE, the highest accuracy was 0.5599 in 2017. In the NNLE, the most impressive result was shown in 2017, with an accuracy of 0.5897, which is also the highest accuracy in our entire evaluation. ChatGPT's performance showed no significant difference in different units, but significant difference in different question types. ChatGPT performed well in a range of subject areas, including clinical epidemiology, human parasitology, and dermatology, as well as in various medical topics such as molecules, health management and prevention, diagnosis and screening.

      CONCLUSIONS: These results indicate ChatGPT failed the NMLE, NPLE and NNLE in China, spanning from year 2017 to 2021. but show great potential of large language models in medical education. In the future high-quality medical data will be required to improve the performance.
    mergedFrom:
      - source: pubmed
        pmid: "38355517"
        doi: 10.1186/s12909-024-05125-7
    finalDecision: exclude
  - title: "Case-based MCQ generator: A custom ChatGPT based on published prompts in the literature for automatic item generation."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not original research; describes an educational innovation tool for MCQ generation without empirical validity data"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Describes an educational tool/proposal without original research, systematic review, meta-analysis, or preprint data."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.1080/0142159X.2024.2314723
    pmid: "38340312"
    authors: K&#x131;yak Y, Kononowicz A
    year: "2024"
    abstract: |-
      WHAT IS THE EDUCATIONAL CHALLENGE?: A fundamental challenge in medical education is creating high-quality, clinically relevant multiple-choice questions (MCQs). ChatGPT-based automatic item generation (AIG) methods need well-designed prompts. However, the use of these prompts is hindered by the time-consuming process of copying and pasting, a lack of know-how among medical teachers, and the generalist nature of standard ChatGPT, which often lacks the medical context.

      WHAT ARE THE PROPOSED SOLUTIONS?: The Case-based MCQ Generator, a custom GPT, addresses these challenges. It has been trained by using GPT Builder, which is a platform designed by OpenAI for customizing ChatGPT to meet specific needs, in order to allow users to generate case-based MCQs. By using this free tool for those who have ChatGPT Plus subscription, health professions educators can easily select a prompt, input a learning objective or item-specific test point, and generate clinically relevant questions.

      WHAT ARE THE POTENTIAL BENEFITS TO A WIDER GLOBAL AUDIENCE?: It enhances the efficiency of MCQ generation and ensures the generation of contextually relevant questions, surpassing the capabilities of standard ChatGPT. It streamlines the MCQ creation process by integrating prompts published in medical education literature, eliminating the need for manual prompt input.

      WHAT ARE THE NEXT STEPS?: Future development aims at sustainability and addressing ethical and accessibility issues. It requires regular updates, integration of new prompts from emerging health professions education literature, and a supportive digital ecosystem around the tool. Accessibility, especially for educators in low-resource countries, is vital, demanding alternative access models to overcome financial barriers.
    mergedFrom:
      - source: pubmed
        pmid: "38340312"
        doi: 10.1080/0142159X.2024.2314723
    finalDecision: exclude
  - title: "Performance of ChatGPT on the Chinese Postgraduate Examination for Clinical Medicine: Survey Study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Postgraduate licensing examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Evaluates ChatGPT on the Chinese Postgraduate Examination for Clinical Medicine, an academic licensing exam, not workplace-based or clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Postgraduate written examination study without workplace-based assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.2196/48514
    pmid: "38335017"
    authors: Yu P, Fang C, Liu X, Fu W, Ling J, Yan Z, Jiang Y, Cao Z, Wu M, Chen Z, Zhu W, Zhang Y, Abudukeremu A, Wang Y, Liu X, Wang J
    year: "2024"
    abstract: |-
      BACKGROUND: ChatGPT, an artificial intelligence (AI) based on large-scale language models, has sparked interest in the field of health care. Nonetheless, the capabilities of AI in text comprehension and generation are constrained by the quality and volume of available training data for a specific language, and the performance of AI across different languages requires further investigation. While AI harbors substantial potential in medicine, it is imperative to tackle challenges such as the formulation of clinical care standards; facilitating cultural transitions in medical education and practice; and managing ethical issues including data privacy, consent, and bias.

      OBJECTIVE: The study aimed to evaluate ChatGPT's performance in processing Chinese Postgraduate Examination for Clinical Medicine questions, assess its clinical reasoning ability, investigate potential limitations with the Chinese language, and explore its potential as a valuable tool for medical professionals in the Chinese context.

      METHODS: A data set of Chinese Postgraduate Examination for Clinical Medicine questions was used to assess the effectiveness of ChatGPT's (version 3.5) medical knowledge in the Chinese language, which has a data set of 165 medical questions that were divided into three categories: (1) common questions (n=90) assessing basic medical knowledge, (2) case analysis questions (n=45) focusing on clinical decision-making through patient case evaluations, and (3) multichoice questions (n=30) requiring the selection of multiple correct answers. First of all, we assessed whether ChatGPT could meet the stringent cutoff score defined by the government agency, which requires a performance within the top 20% of candidates. Additionally, in our evaluation of ChatGPT's performance on both original and encoded medical questions, 3 primary indicators were used: accuracy, concordance (which validates the answer), and the frequency of insights.

      RESULTS: Our evaluation revealed that ChatGPT scored 153.5 out of 300 for original questions in Chinese, which signifies the minimum score set to ensure that at least 20% more candidates pass than the enrollment quota. However, ChatGPT had low accuracy in answering open-ended medical questions, with only 31.5% total accuracy. The accuracy for common questions, multichoice questions, and case analysis questions was 42%, 37%, and 17%, respectively. ChatGPT achieved a 90% concordance across all questions. Among correct responses, the concordance was 100%, significantly exceeding that of incorrect responses (n=57, 50%; P<.001). ChatGPT provided innovative insights for 80% (n=132) of all questions, with an average of 2.95 insights per accurate response.

      CONCLUSIONS: Although ChatGPT surpassed the passing threshold for the Chinese Postgraduate Examination for Clinical Medicine, its performance in answering open-ended medical questions was suboptimal. Nonetheless, ChatGPT exhibited high internal concordance and the ability to generate multiple insights in the Chinese language. Future research should investigate the language-based discrepancies in ChatGPT's performance within the health care context.
    mergedFrom:
      - source: pubmed
        pmid: "38335017"
        doi: 10.2196/48514
    finalDecision: exclude
  - title: "Comparison of the Performance of GPT-3.5 and GPT-4 With That of Medical Students on the Written German Medical Licensing Examination: Observational Study."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares GPT-3.5 and GPT-4 performance to medical students on written licensing exam questions; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.2196/50965
    pmid: "38329802"
    authors: Meyer A, Riese J, Streichert T
    year: "2024"
    abstract: |-
      BACKGROUND: The potential of artificial intelligence (AI)-based large language models, such as ChatGPT, has gained significant attention in the medical field. This enthusiasm is driven not only by recent breakthroughs and improved accessibility, but also by the prospect of democratizing medical knowledge and promoting equitable health care. However, the performance of ChatGPT is substantially influenced by the input language, and given the growing public trust in this AI tool compared to that in traditional sources of information, investigating its medical accuracy across different languages is of particular importance.

      OBJECTIVE: This study aimed to compare the performance of GPT-3.5 and GPT-4 with that of medical students on the written German medical licensing examination.

      METHODS: To assess GPT-3.5's and GPT-4's medical proficiency, we used 937 original multiple-choice questions from 3 written German medical licensing examinations in October 2021, April 2022, and October 2022.

      RESULTS: GPT-4 achieved an average score of 85% and ranked in the 92.8th, 99.5th, and 92.6th percentiles among medical students who took the same examinations in October 2021, April 2022, and October 2022, respectively. This represents a substantial improvement of 27% compared to GPT-3.5, which only passed 1 out of the 3 examinations. While GPT-3.5 performed well in psychiatry questions, GPT-4 exhibited strengths in internal medicine and surgery but showed weakness in academic research.

      CONCLUSIONS: The study results highlight ChatGPT's remarkable improvement from moderate (GPT-3.5) to high competency (GPT-4) in answering medical licensing examination questions in German. While GPT-4's predecessor (GPT-3.5) was imprecise and inconsistent, it demonstrates considerable potential to improve medical education and patient care, provided that medically trained users critically evaluate its results. As the replacement of search engines by AI tools seems possible in the future, further studies with nonprofessional questions are needed to assess the safety and accuracy of ChatGPT for the general population.
    mergedFrom:
      - source: pubmed
        pmid: "38329802"
        doi: 10.2196/50965
    finalDecision: exclude
  - title: Is ChatGPT 'ready' to be a learning tool for medical undergraduates and will it perform equally in different subjects? Comparative study of ChatGPT performance in tutorial and case-based learning questions in physiology and biochemistry.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Tutorial/case-based classroom learning context is not workplace-based clinical assessment."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT on tutorial/case-based exam questions in physiology and biochemistry; not about using AI for assessment, feedback, or analysis of clinical observation records in workplace settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on LLM performance on tutorial/case-based academic questions, not assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1080/0142159X.2024.2308779
    pmid: "38295769"
    authors: Luke W, Seow Chong L, Ban K, Wong A, Zhi Xiong C, Shuh Shing L, Taneja R, Samarasekera D, Yap C
    year: "2024"
    abstract: |-
      PURPOSE: Generative AI will become an integral part of education in future. The potential of this technology in different disciplines should be identified to promote effective adoption. This study evaluated the performance of ChatGPT in tutorial and case-based learning questions in physiology and biochemistry for medical undergraduates. Our study mainly focused on the performance of GPT-3.5 version while a subgroup was comparatively assessed on GPT-3.5 and GPT-4 performances.

      MATERIALS AND METHODS: Answers were generated in GPT-3.5 for 44 modified essay questions (MEQs) in physiology and 43 MEQs in biochemistry. Each answer was graded by two independent examiners. Subsequently, a subset of 15 questions from each subject were selected to represent different score categories of the GPT-3.5 answers; responses were generated in GPT-4, and graded.

      RESULTS: The mean score for physiology answers was 74.7 (SD 25.96). GPT-3.5 demonstrated a statistically significant (p = .009) superior performance in lower-order questions of Bloom's taxonomy in comparison to higher-order questions. Deficiencies in the application of physiological principles in clinical context were noted as a drawback. Scores in biochemistry were relatively lower with a mean score of 59.3 (SD 26.9) for GPT-3.5. There was no statistically significant difference in the scores for higher and lower-order questions of Bloom's taxonomy. The deficiencies highlighted were lack of in-depth explanations and precision. The subset of questions where the GPT-4 and GPT-3.5 were compared demonstrated a better overall performance in GPT-4 responses in both subjects. This difference between the GPT-3.5 and GPT-4 performance was statistically significant in biochemistry but not in physiology.

      CONCLUSIONS: The differences in performance across the two versions, GPT-3.5 and GPT-4 across the disciplines are noteworthy. Educators and students should understand the strengths and limitations of this technology in different fields to effectively integrate this technology into teaching and learning.
    mergedFrom:
      - source: pubmed
        pmid: "38295769"
        doi: 10.1080/0142159X.2024.2308779
    finalDecision: exclude
  - title: "Harnessing the potential of large language models in medical education: promise and pitfalls."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Title suggests perspective/commentary piece, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/editorial publication type"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    doi: 10.1093/jamia/ocad252
    pmid: "38269644"
    authors: Ben&#xed;tez T, Xu Y, Boudreau J, Kow A, Bello F, Van Phuoc L, Wang X, Sun X, Leung G, Lan Y, Wang Y, Cheng D, Tham Y, Wong T, Chung K
    year: "2024"
    abstract: |-
      OBJECTIVES: To provide balanced consideration of the opportunities and challenges associated with integrating Large Language Models (LLMs) throughout the medical school continuum.

      PROCESS: Narrative review of published literature contextualized by current reports of LLM application in medical education.

      CONCLUSIONS: LLMs like OpenAI's ChatGPT can potentially revolutionize traditional teaching methodologies. LLMs offer several potential advantages to students, including direct access to vast information, facilitation of personalized learning experiences, and enhancement of clinical skills development. For faculty and instructors, LLMs can facilitate innovative approaches to teaching complex medical concepts and fostering student engagement. Notable challenges of LLMs integration include the risk of fostering academic misconduct, inadvertent overreliance on AI, potential dilution of critical thinking skills, concerns regarding the accuracy and reliability of LLM-generated content, and the possible implications on teaching staff.
    mergedFrom:
      - source: pubmed
        pmid: "38269644"
        doi: 10.1093/jamia/ocad252
    finalDecision: exclude
  - title: Assessment of Pathology Domain-Specific Knowledge of ChatGPT and Comparison to Human Performance.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT answering pathology board-style questions; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is based on board-style licensing questions in a simulated testing setup."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.5858/arpa.2023-0296-OA
    pmid: "38244054"
    authors: Wang A, Lin S, Tran C, Homer R, Wilsdon D, Walsh J, Goebel E, Sansano I, Sonawane S, Cockenpot V, Mukhopadhyay S, Taskin T, Zahra N, Cima L, Semerci O, &#xd6;zamrak B, Mishra P, Vennavalli N, Chen P, Cecchini M
    year: "2024"
    abstract: |-
      CONTEXT.&#x2014;: Artificial intelligence algorithms hold the potential to fundamentally change many aspects of society. Application of these tools, including the publicly available ChatGPT, has demonstrated impressive domain-specific knowledge in many areas, including medicine.

      OBJECTIVES.&#x2014;: To understand the level of pathology domain-specific knowledge for ChatGPT using different underlying large language models, GPT-3.5 and the updated GPT-4.

      DESIGN.&#x2014;: An international group of pathologists (n = 15) was recruited to generate pathology-specific questions at a similar level to those that could be seen on licensing (board) examinations. The questions (n = 15) were answered by GPT-3.5, GPT-4, and a staff pathologist who recently passed their Canadian pathology licensing exams. Participants were instructed to score answers on a 5-point scale and to predict which answer was written by ChatGPT.

      RESULTS.&#x2014;: GPT-3.5 performed at a similar level to the staff pathologist, while GPT-4 outperformed both. The overall score for both GPT-3.5 and GPT-4 was within the range of meeting expectations for a trainee writing licensing examinations. In all but one question, the reviewers were able to correctly identify the answers generated by GPT-3.5.

      CONCLUSIONS.&#x2014;: By demonstrating the ability of ChatGPT to answer pathology-specific questions at a level similar to (GPT-3.5) or exceeding (GPT-4) a trained pathologist, this study highlights the potential of large language models to be transformative in this space. In the future, more advanced iterations of these algorithms with increased domain-specific knowledge may have the potential to assist pathologists and enhance pathology resident training.
    mergedFrom:
      - source: pubmed
        pmid: "38244054"
        doi: 10.5858/arpa.2023-0296-OA
    finalDecision: exclude
  - title: Using Natural Language Processing to Evaluate the Quality of Supervisor Narrative Comments in Competency-Based Medical Education.
    reviews:
      - decision: uncertain
        comment: NLP for evaluating supervisor narrative comments in CBME; highly relevant to WBA, abstract needed to confirm generative AI use
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP (predictive model) to score narrative assessment comments, not generative AI/LLMs"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP rather than generative AI/LLMs for analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.1097/ACM.0000000000005634
    pmid: "38232079"
    authors: Spadafore M, Yilmaz Y, Rally V, Chan T, Russell M, Thoma B, Singh S, Monteiro S, Pardhan A, Martin L, Monrad S, Woods R
    year: "2024"
    abstract: |-
      PURPOSE: Learner development and promotion rely heavily on narrative assessment comments, but narrative assessment quality is rarely evaluated in medical education. Educators have developed tools such as the Quality of Assessment for Learning (QuAL) tool to evaluate the quality of narrative assessment comments; however, scoring the comments generated in medical education assessment programs is time intensive. The authors developed a natural language processing (NLP) model for applying the QuAL score to narrative supervisor comments.

      METHOD: Samples of 2,500 Entrustable Professional Activities assessments were randomly extracted and deidentified from the McMaster (1,250 comments) and Saskatchewan (1,250 comments) emergency medicine (EM) residency training programs during the 2019-2020 academic year. Comments were rated using the QuAL score by 25 EM faculty members and 25 EM residents. The results were used to develop and test an NLP model to predict the overall QuAL score and QuAL subscores.

      RESULTS: All 50 raters completed the rating exercise. Approximately 50% of the comments had perfect agreement on the QuAL score, with the remaining resolved by the study authors. Creating a meaningful suggestion for improvement was the key differentiator between high- and moderate-quality feedback. The overall QuAL model predicted the exact human-rated score or 1 point above or below it in 87% of instances. Overall model performance was excellent, especially regarding the subtasks on suggestions for improvement and the link between resident performance and improvement suggestions, which achieved 85% and 82% balanced accuracies, respectively.

      CONCLUSIONS: This model could save considerable time for programs that want to rate the quality of supervisor comments, with the potential to automatically score a large volume of comments. This model could be used to provide faculty with real-time feedback or as a tool to quantify and track the quality of assessment comments at faculty, rotation, program, or institution levels.
    mergedFrom:
      - source: pubmed
        pmid: "38232079"
        doi: 10.1097/ACM.0000000000005634
    finalDecision: exclude
  - title: Diagnostic capabilities of ChatGPT in ophthalmology.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About AI diagnostic capabilities, not about assessment or feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focus is diagnostic capability, not learner assessment/feedback or analysis of observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    doi: 10.1007/s00417-023-06363-z
    pmid: "38183467"
    authors: Shemer A, Cohen M, Altarescu A, Atar-Vardi M, Hecht I, Dubinsky-Pertzov B, Shoshany N, Zmujack S, Or L, Einan-Lifshitz A, Pras E
    year: "2024"
    abstract: |-
      PURPOSE: The purpose of this study is to assess the diagnostic accuracy of ChatGPT in the field of ophthalmology.

      METHODS: This is a retrospective cohort study conducted in one academic tertiary medical center. We reviewed data of patients admitted to the ophthalmology department from 06/2022 to 01/2023. We then created two clinical cases for each patient. The first case is according to the medical history alone (Hx). The second case includes an addition of the clinical examination (Hx and Ex). For each case, we asked for the three most likely diagnoses from ChatGPT, residents, and attendings. Then, we compared the accuracy rates (at least one correct diagnosis) of all groups. Additionally, we evaluated the total duration for completing the assignment between the groups.

      RESULTS: ChatGPT, residents, and attendings evaluated 126 cases from 63 patients (history only or history and exam findings for each patient). ChatGPT achieved a significantly lower accurate diagnosis rate (54%) in the Hx, as compared to the residents (75%; p < 0.01) and attendings (71%; p < 0.01). After adding the clinical examination findings, the diagnosis rate of ChatGPT was 68%, whereas for the residents and the attendings, it increased to 94% (p < 0.01) and 86% (p < 0.01), respectively. ChatGPT was 4 to 5 times faster than the attendings and residents.

      CONCLUSIONS AND RELEVANCE: ChatGPT showed low diagnostic rates in ophthalmology cases compared to residents and attendings based on patient history alone or with additional clinical examination findings. However, ChatGPT completed the task faster than the physicians.
    mergedFrom:
      - source: pubmed
        pmid: "38183467"
        doi: 10.1007/s00417-023-06363-z
    finalDecision: exclude
  - title: "Generative Language Models and Open Notes: Exploring the Promise and Limitations."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article, systematic review, meta-analysis, or preprint. Viewpoint/perspective paper exploring potential uses of generative AI in clinical documentation."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Viewpoint paper, not original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.2196/51183
    pmid: "38175688"
    authors: Blease C, Torous J, McMillan B, H&#xe4;gglund M, Mandl K
    year: "2024"
    abstract: "Patients' online record access (ORA) is growing worldwide. In some countries, including the United States and Sweden, access is advanced with patients obtaining rapid access to their full records on the web including laboratory and test results, lists of prescribed medications, vaccinations, and even the very narrative reports written by clinicians (the latter, commonly referred to as \"open notes\"). In the United States, patient's ORA is also available in a downloadable form for use with other apps. While survey studies have shown that some patients report many benefits from ORA, there remain challenges with implementation around writing clinical documentation that patients may now read. With ORA, the functionality of the record is evolving; it is no longer only an aide memoire for doctors but also a communication tool for patients. Studies suggest that clinicians are changing how they write documentation, inviting worries about accuracy and completeness. Other concerns include work burdens; while few objective studies have examined the impact of ORA on workload, some research suggests that clinicians are spending more time writing notes and answering queries related to patients' records. Aimed at addressing some of these concerns, clinician and patient education strategies have been proposed. In this viewpoint paper, we explore these approaches and suggest another longer-term strategy: the use of generative artificial intelligence (AI) to support clinicians in documenting narrative summaries that patients will find easier to understand. Applied to narrative clinical documentation, we suggest that such approaches may significantly help preserve the accuracy of notes, strengthen writing clarity and signals of empathy and patient-centered care, and serve as a buffer against documentation work burdens. However, we also consider the current risks associated with existing generative AI. We emphasize that for this innovation to play a key role in ORA, the cocreation of clinical notes will be imperative. We also caution that clinicians will need to be supported in how to work alongside generative AI to optimize its considerable potential."
    mergedFrom:
      - source: pubmed
        pmid: "38175688"
        doi: 10.2196/51183
      - source: scopus
        doi: 10.2196/51183
        scopusId: SCOPUS_ID:85186076305
    finalDecision: exclude
  - title: Progression of an Artificial Intelligence Chatbot (ChatGPT) for Pediatric Cardiology Educational Knowledge Assessment.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; benchmarks ChatGPT performance on pediatric cardiology board review MCQs."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates chatbot performance on textbook-style knowledge questions, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.1007/s00246-023-03385-6
    pmid: "38170274"
    authors: Gritti M, AlTurki H, Farid P, Morgan C
    year: "2024"
    abstract: Artificial intelligence chatbots, like ChatGPT, have become powerful tools that are disrupting how humans interact with technology. The potential uses within medicine are vast. In medical education, these chatbots have shown improvements, in a short time span, in generalized medical examinations. We evaluated the overall performance and improvement between ChatGPT 3.5 and 4.0 in a test of pediatric cardiology knowledge. ChatGPT 3.5 and ChatGPT 4.0 were used to answer text-based multiple-choice questions derived from a Pediatric Cardiology Board Review textbook. Each chatbot was given an 88 question test, subcategorized into 11 topics. We excluded questions with modalities other than text (sound clips or images). Statistical analysis was done using an unpaired two-tailed t-test. Of the same 88 questions, ChatGPT 4.0 answered 66% of the questions correctly (n = 58/88) which was significantly greater (p < 0.0001) than ChatGPT 3.5, which only answered 38% (33/88). The ChatGPT 4.0 version also did better on each subspeciality topic as compared to ChatGPT 3.5. While acknowledging that ChatGPT does not yet offer subspecialty level knowledge in pediatric cardiology, the performance in pediatric cardiology educational assessments showed a considerable improvement in a short period of time between ChatGPT 3.5 and 4.0.
    mergedFrom:
      - source: pubmed
        pmid: "38170274"
        doi: 10.1007/s00246-023-03385-6
    finalDecision: exclude
  - title: "Large Language Models in Medical Education: Comparing ChatGPT- to Human-Generated Exam Questions."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Comparing ChatGPT-generated vs human-generated exam questions, exam-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Compares LLM-generated vs human-generated MCQs for a formative neurophysiology exam; not in a workplace-based or clinical education setting."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam question generation/comparison study, not workplace-based assessment or feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1097/ACM.0000000000005626
    pmid: "38166323"
    authors: Laupichler M, Rother J, Grunwald Kadow I, Ahmadi S, Raupach T
    year: "2024"
    abstract: |-
      PROBLEM: Creating medical exam questions is time consuming, but well-written questions can be used for test-enhanced learning, which has been shown to have a positive effect on student learning. The automated generation of high-quality questions using large language models (LLMs), such as ChatGPT, would therefore be desirable. However, there are no current studies that compare students' performance on LLM-generated questions to questions developed by humans.

      APPROACH: The authors compared student performance on questions generated by ChatGPT (LLM questions) with questions created by medical educators (human questions). Two sets of 25 multiple-choice questions (MCQs) were created, each with 5 answer options, 1 of which was correct. The first set of questions was written by an experienced medical educator, and the second set was created by ChatGPT 3.5 after the authors identified learning objectives and extracted some specifications from the human questions. Students answered all questions in random order in a formative paper-and-pencil test that was offered leading up to the final summative neurophysiology exam (summer 2023). For each question, students also indicated whether they thought it had been written by a human or ChatGPT.

      OUTCOMES: The final data set consisted of 161 participants and 46 MCQs (25 human and 21 LLM questions). There was no statistically significant difference in item difficulty between the 2 question sets, but discriminatory power was statistically significantly higher in human than LLM questions (mean = .36, standard deviation [SD] = .09 vs mean = .24, SD = .14; P = .001). On average, students identified 57% of question sources (human or LLM) correctly.

      NEXT STEPS: Future research should replicate the study procedure in other contexts (e.g., other medical subjects, semesters, countries, and languages). In addition, the question of whether LLMs are suitable for generating different question types, such as key feature questions, should be investigated.
    mergedFrom:
      - source: pubmed
        pmid: "38166323"
        doi: 10.1097/ACM.0000000000005626
    finalDecision: exclude
  - title: Could ChatGPT Pass the UK Radiology Fellowship Examinations?
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Tests ChatGPT on fellowship exams, not about assessment or feedback of human learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting (fellowship examination context)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1016/j.acra.2023.11.026
    pmid: "38160089"
    authors: Ariyaratne S, Jenko N, Mark Davies A, Iyengar K, Botchu R
    year: "2024"
    abstract: |-
      RATIONALE AND OBJECTIVES: Chat Generative Pre-trained Transformer (ChatGPT) is an artificial intelligence (AI) tool which utilises machine learning to generate original text resembling human language. AI models have recently demonstrated remarkable ability at analysing and solving problems, including passing professional examinations. We investigate the performance of ChatGPT on some of the UK radiology fellowship equivalent examination questions.

      METHODS: ChatGPT was asked to answer questions from question banks resembling the Fellowship of the Royal College of Radiologists (FRCR) examination. The entire physics part 1 question bank (203 5-part true/false questions) was answered by the GPT-4 model and answers recorded. 240 single best answer questions (SBAs) (representing the true length of the FRCR 2A examination) were answered by both GPT-3.5 and GPT-4 models.

      RESULTS: ChatGPT 4 answered 74.8% of part 1 true/false statements correctly. The spring 2023 passing mark of the part 1 examination was 75.5% and ChatGPT thus narrowly failed. In the 2A examination, ChatGPT 3.5 answered 50.8% SBAs correctly, while GPT-4 answered 74.2% correctly. The winter 2022 2A pass mark was 63.3% and thus GPT-4 clearly passed.

      CONCLUSION: AI models such as ChatGPT are able to answer the majority of questions in an FRCR style examination. It is reasonable to assume that further developments in AI will be more likely to succeed in comprehending and solving questions related to medicine, specifically clinical radiology.

      ADVANCES IN KNOWLEDGE: Our findings outline the unprecedented capabilities of AI, adding to the current relatively small body of literature on the subject, which in turn can play a role medical training, evaluation and practice. This can undoubtedly have implications for radiology.
    mergedFrom:
      - source: pubmed
        pmid: "38160089"
        doi: 10.1016/j.acra.2023.11.026
    finalDecision: exclude
  - title: "Twelve tips to leverage AI for efficient and effective medical question generation: A guide for educators using Chat GPT."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: 'Twelve tips' format is a guidelines/tips article, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Tips/guide article type rather than original research/systematic review/meta-analysis/preprint."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    doi: 10.1080/0142159X.2023.2294703
    pmid: "38146711"
    authors: Indran I, Paranthaman P, Gupta N, Mustafa N
    year: "2024"
    abstract: |-
      BACKGROUND: Crafting quality assessment questions in medical education is a crucial yet time-consuming, expertise-driven undertaking that calls for innovative solutions. Large language models (LLMs), such as ChatGPT (Chat Generative Pre-Trained Transformer), present a promising yet underexplored avenue for such innovations.

      AIMS: This study explores the utility of ChatGPT to generate diverse, high-quality medical questions, focusing on multiple-choice questions (MCQs) as an illustrative example, to increase educator's productivity and enable self-directed learning for students.

      DESCRIPTION: Leveraging 12 strategies, we demonstrate how ChatGPT can be effectively used to generate assessment questions aligned with Bloom's taxonomy and core knowledge domains while promoting best practices in assessment design.

      CONCLUSION: Integrating LLM tools like ChatGPT into generating medical assessment questions like MCQs augments but does not replace human expertise. With continual instruction refinement, AI can produce high-standard questions. Yet, the onus of ensuring ultimate quality and accuracy remains with subject matter experts, affirming the irreplaceable value of human involvement in the artificial intelligence-driven education paradigm.
    mergedFrom:
      - source: pubmed
        pmid: "38146711"
        doi: 10.1080/0142159X.2023.2294703
    finalDecision: exclude
  - title: Performance of Generative Pre-trained Transformer-4 (GPT-4) in Membership of the Royal College of General Practitioners (MRCGP)-style examination questions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Study tests GPT-4 performance on MRCGP-style written examination questions. This is an exam-based assessment with no workplace-based component. The study evaluates AI performance on standardized exam questions, not assessment or feedback in a clinical/workplace education setting."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:56.143000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on GP opinions about LLM uses in primary care rather than assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:04.368000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: GPT-4 performance on MRCGP-style written exam questions, not workplace-based. Codex comment (Cr.2 re GP opinions) likely reflects fulltext mismatch."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1093/postmj/qgad128
    pmid: "38142282"
    authors: Armitage R
    year: "2024"
    mergedFrom:
      - source: pubmed
        pmid: "38142282"
        doi: 10.1093/postmj/qgad128
    finalDecision: exclude
  - title: OpenAI's GPT-4 performs to a high degree on board-style dermatology questions.
    reviews:
      - decision: uncertain
        comment: GPT-4 on dermatology board questions; abstract needed to determine WBA relevance
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: GPT-4 performance on dermatology board-style questions; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is conducted on board-prep question banks only."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.1111/ijd.16913
    pmid: "38131454"
    authors: Elias M, Burshtein J, Sharon V
    year: "2024"
    abstract: |-
      BACKGROUND: Artificial intelligence tools such as OpenAI's GPT-4 have shown promise in medical education, but their potential in dermatology remains unexplored.

      OBJECTIVES: To assess GPT-4's performance on dermatology board-style questions and determine its value as a supplementary educational tool for trainees and educators.

      METHODS: This cross-sectional study evaluated GPT-4's performance on 250 random dermatology board-style questions sampled from the American Academy of Dermatology's Board Prep Plus resource. Questions were divided into five subspecialties and various difficulty levels. GPT-4 responses were compared to the correct answers and evaluated by two physicians.

      RESULTS: GPT-4 achieved an overall accuracy of 75% on the 250 questions, with no significant variation based on subspecialty or question difficulty. The most common errors were factual and misunderstanding inaccuracies. Responses scored high in clarity, accuracy, and relevance but frequently lacked depth and completeness.

      CONCLUSION: GPT-4 performed to a high degree and demonstrated promising performance as an educational adjunct in dermatology. Improvements in response depth and completeness are needed before its use as an unsupervised learning tool is established.
    mergedFrom:
      - source: pubmed
        pmid: "38131454"
        doi: 10.1111/ijd.16913
    finalDecision: exclude
  - title: "Decoding medical school narrative evaluations: Is natural language processing an antidote to the leniency bias?"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: uncertain
        comment: Abstract does not clarify whether generative AI/LLMs are used or conventional NLP; topic is relevant (NLP analysis of narrative clinical assessments to address leniency bias); forwarded to full-text screening to determine AI method
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses natural language processing broadly without clear use of generative AI/LLMs."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP techniques (keyword-based qualifying word lists and computer-based sentiment analysis), not generative AI or LLMs. No mention of ChatGPT, GPT, LLMs, or any generative AI model. The approach relies on faculty-identified qualifying words to inform traditional sentiment analysis of narrative evaluations."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 03:36:32.323000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses general NLP/sentiment analysis rather than generative AI or LLMs."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 03:36:33.873000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Conventional NLP (keyword-based sentiment analysis), not generative AI/LLMs."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1016/j.amjsurg.2023.11.028
    pmid: "38065720"
    authors: Harvey A
    year: "2024"
    abstract: Observational assessments in the clinical context are a cornerstone of evaluation in medical education. Leniency bias, described in performance management in the business arena appears to widely impact these assessments with medical training. Natural language processing provides a potential tool that medical educators may leverage to decipher underlying meaning in narrative assessment. A "proof-of-concept" study at the Cumming School of Medicine supports this notion and suggests further work would be a worthwhile pursuit in this field.
    mergedFrom:
      - source: pubmed
        pmid: "38065720"
        doi: 10.1016/j.amjsurg.2023.11.028
    finalDecision: exclude
  - title: Reimagining Core Entrustable Professional Activities for Undergraduate Medical Education in the Era of Artificial Intelligence.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not original research; perspective/commentary proposing revised EPAs for the AI era without empirical data"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Perspective article proposing EPA changes; not original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.2196/50903
    pmid: "38052721"
    authors: Jacobs S, Lundy N, Issenberg S, Chandran L
    year: "2023"
    abstract: The proliferation of generative artificial intelligence (AI) and its extensive potential for integration into many aspects of health care signal a transformational shift within the health care environment. In this context, medical education must evolve to ensure that medical trainees are adequately prepared to navigate the rapidly changing health care landscape. Medical education has moved toward a competency-based education paradigm, leading the Association of American Medical Colleges (AAMC) to define a set of Entrustable Professional Activities (EPAs) as its practical operational framework in undergraduate medical education. The AAMC's 13 core EPAs for entering residencies have been implemented with varying levels of success across medical schools. In this paper, we critically assess the existing core EPAs in the context of rapid AI integration in medicine. We identify EPAs that require refinement, redefinition, or comprehensive change to align with the emerging trends in health care. Moreover, this perspective proposes a set of "emerging" EPAs, informed by the changing landscape and capabilities presented by generative AI technologies. We provide a practical evaluation of the EPAs, alongside actionable recommendations on how medical education, viewed through the lens of the AAMC EPAs, can adapt and remain relevant amid rapid technological advancements. By leveraging the transformative potential of AI, we can reshape medical education to align with an AI-integrated future of medicine. This approach will help equip future health care professionals with technological competence and adaptive skills to meet the dynamic and evolving demands in health care.
    mergedFrom:
      - source: pubmed
        pmid: "38052721"
        doi: 10.2196/50903
      - source: scopus
        doi: 10.2196/50903
        scopusId: SCOPUS_ID:85182551145
    finalDecision: exclude
  - title: "Performance of large language models at the MRCS Part A: a tool for medical education?"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarking LLMs on MRCS Part A surgical exam questions; not about using AI for assessment/feedback of learners in workplace settings."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on MRCS exam question performance rather than workplace-based assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.1308/rcsann.2023.0085
    pmid: "38037955"
    authors: Yiu A, Lam K
    year: "2025"
    abstract: |-
      INTRODUCTION: The Intercollegiate Membership of the Royal College of Surgeons examination (MRCS) Part A assesses generic surgical sciences and applied knowledge using 300 multiple-choice Single Best Answer items. Large Language Models (LLMs) are trained on vast amounts of text to generate natural language outputs, and applications in healthcare and medical education are rising.

      METHODS: Two LLMs, ChatGPT (OpenAI) and Bard (Google AI), were tested using 300 questions from a popular MRCS Part A question bank without/with need for justification (NJ/J). LLM outputs were scored according to accuracy, concordance and insight.

      RESULTS: ChatGPT achieved 85.7%/84.3% accuracy for NJ/J encodings. Bard achieved 64%/64.3% accuracy for NJ/J encodings. ChatGPT and Bard displayed high levels of concordance for NJ (95.3%; 81.7%) and J (93.7%; 79.7%) encodings, respectively. ChatGPT and Bard provided an insightful statement in >98% and >86% outputs, respectively.

      DISCUSSION: This study demonstrates that ChatGPT achieves passing-level accuracy at MRCS Part A, and both LLMs achieve high concordance and provide insightful responses to test questions. Instances of clinically inappropriate or inaccurate decision-making, incomplete appreciation of nuanced clinical scenarios and utilisation of out-of-date guidance was, however, noted. LLMs are accessible and time-efficient tools, access vast clinical knowledge, and may reduce the emphasis on factual recall in medical education and assessment.

      CONCLUSION: ChatGPT achieves passing-level accuracy for MRCS Part A with concordant and insightful outputs. Future applications of LLMs in healthcare must be cautious of hallucinations and incorrect reasoning but have the potential to develop AI-supported clinicians.
    mergedFrom:
      - source: pubmed
        pmid: "38037955"
        doi: 10.1308/rcsann.2023.0085
    finalDecision: exclude
  - title: ChatGPT's performance before and after teaching in mass casualty incident triage.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Title indicates triage teaching/performance context, not workplace-based or clinical education assessment setting."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment/feedback of learners; evaluates ChatGPT's own triage performance on validated questionnaire scenarios"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Assesses chatbot triage test performance, not assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1038/s41598-023-46986-0
    pmid: "37989755"
    authors: Gan R, Uddin H, Gan A, Yew Y, Gonz&#xe1;lez P
    year: "2023"
    abstract: Since its initial launching, ChatGPT has gained significant attention from the media, with many claiming that ChatGPT's arrival is a transformative milestone in the advancement of the AI revolution. Our aim was to assess the performance of ChatGPT before and after teaching the triage of mass casualty incidents by utilizing a validated questionnaire specifically designed for such scenarios. In addition, we compared the triage performance between ChatGPT and medical students. Our cross-sectional study employed a mixed-methods analysis to assess the performance of ChatGPT in mass casualty incident triage, pre- and post-teaching of Simple Triage And Rapid Treatment (START) triage. After teaching the START triage algorithm, ChatGPT scored an overall triage accuracy of 80%, with only 20% of cases being over-triaged. The mean accuracy of medical students on the same questionnaire yielded 64.3%. Qualitative analysis on pre-determined themes on 'walking-wounded', 'respiration', 'perfusion', and 'mental status' on ChatGPT showed similar performance in pre- and post-teaching of START triage. Additional themes on 'disclaimer', 'prediction', 'management plan', and 'assumption' were identified during the thematic analysis. ChatGPT exhibited promising results in effectively responding to mass casualty incident questionnaires. Nevertheless, additional research is necessary to ensure its safety and efficacy before clinical implementation.
    mergedFrom:
      - source: pubmed
        pmid: "37989755"
        doi: 10.1038/s41598-023-46986-0
      - source: scopus
        doi: 10.1038/s41598-023-46986-0
        scopusId: SCOPUS_ID:85177441540
    finalDecision: exclude
  - title: Making Use of Natural Language Processing to Better Understand Medical Students' Self-Assessment of Clinical Skills.
    reviews:
      - decision: uncertain
        comment: NLP for analyzing self-assessment of clinical skills; abstract needed to determine if generative AI and WBA-related
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses TopEx, a conventional NLP tool for topic extraction, not generative AI/LLMs."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP (TopEx), not generative AI/LLMs."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.1097/ACM.0000000000005527
    pmid: "37976396"
    authors: Turner L, Weber D, Santen S, Olex A, Baker P, Overla S, Shu D, Randolph A, Kelleher M
    year: "2024"
    abstract: |-
      PROBLEM: Reflective practice is necessary for self-regulated learning. Helping medical students develop these skills can be challenging since they are difficult to observe. One common solution is to assign students' reflective self-assessments, which produce large quantities of narrative assessment data. Reflective self-assessments also provide feedback to faculty regarding students' understanding of content, reflective abilities, and areas for course improvement. To maximize student learning and feedback to faculty, reflective self-assessments must be reviewed and analyzed, activities that are often difficult for faculty due to the time-intensive and cumbersome nature of processing large quantities of narrative assessment data.

      APPROACH: The authors collected narrative assessment data (2,224 students' reflective self-assessments) from 344 medical students' reflective self-assessments. In academic years 2019-2020 and 2021-2022, students at the University of Cincinnati College of Medicine responded to 2 prompts (aspects that surprised students, areas for student improvement) after reviewing their standardized patient encounters. These free-text entries were analyzed using TopEx, an open-source natural language processing (NLP) tool, to identify common topics and themes, which faculty then reviewed.

      OUTCOMES: TopEx expedited theme identification in students' reflective self-assessments, unveiling 10 themes for prompt 1 such as question organization and history analysis, and 8 for prompt 2, including sensitive histories and exam efficiency. Using TopEx offered a user-friendly, time-saving analysis method without requiring complex NLP implementations. The authors discerned 4 education enhancement implications: aggregating themes for future student reflection, revising self-assessments for common improvement areas, adjusting curriculum to guide students better, and aiding faculty in providing targeted upcoming feedback.

      NEXT STEPS: The University of Cincinnati College of Medicine aims to refine and expand the utilization of TopEx for deeper narrative assessment analysis, while other institutions may model or extend this approach to uncover broader educational insights and drive curricular advancements.
    mergedFrom:
      - source: pubmed
        pmid: "37976396"
        doi: 10.1097/ACM.0000000000005527
    finalDecision: exclude
  - title: Performance of ChatGPT and Bard on the official part 1 FRCOphth practice questions.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT and Bard on FRCOphth MCQ exam questions; not about using AI for assessment or feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Practice-question performance study, not workplace-based assessment feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
    doi: 10.1136/bjo-2023-324091
    pmid: "37932006"
    authors: Fowler T, Pullen S, Birkett L
    year: "2024"
    abstract: |-
      BACKGROUND: Chat Generative Pre-trained Transformer (ChatGPT), a large language model by OpenAI, and Bard, Google's artificial intelligence (AI) chatbot, have been evaluated in various contexts. This study aims to assess these models' proficiency in the part 1 Fellowship of the Royal College of Ophthalmologists (FRCOphth) Multiple Choice Question (MCQ) examination, highlighting their potential in medical education.

      METHODS: Both models were tested on a sample question bank for the part 1 FRCOphth MCQ exam. Their performances were compared with historical human performance on the exam, focusing on the ability to comprehend, retain and apply information related to ophthalmology. We also tested it on the book 'MCQs for FRCOpth part 1', and assessed its performance across subjects.

      RESULTS: ChatGPT demonstrated a strong performance, surpassing historical human pass marks and examination performance, while Bard underperformed. The comparison indicates the potential of certain AI models to match, and even exceed, human standards in such tasks.

      CONCLUSION: The results demonstrate the potential of AI models, such as ChatGPT, in processing and applying medical knowledge at a postgraduate level. However, performance varied among different models, highlighting the importance of appropriate AI selection. The study underlines the potential for AI applications in medical education and the necessity for further investigation into their strengths and limitations.
    mergedFrom:
      - source: pubmed
        pmid: "37932006"
        doi: 10.1136/bjo-2023-324091
    finalDecision: exclude
  - title: AI did not pass Finnish plastic surgery written board examination.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. This study tests ChatGPT and Microsoft Bing on Finnish plastic surgery written board examination questions. It is a board exam performance study with no workplace-based assessment component."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17T07:34:39.189Z
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is a written board exam setting, i.e., simulated/non-workplace rather than workplace-based clinical assessment."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17T07:34:41.875Z
      - decision: exclude
        comment: "Exclusion criterion 4: Written board exam, not workplace-based. Cr.3 (simulation) is not appropriate as this is a written exam, not a clinical simulation (OSCE)."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1016/j.bjps.2023.10.059
    pmid: "37871487"
    authors: Kelloniemi M, Koljonen V
    year: "2023"
    mergedFrom:
      - source: pubmed
        pmid: "37871487"
        doi: 10.1016/j.bjps.2023.10.059
    finalDecision: exclude
  - title: "An explorative assessment of ChatGPT as an aid in medical education: Use it with caution."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records. Study explores ChatGPT for generating educational content outlines, learning objectives, and exam questions."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Educational prompting study in a medical school context is not workplace-based clinical education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
    doi: 10.1080/0142159X.2023.2271159
    pmid: "37862566"
    authors: Han Z, Battaglia F, Udaiyar A, Fooks A, Terlecky S
    year: "2024"
    abstract: |-
      OBJECTIVE: To explore the use of ChatGPT by educators and students in a medical school setting.

      METHOD: This study used the public version of ChatGPT launched by OpenAI on November 30, 2022 (https://openai.com/blog/chatgpt/). We employed prompts to ask ChatGPT to 1) generate a content outline for a session on the topics of cholesterol, lipoproteins, and hyperlipidemia for medical students; 2) produce a list of learning objectives for the session; and 3) write assessment questions with and without clinical vignettes related to the identified learning objectives. We assessed the responses by ChatGPT for accuracy and reliability to determine the potential of the chatbot as an aid to educators and as a "know-it-all" medical information provider for students.

      RESULTS: ChatGPT can function as an aid to educators, but it is not yet suitable as a reliable information resource for educators and medical students.

      CONCLUSION: ChatGPT can be a useful tool to assist medical educators in drafting course and session content outlines and create assessment questions. At the same time, caution must be taken as ChatGPT is prone to providing incorrect information; expert oversight and caution are necessary to ensure the information generated is accurate and beneficial to students. Therefore, it is premature for medical students to use the current version of ChatGPT as a "know-it-all" information provider. In the future, medical educators should work with programming experts to explore and grow the full potential of AI in medical education.
    mergedFrom:
      - source: pubmed
        pmid: "37862566"
        doi: 10.1080/0142159X.2023.2271159
    finalDecision: exclude
  - title: "Advantages and pitfalls in utilizing artificial intelligence for crafting medical examinations: a medical education pilot study with GPT-4."
    reviews:
      - decision: uncertain
        comment: GPT-4 for exam creation; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Uses GPT-4 to generate MCQ exam items; not related to workplace-based or clinical education assessment/feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Addresses AI-generated MCQ exam item writing, not workplace-based assessment, feedback generation, or analysis of observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1186/s12909-023-04752-w
    pmid: "37848913"
    authors: E K, S P, R G, R K, A B, M G, T O, S R, V R, H M, G S
    year: "2023"
    abstract: |-
      BACKGROUND: The task of writing multiple choice question examinations for medical students is complex, timely and requires significant efforts from clinical staff and faculty. Applying artificial intelligence algorithms in this field of medical education may be advisable.

      METHODS: During March to April 2023, we utilized GPT-4, an OpenAI application, to write a 210 multi choice questions-MCQs examination based on an existing exam template and thoroughly investigated the output by specialist physicians who were blinded to the source of the questions. Algorithm mistakes and inaccuracies, as identified by specialists were classified as stemming from age, gender or geographical insensitivities.

      RESULTS: After inputting a detailed prompt, GPT-4 produced the test rapidly and effectively. Only 1 question (0.5%) was defined as false; 15% of questions necessitated revisions. Errors in the AI-generated questions included: the use of outdated or inaccurate terminology, age-sensitive inaccuracies, gender-sensitive inaccuracies, and geographically sensitive inaccuracies. Questions that were disqualified due to flawed methodology basis included elimination-based questions and questions that did not include elements of integrating knowledge with clinical reasoning.

      CONCLUSION: GPT-4 can be used as an adjunctive tool in creating multi-choice question medical examinations yet rigorous inspection by specialist physicians remains pivotal.
    mergedFrom:
      - source: pubmed
        pmid: "37848913"
        doi: 10.1186/s12909-023-04752-w
    finalDecision: exclude
  - title: "Reshaping medical education: Performance of ChatGPT on a PES medical examination."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT-4 performance on Polish medical specialization licensing exam; not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on exam question-answering performance, not workplace-based assessment or feedback records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
    doi: 10.5603/cj.97517
    pmid: "37830257"
    authors: W&#xf3;jcik S, Rulkiewicz A, Pruszczyk P, Lisik W, Pobo&#x17c;y M, Domienik-Kar&#x142;owicz J
    year: "2024"
    abstract: |-
      BACKGROUND: We are currently experiencing a third digital revolution driven by artificial intelligence (AI), and the emergence of new chat generative pre-trained transformer (ChatGPT) represents a significant technological advancement with profound implications for global society, especially in the field of education.

      METHODS: The aim of this study was to see how well ChatGPT performed on medical school exams and to highlight how it might change medical education and practice. Recently, OpenAI's ChatGPT (OpenAI, San Francisco; GPT-4 May 24 Version) was put to the test against a significant Polish medical specialization licensing exam (PES), and the results are in. The version of ChatGPT-4 used in this study was the most up-to-date model at the time of publication (GPT-4). ChatGPT answered questions from June 28, 2023, to June 30, 2023.

      RESULTS: ChatGPT demonstrates notable advancements in natural language processing models on the tasks of medical question answering. In June 2023, the performance of ChatGPT was assessed based on its ability to answer a set of 120 questions, where it achieved a correct response rate of 67.1%, accurately responding to 80 questions.

      CONCLUSIONS: ChatGPT may be used as an assistance tool in medical education. While ChatGPT can serve as a valuable tool in medical education, it cannot fully replace human expertise and knowledge due to its inherent limitations.
    mergedFrom:
      - source: pubmed
        pmid: "37830257"
        doi: 10.5603/cj.97517
    finalDecision: exclude
  - title: Influence of the embedded participant on learners' performance during high-fidelity simulation sessions in healthcare.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About embedded participants in simulation sessions, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is explicitly in high-fidelity simulation sessions only."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    doi: 10.1186/s12909-023-04724-0
    pmid: "37821900"
    authors: Koca A, Schlatter S, Delas Q, Denoyel L, Lehot J, Lilot M, Rimmel&#xe9; T
    year: "2023"
    abstract: |-
      BACKGROUND: The embedded participant (EP) plays a key role during a full scale/high-fidelity simulation (HFS) session. He/she is expected to guide the learner towards the achievement of the educational objectives of the scenario. However, his/her influence on learners' performance stands undetermined and this effect remains briefly addressed in the literature. This study primarily aims to assess whether the EP could have an influence on the performance of the learner during a HFS scenario. The secondary aim was to establish an inventory of the EP practices in France.

      METHODS: This retrospective study was conducted in Lyon Claude Bernard University Health Simulation Centre (France). Anaesthesia and critical care residents in postgraduate years 1 to 5 who were scheduled for their HFS sessions during the 2016 to 2021 academic years were included. Two investigators independently evaluated the resident performance regarding both technical and non-technical skills from video recordings. In addition, a nationwide survey was sent out by email through the networks of the Francophone Healthcare Simulation Society (SoFraSimS, Société Francophone de Simulation en Santé) to collect information on EP practices in French-speaking Simulation centres.

      RESULTS: From a total of 344 HFS videos analysed, a cohort of 19 experienced EPs was identified. The EPs had an influence on the technical and non-technical performances of the learners. The 147 responses to the survey showed that predefined rules of EP good practice existed in only 36% of the simulation centres and 65% of respondents believed that specific EP training would be justified.

      CONCLUSION: The EP can exert an influence on the performance of the learners during HFS. For acting as an EP, a great variability of practices and a lack of training are reported by professionals working in simulation centres. These results suggest that more attention must be paid to EP training and attitudes during simulation, especially if summative simulations are extensively developed.
    mergedFrom:
      - source: pubmed
        pmid: "37821900"
        doi: 10.1186/s12909-023-04724-0
    finalDecision: exclude
  - title: "ChatGPT Surpasses 1000 Publications on PubMed: Envisioning the Road Ahead."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/editorial about ChatGPT publication trends, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Appears to be commentary/editorial rather than original research or evidence synthesis."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    doi: 10.7759/cureus.44769
    pmid: "37809155"
    authors: Temsah M, Altamimi I, Jamal A, Alhasan K, Al-Eyadhy A
    year: "2023"
    abstract: The exponential growth of ChatGPT in medical literature, amassing over 1000 PubMed citations by August 2023, underscores a pivotal juncture in the convergence of artificial intelligence (AI) and healthcare. This remarkable rise not only showcases its potential to revolutionize medical academia but also indicates its impending influence on patient care and healthcare systems. Notwithstanding this enthusiasm, one-third of these citations are editorials or commentaries, stressing a gap in empirical research. Alongside its potential, there are concerns about ChatGPT becoming a "Weapon of Mass Deception" and the need for rigorous evaluations to counter inaccuracies. The World Association of Medical Editors has released guidelines emphasizing that AI tools should not be manuscript co-authors and advocates for clear disclosures in AI-assisted academic works. Interestingly, ChatGPT achieved its citation milestone within nine months, compared to Google's 14 years. As Large Language Models (LLMs), like ChatGPT, become more integral in healthcare, issues surrounding data protection, patient privacy, and ethical implications gain prominence. As the future of LLM research unfolds, key areas of interest include its efficacy in clinical settings, its role in telemedicine, and its potential in medical education. The journey ahead necessitates a harmonious partnership between the medical community and AI developers, emphasizing both technological advancements and ethical considerations.
    mergedFrom:
      - source: pubmed
        pmid: "37809155"
        doi: 10.7759/cureus.44769
    finalDecision: exclude
  - title: "The Scientific Knowledge of Bard and ChatGPT in Endocrinology, Diabetes, and Diabetes Technology: Multiple-Choice Questions Examination-Based Performance."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; benchmarks Bard and ChatGPT on endocrinology/diabetes MCQ examinations."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Evaluates AI models' knowledge without learner participants in workplace-based assessment contexts."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.1177/19322968231203987
    pmid: "37798960"
    authors: Meo S, Al-Khlaiwi T, AbuKhalaf A, Meo A, Klonoff D
    year: "2025"
    abstract: |-
      BACKGROUND: The present study aimed to investigate the knowledge level of Bard and ChatGPT in the areas of endocrinology, diabetes, and diabetes technology through a multiple-choice question (MCQ) examination format.

      METHODS: Initially, a 100-MCQ bank was established based on MCQs in endocrinology, diabetes, and diabetes technology. The MCQs were created from physiology, medical textbooks, and academic examination pools in the areas of endocrinology, diabetes, and diabetes technology and academic examination pools. The study team members analyzed the MCQ contents to ensure that they were related to the endocrinology, diabetes, and diabetes technology. The number of MCQs from endocrinology was 50, and that from diabetes and science technology was also 50. The knowledge level of Google's Bard and ChatGPT was assessed with an MCQ-based examination.

      RESULTS: In the endocrinology examination section, ChatGPT obtained 29 marks (correct responses) of 50 (58%), and Bard obtained a similar score of 29 of 50 (58%). However, in the diabetes technology examination section, ChatGPT obtained 23 marks of 50 (46%), and Bard obtained 20 marks of 50 (40%). Overall, in the entire three-part examination, ChatGPT obtained 52 marks of 100 (52%), and Bard obtained 49 marks of 100 (49%). ChatGPT obtained slightly more marks than Bard. However, both ChatGPT and Bard did not achieve satisfactory scores in endocrinology or diabetes/technology of at least 60%.

      CONCLUSIONS: The overall MCQ-based performance of ChatGPT was slightly better than that of Google's Bard. However, both ChatGPT and Bard did not achieve appropriate scores in endocrinology and diabetes/diabetes technology. The study indicates that Bard and ChatGPT have the potential to facilitate medical students and faculty in academic medical education settings, but both artificial intelligence tools need more updated information in the fields of endocrinology, diabetes, and diabetes technology.
    mergedFrom:
      - source: pubmed
        pmid: "37798960"
        doi: 10.1177/19322968231203987
    finalDecision: exclude
  - title: Can ChatGPT pass China's national medical licensing examination?
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on national licensing examination, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. This letter to the editor evaluates ChatGPT on China's National Medical Licensing Examination questions with no workplace-based assessment or feedback component."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17T07:34:40.555Z
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on national licensing exam question performance in a simulated test environment, not workplace-based clinical education."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17T07:34:43.172Z
      - decision: exclude
        comment: "Exclusion criterion 4: National licensing exam, not workplace-based."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1016/j.asjsur.2023.09.089
    pmid: "37775381"
    authors: Shang L, Xue M, Hou Y, Tang B
    year: "2023"
    mergedFrom:
      - source: pubmed
        pmid: "37775381"
        doi: 10.1016/j.asjsur.2023.09.089
    finalDecision: exclude
  - title: "The Rapid Development of Artificial Intelligence: GPT-4's Performance on Orthopedic Surgery Board Questions."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Benchmarks GPT-4 on orthopedic surgery board-style questions (OITE), an academic exam setting, not workplace-based or clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Orthopedic board examination performance study outside workplace-based assessment context."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.3928/01477447-20230922-05
    pmid: "37757748"
    authors: Hofmann H, Guerra G, Le J, Wong A, Hofmann G, Mayfield C, Petrigliano F, Liu J
    year: "2024"
    abstract: Advances in artificial intelligence and machine learning models, like Chat Generative Pre-trained Transformer (ChatGPT), have occurred at a remarkably fast rate. OpenAI released its newest model of ChatGPT, GPT-4, in March 2023. It offers a wide range of medical applications. The model has demonstrated notable proficiency on many medical board examinations. This study sought to assess GPT-4's performance on the Orthopaedic In-Training Examination (OITE) used to prepare residents for the American Board of Orthopaedic Surgery (ABOS) Part I Examination. The data gathered from GPT-4's performance were additionally compared with the data of the previous iteration of ChatGPT, GPT-3.5, which was released 4 months before GPT-4. GPT-4 correctly answered 251 of the 396 attempted questions (63.4%), whereas GPT-3.5 correctly answered 46.3% of 410 attempted questions. GPT-4 was significantly more accurate than GPT-3.5 on orthopedic board-style questions (P<.00001). GPT-4's performance is most comparable to that of an average third-year orthopedic surgery resident, while GPT-3.5 performed below an average orthopedic intern. GPT-4's overall accuracy was just below the approximate threshold that indicates a likely pass on the ABOS Part I Examination. Our results demonstrate significant improvements in OpenAI's newest model, GPT-4. Future studies should assess potential clinical applications as AI models continue to be trained on larger data sets and offer more capabilities. [Orthopedics. 2024;47(2):e85-e89.].
    mergedFrom:
      - source: pubmed
        pmid: "37757748"
        doi: 10.3928/01477447-20230922-05
    finalDecision: exclude
  - title: ChatGPT versus human in generating medical graduate exam multiple choice questions-A multinational prospective study (Hong Kong S.A.R., Singapore, Ireland, and the United Kingdom).
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Compares AI-generated vs human-generated MCQs for graduate medical exams; about exam item generation, not assessment/feedback of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates AI for generating MCQ exam items, not workplace-based assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    doi: 10.1371/journal.pone.0290691
    pmid: "37643186"
    authors: Cheung B, Lau G, Wong G, Lee E, Kulkarni D, Seow C, Wong R, Co M
    year: "2023"
    abstract: |-
      INTRODUCTION: Large language models, in particular ChatGPT, have showcased remarkable language processing capabilities. Given the substantial workload of university medical staff, this study aims to assess the quality of multiple-choice questions (MCQs) produced by ChatGPT for use in graduate medical examinations, compared to questions written by university professoriate staffs based on standard medical textbooks.

      METHODS: 50 MCQs were generated by ChatGPT with reference to two standard undergraduate medical textbooks (Harrison's, and Bailey & Love's). Another 50 MCQs were drafted by two university professoriate staff using the same medical textbooks. All 100 MCQ were individually numbered, randomized and sent to five independent international assessors for MCQ quality assessment using a standardized assessment score on five assessment domains, namely, appropriateness of the question, clarity and specificity, relevance, discriminative power of alternatives, and suitability for medical graduate examination.

      RESULTS: The total time required for ChatGPT to create the 50 questions was 20 minutes 25 seconds, while it took two human examiners a total of 211 minutes 33 seconds to draft the 50 questions. When a comparison of the mean score was made between the questions constructed by A.I. with those drafted by humans, only in the relevance domain that the A.I. was inferior to humans (A.I.: 7.56 +/- 0.94 vs human: 7.88 +/- 0.52; p = 0.04). There was no significant difference in question quality between questions drafted by A.I. versus humans, in the total assessment score as well as in other domains. Questions generated by A.I. yielded a wider range of scores, while those created by humans were consistent and within a narrower range.

      CONCLUSION: ChatGPT has the potential to generate comparable-quality MCQs for medical graduate examinations within a significantly shorter time.
    mergedFrom:
      - source: pubmed
        pmid: "37643186"
        doi: 10.1371/journal.pone.0290691
    finalDecision: exclude
  - title: "Teaching clinical empathy skills in medical education: Can ChatGPT assist the educator?"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article. This is a Letter to the Editor in Medical Teacher discussing ChatGPT's potential for teaching clinical empathy skills, presenting no original data."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17T07:34:39.189Z
      - decision: exclude
        comment: "Exclusion criterion 7: This is a Dear Editor letter/commentary rather than an original research study, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17T07:34:41.875Z
      - decision: exclude
        comment: "Exclusion criterion 7: Letter to the Editor, not original research/SR/MA."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1080/0142159X.2023.2247144
    pmid: "37591768"
    authors: Ennab F
    year: "2023"
    mergedFrom:
      - source: pubmed
        pmid: "37591768"
        doi: 10.1080/0142159X.2023.2247144
    finalDecision: exclude
  - title: The role of ChatGPT in enhancing ENT surgical training - a trainees' perspective.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article; this is a guidance/opinion piece reviewing how ChatGPT can supplement otolaryngology education, without presenting empirical data."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Perspective/guidance article rather than original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.1017/S0022215123001354
    pmid: "37583313"
    authors: Brennan L, Balakumar R, Bennett W
    year: "2024"
    abstract: |-
      OBJECTIVE: ChatGPT, developed by Open AI (November 2022) is a powerful artificial intelligence language model, designed to produce human-like text from user-written prompts. Prompts must give context-specific information to produce valuable responses. Otolaryngology is a specialist field that sees limited exposure during undergraduate and postgraduate education. Additionally, otolaryngology trainees have seen a reduction in learning opportunities since the coronavirus disease 2019 pandemic.

      METHOD: This article aims to give guidance on optimising the ChatGPT system in the context of education for otolaryngology by reviewing barriers to otolaryngology education and suggesting ways that ChatGPT can overcome them by providing examples using the authors' experience.

      RESULTS: Overall, the authors saw that ChatGPT demonstrated some useful qualities, particularly with regards to assistance with communication skills and individualised patient responses.

      CONCLUSION: Although ChatGPT cannot replace traditional mentorship and practical surgical experience, it can serve as an invaluable supplementary resource to education in otolaryngology.
    mergedFrom:
      - source: pubmed
        pmid: "37583313"
        doi: 10.1017/S0022215123001354
    finalDecision: exclude
  - title: ChatGPT Performs on the Chinese National Medical Licensing Examination.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment/feedback of learners; evaluates ChatGPT's own performance on the Chinese NMLE"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:41.361000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Licensing exam performance study not tied to WBA assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:47.316000+00:00
    doi: 10.1007/s10916-023-01961-0
    pmid: "37581690"
    authors: Wang X, Gong Z, Wang G, Jia J, Xu Y, Zhao J, Fan Q, Wu S, Hu W, Li X
    year: "2023"
    abstract: ChatGPT, a language model developed by OpenAI, uses a 175 billion parameter Transformer architecture for natural language processing tasks. This study aimed to compare the knowledge and interpretation ability of ChatGPT with those of medical students in China by administering the Chinese National Medical Licensing Examination (NMLE) to both ChatGPT and medical students. We evaluated the performance of ChatGPT in three years' worth of the NMLE, which consists of four units. At the same time, the exam results were compared to those of medical students who had studied for five years at medical colleges. ChatGPT's performance was lower than that of the medical students, and ChatGPT's correct answer rate was related to the year in which the exam questions were released. ChatGPT's knowledge and interpretation ability for the NMLE were not yet comparable to those of medical students in China. It is probable that these abilities will improve through deep learning.
    mergedFrom:
      - source: pubmed
        pmid: "37581690"
        doi: 10.1007/s10916-023-01961-0
    finalDecision: exclude
  - title: "The Role of Large Language Models in Medical Education: Applications and Implications."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Editorial/viewpoint piece written from medical students' perspective about LLM use cases; not original research."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:26.473000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Editorial/perspective article, not original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:31.024000+00:00
    doi: 10.2196/50945
    pmid: "37578830"
    authors: Safranek C, Sidamon-Eristoff A, Gilson A, Chartash D
    year: "2023"
    abstract: "Large language models (LLMs) such as ChatGPT have sparked extensive discourse within the medical education community, spurring both excitement and apprehension. Written from the perspective of medical students, this editorial offers insights gleaned through immersive interactions with ChatGPT, contextualized by ongoing research into the imminent role of LLMs in health care. Three distinct positive use cases for ChatGPT were identified: facilitating differential diagnosis brainstorming, providing interactive practice cases, and aiding in multiple-choice question review. These use cases can effectively help students learn foundational medical knowledge during the preclinical curriculum while reinforcing the learning of core Entrustable Professional Activities. Simultaneously, we highlight key limitations of LLMs in medical education, including their insufficient ability to teach the integration of contextual and external information, comprehend sensory and nonverbal cues, cultivate rapport and interpersonal interaction, and align with overarching medical education and patient care goals. Through interacting with LLMs to augment learning during medical school, students can gain an understanding of their strengths and weaknesses. This understanding will be pivotal as we navigate a health care landscape increasingly intertwined with LLMs and artificial intelligence."
    mergedFrom:
      - source: pubmed
        pmid: "37578830"
        doi: 10.2196/50945
      - source: scopus
        doi: 10.2196/50945
        scopusId: SCOPUS_ID:85168841910
    finalDecision: exclude
  - title: "Sailing the Seven Seas: A Multinational Comparison of ChatGPT's Performance on Medical Licensing Examinations."
    reviews:
      - decision: uncertain
        comment: ChatGPT on licensing exams; abstract may reveal WBA-relevant elements
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Study benchmarks ChatGPT performance on medical licensing exams rather than using AI to assess learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates performance on licensing exam questions rather than workplace-based assessment feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1007/s10439-023-03338-3
    pmid: "37553555"
    authors: Alfertshofer M, Hoch C, Funk P, Hollmann K, Wollenberg B, Knoedler S, Knoedler L
    year: "2024"
    abstract: |-
      PURPOSE: The use of AI-powered technology, particularly OpenAI's ChatGPT, holds significant potential to reshape healthcare and medical education. Despite existing studies on the performance of ChatGPT in medical licensing examinations across different nations, a comprehensive, multinational analysis using rigorous methodology is currently lacking. Our study sought to address this gap by evaluating the performance of ChatGPT on six different national medical licensing exams and investigating the relationship between test question length and ChatGPT's accuracy.

      METHODS: We manually inputted a total of 1,800 test questions (300 each from US, Italian, French, Spanish, UK, and Indian medical licensing examination) into ChatGPT, and recorded the accuracy of its responses.

      RESULTS: We found significant variance in ChatGPT's test accuracy across different countries, with the highest accuracy seen in the Italian examination (73% correct answers) and the lowest in the French examination (22% correct answers). Interestingly, question length correlated with ChatGPT's performance in the Italian and French state examinations only. In addition, the study revealed that questions requiring multiple correct answers, as seen in the French examination, posed a greater challenge to ChatGPT.

      CONCLUSION: Our findings underscore the need for future research to further delineate ChatGPT's strengths and limitations in medical test-taking across additional countries and to develop guidelines to prevent AI-assisted cheating in medical examinations.
    mergedFrom:
      - source: pubmed
        pmid: "37553555"
        doi: 10.1007/s10439-023-03338-3
    finalDecision: exclude
  - title: "Dr. ChatGPT: Utilizing Artificial Intelligence in Surgical Education."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Evaluates ChatGPT-generated surgical procedural descriptions and resident preferences, not related to workplace-based assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates AI-generated procedural explanations, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.1177/10556656231193966
    pmid: "37545428"
    authors: Lebhar M, Velazquez A, Goza S, Hoppe I
    year: "2024"
    abstract: |-
      OBJECTIVE: This study sought to explore the unexamined capabilities of ChatGPT in describing the surgical steps of a specialized operation, the Fisher cleft lip repair.

      DESIGN: A chat log within ChatGPT was created to generate the procedural steps of a cleft lip repair utilizing the Fisher technique. A board certified craniomaxillofacial (CMF) surgeon then wrote the Fisher repair in his own words blinded to the ChatGPT response. Using both responses, a voluntary survey questionnaire was distributed to residents of plastic and reconstructive surgery (PRS), general surgery (GS), internal medicine (IM), and medical students at our institution in a blinded study.

      SETTING: Authors collected information from residents (PRS, GS, IM) and medical students at one institution.

      MAIN OUTCOME MEASURES: Primary outcome measures included understanding, preference, and author identification of the procedural prompts.

      RESULTS: Results show PRS residents were able to detect more inaccuracies of the ChatGPT response as well as prefer the CMF surgeon's prompt in performing the surgery. Residents with less expertise in the procedure not only failed to detect who wrote what procedure, but preferred the ChatGPT response in explaining the concept and chose it to perform the surgery.

      CONCLUSIONS: In applications to surgical education, ChatGPT was found to be effective in generating easy to understand procedural steps that can be followed by medical personnel of all specialties. However, it does not have expert capabilities to provide the minute detail of measurements and specific anatomy required to perform medical procedures.
    mergedFrom:
      - source: pubmed
        pmid: "37545428"
        doi: 10.1177/10556656231193966
    finalDecision: exclude
  - title: Will code one day run a code? Performance of language models on ACEM primary examinations and implications.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Study benchmarks LLM performance on an emergency medicine practice examination."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates LLMs on specialist exam questions rather than workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    doi: 10.1111/1742-6723.14280
    pmid: "37414729"
    authors: Smith J, Choi P, Buntine P
    year: "2023"
    abstract: |-
      OBJECTIVE: Large language models (LLMs) have demonstrated mixed results in their ability to pass various specialist medical examination and their performance within the field of emergency medicine remains unknown.

      METHODS: We explored the performance of three prevalent LLMs (OpenAI's GPT series, Google's Bard, and Microsoft's Bing Chat) on a practice ACEM primary examination.

      RESULTS: All LLMs achieved a passing score, with scores with GPT 4.0 outperforming the average candidate.

      CONCLUSION: Large language models, by passing the ACEM primary examination, show potential as tools for medical education and practice. However, limitations exist and are discussed.
    mergedFrom:
      - source: pubmed
        pmid: "37414729"
        doi: 10.1111/1742-6723.14280
    finalDecision: exclude
  - title: "ChatGPT: the threats to medical education."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Title suggests commentary/opinion piece, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/editorial publication type"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    doi: 10.1093/postmj/qgad046
    pmid: "37410672"
    authors: Armitage R
    year: "2023"
    abstract: While it offers abundant advantages, ChatGPT threatens to significantly harm the educational attainment, and the intellectual life, of students of medicine and the subjects that compliment it. This technology poses a serious threat to the ability of such students to deliver safe and effective medical care once they graduate to clinical practice. Institutions that providemedical education must react to the existence, availability, and rapidly increasing competency of GPT models. This article suggests an intervention by which this could be, at least partially, achieved.
    mergedFrom:
      - source: pubmed
        pmid: "37410672"
        doi: 10.1093/postmj/qgad046
    finalDecision: exclude
  - title: The ChatGPT (Generative Artificial Intelligence) Revolution Has Made Artificial Intelligence Approachable for Medical Professionals.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/editorial about the ChatGPT revolution, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Title indicates a broad perspective/opinion piece rather than eligible empirical/review study type."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.2196/48392
    pmid: "37347508"
    authors: Mesko B
    year: "2023"
    abstract: In November 2022, OpenAI publicly launched its large language model (LLM), ChatGPT, and reached the milestone of having over 100 million users in only 2 months. LLMs have been shown to be useful in a myriad of health care-related tasks and processes. In this paper, I argue that attention to, public access to, and debate about LLMs have initiated a wave of products and services using generative artificial intelligence (AI), which had previously found it hard to attract physicians. This paper describes what AI tools have become available since the beginning of the ChatGPT revolution and contemplates how it they might change physicians' perceptions about this breakthrough technology.
    mergedFrom:
      - source: pubmed
        pmid: "37347508"
        doi: 10.2196/48392
      - source: scopus
        doi: 10.2196/48392
        scopusId: SCOPUS_ID:85164040035
    finalDecision: exclude
  - title: "ChatGPT in medical school: how successful is AI in progress testing?"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Benchmarks ChatGPT on progress test MCQs in an academic exam setting, not workplace-based or clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:36.268000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Progress testing study in medical school context, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:40.229000+00:00
    doi: 10.1080/10872981.2023.2220920
    pmid: "37307503"
    authors: Friederichs H, Friederichs W, M&#xe4;rz M
    year: "2023"
    abstract: |-
      BACKGROUND: As generative artificial intelligence (AI), ChatGPT provides easy access to a wide range of information, including factual knowledge in the field of medicine. Given that knowledge acquisition is a basic determinant of physicians' performance, teaching and testing different levels of medical knowledge is a central task of medical schools. To measure the factual knowledge level of the ChatGPT responses, we compared the performance of ChatGPT with that of medical students in a progress test.

      METHODS: A total of 400 multiple-choice questions (MCQs) from the progress test in German-speaking countries were entered into ChatGPT's user interface to obtain the percentage of correctly answered questions. We calculated the correlations of the correctness of ChatGPT responses with behavior in terms of response time, word count, and difficulty of a progress test question.

      RESULTS: Of the 395 responses evaluated, 65.5% of the progress test questions answered by ChatGPT were correct. On average, ChatGPT required 22.8 s (SD 17.5) for a complete response, containing 36.2 (SD 28.1) words. There was no correlation between the time used and word count with the accuracy of the ChatGPT response (correlation coefficient for time rho = -0.08, 95% CI [-0.18, 0.02], t(393) = -1.55, p = 0.121; for word count rho = -0.03, 95% CI [-0.13, 0.07], t(393) = -0.54, p = 0.592). There was a significant correlation between the difficulty index of the MCQs and the accuracy of the ChatGPT response (correlation coefficient for difficulty: rho = 0.16, 95% CI [0.06, 0.25], t(393) = 3.19, p = 0.002).

      CONCLUSION: ChatGPT was able to correctly answer two-thirds of all MCQs at the German state licensing exam level in Progress Test Medicine and outperformed almost all medical students in years 1-3. The ChatGPT answers can be compared with the performance of medical students in the second half of their studies.
    mergedFrom:
      - source: pubmed
        pmid: "37307503"
        doi: 10.1080/10872981.2023.2220920
    finalDecision: exclude
  - title: Can Artificial Intelligence Pass the American Board of Orthopaedic Surgery Examination? Orthopaedic Residents Versus ChatGPT.
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; evaluates ChatGPT performance on orthopaedic in-training examination MCQs"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Board-exam question-answering study, not workplace-based assessment or feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.1097/CORR.0000000000002704
    pmid: "37220190"
    authors: Lum Z
    year: "2023"
    abstract: |-
      BACKGROUND: Advances in neural networks, deep learning, and artificial intelligence (AI) have progressed recently. Previous deep learning AI has been structured around domain-specific areas that are trained on dataset-specific areas of interest that yield high accuracy and precision. A new AI model using large language models (LLM) and nonspecific domain areas, ChatGPT (OpenAI), has gained attention. Although AI has demonstrated proficiency in managing vast amounts of data, implementation of that knowledge remains a challenge.

      QUESTIONS/PURPOSES: (1) What percentage of Orthopaedic In-Training Examination questions can a generative, pretrained transformer chatbot (ChatGPT) answer correctly? (2) How does that percentage compare with results achieved by orthopaedic residents of different levels, and if scoring lower than the 10th percentile relative to 5th-year residents is likely to correspond to a failing American Board of Orthopaedic Surgery score, is this LLM likely to pass the orthopaedic surgery written boards? (3) Does increasing question taxonomy affect the LLM's ability to select the correct answer choices?

      METHODS: This study randomly selected 400 of 3840 publicly available questions based on the Orthopaedic In-Training Examination and compared the mean score with that of residents who took the test over a 5-year period. Questions with figures, diagrams, or charts were excluded, including five questions the LLM could not provide an answer for, resulting in 207 questions administered with raw score recorded. The LLM's answer results were compared with the Orthopaedic In-Training Examination ranking of orthopaedic surgery residents. Based on the findings of an earlier study, a pass-fail cutoff was set at the 10th percentile. Questions answered were then categorized based on the Buckwalter taxonomy of recall, which deals with increasingly complex levels of interpretation and application of knowledge; comparison was made of the LLM's performance across taxonomic levels and was analyzed using a chi-square test.

      RESULTS: ChatGPT selected the correct answer 47% (97 of 207) of the time, and 53% (110 of 207) of the time it answered incorrectly. Based on prior Orthopaedic In-Training Examination testing, the LLM scored in the 40th percentile for postgraduate year (PGY) 1s, the eighth percentile for PGY2s, and the first percentile for PGY3s, PGY4s, and PGY5s; based on the latter finding (and using a predefined cutoff of the 10th percentile of PGY5s as the threshold for a passing score), it seems unlikely that the LLM would pass the written board examination. The LLM's performance decreased as question taxonomy level increased (it answered 54% [54 of 101] of Tax 1 questions correctly, 51% [18 of 35] of Tax 2 questions correctly, and 34% [24 of 71] of Tax 3 questions correctly; p = 0.034).

      CONCLUSION: Although this general-domain LLM has a low likelihood of passing the orthopaedic surgery board examination, testing performance and knowledge are comparable to that of a first-year orthopaedic surgery resident. The LLM's ability to provide accurate answers declines with increasing question taxonomy and complexity, indicating a deficiency in implementing knowledge.

      CLINICAL RELEVANCE: Current AI appears to perform better at knowledge and interpretation-based inquires, and based on this study and other areas of opportunity, it may become an additional tool for orthopaedic learning and education.
    mergedFrom:
      - source: pubmed
        pmid: "37220190"
        doi: 10.1097/CORR.0000000000002704
    finalDecision: exclude
  - title: "Utilizing Natural Language Processing of Narrative Feedback to Develop a Predictive Model of Pre-Clerkship Performance: Lessons Learned."
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP/predictive modeling, not generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP (manual topic grouping, predictive modeling) to analyze narrative feedback, not generative AI or LLMs."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Uses conventional NLP predictive modeling rather than generative AI/LLMs."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.5334/pme.40
    pmid: "37151853"
    authors: Maimone C, Dolan B, Green M, Sanguino S, Garcia P, O'Brien C
    year: "2023"
    abstract: |-
      BACKGROUND: Natural language processing is a promising technique that can be used to create efficiencies in the review of narrative feedback to learners. The Feinberg School of Medicine has implemented formal review of pre-clerkship narrative feedback since 2014 through its portfolio assessment system but this process requires considerable time and effort. This article describes how natural language processing was used to build a predictive model of pre-clerkship student performance that can be utilized to assist competency committee reviews.

      APPROACH: The authors took an iterative and inductive approach to the analysis, which allowed them to identify characteristics of narrative feedback that are both predictive of performance and useful to faculty reviewers. Words and phrases were manually grouped into topics that represented concepts illustrating student performance. Topics were reviewed by experienced reviewers, tested for consistency across time, and checked to ensure they did not demonstrate bias.

      OUTCOMES: Sixteen topic groups of words and phrases were found to be predictive of performance. The best-fitting model used a combination of topic groups, word counts, and categorical ratings. The model had an AUC value of 0.92 on the training data and 0.88 on the test data.

      REFLECTION: A thoughtful, careful approach to using natural language processing was essential. Given the idiosyncrasies of narrative feedback in medical education, standard natural language processing packages were not adequate for predicting student outcomes. Rather, employing qualitative techniques including repeated member checking and iterative revision resulted in a useful and salient predictive model.
    mergedFrom:
      - source: pubmed
        pmid: "37151853"
        doi: 10.5334/pme.40
    finalDecision: exclude
  - title: Evaluating ChatGPT's Ability to Solve Higher-Order Questions on the Competency-Based Medical Education Curriculum in Medical Biochemistry.
    reviews:
      - decision: uncertain
        comment: ChatGPT on CBME questions; may relate to assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT answering biochemistry exam questions; not in a workplace-based or clinical education setting"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Conducted in classroom/curriculum question-bank context, not workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.7759/cureus.37023
    pmid: "37143631"
    authors: Ghosh A, Bir A
    year: "2023"
    abstract: "Background Healthcare-related artificial intelligence (AI) is developing. The capacity of the system to carry out sophisticated cognitive processes, such as problem-solving, decision-making, reasoning, and perceiving, is referred to as higher cognitive thinking in AI. This kind of thinking requires more than just processing facts; it also entails comprehending and working with abstract ideas, evaluating and applying data relevant to the context, and producing new insights based on prior learning and experience. ChatGPT is an artificial intelligence-based conversational software that can engage with people to answer questions and uses natural language processing models. The platform has created a worldwide buzz and keeps setting an ongoing trend in solving many complex problems in various dimensions. Nevertheless, ChatGPT's capacity to correctly respond to queries requiring higher-level thinking in medical biochemistry has not yet been investigated. So, this research aimed to evaluate ChatGPT's aptitude for responding to higher-order questions on medical biochemistry. Objective In this study, our objective was to determine whether ChatGPT can address higher-order problems related to medical biochemistry.​​​​​​ Methods​​​ This cross-sectional study was done online by conversing with the current version of ChatGPT (14 March 2023, which is presently free for registered users). It was presented with 200 medical biochemistry reasoning questions that require higher-order thinking. These questions were randomly picked from the institution's question bank and classified according to the Competency-Based Medical Education (CBME) curriculum's competency modules. The responses were collected and archived for subsequent research. Two expert biochemistry academicians examined the replies on a zero to five scale. The score's accuracy was determined by a one-sample Wilcoxon signed rank test using hypothetical values. Result The AI software answered 200 questions requiring higher-order thinking with a median score of 4.0 (Q1=3.50, Q3=4.50). Using a single sample Wilcoxon signed rank test, the result was less than the hypothetical maximum of five (p=0.001) and comparable to four (p=0.16). There was no difference in the replies to questions from different CBME modules in medical biochemistry (Kruskal-Wallis p=0.39). The inter-rater reliability of the scores scored by two biochemistry faculty members was outstanding (ICC=0.926 (95% CI: 0.814-0.971); F=19; p=0.001)​​​​​​ Conclusion The results of this research indicate that ChatGPT has the potential to be a successful tool for answering questions requiring higher-order thinking in medical biochemistry, with a median score of four out of five. However, continuous training and development with data of recent advances are essential to improve performance and make it functional for the ever-growing field of academic medical usage."
    mergedFrom:
      - source: pubmed
        pmid: "37143631"
        doi: 10.7759/cureus.37023
    finalDecision: exclude
  - title: "Medical School Admissions: Focusing on Producing a Physician Workforce That Addresses the Needs of the United States."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about medical school admissions policy"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    doi: 10.1097/ACM.0000000000005262
    pmid: "37130009"
    authors: Prober C, Desai S
    year: "2023"
    abstract: The aging population, burnout, and earlier retirement of physicians along with the static number of training positions are likely to worsen the current physician shortage. There is an urgent need to transform the process for selecting medical students. In this Invited Commentary, the authors suggest that to build the physician workforce that the United States needs for the future, academic medicine should focus on building capacity in 3 overarching areas. First, medical schools need to develop a more diverse pool of capable applicants that better matches the demographic characteristics of health care trainees with those of the population, and they need to nurture applicants with diverse career aspirations. Second, medical schools should recalibrate their student selection process, aligning criteria for admission with competencies expected of medical school graduates, whether they choose to become practicing clinicians, physician-scientists, members of the public health workforce, or policy makers. Selection criteria that overweight the results of standardized test scores should be replaced by assessments that value and predict academic capacity, adaptive learning skills, curiosity, compassion, empathy, emotional maturity, and superior communication skills. Finally, to improve the equity and effectiveness of the selection processes, medical schools should leverage innovations in data science and generative artificial intelligence platforms. The ability of ChatGPT to pass the United States Medical Licensing Examination (USMLE) demonstrates the decreasing importance of memorization in medicine in favor of critical thinking and problem-solving skills. The 2022 change in the USMLE Step 1 to pass/fail plus the exodus of several prominent medical schools from the U.S. News and World Report rankings have exposed limitations of the current selection processes. Newer approaches that use precision education systems to leverage data and technology can help address these limitations.
    mergedFrom:
      - source: pubmed
        pmid: "37130009"
        doi: 10.1097/ACM.0000000000005262
    finalDecision: exclude
  - title: Assessing the Capability of ChatGPT in Answering First- and Second-Order Knowledge Questions on Microbiology as per Competency-Based Medical Education Curriculum.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT answering microbiology knowledge questions, exam/curriculum-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Evaluates ChatGPT's ability to answer microbiology knowledge questions from the CBME curriculum; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.7759/cureus.36034
    pmid: "37056538"
    authors: Das D, Kumar N, Longjam L, Sinha R, Deb Roy A, Mondal H, Gupta P
    year: "2023"
    abstract: "Background and objective ChatGPT is an artificial intelligence (AI) language model that has been trained to process and respond to questions across a wide range of topics. It is also capable of solving problems in medical educational topics. However, the capability of ChatGPT to accurately answer first- and second-order knowledge questions in the field of microbiology has not been explored so far. Hence, in this study, we aimed to analyze the capability of ChatGPT in answering first- and second-order questions on the subject of microbiology. Materials and methods Based on the competency-based medical education (CBME) curriculum of the subject of microbiology, we prepared a set of first-order and second-order questions. For the total of eight modules in the CBME curriculum for microbiology, we prepared six first-order and six second-order knowledge questions according to the National Medical Commission-recommended CBME curriculum, amounting to a total of (8 x 12) 96 questions. The questions were checked for content validity by three expert microbiologists. These questions were used to converse with ChatGPT by a single user and responses were recorded for further analysis. The answers were scored by three microbiologists on a rating scale of 0-5. The average of three scores was taken as the final score for analysis. As the data were not normally distributed, we used a non-parametric statistical test. The overall scores were tested by a one-sample median test with hypothetical values of 4 and 5. The scores of answers to first-order and second-order questions were compared by the Mann-Whitney U test. Module-wise responses were tested by the Kruskall-Wallis test followed by the post hoc test for pairwise comparisons. Results The overall score of 96 answers was 4.04 ±0.37 (median: 4.17, Q1-Q3: 3.88-4.33) with the mean score of answers to first-order knowledge questions being 4.07 ±0.32 (median: 4.17, Q1-Q3: 4-4.33) and that of answers to second-order knowledge questions being 3.99 ±0.43 (median: 4, Q1-Q3: 3.67-4.33) (Mann-Whitney p=0.4). The score was significantly below the score of 5 (one-sample median test p<0.0001) and similar to 4 (one-sample median test p=0.09). Overall, there was a variation in median scores obtained in eight categories of topics in microbiology, indicating inconsistent performance in different topics. Conclusion The results of the study indicate that ChatGPT is capable of answering both first- and second-order knowledge questions related to the subject of microbiology. The model achieved an accuracy of approximately 80% and there was no difference between the model's capability of answering first-order questions and second-order knowledge questions. The findings of this study suggest that ChatGPT has the potential to be an effective tool for automated question-answering in the field of microbiology. However, continued improvement in the training and development of language models is necessary to enhance their performance and make them suitable for academic use."
    mergedFrom:
      - source: pubmed
        pmid: "37056538"
        doi: 10.7759/cureus.36034
    finalDecision: exclude
  - title: "Are ChatGPT’s knowledge and interpretation ability comparable to those of medical students in Korea for taking a parasitology examination?: a descriptive study."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: LLM performance on parasitology examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarking ChatGPT on a parasitology exam vs. medical students; not about using AI for assessment/feedback in clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Parasitology examination performance comparison, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.3352/jeehp.2023.20.1
    pmid: "36627845"
    authors: Huh S
    year: "2023"
    abstract: This study aimed to compare the knowledge and interpretation ability of ChatGPT, a language model of artificial general intelligence, with those of medical students in Korea by administering a parasitology examination to both ChatGPT and medical students. The examination consisted of 79 items and was administered to ChatGPT on January 1, 2023. The examination results were analyzed in terms of ChatGPT’s overall performance score, its correct answer rate by the items’ knowledge level, and the acceptability of its explanations of the items. ChatGPT’s performance was lower than that of the medical students, and ChatGPT’s correct answer rate was not related to the items’ knowledge level. However, there was a relationship between acceptable explanations and correct answers. In conclusion, ChatGPT’s knowledge and interpretation ability for this parasitology examination were not yet comparable to those of medical students in Korea.
    mergedFrom:
      - source: pubmed
        pmid: "36627845"
        doi: 10.3352/jeehp.2023.20.1
    finalDecision: exclude
  - title: "Methodology paper for the General Medicine Inpatient Initiative Medical Education Database (GEMINI MedED): a retrospective cohort study of internal medicine resident case-mix, clinical care and patient outcomes."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; database methodology paper for medical education research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    doi: 10.1136/bmjopen-2022-062264
    pmid: "36153026"
    authors: Lam A, Tang B, Lalwani A, Verma A, Wong B, Razak F, Ginsburg S
    year: "2022"
    abstract: |-
      INTRODUCTION: Unwarranted variation in patient care among physicians is associated with negative patient outcomes and increased healthcare costs. Care variation likely also exists for resident physicians. Despite the global movement towards outcomes-based and competency-based medical education, current assessment strategies in residency do not routinely incorporate clinical outcomes. The widespread use of electronic health records (EHRs) may enable the implementation of in-training assessments that incorporate clinical care and patient outcomes.

      METHODS AND ANALYSIS: The General Medicine Inpatient Initiative Medical Education Database (GEMINI MedED) is a retrospective cohort study of senior residents (postgraduate year 2/3) enrolled in the University of Toronto Internal Medicine (IM) programme between 1 April 2010 and 31 December 2020. This study focuses on senior IM residents and patients they admit overnight to four academic hospitals. Senior IM residents are responsible for overseeing all overnight admissions; thus, care processes and outcomes for these clinical encounters can be at least partially attributed to the care they provide. Call schedules from each hospital, which list the date, location and senior resident on-call, will be used to link senior residents to EHR data of patients admitted during their on-call shifts. Patient data will be derived from the GEMINI database, which contains administrative (eg, demographic and disposition) and clinical data (eg, laboratory and radiological investigation results) for patients admitted to IM at the four academic hospitals. Overall, this study will examine three domains of resident practice: (1) case-mix variation across residents, hospitals and academic year, (2) resident-sensitive quality measures (EHR-derived metrics that are partially attributable to resident care) and (3) variations in patient outcomes across residents and factors that contribute to such variation.

      ETHICS AND DISSEMINATION: GEMINI MedED was approved by the University of Toronto Ethics Board (RIS#39339). Results from this study will be presented in academic conferences and peer-reviewed journals.
    mergedFrom:
      - source: pubmed
        pmid: "36153026"
        doi: 10.1136/bmjopen-2022-062264
      - source: scopus
        doi: 10.1136/bmjopen-2022-062264
        scopusId: SCOPUS_ID:85138459404
    finalDecision: exclude
  - title: "Words matter: using natural language processing to predict neurosurgical residency match outcomes."
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Traditional NLP for predicting match outcomes, not generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses on conventional NLP, not generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    doi: 10.3171/2022.5.JNS22558
    pmid: "35901704"
    authors: Ortiz A, Feldman M, Yengo-Kahn A, Roth S, Dambrino R, Chitale R, Chambless L
    year: "2023"
    abstract: |-
      OBJECTIVE: Narrative letters of recommendation (NLORs) are considered by neurosurgical program directors to be among the most important parts of the residency application. However, the utility of these NLORs in predicting match outcomes compared to objective measures has not been determined. In this study, the authors compare the performance of machine learning models trained on applicant NLORs and demographic data to predict match outcomes and investigate whether narrative language is predictive of standardized letter of recommendation (SLOR) rankings.

      METHODS: This study analyzed 1498 NLORs from 391 applications submitted to a single neurosurgery residency program over the 2020-2021 cycle. Applicant demographics and match outcomes were extracted from Electronic Residency Application Service applications and training program websites. Logistic regression models using least absolute shrinkage and selection operator were trained to predict match outcomes using applicant NLOR text and demographics. Another model was trained on NLOR text to predict SLOR rankings. Model performance was estimated using area under the curve (AUC).

      RESULTS: Both the NLOR and demographics models were able to discriminate similarly between match outcomes (AUCs 0.75 and 0.80; p = 0.13). Words including "outstanding," "seamlessly," and "AOA" (Alpha Omega Alpha) were predictive of match success. This model was able to predict SLORs ranked in the top 5%. Words including "highest," "outstanding," and "best" were predictive of the top 5% SLORs.

      CONCLUSIONS: NLORs and demographic data similarly discriminate whether applicants will or will not match into a neurosurgical residency program. However, NLORs potentially provide further insight regarding applicant fit. Because words used in NLORs are predictive of both match outcomes and SLOR rankings, continuing to include narrative evaluations may be invaluable to the match process.
    mergedFrom:
      - source: pubmed
        pmid: "35901704"
        doi: 10.3171/2022.5.JNS22558
    finalDecision: exclude
  - title: Intelligent virtual case learning system based on real medical records and natural language processing.
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Based on traditional NLP, not generative AI; also PMID 35246134 suggests publication before 2022"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses on natural language processing, not clearly generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1186/s12911-022-01797-7
    pmid: "35246134"
    authors: Wang M, Sun Z, Jia M, Wang Y, Wang H, Zhu X, Chen L, Ji H
    year: "2022"
    abstract: |-
      BACKGROUND: Modernizing medical education by using artificial intelligence and other new technologies to improve the clinical thinking ability of medical students is an important research topic in recent years. Prominent medical universities are actively conducting research and exploration in this area. In particular, given the shortage of human resources, the need to maintain social distancing to prevent the spread of the epidemics, and the increase in the cost of medical education, it is critical to harness online learning to promote medical education. A virtual case learning system that uses natural language processing technology to process and present a hospital's real medical records and evaluate student responses can effectively improve medical students' clinical thinking abilities.

      OBJECTIVE: The purpose of this study is to develop a virtual case system, AIteach, based on actual complete hospital medical records and natural language processing technology, and achieve clinical thinking ability improvement through a contactless, self-service, trial-and-error system application.

      METHODS: Case extraction is performed on a hospital's case data center and the best-matching cases are produced through natural language processing, word segmentation, synonym conversion, and sorting. A standard clinical questioning data module, virtual case data module, and student learning difficulty module are established to achieve simulation. Students can view the objective examination and inspection data of actual cases, including details of the consultation and physical examination, and automatically provide their learning response via a multi-dimensional evaluation system. In order to assess the changes in students' clinical thinking after using AIteach, 15 medical graduate students were subjected to two simulation tests before and after learning through the virtual case system. The tests, which included the full-process case examination of cases having the same difficulty level, examined core clinical thinking test points such as consultation, physical examination, and disposal, and generated multi-dimensional evaluation indicators (rigor, logic, system, agility, and knowledge expansion). Thus, a complete and credible evaluation system is developed.

      RESULTS: The AIteach system used an internal and external double-cycle learning model. Students collect case information through online inquiries, physical examinations, and other means, analyze the information for feedback verification, and generate their detailed multi-dimensional clinical thinking after learning. The feedback report can be evaluated and its knowledge gaps analyzed. Such learning based on real cases is in line with traditional methods of disease diagnosis and treatment, and addresses the practical difficulties in reflecting actual disease progression while keeping pace with recent research. Test results regarding short-term learning showed that the average score (P < 0.01) increased from 69.87 to 85.6, the five indicators of clinical thinking evaluation improved, and there was obvious logical improvement, reaching 47%.

      CONCLUSION: By combining real cases and natural language processing technology, AIteach can provide medical students (including undergraduates and postgraduates) with an online learning tool for clinical thinking training. Virtual case learning helps students to cultivate clinical thinking abilities even in the absence of clinical tutor, such as during pandemics or natural disasters.
    mergedFrom:
      - source: pubmed
        pmid: "35246134"
        doi: 10.1186/s12911-022-01797-7
    finalDecision: exclude
  - title: Claude, ChatGPT, Copilot, and Gemini Performance versus Students in Different Topics of Neuroscience
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarking LLM performance on USMLE-style neuroscience MCQs; not about using generative AI for assessment or feedback of learners. Also criterion 4: not workplace-based clinical education."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Exam-focused MCQ performance study, not assessment/feedback or analysis of clinical observation records in workplace-based training."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    ericId: EJ1467812
    authors: Volodymyr Mavrych, Ahmed Yaqinuddin, Olena Bolgova
    year: "2025"
    abstract: Despite extensive studies on large language models and their capability to respond to questions from various licensed exams, there has been limited focus on employing chatbots for specific subjects within the medical curriculum, specifically medical neuroscience. This research compared the performances of Claude 3.5 Sonnet (Anthropic), GPT-3.5 and GPT-4-1106 (OpenAI), Copilot free version (Microsoft), and Gemini 1.5 Flash (Google) versus students on multiple-choice questions (MCQs) from the medical neuroscience course database to evaluate chatbot reliability. Five successive attempts of each chatbot to answer 200 United States Medical Licensing Examination (USMLE)-style questions were evaluated based on accuracy, relevance, and comprehensiveness. MCQs were categorized into 12 categories/topics. The results indicated that, at the current level of development, selected AI-driven chatbots, on average, can accurately answer 67.2% of MCQs from the medical neuroscience course, which is 7.4% below the students' average. However, Claude and GPT-4 outperformed other chatbots, with 83% and 81.7% correct answers, which is better than the average student result. They were followed by Copilot (59.5%), GPT-3.5 (58.3%), and Gemini (53.6%). Concerning different categories, Neurocytology, Embryology, and Diencephalon were the three best topics, with average results of 78.1-86.7%, and the lowest results were for Brain stem, Special senses, and Cerebellum, with 54.4-57.7% correct answers. Our study suggested that Claude and GPT-4 are currently two of the most evolved chatbots. They exhibit proficiency in answering MCQs related to neuroscience that surpasses that of the average medical student. This breakthrough indicates a significant milestone in how AI can supplement and enhance educational tools and techniques.
    mergedFrom:
      - source: eric
        ericId: EJ1467812
    finalDecision: exclude
  - title: "ChatGPT as a Life Coach for Professional Identity Formation in Medical Education: A Self-Regulated Learning Perspective"
    reviews:
      - decision: uncertain
        comment: ChatGPT in medical education; abstract may reveal assessment or WBA elements
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Uses ChatGPT as a life coach for professional identity formation via self-regulated learning; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:33.945000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on ChatGPT-supported professional identity learning activity, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:37.441000+00:00
    ericId: EJ1437405
    authors: Hsin Huang, Hui-Chen Lin
    year: "2024"
    abstract: Professional Identity Formation (PIF) is considered a crucial process in medical education. It involves how medical students identify their role as physicians, discover their professional positioning, and gradually develop their professional identity through social interactions. This qualitative descriptive study adopted the phenomenological method; it proposed the DSCOR (Diverse thinking, Seeking advice, Construction, Organizing and sharing, and Reflection) model based on the ChatGPT as a life coach (ChatGPT-LC) selfregulated learning (SRL) approach to design a PIF course for six medical students. Data collection included digital storytelling created using AI-generated techniques, learning sheets, direct observations, reflective feedback forms, and semi-structured interviews. The data transcription and analysis were conducted using Colaizzi's method. The results revealed three benefits of Generative AI (i.e., ChatGPT), namely &quot;increasing motivation for planning PIF,&quot; &quot;strengthening the mastery of PIF,&quot; and &quot;broadening the perspectives of PIF.&quot; Moreover, the ChatGPT-LC SRL approach had a positive influence on students, helping them understand the significance of PIF in their personal development at the early stage. The artificial intelligence-generated content provided positive guidance and supportive learning, offering specific suggestions and assistance. This brought about benefits for learning, and provided initial evidence for the application of ChatGPT-LC in medical education.
    mergedFrom:
      - source: eric
        ericId: EJ1437405
    finalDecision: exclude
  - title: "Theoretical and Clinical Learning Experiences and Expectations of Midwifery Students Using the Artificial Intelligence Tool ChatGPT: A Qualitative Study"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Study explores midwifery students' general learning experiences and expectations using ChatGPT, not AI applied to assessment or feedback."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:55.712000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Explores general learning experiences with ChatGPT, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:01.295000+00:00
    ericId: EJ1461262
    authors: Nazli Unlu Bidik
    year: "2025"
    abstract: ChatGPT is one of the most popular artificial intelligence tools today. The number of users is increasing rapidly every day. ChatGPT, which is used in many fields for different purposes, has the potential to revolutionise the field of education and health. The study aims of the study is to investigate the learning experiences and expectations of midwifery students who use ChatGPT in their theoretical and clinical education. This study was conducted through an interpretive paradigm based on Heideggerian hermeneutic phenomenology, a qualitative research method. In the study, the maximum diversity sampling method, one of the purposive sampling methods, was used. One-to-one in-depth interviews were conducted with a total of 17 midwifery students. The study data were collected through a three-part interview form. The data obtained were analysed with the MAXQDA program. As a result of the data analysis, three main themes and seven sub-themes were identified. The main themes are The Role of ChatGPT in Midwifery Education, The Effect of ChatGPT on Student Development and Concerns about the Use of ChatGPT. Midwifery students use ChatGPT for various purposes in theoretical and clinical courses. Although students are satisfied that artificial intelligence tools save time and make learning practical, students have some concerns about ChatGPT.
    mergedFrom:
      - source: eric
        ericId: EJ1461262
    finalDecision: exclude
  - title: "Transforming Combat Casualty Care Training: Generative AI-Enabled Adaptive Learning in Forward Medical Settings"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Uses LLMs to generate multiple-choice question sets for combat casualty care training; focuses on item generation quality, not assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    ericId: ED676628
    authors: Alan D. Koenig, John J. Lee, Eric Savitsky, Gabriele Nataneli, Karson Lindstrom, David L. Schriger, Tyler Savitsky
    year: "2025"
    abstract: The urgent need to train military and civilian responders in combat casualty care during large-scale operations presents challenges due to the variability of learner preparedness and the resource demands of traditional curriculum development. This study examines the application of generative artificial intelligence (AI) in authoring and evaluating multiple-choice question-and-answer (QA) sets for medical training, with a specific focus on far-forward combat environments. Leveraging OpenAI's latest large language models (LLMs)--including GPT-4 (Open AI, 2023), GPT-4o (OpenAI, 2024a), o1, (OpenAI, 2024c) and o1-mini (OpenAI, 2024d)--the study compares AI-generated QA sets to those created by a seasoned human subject matter expert (SME), using National Board of Medical Examiners (NBME) guidelines as the benchmark. Results show that GPT-4o produced high-quality QA sets in 86.6% of cases, while interrater agreement between human and AI raters was strong (Krippendorff's [alpha] = 0.85; Gwet's AC2 = 0.96). The AI-generated QA sets were created with a 31-fold time savings and over 4,000-fold cost reduction relative to SME-authored items. Beyond performance metrics, the study introduces a replicable human-in-the-loop methodology for AI-assisted educational assessment design, striking a balance between scalability and pedagogical integrity. This framework provides a viable path for integrating LLMs into adaptive learning systems across various domains, while emphasizing the continued need for expert oversight to ensure contextual fidelity, instructional relevance, and quality assurance.
    mergedFrom:
      - source: eric
        ericId: ED676628
    finalDecision: exclude
  - title: Incorporating Competency-Based Medical Education in the Gross Anatomy Lab Context. A Mixed-Methods Study of First-Year Medical Students&apos; Communication Competency Development over the Gross Anatomy Course
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on communication competency in anatomy lab"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: EJ1476020
    authors: Emily M. Porta-Miller, Jennifer K. Brueckner-Collins
    year: "2025"
    abstract: While adoption of competency-based medical education for clinical training is well documented, it is less common in preclinical medical education. The gross anatomy laboratory is an opportune venue to implement competency assessment during preclinical training. This mixed-methods study determined how first-year medical student assessments of communication skills in the gross anatomy lab change over time and analyzed student reflections to contextualize the experiences they had developing the Communication competency. First-year medical students at the University of Louisville completed self- and peer-assessments using the Communication Assessment Tool (CAT) at the beginning and end of their gross anatomy course (n = 83). The students also completed three competency development portfolio (CDP) entries throughout the course (n = 83). Qualitative thematic analysis with grounded theory was used to analyze comments related to communication in the CDP reflections during the course. There was a significant increase in CAT-peer-assessment scores from Time 1 (M = 4.70, SD = 0.331) to Time 2 (M = 4.87, SD = 0.264), p &lt; 0.001. There was also a significant increase in CAT-self-assessment scores from Time 1 (M = 4.50, SD = 0.472) to Time 2 (M = 4.77, SD = 0.386), p &lt; 0.001. Thematic analysis of CDPs revealed that students developed skills in imparting information, gathering information, and team communication skills throughout the gross anatomy course. First-year medical students reflected upon and demonstrated self- and peer-perceived growth in the Communication competency in the gross anatomy lab context.
    mergedFrom:
      - source: eric
        ericId: EJ1476020
    finalDecision: exclude
  - title: Linking Assessment to Real Life Practice -- Comparing Work Based Assessments and Objective Structured Clinical Examinations Using Mystery Shopping
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; compares WBA and OSCE using mystery shopping"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    ericId: EJ1429017
    authors: Angelina Lim, Sunanthiny Krishnan, Harjit Singh, Simon Furletti, Mahbub Sarkar, Derek Stewart, Daniel Malone
    year: "2024"
    abstract: Objective Structured Clinical Examinations (OSCEs) and Work Based Assessments (WBAs) are the mainstays of assessing clinical competency in health professions' education. Underpinned by the extrapolation inference in Kane's Validity Framework, the purpose of this study is to determine whether OSCEs translate to real life performance by comparing students' OSCE performance to their performance in real-life (as a WBA) using the same clinical scenario, and to understand factors that affect students' performance. A sequential explanatory mixed methods approach where a grade comparison between students' performance in their OSCE and WBA was performed. Students were third year pharmacy undergraduates on placement at a community pharmacy in 2022. The WBA was conducted by a simulated patient, unbeknownst to students and indistinguishable from a genuine patient, visiting the pharmacy asking for health advice. The simulated patient was referred to as a 'mystery shopper' and the process to 'mystery shopping' in this manuscript. Community pharmacy is an ideal setting for real-time observation and mystery shopping as staff can be accessed without appointment. The students' provision of care and clinical knowledge was assessed by the mystery shopper using the same clinical checklist the student was assessed from in the OSCE. Students who had the WBA conducted were then invited to participate in semi-structured interviews to discuss their experiences in both settings. Overall, 92 mystery shopper (WBA) visits with students were conducted and 36 follow-up interviews were completed. The median WBA score was 41.7% [IQR 28.3] and significantly lower compared to the OSCE score 80.9% [IQR 19.0] in all participants (p &lt; 0.001). Interviews revealed students knew they did not perform as well in the WBA compared to their OSCE, but reflected that they still need OSCEs to prepare them to manage real-life patients. Many students related their performance to how they perceived their role in OSCEs versus WBAs, and that OSCEs allowed them more autonomy to manage the patient as opposed to an unfamiliar workplace. As suggested by the activity theory, the performance of the student can be driven by their motivation which differed in the two contexts.
    mergedFrom:
      - source: eric
        ericId: EJ1429017
    finalDecision: exclude
  - title: Feedback and Formative Assessment--Looking Backwards to Move Forward
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About feedback and formative assessment broadly, no mention of generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: EJ1461134
    authors: Nora McCarthy, Karen Neville, Andrew Pope
    year: "2025"
    abstract: The terms 'feedback' and 'formative assessment' are ubiquitous in education, but their conceptual boundaries are ill-defined. This perspective piece explores the meaning of 'feedback' and 'formative assessment', revealing the entanglement and confusion that exists between these two terms. An argument for clarity of terms is made, to avoid ambiguity and to create a common language. A suggestion is made to re-embrace the original definition components of 'feedback', as all of the original parameters of this term, when well established in industry and biology, did not migrate into education. Incorporating the original definition components into the current definition of 'feedback' in education circles may negate the need for the term 'formative assessment' altogether, which carries with it the inevitable negative connotations associated with the word assessment. Words matter. Medical education, with the introduction of competency-based medical education (CBME) and entrustable professional activities offers a timely opportunity to re-visit such terms, with 'feedback' and 'formative assessment' being central to these processes. While we use medical education as an example to explore how looking backwards and incorporating all aspects of the original feedback definition can help us to move forward with clarity of terms, we mainly seek to bring new perspectives and hope to encourage necessary conversations on 'feedback' and 'formative assessment' entanglement.
    mergedFrom:
      - source: eric
        ericId: EJ1461134
    finalDecision: exclude
  - title: A Pilot Study of the Anatomical Self-Efficacy Instrument for Clinical Clerkships (ASEI-CC)
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About an anatomical self-efficacy instrument, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: EJ1488703
    authors: Bradley R. Collins, Ashley N. Walker, Phuong B. Huynh, Alyson Adams, Daniel B. Topping, Venkatesh Nonabur, Kyle E. Rarey
    year: "2025"
    abstract: Self-efficacy and anatomical knowledge have been shown to be important in the development of medical students. Validated instruments designed to measure the construct of anatomical self-efficacy during the clinical years of medical school are limited. In this study, the Anatomical Self-Efficacy Instrument for Clinical Clerkships (ASEI-CC) was developed, and evidence for the reliability of the scores and the validity of the interpretations of the scores was gathered. The ASEI-CC consisted of 10- Likert-type items designed to measure anatomical self-efficacy, with higher scores indicating higher levels of anatomical self-efficacy. To conduct pilot testing for the ASEI-CC, a sample of 99 medical students rotating through the medicine, surgery, pediatrics, neurology, family medicine, and obstetrics and gynecology clerkships at a Southeastern institution in 2023 was recruited to complete an anonymous survey at the conclusion of an anatomy workshop. In the sample of 99 medical students in this study, the observed means of the scores on the items of the ASEI-CC ranged from 3.84 to 4.37, representing an average response of &quot;fairly confident&quot; to &quot;very confident&quot; on each item. Exploratory factor analysis with principal axis factoring yielded a unidimensional factor structure that explained 62.6% of the variance, with all 10 items having a factor loading greater than 0.4. This study provides evidence that supports the reliability of scores and the validity of the interpretations of scores on the ASEI-CC and extends scholarship about the anatomical self-efficacy of medical students to the clinical years of the medical school curriculum.
    mergedFrom:
      - source: eric
        ericId: EJ1488703
    finalDecision: exclude
  - title: "The Importance of Professional Development in a Programmatic Assessment System: One Medical School&apos;s Experience"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About programmatic assessment professional development, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Does not indicate generative AI/LLM use"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
    ericId: EJ1353865
    authors: Colbert C, Bierer S
    year: "2022"
    abstract: The Cleveland Clinic Lerner College of Medicine of Case Western Reserve University (CCLCM) was created in 2004 as a 5-year undergraduate medical education program with a mission to produce future physician-investigators. CCLCM's assessment system aligns with the principles of programmatic assessment. The curriculum is organized around nine competencies, where each competency has milestones that students use to self-assess their progress and performance. Throughout the program, students receive low-stakes feedback from a myriad of assessors across courses and contexts. With support of advisors, students construct portfolios to document their progress and performance. A separate promotion committee makes high-stakes promotion decisions after reviewing students' portfolios. This case study describes a systematic approach to provide both student and faculty professional development essential for programmatic assessment. Facilitators, barriers, lessons learned, and future directions are discussed.
    mergedFrom:
      - source: eric
        ericId: EJ1353865
    finalDecision: exclude
  - title: The Power of Rotation Schedules on the Career Selection Decisions of Medical Students
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About rotation schedules and career decisions, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Does not indicate generative AI/LLM use"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
    ericId: EJ1403072
    authors: Bechara J, Shah P, Lindor K
    year: "2023"
    abstract: Choosing a career pathway in medicine is a high stakes decision for both medical students and the field of medicine as a whole. While past research has examined how characteristics of the medical student or specialties influence this decision, we introduce temporal elements as novel variables influencing career selection decisions in medicine. Specifically, we investigate how timing and duration of residency options, based on a rotation schedule that medical students have limited control over, influence their career selection decisions. An archival study investigating 5 years of medical student rotation schedules (N = 115) reveals that clinical rotation options appearing earlier and more often in the schedule were more likely to be selected. Moreover, timing and duration of exposure interacted such that residency options appearing later in the schedules were more likely to be selected if they also appeared more often. Conditional logistic regressions using student fixed-effects to control for idiosyncratic medical student differences (i.e., gender, &amp; debt, etc.), and residency fixed-effects to control for idiosyncratic residency differences (income, and lifestyle, etc.), revealed the rotation schedule had a significant impact on residency selection decisions even when controlling for factors typically influencing this decision. Medical students' career decisions are influenced by when and how long different choice options appear in their rotation schedule, especially when they have limited influence over this schedule. The results have implications for healthcare policy by highlighting a tool for adjusting physician workforce composition by broadening exposure to a greater array of career options.
    mergedFrom:
      - source: eric
        ericId: EJ1403072
    finalDecision: exclude
  - title: Using Cultural Historical Activity Theory to Reflect on the Sociocultural Complexities in OSCE Examiners&apos; Judgements
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About OSCE examiner judgements using activity theory, no generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: EJ1368711
    authors: Wong W, Thistlethwaite J, Moni K, Roberts C
    year: "2023"
    abstract: "Examiners' judgements play a critical role in competency-based assessments such as objective structured clinical examinations (OSCEs). The standardised nature of OSCEs and their alignment with regulatory accountability assure their wide use as high-stakes assessment in medical education. Research into examiner behaviours has predominantly explored the desirable psychometric characteristics of OSCEs, or investigated examiners' judgements from a cognitive rather than a sociocultural perspective. This study applies cultural historical activity theory (CHAT) to address this gap in exploring examiners' judgements in a high-stakes OSCE. Based on the idea that OSCE examiners' judgements are socially constructed and mediated by their clinical roles, the objective was to explore the sociocultural factors that influenced examiners' judgements of student competence and use the findings to inform examiner training to enhance assessment practice. Seventeen semi-structured interviews were conducted with examiners who assessed medical student competence in progressing to the next stage of training in a large-scale OSCE at one Australian university. The initial thematic analysis provided a basis for applying CHAT iteratively to explore the sociocultural factors and, specifically, the contradictions created by interactions between different elements such as examiners and rules, thus highlighting the factors influencing examiners' judgements. The findings indicated four key factors that influenced examiners' judgements: examiners' contrasting beliefs about the purpose of the OSCE; their varying perceptions of the marking criteria; divergent expectations of student competence; and idiosyncratic judgement practices. These factors were interrelated with the activity systems of the medical school's assessment practices and the examiners' clinical work contexts. Contradictions were identified through the guiding principles of multi-voicedness and historicity. The exploration of the sociocultural factors that may influence the consistency of examiners' judgements was facilitated by applying CHAT as an analytical framework. Reflecting upon these factors at organisational and system levels generated insights for creating fit-for-purpose examiner training to enhance assessment practice."
    mergedFrom:
      - source: eric
        ericId: EJ1368711
    finalDecision: exclude
  - title: The Influence of a Digital Clinical Reasoning Test on Medical Student Learning Behavior during Clinical Clerkships
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About a digital clinical reasoning test during clerkships, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: EJ1428966
    authors: Larissa I. A. Ruczynski, Bas J. J. W. Schouwenberg, Eugène Custers, Cornelia R. M. G. Fluit, Marjolein H. J. van de Pol
    year: "2024"
    abstract: "Recently, a new digital clinical reasoning test (DCRT) was developed to evaluate students' clinical-reasoning skills. Although an assessment tool may be soundly constructed, it may still prove inadequate in practice by failing to function as intended. Therefore, more insight is needed into the effects of the DCRT in practice. Individual semi-structured interviews and template analysis were used to collect and process qualitative data. The template, based on the interview guide, contained six themes: (1) DCRT itself, (2) test debriefing, (3) reflection, (4) practice/workplace, (5) DCRT versus practice and (6) 'other'. Thirteen students were interviewed. The DCRT encourages students to engage more in formal education, self-study and workplace learning during their clerkships, particularly for those who received insufficient results. Although the faculty emphasizes the different purposes of the DCRT (assessment of/as/for learning), most students perceive the DCRT as an assessment of learning. This affects their motivation and the role they assign to it in their learning process. Although students appreciate the debriefing and reflection report for improvement, they struggle to fill the identified knowledge gaps due to the timing of receiving their results. Some students are supported by the DCRT in exhibiting lifelong learning behavior. This study has identified several ways in which the DCRT influences students' learning practices in a way that can benefit their clinical-reasoning skills. Additionally, it stresses the importance of ensuring the alignment of theoretical principles with real-world practice, both in the development and utilization of assessment tools and their content. Further research is needed to investigate the long-term impact of the DCRT on young physicians' working practice."
    mergedFrom:
      - source: eric
        ericId: EJ1428966
    finalDecision: exclude
  - title: "The Concept of Errors in Medical Education: A Scoping Review"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about the concept of errors in medical education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: EJ1344800
    authors: Dyre L, Grierson L, Rasmussen K, Ringsted C, Tolsgaard M
    year: "2022"
    abstract: "The purpose of this scoping review was to explore how errors are conceptualized in medical education contexts by examining different error perspectives and practices. This review used a scoping methodology with a systematic search strategy to identify relevant studies, written in English, and published before January 2021. Four medical education journals (Medical Education, Advances in Health Science Education, Medical Teacher, and Academic Medicine) and four clinical journals (Journal of the American Medical Association, Journal of General Internal Medicine, Annals of Surgery, and British Medical Journal) were purposively selected. Data extraction was charted according to a data collection form. Of 1505 screened studies, 79 studies were included. Three overarching perspectives were identified: &quot;understanding errors&quot;) (n = 31), &quot;avoiding errors&quot; (n = 25), &quot;learning from errors&quot; (n = 23). Studies that aimed at 'understanding errors' used qualitative methods (19/31, 61.3%) and took place in the clinical setting (19/31, 61.3%), whereas studies that aimed at 'avoiding errors' and 'learning from errors' used quantitative methods ('avoiding errors': 20/25, 80%, and 'learning from errors': 16/23, 69.6%, p = 0.007) and took place in pre-clinical (14/25, 56%) and simulated settings (10/23, 43.5%), respectively (p &lt; 0.001). The three perspectives differed significantly in terms of inclusion of educational theory: 'Understanding errors' studies 16.1% (5/31),'avoiding errors' studies 48% (12/25), and 'learning from errors' studies 73.9% (17/23), p &lt; 0.001. Errors in medical education and clinical practice are defined differently, which makes comparisons difficult. A uniform understanding is not necessarily a goal but improving transparency and clarity of how errors are currently conceptualized may improve our understanding of when, why, and how to use and learn from errors in the future."
    mergedFrom:
      - source: eric
        ericId: EJ1344800
    finalDecision: exclude
  - title: Starting Research in Clinical Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; general clinical education research topic"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: ED631335
    authors: Rees, Eliot L. E, Ledger, Alison E, Walker, Kim A. E
    year: "2023"
    abstract: "&quot;Starting Research in Clinical Education&quot; is written by a global team of experienced and emerging clinical education researchers who have a wealth of knowledge designing rigorous research projects and expertise in contemporary methods. Covering a broad spectrum of methods used by clinical education researchers, the book is split into five parts: research design, evidence synthesis and mixed methods research, qualitative research, quantitative research and succeeding in clinical education research. These sections are also accompanied by a companion website which provides further resources. The methods discussed are illustrated with real life examples and case studies to support the reader in designing their own project. The new edition includes information on: (1) Getting started in clinical education research, constructing a research question, clarifying research paradigms and design, using educational theory, involving stakeholders, sampling and recruiting participants and conducting ethical research; (2) Evidence synthesis, realist research, mixed methods research, action research and emerging possibilities in online data collection; (3) Interviews and focus groups, visual elicitation, ethnography, narrative research, thematic analysis and struggles new researchers often face in qualitative research; (4) Survey research, experimental methods, statistical analysis and big data; and (5) Maximising opportunities, project management, writing dissertations, writing for publication, research dissemination and career development. This edition is designed to support those new to clinical education research, including those undertaking intercalated or postgraduate degrees in clinical, medical, dental or health professions education."
    mergedFrom:
      - source: eric
        ericId: ED631335
    finalDecision: exclude
  - title: "Instructional Practices to Foster the Development of Autonomy: A Qualitative Phenomenological Study of the Resident Experience"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About instructional practices for resident autonomy, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: ED658008
    authors: Rachelle Musgrave Aker
    year: "2024"
    abstract: "A growing concern in graduate medical education (GME), research indicates that some graduating residents are unprepared to enter unsupervised practice. The aim of this qualitative phenomenological study is to illuminate the resident experience surrounding the development of autonomy in clinical decision making relative to teaching practices. Research questions include: (1) What is the dual role of learner/teacher like for residents as they attend to their own and others' development of autonomy in clinical reasoning and decision making? (2) Which teaching methods do they commonly experience as the learner and practice as the teacher relative to this development, and which methods are associated with experiences most and least valued for growth in autonomy? (3) How are these teaching methods acquired? (4) How do residents experience and interpret the provision of autonomy by supervising teachers? The researcher obtained a purposive sample of eight participants from GME programs in a large public university in the south, representing residents in early training to post-training as chief residents, from the specialties of Internal Medicine, Pediatrics, and the combined specialties of Internal Medicine-Pediatrics and Internal Medicine-Psychiatry. The researcher conducted in-depth interviews supported by member checks and memos. Consistent with the phenomenological framework, In-Vivo and Process Coding enabled identification of meaning units and experiential themes. The interpretative phenomenological analysis (IPA) method was used to conduct data analysis. Findings revealed that these residents experience difficulty and uncertainty in their dual role of learner/teacher, attaching meaning to the experience as an unsteady balance between the two roles. Despite this challenge, they particularly value their teaching role for their own learning and autonomy development. A lack of formal teaching training, however, means that residents rely on acquiring teaching practices through observation and imitation of others, often resulting in the adoption of underdeveloped or misaligned methods for teaching clinical reasoning. Teaching methods included the traditional apprenticeship methods of behavioral modeling, coaching and scaffolding early in the intern year, transitioning to cognitive apprenticeship limited to learner articulation of clinical reasoning, coaching, scaffolding and most often, modeling of the clinical decision or plan of care. Exceptional teaching for growth in autonomy was differentiated by modeling of the clinical reasoning process and a climate of psychological safety, often including self-directed learner reflection and exploration. All participants experienced a variable provision of autonomy, attributing the willingness of some faculty to grant autonomy to other factors besides patient needs and entrustment in resident ability, which may jeopardize the experience needed for unsupervised practice. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED658008
    finalDecision: exclude
  - title: Focused Anatomy Workshops for Clerkships and the USMLE Step 1 Examination
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on anatomy workshops"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI/LLM use in the title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    ericId: EJ1454933
    authors: Anna Ricci, Ian Minearo, Abigail Hielscher
    year: "2025"
    abstract: Anatomy is essential for understanding healthy and disease states as well as for the successful completion of clinical clerkships and board examinations. This project provided structured workshops aimed to review anatomical concepts for clerkships and Step 1 and provided a means for medical students to assess their anatomical knowledge. We provided six optional anatomy workshops, in which students (1) took a pre-session quiz, (2) faculty reviewed key anatomy of a particular system (e.g., musculoskeletal), (3) students worked through clinical cases in small groups, and (4) students took a post-session quiz and responded to a post-session survey to rate satisfaction of session content and delivery on a five-point Likert scale. One session was excluded due to small sample size (n = 2). Results from five workshops, including brachial plexus, musculoskeletal, pelvic, gastrointestinal, and head and neck anatomy, showed that students performed significantly (p [less than or equal to] 0.05) better on the post-session quizzes compared to pre-session quizzes in all sessions. Post-session survey results indicated that students were satisfied with session content and facilitation, would attend future workshops, and would use session materials to study for Step 1. Based on these short-term benefits of the sessions, we plan to continue offering monthly workshops to medical students to increase knowledge retention of key anatomical concepts and increase preparedness for clerkships and Step 1. Future studies will longitudinally follow up with students post-Step 1 and clerkships to determine the long-term benefits of offering these workshops.
    mergedFrom:
      - source: eric
        ericId: EJ1454933
    finalDecision: exclude
  - title: "Promoting Clinical Reasoning in Undergraduate Family Medicine Curricula through Concept Mapping: A Qualitative Approach"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about concept mapping pedagogy"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus in the title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    ericId: EJ1484730
    authors: Marta Fonseca, Pedro Marvão, Patrícia Rosado-Pinto, António Rendas, Bruno Heleno
    year: "2025"
    abstract: "Clinical reasoning is a crucial skill for physicians, enabling them to bridge theoretical knowledge with practical application. The gap between basic sciences and clinical practice persists as a challenge, with traditional teaching methods yet to effectively bridge it. Concept maps (CMs), visual tools for organizing and connecting knowledge, hold promise for enhancing clinical reasoning in the undergraduate medical curriculum. However, further research is required to ascertain if CMs facilitate clinical reasoning development in medical students transitioning from basic sciences to clinical practice. This study aims to delineate how CMs can facilitate clinical reasoning in patients with multimorbidity within undergraduate Family Medicine curricula, as perceived by students and tutors, and to understand the implementation process and resources required. This exploratory qualitative study formed a part of an action research project. While introducing an educational intervention to 5th-year medical students, we conducted a qualitative evaluation. Subsequently, semi-structured group interviews were conducted with students, and a focus group was conducted with tutors. Three main educational impacts were identified: integration of clinical information, support for patient management and care plan, and collaborative learning. Key aspects for successful CM implementation included clear instructions for map construction, using user-friendly software, allocating sufficient time for the task, encouraging group discussion of CMs, and incorporating tutor feedback. CMs are pedagogical tools that facilitate clinical information integration and support management and treatment plans, helping students better understand multimorbidity patients and promoting some components of clinical reasoning in undergraduate medical education."
    mergedFrom:
      - source: eric
        ericId: EJ1484730
    finalDecision: exclude
  - title: "Beyond Empathy Decline: Do the Barriers to Compassion Change across Medical Training?"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About empathy/compassion barriers in medical training, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: EJ1336021
    authors: Wang C, Pavlova A, Fernando A, Consedine N
    year: "2022"
    abstract: "Background: Despite being a mandated, foundational value in healthcare, research on compassion remains limited. Studying the individual, patient, clinical, and contextual factors that interfere with compassion--the &quot;barriers&quot;--may clarify our understanding of the origins of compassion and identify potential targets for improving patient-centred care. Studies of the related construct of empathy have suggested that medical students report declines with increasing clinical experience. In contrast, when comparing physicians with medical students, increased clinical experience predicts lower barriers to compassion. Whether--and how--a similar experience-related decline in the factors that interfere with compassion occurs across medical training remains unknown. Aims: To describe how the barriers to compassion vary across clinical training in medical students. Method: New Zealand medical students (N = 351) in their clinical years (Years 4-6) completed measures of the Barriers to Physician Compassion (BPCQ) and potential covariates such as demographics, work burden factors, and dispositional factors. The BPCQ indexes the extent to which barriers in four domains (individual, patient, clinical, and contextual) interfere with a physician/student's compassion towards patients. Analyses of variance and regression analyses were used to explore the effect of year level on the four types of barriers. Results: Year 4 students reported slightly lower student-related, environmental and patient/family-related (but not clinical) barriers than Year 6 students (effect size: [omega][superscript 2] &lt; 0.05); all barriers increased comparably across training. Controlling for relevant confounds, regression analyses confirmed that lower year level predicted lower barriers to compassion. Higher self-compassion, but not gender, predicted lower barriers. Conclusions: In extending studies of empathy decline, this report suggests that students experience higher barriers to compassion as clinical training progresses. This is in contrast to existing studies contrasting physicians with medical students, where greater experience was associated with lower perceived barriers to compassion. Self-compassion may offset increases in barriers to care."
    mergedFrom:
      - source: eric
        ericId: EJ1336021
    finalDecision: exclude
  - title: "Transforming Medical Education through Simulation Design: The Development and Validation of a Central Line Training System"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about simulation-based central line training system"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: ED661369
    authors: Haroula M. Tzamaras
    year: "2024"
    abstract: "Central Venous Catheterization (CVC) is a commonly performed medical procedure used for medication delivery to the heart. While CVC is conducted over 5 million times annually it is plagued with high complication rates, resulting in adverse effects on patients, and in the worst cases, death. These complications are directly related to the experience level of the performing physician. A physician who has conducted less than 50 CVCs, is &quot;twice as likely&quot; to incur complications than a physician with more experience, reiterating the need for robust training of CVC for new medical residents. To better train physicians in complex procedures like CVC, many residency programs utilize simulation-based training (SBT). SBT is an imitation of a procedure or environment that allows trainees to practice hands-on medical procedures risk-free to a predefined mastery level of performance before conducting the procedure on patients. When determining the effectiveness of SBT, instructors will sometimes employ self-assessment to gauge trainee knowledge gains. Self-assessment can be useful; however, when using self-assessment for measuring learning success, other factors like gender can potentially cause trainees to rate themselves lower even if their learning and performance is equivalent. This gender-gap is not widely researched in SBT, but is important to understand in the context of learning. Additionally, most SBT methods require residents to already know how to conduct the steps of the procedure on their first usage, without checking for understanding. In this way, many simulators are designed for practicing procedures, but not for effective learning, indicating a need for innovative training methods that can do both. For CVC SBT, manikin trainers are commonly utilized and are useful because they provide hands-on practice but are limited in that they only provide practice on one anatomy and do not provide automated feedback to the trainee. The Dynamic Haptic Robotic Trainer (DHRT) addresses these deficits of manikin training for CVC by providing force tissue profiles to simulate multiple patient anatomies, along with providing automated, personalized feedback on performance to help the trainee learn and improve. While the DHRT has been shown to train residents as effectively as manikin trainers without the need of a trained preceptor, it only teaches &quot;part of the mechanical portions&quot; of CVC residents need to know to be proficient in the clinical environment. In addition, while DHRT has been validated for its educational effectiveness, it lacks clinical validation. Considering these gaps in medical training for CVC SBT, the objective of this dissertation was to transform CVC education through assessment of current training methods, development of new training methods, and validation of new training methods. Specifically, this dissertation focused on: (1) evaluating the impact of sequential learning on initial skill gain and learning over time, (2) assessing DHRT training for differences in self-efficacy between men and women, (3) developing and analyzing the impact of a novel comprehensive simulator on resident self-efficacy and proficiency, and (4) validating the comprehensive simulator through eye gaze in the operating room and on the simulator with novice and expert physicians. The results of this dissertation indicate that sequential learning significantly increased initial skill gain, decreased the number of trials required to complete training, and reduced learning curves, women rate their self-efficacy significantly lower than men despite no performance differences for SBT and neither men nor women are able to accurately self-assess performance, comprehensive simulation is more effective than the original DHRT training for resident performance and self-efficacy, and the comprehensive simulator exhibits both predictive validity by aligning expert gaze between the simulator and the operating room and construct validity by distinguishing between expert and novices. This dissertation also provides novel methodology for conducting validity studies in the clinical environment. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED661369
    finalDecision: exclude
  - title: "It Takes a Village: An Ethnographic Study on How Undergraduate Medical Students Use Each Other to Learn Clinical Reasoning in the Workplace"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; ethnographic study about peer learning in clinical reasoning"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: EJ1492634
    authors: Larissa I. A. Ruczynski, Marjolein H. J. van de Pol, Shiba Hashmi, Erwin J. H. Vos, Cornelia R. M. G. Fluit, Bas J. J. W. Schouwenberg
    year: "2025"
    abstract: "When students learn with--and from--other students, it is called peer-assisted learning (PAL). How undergraduate medical students use their peers for their clinical-reasoning learning process remains unclear, although literature suggests that it is a promising learning strategy at this stage. This research therefore explores the question: 'How is PAL manifested in the clinical learning environment of undergraduate medical students with regard to developing clinical-reasoning skills?'. A constructivist paradigm with a sociocultural theoretical framework was adopted for this research, using PAL and workplace learning as theoretical background. Focused ethnography and a combination of template and open coding was used to gather and analyze qualitative data. Twenty semi-structured interviews were conducted with nine students, four residents, and seven clinical supervisors. A total number of 31.5 h were used for five clinical observations. Following categories were used to describe the data: (1) the role of PAL in the clinical-reasoning learning practice, in which PAL-theory was placed alongside clinical practice, (2) the role of different actors during PAL and (3) the PAL environment, in which contextual factors have been scrutinized. Students deploy various categories of PAL to advance their clinical-reasoning skills, although they were largely unaware of these processes, and facilitation of PAL is not consistently provided. Three topics of discussion are identified that need to be acknowledged: (1) the design of a PAL environment that is conducive to collaborative learning, (2) the shifting roles of peers when they enter clinical practice, and (3) the individualistic tendencies of students that can hamper PAL. Future research could focus on stimulating and facilitating PAL among the next generation of students and integrating PAL into the clinical practice workflow."
    mergedFrom:
      - source: eric
        ericId: EJ1492634
    finalDecision: exclude
  - title: Programmatic Efforts to Provide Progressively Autonomous Clinical Education Experiences
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About clinical education programming, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM is indicated"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    ericId: EJ1409453
    authors: Brittany E. Carlson, Justin P. Young, Elizabeth R. Neil, Jessica Barrett, Lindsey E. Eberman
    year: "2024"
    abstract: "Context: In previous research, athletic trainers (ATs) have identified the formal coursework and clinical experiences of their professional program as contributing to preparedness for autonomous practice. However, new graduates have reported a perceived lack of skills necessary for practicing autonomously. Objective: The purpose of this study was to evaluate how programs provide progressively autonomous clinical education experiences and the role of these experiences in preparing future ATs. Design: Qualitative study. Setting: Virtual interviews. Patients or Other Participants: A sample of 17 program administrators (program directors = 12, coordinators of clinical education = 5) representing 16 master of science in athletic training programs participated in this study. Main Outcome Measure(s): Each program completed a virtual interview guided by a semistructured interview protocol. A 3-person data analysis team identified emerging domains and categories through a multiphase approach. Member checking, multiple researcher triangulation, and auditing were used to establish trustworthiness. Results: Four domains emerged from participant responses: (1) &quot;curricular design,&quot; (2) &quot;preparatory experience and outcomes,&quot; (3) &quot;preceptor role,&quot; and (4) &quot;assessments.&quot; We found that intentionality in curricular design to promote progressive autonomy was beneficial in facilitating transition to practice through improved knowledge, skills, and confidence. Due to the importance of clinical education in AT preparation, preceptors played a critical role in creating opportunities for learning in the clinical environment. However, the degree to which experiences were autonomous and the effectiveness of the student in those encounters were rarely measured. Limitations outlined by participants included accreditation and program length. Conclusions: The development of knowledge, skills, and confidence through intentionally selected clinical experiences and guided by preceptor feedback are critical to establishing an autonomous practitioner. However, the program's evaluations were predominantly traditional practice assessments and informal feedback, instead of an assessment of readiness for practice."
    mergedFrom:
      - source: eric
        ericId: EJ1409453
    finalDecision: exclude
  - title: An Early-Curricular Team Learning Activity to Foster Integration of Biochemical Concepts and Clinical Sciences in Undergraduate Medical Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About team learning activity for biochemistry, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI/LLM component is indicated."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    ericId: EJ1425267
    authors: Martin Schmidt, Brian Pinney, Craig Canby, April Vargus, Marianka Pille
    year: "2024"
    abstract: The ability to connect key concepts of biochemistry with clinical presentations is essential for the development of clinical reasoning skills and adaptive expertise in medical trainees. To support the integration of foundational and clinical sciences in our undergraduate health science curricula, we developed a small group active learning exercise during which interprofessional groups of students use clinical cases to explore the biochemistry, diagnostic strategy, and evidence-based treatment options of inborn errors of metabolism (IEM). We designed multistage learning modules consisting of (1.) low-fidelity case simulations of pediatric patients presenting with IEMs, (2.) guided group discussions on clinical biochemistry, differential diagnoses, and diagnostic strategies, (3.) oral presentations of clinical reasoning strategies, and (4.) discussion of relevant evidence-based medicine topics related to the cases. These modules Scientific Knowledge Integrated in Patient Presentations (SKIPPs) were added to a first-semester foundational sciences course serving five health professions programs. The assessment of learning outcomes by students and faculty shows that SKIPPs sessions are well-received activities that significantly improve trainees' ability to integrate foundational science concepts into clinical scenarios, to practice interprofessional teamwork and to develop clinical reasoning skills.
    mergedFrom:
      - source: eric
        ericId: EJ1425267
    finalDecision: exclude
  - title: "Essential Anatomy for Core Clerkships: A Clinical Perspective"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About anatomy education for clerkships, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: EJ1391203
    authors: Keim S, Harmon D, Martindale J, Lopez E, Sanky C, Brooks W, Cotter M, Davies D, Doroudi M, Fahl J, Farias A, Granite G, Harrell K, Kar R, Kramer K, Jackson J, Jones S, Lackey-Cornelison W, Laitman J, Latacha K, Lewis S, Lovejoy Mork A, Marzban H, McNary T, McWhorter D, Merchant A, Mussell J, Quinn M, Reidenberg J, Royer D, Sakaguchi A, Sawyer F, Topping D, Wainman B, Wineski L, Zumwalt A, Hankin M
    year: "2023"
    abstract: "Clerkships are defining experiences for medical students in which students integrate basic science knowledge with clinical information as they gain experience in diagnosing and treating patients in a variety of clinical settings. Among the basic sciences, there is broad agreement that anatomy is foundational for medical practice. Unfortunately, there are longstanding concerns that student knowledge of anatomy is below the expectations of clerkship directors and clinical faculty. Most allopathic medical schools require eight &quot;core&quot; clerkships: internal medicine (IM), pediatrics (PD), general surgery (GS), obstetrics and gynecology (OB), psychiatry (PS), family medicine (FM), neurology (NU), and emergency medicine (EM). A targeted needs assessment was conducted to determine the anatomy considered important for each core clerkship based on the perspective of clinicians teaching in those clerkships. A total of 525 clinical faculty were surveyed at 24 United States allopathic medical schools. Participants rated 97 anatomical structure groups across all body regions on a 1-4 Likert-type scale (1 = not important, 4 = essential). Non-parametric ANOVAs determined if differences existed between clerkships. Combining all responses, 91% of anatomical structure groups were classified as &quot;essential&quot; or &quot;more important.&quot; Clinicians in FM, EM, and GS rated anatomical structures in most body regions significantly higher than at least one other clerkship (p = 0.006). This study provides an evidence-base of anatomy content that should be considered important for each core clerkship and may assist in the development and/or revision of preclinical curricula to support the clinical training of medical students."
    mergedFrom:
      - source: eric
        ericId: EJ1391203
    finalDecision: exclude
  - title: "&quot;Pet Things&quot; and &quot;Penny Drops&quot;--Factors Influencing Clinicians&apos; Teaching of Pelvic Anatomy"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on anatomy teaching factors"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    ericId: EJ1411990
    authors: Lilian H. Fellner, Svetlana M. King, Christine M. Barry
    year: "2024"
    abstract: "Pelvic anatomy is considered challenging to teach and learn, partly because its complexity can make it difficult to conceptualize. Educational researchers recognize the value of a spiraling curriculum to develop clinically orientated anatomy knowledge for health professionals, but most studies have focused on the preclinical years. It is unclear how the complexities of pelvic anatomy are taught in the clinical setting by clinician educators. Understanding pelvic anatomy teaching from the perspectives of clinician educators is important because of their critical role in supporting medical students to become knowledgeable, safe practitioners. This study sought to examine factors that influence clinician educators' teaching of pelvic anatomy to medical students during clinical placement. Using established qualitative research methods, this exploratory study used semi-structured interviews with 10 participants from obstetrics/gynecology (n = 4) and urology (n = 6) in Australia. Interviews were transcribed, and data were analyzed using a reflexive thematic analysis framework. Two overarching factors were identified as influencing clinician educators' teaching: (1) how they defined the scope of knowledge, and (2) pedagogical understanding. Scope of knowledge was underpinned by patient safety considerations and guided by past experience and student aspirations. Pedagogical understanding also relied on experience and encompassed multiple teaching strategies, including approaches informed by sensitivities surrounding pelvic anatomy. Multiple factors influence how clinician educators define the scope of pelvic anatomy knowledge required of medical students. Clinician educators' awareness of syllabus and teaching approaches can enable them to support students to attain threshold concepts such as three-dimensional spatial relationships and sociocultural sensitivities associated with pelvic anatomy."
    mergedFrom:
      - source: eric
        ericId: EJ1411990
    finalDecision: exclude
  - title: Clinical Education Coordinators&apos; Selection and Deselection Criteria of Clinical Education Experiences
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About clinical education coordinators' selection criteria, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Does not indicate generative AI/LLM use"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
    ericId: EJ1329969
    authors: Benedict J, Neil E, Dougal Z, Walker S, Eberman L
    year: "2022"
    abstract: "Context: Athletic training students have identified clinical education as the most important aspect of their education when transitioning to practice. However, athletic training students have been frustrated with a lack of engagement, mentorship, and diversity within their clinical education experiences. As such, the selection and deselection of clinical sites is critical to creating effective learning experiences. Objective: To explore how clinical education coordinators (CECs) select and deselect clinical education experiences (sites and preceptors) for clinical integration and immersion. Design: Consensual qualitative research. Setting: Individual teleconference interviews. Patients or Other Participants: Thirteen CECs (age = 42 ± 8 years, experience in current role = 8 ± 8 years) from accredited professional master's programs who were in their position for at least 1 year and had at least 1 immersive rotation. Data Collection and Analysis: All interviews were audio recorded and transcribed. A 3-person data analysis team used a multiphase process to identify the emerging domains and categories. Trustworthiness was established through member checking, multiple researcher triangulation, and auditing. Results: Two themes emerged from the participant responses: accreditation compliance and strategic choices. We found participants expressed the theme of accreditation compliance as a major facilitator when selecting or deselecting clinical education placements. Strategic choices, such as student aspirations and focus on the quantity over the quality, were used by CECs to select clinical education placements which develop student autonomy and provide diverse experiences. However, the CECs engaged in convenient preceptor selection based on geographical location and previous relationships. Conclusion(s): Our findings suggest CECs leverage convenient clinical education opportunities that comply with accreditation expectations. CECs should be strategically selecting clinical education opportunities that move beyond accreditation minimum standards and focus on high-quality experiential learning, which leads to autonomous practice and embraces the diversity of the profession."
    mergedFrom:
      - source: eric
        ericId: EJ1329969
    finalDecision: exclude
  - title: "Veterinary Medical Education: A Practical Guide, 2nd Edition"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: This is a book/guide, not an original research article, systematic review, or meta-analysis"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Book/guide, not an original research article, systematic review, meta-analysis, or preprint"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
    ericId: ED661335
    authors: Jennifer L. Hodgson E, Jacquelyn M. Pelzer E
    year: "2024"
    abstract: "&quot;Veterinary Medical Education: A Practical Guide, Second Edition&quot; offers a comprehensive reference to all aspects of veterinary medical education, providing concrete guidance for instructors in a variety of settings. The book gives real-world, practical, veterinary-specific advice on all aspects of designing and implementing a veterinary curriculum. This Second Edition includes new and expanded information on widening access on admissions, competency-based veterinary education, academic advising and student support, eLearning, transition to practice and career opportunities, educational leadership and global veterinary education. This revised edition has been significantly enhanced and updated, featuring twelve new chapters and many expanded chapters. It includes diagrams, figures, and informational boxes that highlight key points, clarify concepts, provide helpful tips and evidence from the literature, and examples of educational innovations that could be adopted in veterinary programs. &quot;Veterinary Medical Education&quot; covers: (1) Student selection, including widening access; (2) Curricular innovations and competency-based veterinary education; (3) Learning theories, eLearning, and their application in the classroom; (4) Teaching in clinical and non-clinical settings and creating safe, inclusive learning environments; (5) Programmatic and technology-enhanced assessment, academic advising and study skills, coaching, and mentoring; (6) Professionalism and professional identity, cultural humility, and transition to practice; and (7) Program evaluation, educational leadership, and global trends. With comprehensive coverage of the field and a wealth of new and updated information, the Second Edition of &quot;Veterinary Medical Education&quot; is an indispensable resource for anyone involved with veterinary education, including instructors and faculty at veterinary colleges, continuing education instructors, veterinary technology instructors, and veterinarians training in internships and residencies."
    mergedFrom:
      - source: eric
        ericId: ED661335
    finalDecision: exclude
  - title: "Understanding Learner Trauma in the Emergency Medicine Clerkship: An Analysis of Self-Efficacy and Psychological Safety in the Clinical Learning Environment"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about learner trauma and psychological safety in EM clerkship"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: ED652987
    authors: Dimitrios Papanagnou
    year: "2024"
    abstract: "As third-year medical students transition from the classroom to the high-stakes, high-stress environment of the emergency department (ED), they confront a unique set of challenges that result in significant personal trauma. The literature offers limited insight into the trauma experienced specifically during the shift to emergency medicine (EM) as medical students' first clinical rotation. The purpose of this study was to bridge this gap by examining the interplay between students' perceived psychological safety of their ED teams and their own self-efficacy on the trauma they experienced as learners when working in this unique learning environment. This mixed-methods study included interviews with 17 third-year medical students who immediately completed the EM clerkship at an urban, academic ED. The study addressed four main questions: (1.) What types of trauma do students experience in the EM clerkship as they transition from the classroom into the clinical learning environment for the first time in their training? What are the factors of the learning environment that trigger trauma? (2.) In what ways, if any, do students' intersectional demographics affect their experiences of trauma during the EM clerkship? (3.) To what extent does general self-efficacy predict medical students' perceptions of the psychological safety afforded by their clinical team during the EM clerkship? (4.) How are students' experiences of trauma associated, if at all, with perceived psychological safety? What factors in the clinical learning environment contribute to psychological safety or its lack? This study utilized several data collection methods: (a) a pre-interview questionnaire soliciting information on student demographics and responses to items on the General Self-Efficacy Scale, (b) in-depth interviews using the critical incident technique, and (c) responses to items from the Team Psychological Safety Questionnaire. Several key findings emerged. A substantial amount of trauma that students experienced was rooted in a lack of peer support and student empowerment. Various triggers for trauma were identified that transcended different types of trauma. Demographic factors, such as race/ethnicity and gender, influenced the prevalence and nature of these traumatic experiences, with students from underrepresented backgrounds reporting deeper emotional connections with patients. While student self-efficacy was generally high, it did not correlate with the perceived psychological safety provided by their clinical teams. Furthermore, the perception of psychological safety within ED teams correlated with the nature of trauma experienced; those with lower safety scores reported trauma connected to peer support or issues related to cultural, historical, and gender considerations. Lastly, the opportunity for students to safely take risks or learn from mistakes, coupled with their own medical knowledge limitations, emerged as central to their perception of psychological safety within the team dynamic. Deeper insights into the data were revealed through a cross-interview analysis, and several analytical categories were used to further synthesize and interpret the data. Six conclusions were drawn from the study's findings and analysis: (1.) Medical students experience different types of primary trauma when immersed in the ED. (2.) Several forces that are intrinsic to the ED workplace influence the trauma students experience. (3.) Clerkship leadership must be aware of the unique experiences underrepresented students have in the EM clerkship. (4.) The psychological safety provided to students by their teams impacts their experiences of trauma in the ED. (5.) Self-efficacy offers a lens to understand students' experiences of trauma in the ED, but it is insufficient. (6.) Clerkship-specific interventions exist to amplify the team psychological safety afforded to medical students. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED652987
    finalDecision: exclude
  - title: "Students&apos; Insights on the Suitability and Adaptability of Two Undergraduate Medical and Dental Curricular Pathways before and during COVID-19 Pandemic: A Cross-Sectional Survey"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; COVID curriculum survey"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: EJ1362235
    authors: Okoromah C, Okei J, Udotong G, Buchi-Njere O, Ike J, Adeyeye A, Savage F, Nwobu M, Owate O, Eze U
    year: "2022"
    abstract: "The COVID-19 pandemic and lockdown disrupted every human endeavour including the world of medical education, halting pre-clinical and clinical trainings as well as graduation of physicians for the healthcare workforce thus posing a double jeopardy at a time of dire need for increased healthcare personnel. The adaptability and preparedness of training curricula for emergencies such as natural and man-made disasters ultimately determine the degree of disruption in medical education. This study explored students' insights on the efficacy and suitability of two curriculum pathways before and during the COVID-19 pandemic. This is a descriptive, cross-sectional study conducted between 2020 and 2021 in a public medical school using 102-item questionnaire administered online to 300 medical and dental students exposed to different training curricula- a &quot;Traditional Curriculum&quot;, and a new &quot;SPICCES Curriculum&quot; that emphasizes student-centred designs. The questionnaire response rate was 93.3% comprising 55.7% and 44.3% female and male respondents, and 78.6% and 21.4% medical and dental students respectively with a mean age of 22.1 ± 2.39 years (range: 18-33 years). A total of 65.4% and 34.6% respondents were on the SPICCES and Traditional curriculum pathways respectively. More respondents in the SPICCES curriculum compared with the traditional curriculum track (70.5% vs 52.6%; X[superscript 2] = 8.862, p = 0.003, [phi] = -0.178) had favourable perspectives on the relevance of their program and the overall quality of the curriculum in preparing them to be rounded physicians for the 21st-century (59.0% vs 27.8%). Approximately 47.0% and 33.0% (X[superscript 2] = 3.990, p = 0.046, [phi] = -0.119) respondents in dentistry and medicine respectively had unfavourable perception about the relevance of the SPICCES curriculum. Findings in this study echo the need for student-centred curriculum designs and conducive educational environments that are resilient to emerging and established threats to educational pedagogy and programs. The study highlights the need for programs that support social and extracurricular activities for students."
    mergedFrom:
      - source: eric
        ericId: EJ1362235
    finalDecision: exclude
  - title: "The Impact of Prior Performance Information on Subsequent Assessment: Is There Evidence of Retaliation in an Anonymous Multisource Assessment System?"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about multisource assessment bias"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus in the title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    ericId: EJ1423783
    authors: Bahar Saberzadeh-Ardestani, Ali Reza Sima, Bardia Khosravi, Meredith Young, Sara Mortaz Hejri
    year: "2024"
    abstract: "Few studies have engaged in data-driven investigations of the presence, or frequency, of what could be considered retaliatory assessor behaviour in Multi-source Feedback (MSF) systems. In this study, authors explored how assessors scored others if, before assessing others, they received their own assessment score. The authors examined assessments from an established MSF system in which all clinical team members - medical students, interns, residents, fellows, and supervisors - anonymously assessed each other. The authors identified assessments in which an assessor (i.e., any team member providing a score to another) gave an aberrant score to another individual. An aberrant score was defined as one that was more than two standard deviations from the assessment receiver's average score. Assessors who gave aberrant scores were categorized according to whether their behaviour was preceded by: (1) receiving a score or not from another individual in the MSF system (2) whether the score they received was aberrant or not. The authors used a multivariable logistic regression model to investigate the association between the type of score received and the type of score given by that same individual. In total, 367 unique assessors provided 6091 scores on the performance of 484 unique individuals. Aberrant scores were identified in 250 forms (4.1%). The chances of giving an aberrant score were 2.3 times higher for those who had received a score, compared to those who had not (odds ratio 2.30, 95% CI:1.54-3.44, P &lt; 0.001). Individuals who had received an aberrant score were also 2.17 times more likely to give an aberrant score to others compared to those who had received a non-aberrant score (2.17, 95% CI:1.39-3.39, P &lt; 0.005) after adjusting for all other variables. This study documents an association between receiving scores within an anonymous multi-source feedback (MSF) system and providing aberrant scores to team members. These findings suggest care must be given to designing MSF systems to protect against potential downstream consequences of providing and receiving anonymous feedback."
    mergedFrom:
      - source: eric
        ericId: EJ1423783
    finalDecision: exclude
  - title: "The Impact of Social Interaction, Academic Achievement, and Cognitive Flexibility Levels on Clinical Reasoning: Statistical Discourse Analysis"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on social interaction, academic achievement, and cognitive flexibility"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Does not indicate generative AI/LLM use"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
    ericId: EJ1379327
    authors: Toraman C, Akman C, Aytug-Kosan A, Korkmaz G
    year: "2023"
    abstract: This study aims to analyze the impact of intern physicians' social interaction during teamwork, their cognitive flexibility and academic achievement on clinical reasoning. The study, designed through sequential exploratory mixed method, was carried out with 20 intern physicians working in 4 teams. The process of how they approach to the case was recorded. The verbal interactions made during the discussions in the teams were analyzed, the discourses obtained from the videos were coded and modelled by regression. In addition, the cognitive flexibility and academic achievement levels of intern physicians were also included in the regression modelling. The results showed that, according to the initial signs and symptoms of the first period of the case (when uncertainty was high), the team members informing each other resulted in success in clinical reasoning. Although guiding in the first period was not very effective, excessive guiding in the second period played a negative role in clinical reasoning. In the second period, ignoring, reminding, and suggesting had a negative impact on clinical reasoning. Our study demonstrated that academic achievement and cognitive flexibility levels of the students in teamwork has a positive impact on the quality of clinical reasoning. Moreover, for effective clinical reasoning, the quality of discourses in the team is more important than how many discourses are created during the discussions.
    mergedFrom:
      - source: eric
        ericId: EJ1379327
    finalDecision: exclude
  - title: Art in Anatomy Session as a Method of Formative Feedback in Preclerkship Medical Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About using art for formative feedback in anatomy, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI/LLM use is indicated."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    ericId: EJ1463922
    authors: Saewon Chun, Cindy Liang, Charity Thomann, Shaimaa N. Amin, Christina Trinh, Camila Araujo, Sherif S. Hassan
    year: "2025"
    abstract: Medical schools were incorporating active learning strategies in anatomy teaching to accommodate diverse student bodies. Formative assessment and art as a hands-on learning method had been explored as alternatives to traditional teaching methods. Those methods allowed students to practice and assess their understanding of anatomy as they progress. The present study investigated the effectiveness of &quot;Art in Anatomy&quot; lab sessions in enhancing preclerkship medical students' comprehension of challenging anatomical topics and determining whether differences were related to their year in medical school. This study involved 41 pre-clinical year medical students at California University of Science and Medicine--School of Medicine (CUSM-SOM) who participated in Art in Anatomy sessions. Results showed presession and postsession quiz scores, with differences calculated for first-year and second-year medical students. The study revealed a significant skew in pre- and postsession data, with Year 2 students showing lower mean and smaller range on presession quiz scores. Postsession quiz scores showed higher mean and median scores but reversed on postsession. Both Year 1 and Year 2 students showed improved scores, with 68% experiencing a score increase of 0, 1, 3, or 4 points and 32% experiencing a 2-point increase. Art in Anatomy sessions could effectively support medical students in learning human anatomy during preclerkship years. The method provided formative feedback, aiding immediate recall of anatomical knowledge. Future research should explore different art forms and correlate postsession quiz scores with other students' exam scores, such as end-of-course, National Board of Medical Examiners (NBME), and practical exam scores.
    mergedFrom:
      - source: eric
        ericId: EJ1463922
    finalDecision: exclude
  - title: Evaluation of Georgia State University&apos;s CREATE Teacher Residency Program -- Cohorts 3 through 5. A Final Report of a Quasi-Experiment in Georgia. Appendix
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 6: Teacher residency program, not medical/health-professional learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Teacher residency participants are not medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: ED639277
    authors: Andrew P. Jaciw, Sze-Shun Lau, Jenna Zacamy, Li Lin
    year: "2023"
    abstract: "The Collaboration and Reflection to Enhance Atlanta Teacher Effectiveness (CREATE) is a three-year teacher residency program that seeks to recruit, support, and retain a diverse, highly effective, and culturally competent educator workforce that is committed to working in high-needs schools and to improving student outcomes. This evaluation focused on the effectiveness of CREATE for three cohorts of pre-service and early career teachers from 2017-18 through 2021-2022. Using a quasi-experimental design, participants included CREATE residents and comparison subjects, all of whom began participation in the study when they were enrolled in Georgia State University's College of Education and Human Development (GSU CEHD). The study followed participants for three years. We assessed primary impacts after participants completed two years as teachers of record. Program implementation and the experiences of early career teachers were measured across the study years. Appendices included in the document: (1) Description of CREATE Programming; (2) Participant Recruitment; (3) Details about Study Participation; (4) Data Collection Sources; (5) Schedule of Major Milestones; (6) Survey Response Rates; (7) Fidelity of Implementation Matrix; (8) Fidelity of Implementation Results; (9) Technical Details for Exploratory Intermediate Impacts on Teachers; (10) Supplementary Results for Analysis of Impacts on Student ELA Achievement; (11) Technical Details for Impacts on Early Career Teacher Retention; and (12) Impacts on Retention Through the Second Year of Teaching Among Teachers Who Were Retained Through the First Year of CREATE."
    mergedFrom:
      - source: eric
        ericId: ED639277
    finalDecision: exclude
  - title: Innovative Self-Directed, Problem-Oriented, Lifelong Learning, Integrated Clinical Case Exercise (SPLICE) Modules Promote Critical Thinking Skills, Early Clinical Exposure, and Contextual Learning among First Professional-Year Medical Students
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About SPLICE teaching modules, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: EJ1419461
    authors: Krishna Mohan Surapaneni
    year: "2024"
    abstract: Medical education is undergoing various transformations to promote a more personalized and contextual way of learning. In light of this, the innovative &quot;Self-directed, Problem-oriented, Lifelong learning, Integrated Clinical case Exercise&quot; (SPLICE) modules were designed, implemented, and evaluated for medical students in the first professional year as a strategy for early clinical exposure in a collaborative and self-directed way of learning. This is a mixed methods study involving first-year medical students. Students were divided randomly into the control and the intervention groups. Six SPLICE modules were administered to the intervention while the control group followed the traditional curricula. The educational outcome was compared using an end-of-module assessment. In addition, 13-item and 8-item questionnaires were administered to students to evaluate the SPLICE and plenary sessions on a 5-point Likert scale. Furthermore, students' feedback was obtained on a 10-point rating scale and in in-depth small-group interviews. The majority of students perceived that the SPLICE module improved their communication and encouraged meaningful, active learning. Students found the plenary sessions to be well organized, with sufficient interaction with professionals. Students also gave excellent scores for feedback on SPLICE modules, demonstrating the effectiveness of the innovation. In terms of test scores used in assessing learning outcomes, the intervention group outperformed the control group (P &lt; 0.0001). The innovative SPLICE curriculum facilitated early clinical exposure and active self-directed learning. Students perceived SPLICE modules to be highly helpful in terms of promoting meaningful learning and the future application of knowledge.
    mergedFrom:
      - source: eric
        ericId: EJ1419461
    finalDecision: exclude
  - title: Athletic Training Residency Program Development and Assessment of Advanced Clinical Reasoning
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about athletic training residency program development"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: EJ1329160
    authors: Hofmann D, Welch Bacon C, Rivera M, Eberman L
    year: "2022"
    abstract: "Context: Clinical reasoning is an inferential process of collecting and evaluating data in patient cases, necessary in all care. Athletic training residency programs are emerging rapidly as the industry standard for developing clinical specialists who possess advanced clinical reasoning needed for complex cases. Objective: The purpose of this study was to explore current practices of advanced clinical reasoning development and assessment in athletic training residency programs. Design: Inductive qualitative research. Setting: Web-based teleconferencing platform (Zoom Video Communications). Patients or Other Participants: Seven of 10 Commission on Accreditation of Athletic Training Education--accredited residency program directors (RPDs) (age = 46 ± 9 years, years as RPD = 5 ± 6). Data Collection and Analysis: We used qualitative, semistructured interviews. Audio files were transcribed verbatim, checked for accuracy, deidentified, and member checked before analysis. We used a 2-person data analysis team and an inductive coding approach. The researchers reviewed the transcripts independently, met to develop a codebook, applied the codes, and conducted internal auditing. Trustworthiness was established through member checking, multiple-analyst coding, and auditing. Results: Three themes emerged from the RPDs about developing advanced clinical reasoning: (1) defining clinical reasoning; (2) instructional tactics; and (3) assessment tactics. RPDs defined clinical reasoning as the process of the resident justifying their decision-making and the accuracy of their decisions. RPDs described both clinical and didactic instructional tactics, including mentoring, lectures and discussions, case presentations, and journal club. RPDs described assessing advanced clinical reasoning through patient and preceptor feedback in structured and unstructured formats, self-reflection, and knowledge testing to measure clinical reasoning. RPDs highly relied on preceptors and clinical mentoring, but also described a singular reliance on residents to appraise the clinical reasoning capacity of their own preceptors. Conclusions: RPDs define clinical reasoning as rationalization and accuracy of decisions. Although the programs are engaged in effective clinical and didactic approaches to teach advanced clinical reasoning, they rely heavily on the subjectivity of preceptors and residents to assess these outcomes."
    mergedFrom:
      - source: eric
        ericId: EJ1329160
    finalDecision: exclude
  - title: The Influence of Occupational Therapy Students&apos; Preferred Language on Academic and Clinical Performance in a Canadian University Program
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on language influence on academic performance"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    ericId: EJ1403091
    authors: Guitard P, Toal-Sullivan D
    year: "2023"
    abstract: "The Occupational Therapy Program at this Canadian university is a French program however, students must be bilingual to function in English or French clinical fieldwork settings. An understanding of the role of language in successful completion of program requirements was needed to effectively support students' education. The study objectives were to identify the role of linguistic factors in students' academic and clinical performance and to provide recommendations for strategies to address areas of learning difficulty. A multimethod approach used 4 data sources: (1) Multiple Mini Interview (MMI) informal language assessment scores, (2) grade point average (GPA), (3) fieldwork evaluation reports, and (4) an online survey of program graduates. The GPA on admission and MMI scores of 140 students predicted respectively only 20% and 2% of the variation in GPA on program completion. The areas of poorest performance in failed clinical fieldwork reports were in clinical reasoning and communication competencies. Among survey respondents (n = 47), 44.5% reported that a clinical placement in their second language with related charting (51.6%) and client communication (40.9%) were the most significant difficulties encountered in the program. Clients with mental health issues (45.4%) were the most challenging population to work with, attributed to communication barriers in the students' second language. Strategies are proposed to support occupational therapy students' academic and clinical language proficiency, including conversational training courses, problem-based learning activities in students' second language, focussed teaching on the clinical reasoning process and reflective skills, and language coaching to address early signs of difficulty in clinical fieldwork."
    mergedFrom:
      - source: eric
        ericId: EJ1403091
    finalDecision: exclude
  - title: Identifying Entrustable Professional Activities for MD Program in Biochemistry--A Modified Delphi Approach
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on EPA identification using Delphi method"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: EJ1387951
    authors: Rajaragupathy S, Govindarajan S, Ganesan J, Subramanian K, Doraiswamy V, Balasubramaniam G, Andra Suryanarayana M, Veeruswamy A
    year: "2023"
    abstract: Entrustable professional activities (EPAs) facilitate competency-based assessments. India is on the verge of implementing competency-based training for postgraduate programs. MD degree in Biochemistry is a unique program available exclusively in India. Postgraduate programs in most specialties have started working toward EPA-based curriculum, in both India and other countries. However, EPAs for MD Biochemistry course are yet to be defined. This study aims to identify EPAs for postgraduate training program in Biochemistry. Identification and attaining consensus on the list of EPAs for MD Biochemistry curriculum was done by modified Delphi method. The study was conducted in three rounds. In round 1, tasks expected from an MD Biochemistry graduate were identified by working group followed by expert panel validation. The tasks were organized and reframed to EPAs. Two rounds of online survey were conducted to achieve a consensus on the list of EPAs. Consensus measure was calculated. A cut-off value of 80% and above was considered to reflect good consensus. The working group identified 59 tasks. This was validated by 10 experts based on which, 53 items were retained. These tasks were reframed into 27 EPAs. In round 2, 11 EPAs achieved good consensus. Among the remaining EPAs, 13 achieved consensus of 60%-80% and were selected for round 3. Five EPAs achieved good consensus in this round. A total of 16 EPAs were identified for MD Biochemistry curriculum. This study provides a frame of reference for experts to develop an EPA-based curriculum in the future.
    mergedFrom:
      - source: eric
        ericId: EJ1387951
    finalDecision: exclude
  - title: "Certainty-Based Marking in Multiple-Choice Assessments in Physiology: A Web-Based Implementation Using an AI Assistant"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; AI assistant used to develop a certainty-based marking tool for physiology MCQs in small group classroom teaching."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Physiology classroom assessment study without a workplace-based or clinical education setting."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    ericId: EJ1490857
    authors: Chinmay Suryavanshi, Kirtana Raghurama Nayak
    year: "2025"
    abstract: Certainty-based marking (CBM) requires students to indicate their certainty levels alongside their answers. CBM has been shown to enhance self-assessment and metacognitive awareness. This study aimed to explore the implementation of CBM in multiple-choice assessments in physiology. The CBM assessment tool was developed with an artificial intelligence (AI) assistant, Claude 3.5, with prompts focused on functional rather than technical requirements. The assessment consisted of 15 multiple-choice questions (MCQs), which were administered as a pretest and posttest during a small group teaching session to first-year medical students. Following the assessment, students completed a survey to evaluate their perceptions regarding the format, knowledge-gap identification, and overall acceptability. Answers from 195 students were analyzed, and significant improvements were observed in performance measures and certainty indices from the pretest to the posttest. Most students (80.9%) found the certainty scale beneficial, and 78.3% changed their answers after reflecting on their certainty. CBM demonstrated metacognitive benefits, with 86.4% of students better recognizing their knowledge gaps and 85.8% feeling more aware of their learning progress. About 73% of students preferred the CBM format and expressed greater engagement (82.8%) than traditional MCQs. CBM implemented through a web-based platform functioned as an assessment tool and an instructional intervention that enhanced students' metacognitive awareness and self-monitoring skills in physiology education. Our study focused on a single physiology topic and showed improvements in knowledge retention and certainty calibration. However, further longitudinal studies across multiple topics are needed to determine whether students maintain these self-assessment skills over time.
    mergedFrom:
      - source: eric
        ericId: EJ1490857
    finalDecision: exclude
  - title: OBGYN Resident Research Program Practices and Outcomes after Implementation in an Academically Affiliated Residency Program
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About a resident research program, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI/LLM component is indicated."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    ericId: EJ1440156
    authors: Emily A. Evans-Hoeker, Mariah J. Rudd, Amanda B. Murchison, Hunter D. Sharp, Samantha M. Harden
    year: "2024"
    abstract: "Unlike other aspects of Obstetrics and Gynecology (OBGYN) residency training, the research education curriculum and requirements are not standardized. It is unclear what constitutes core curricula in educational research programs, which components are most useful and efficacious, and how to use these findings to adapt existing programs. A multi-phase approach to (1) garner cross-sectional data from United States OBGYN Residency Program Directors and review previously published literature, (2) use Phase I and input from clinical, research and education faculty to develop a comprehensive program, and (3) evaluate the program for resident research productivity, knowledge and satisfaction. Phase I: Sixty-one (31%) Program Directors completed the survey. Most reported [less than or equal] 4 research didactics per year and no dedicated research rotation. Programs with post graduate year (PGY) level specific didactics, a research rotation, or allocated time for the Research Director role noted higher odds of resident research resulting in peer-reviewed publication. Phase II: A Resident Research Program was adapted to include a clinical and research faculty co-directorship and seven additional didactics (all of which were PGY-specific). A departmentally funded resident research grant and a repository to monitor completion of milestones and program requirements have been developed. Surveys were used to track residents' research competence and satisfaction with the research program. While resident involvement in research activities and satisfaction with opportunities for scholarly activity have improved, resident self-evaluations and mentor evaluations have not. In addition, we have not seen an increase in publications or presentations. Implementation of components of a research program associated with increased productivity did not result in increased productivity in our program, however, resident satisfaction with scholarly activity did improve."
    mergedFrom:
      - source: eric
        ericId: EJ1440156
    finalDecision: exclude
  - title: "Humanizing Teacher Preparation: Claremont Graduate University&apos;s Teacher Residency"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 6: About teacher preparation, not medical or health-professional learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: ED674736
    authors: Cathy Yun, Julie Fitz
    year: "2025"
    abstract: Claremont Graduate University (Claremont), a nonprofit, private institution in southern California, offers a flexible Teacher Education Program that is a 12- to 18-month, 100% online residency program. Claremont's residency model was launched as a pilot in 2020 with 5 residents and grew to include 34 fulltime residents in the 2022-23 academic year, when this study was conducted. Candidates earn a Master of Arts in Education in addition to a preliminary Multiple Subject (elementary), Single Subject (secondary), or Education Specialist (special education) credential. Within the program, there are several timing, placement, and funding options for residents, of which the Claremont Fellows option was the most commonly used from 2020 through 2023. The case study presented in this report was conducted in 2023 and was guided by the overarching question &quot;How do successful residencies do their work?&quot; It is part of a larger multiple-case study of five California teacher residency programs across four different institutions of higher education, conducted with the goal of documenting the details of program infrastructure; program design; recruitment strategies; resident, mentor teacher, and graduate supports; partnerships; and financial sustainability. By understanding the details of how these residencies developed and operated their programming, we are able to share insights that can inform the design and continuous improvement of residency programs across the country.
    mergedFrom:
      - source: eric
        ericId: ED674736
    finalDecision: exclude
  - title: An Exploration of Psychological Safety in Allied Healthcare Clinical Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About psychological safety in clinical education, no mention of generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: EJ1450021
    authors: Kelsey J. Robinson, Heather David, Beth Kinslow, Jennifer Austin
    year: "2024"
    abstract: Psychological safety impacts learning in various professions and settings, including healthcare. While the construct has been investigated in medical and nursing education, less is known within allied health education. This study aimed to investigate athletic training and physical therapy students' perceptions of psychological safety within clinical experiences using the Psychological Safety Scale. Participants of this quantitative, cross-sectional survey were students and recent graduates of Master of Athletic Training and Doctor of Physical Therapy programs. Data was collected anonymously via Qualtrics using a convenience sample. Following data cleaning and imputation, the study returned 338 usable responses, 169 from each profession. Descriptive statistics were used to analyze demographic data, and ANOVA analyses were used for between-group comparisons. No differences in psychological safety were found based on age, gender identity, profession, student versus graduate status, degree progress, or number of clinical experiences completed. Although not statistically significant, data analysis revealed students early in their degree progression reported the highest levels of psychological safety, 5.73 (SD = 1.16) which declined with degree progress, reaching the lowest levels among recent graduates at 5.46 (SD = 1.20). Psychological safety was remarkably similar between athletic training and physical therapy students and was higher than reported findings for nursing and medical students. This study provides a baseline understanding of psychological safety within allied health professions' clinical experiences. Further research is needed to understand the influence of psychological safety in clinical experiences.
    mergedFrom:
      - source: eric
        ericId: EJ1450021
    finalDecision: exclude
  - title: "A Creative Medical Investigation: Bridging the Soft Skill Gap"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about soft skills in medical education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: EJ1397868
    authors: Wood T, Grahovec N, Sanfilippo C, Hill C, Gockel-Blessing E
    year: "2023"
    abstract: The refinement of soft skills is essential to the success of athletic training students. Yet, more work is needed to identify desirable educational techniques to support the progression of soft skills in athletic training education. The current study identified professional athletic training students' (10 students; 21.2 ± 2.1 years) perceptions of the five-part creative medical investigation (CMI) and described how the CMI could contribute to clinical reasoning, collaboration, and reflection, along with integrating soft skills with hard skills. The consensual qualitative research (CQR) approach utilized semi-structured interviews after the five CMI parts. Three qualitative researchers conducted CQR analysis after all interviews were complete. The researchers independently coded the interview transcripts and met to triangulate common themes; member checks were conducted to validate the findings. A 10-month follow-up survey evaluated the long-term effects of the CMI on selected subdomains of clinical reasoning. Five themes were identified from the consensual qualitative analysis. This CMI (1) Facilitated clinical reasoning, (2) Encouraged collaboration and reflection, (3) Promoted soft and hard skill integration, (4) Improved student confidence, (5) Enjoyed by the students. The follow-up survey indicated that students agreed (4.1-4.5 out of 5) this CMI leads to improvement in clinical reasoning subdomains. The current CMI facilitated desirable soft skills and integrated soft skills with hard skills through an enjoyable experience. Thus, the CMI might be a desirable educational technique with long-term implications to support the progression of soft skills in health care education.
    mergedFrom:
      - source: eric
        ericId: EJ1397868
    finalDecision: exclude
  - title: "Curricular Praxis: Preparing Athletic Trainers by Combining Formal Debriefing with Clinical Experiences"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about debriefing in athletic training education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: EJ1484931
    authors: Christopher J. Hamlyn, Thalia Mulvihill
    year: "2025"
    abstract: "Background: This study examined the integration of formal debriefing in athletic training clinical education, addressing a gap in understanding its role during the transition to professional master's programs. It focused on how formal debriefing can bridge the shift from traditional learning models, ensuring effective student learning and knowledge application in diverse clinical scenarios. The study specifically explored the perceptions of preceptors and students about the incorporation of debriefing in a Commission on Accreditation of Athletic Training Education accredited athletic training program. Methods: This qualitative study used a purposive sample of five preceptors for an accredited athletic training program with training in formal debriefing as well as the seven students with whom they worked. Data were gathered through semistructured interviews and transcribed verbatim. Analysis involved a general inductive reasoning process, progressing from specific information to broader thematic units. Trustworthiness was ensured through peer-debriefing, member checking, and researcher reflexivity. Findings: Within this research, six themes emerged from the experiences shared by the preceptors. These included: (1) lack of familiarity, (2) structured reflection on thought process, (3) cumulative learning, (4) commitment, (5) challenge, and (6) obstacles. These themes were derived from participants' statements and collectively contributed to a synthesized understanding of their experiences. Conclusions: This study demonstrates that formal debriefing techniques enhance clinical education for athletic training students by improving clinical reasoning and readiness for practice. Successful adoption requires a cultural shift in athletic training education, with preceptors embracing their dual role as educators and healthcare providers, supported by comprehensive training and clear program expectations. Students must actively engage in reflective learning, while programs should move beyond traditional apprenticeship models toward structured educational experiences. Despite implementation challenges, integrating tailored debriefing methods offers a promising strategy to advance clinical education and strengthen the profession's commitment to evidence-based practice."
    mergedFrom:
      - source: eric
        ericId: EJ1484931
    finalDecision: exclude
  - title: Physical Therapy Students&apos; Attitudes toward Older Adults
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on student attitudes"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    ericId: ED639964
    authors: Kelly D. Hawthorne
    year: "2023"
    abstract: "The purpose of this study was to investigate physical therapy students' attitudes toward working with older adults. A basic qualitative design geared toward discovering physical therapy students' attitudes, where themes emerge from the findings, was used. The setting was a CAPTE Accredited Doctor of Physical Therapy (DPT) program in a 4-year, private not-for-profit university in a large city in the Midwest with a Carnegie Classification of Doctoral Universities: High Research Activity. The major themes that emerged from semi-structured participant interviews were 1) Congruence of emotion and thought due to early exposure, 2) A potpourri of emotions and thoughts, 3) Peer influence, 4) Relevancy of clinical experiences, 5) Clinical mentorship, and 6) University mission. Overall, built on the long-standing relationship with grandparents, physical therapy students interviewed in this study had emotional and positive attitudes toward older adults. In addition, the mentoring provided by clinical instructors in the clinical education portion of the curriculum was perceived to be the most beneficial when it came to preparing physical therapy students for practice with older adults. Of course, more research on physical therapy students' attitudes toward older adults is needed. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED639964
    finalDecision: exclude
  - title: Reimagining Clinical Education Practices for Autism through the Multi-Client Multilevel Mentorship Model
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About mentorship model for autism clinical education, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: EJ1445300
    authors: George W. Wolford, Schea Fissel Brannick
    year: "2024"
    abstract: Speech-language pathology students require comprehensive graduate education to address the needs of their future autistic clients. Despite this need, survey research suggests that students receive limited didactic and clinical graduate training that sufficiently prepares them to work with autistic clients. Contemporary research into clinical education for autism includes several features, such as more support and group-based services, that do not align with traditional clinical education in the field (Anderson, 1988; Dudding et al., 2017). The purpose of this study is to describe feasibility (by acceptability and implementation) of a new clinical education protocol, the Multi-client Multilevel Mentorship (M[superscript 3]) model. The M[superscript 3] model is a collaborative clinical education model that emphasizes in-the-room clinical supervision of group-based service delivery for a team of students. Two cohorts of student clinicians (N = 9) participated in two ten-week rotations where they provided (a) and a literacy intervention (b) an intervention targeting executive function for two groups of clients with mixed diagnoses including autism spectrum disorder. Two clinical educators supervised the sessions with additional support by peer mentors. Survey feedback from participants showed that they rated the clinical education experience highly, suggesting adequate acceptability of the M[superscript 3] model. Participants demonstrated strong fidelity to one protocol and fair fidelity to the other, which was a positive indicator of implementation. Overall, student participants appear to benefit from the M[superscript 3] model during an adapted group intervention protocol designed for autistic clients. Further testing of the M[superscript 3] model's effectiveness is warranted given the positive feasibility indicators.
    mergedFrom:
      - source: eric
        ericId: EJ1445300
    finalDecision: exclude
  - title: Does It Make the Grade? Clinical Grading in an Optometric Program
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on clinical grading practices in optometry"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    ericId: ED649371
    authors: Marc B. Taub
    year: "2022"
    abstract: "In this naturalistic and formative evaluation, The purpose of the study was to examine and understand how the clinical grading system at Southern College of Optometry (SCO) is being used, whether it is effective as a grading system and in teaching, and how it can be improved to better suit the needs of administration, faculty, and students. I used Experiential Learning Theory to view clinical grading as an opportunity for reflection and investigated whether the grading system was being used for that purpose. I interviewed three administrators and conducted both a faculty and student focus group with six participants each. Through thematic analysis, five themes developed: (1) Faculty expectations develop with experience, are highly personal, and have an impact on learning; (2) Faculty feedback can have a positive or negative impact on student learning; (3) The clinical grading system is used in a variety of ways and for different reasons by the faculty, administrators, and students; (4) Clinical grading is subjective and has challenges that inhibit its effective use; and (5) The clinical grading system continues to evolve and grow to meet the needs of all parties. The current clinical grading system at SCO is partially effective for grading and teaching but has barriers that hamper student reflection. It has a variable impact on shaping student learning and performance based on how it is being used by both faculty and students. The grading system mostly meets the needs of the various stakeholders, but I make recommendations toward process improvement. The recommendations are both specific to SCO and more broadly, to optometric education. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED649371
    finalDecision: exclude
  - title: "&quot;Fun Slipping into the Doctor&apos;s Role&quot;--The Relationship between Sonoanatomy Teaching and Professional Identity Formation before and during the COVID-19 Pandemic"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Sonoanatomy teaching and professional identity, not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    ericId: EJ1335228
    authors: Darici D, Missler M, Schober A, Masthoff M, Schnittler H, Schmitz M
    year: "2022"
    abstract: The various psychological dimensions of professional identity formation (PIF) are an important aspect of the study course for undergraduate medical students. Anatomical learning environments have been repeatedly shown to play a critical role in forming such an identity; however, relevance of PIF during sonoanatomical training remains underexplored. At the end of their basic anatomy studies, third-semester medical students took part in a four-day block course on anatomy and imaging. Anatomical content was revised in small groups using peer teaching and imaging methods, including one hour of hands-on sonoanatomy sessions each day. On-site sonoanatomy was identified as an excellent format to support students' transition from the pre-clinical to clinical phase as medical experts-to-be. Students enjoyed practical exercises and the clinical input, which increased their interest in the medical profession and their academic studies. This study further examined the effects of the transition into an online-only format, necessitated by the current Covid-19 pandemic. A comparison was made between the quantitative and qualitative evaluation data, and the written results of examinations of several on-site (n = 1096, &quot;mean age&quot; = 22.4 years ± 2.18), and online-only cohorts (n = 230, &quot;mean age&quot; = 22.6 years ± 2.21). The online-only transition led to a reduction of all PIF-related variables measured, losing identity-related variables, increasing students' stress levels, and reducing their long-term academic performance. Together, this study demonstrates presence of PIF in undergraduate sonoanatomy teaching, and cautions against the uncritical online-only substitution of hands-on learning environments.
    mergedFrom:
      - source: eric
        ericId: EJ1335228
    finalDecision: exclude
  - title: Athletic Training Students&apos; Preparation and Perception of Resources Available to Them Following a Critical Incident
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about athletic training students' experiences with critical incidents"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: EJ1405506
    authors: Julie A. Wonch, Jamie L. Mansell, Zachary K. Winkelmann, Lindsey E. Eberman, Elizabeth R. Neil
    year: "2024"
    abstract: "Context: Although catastrophic injuries are rare, athletic trainers must be ready for emergencies. It is unknown how athletic training students (ATSs) are prepared for postcritical incident management. Objective: To explore the perceived availability and helpfulness of resources available to ATSs after critical incidents. Design: Qualitative. Setting: Individual interviews. Patients or Other Participants: Eleven current ATSs and/or recent graduates (5 women, 4 men, and 2 prefer to not share; age = 24 ± 4 years) who experienced a critical incident during clinical education. Data Collection and Analysis: We used criterion sampling for recruiting participants in multiple modes until data saturation was achieved. Participants completed demographic surveys and participated in semistructured interviews regarding their involvement, resources available after the incident, and perceived helpfulness of the resources. All transcripts were deidentified and coded by a 3-person team following the consensual qualitative research tradition. Member checking, triangulation, external auditing, and peer discussion among researchers established credibility and trustworthiness for the analysis. Results: Three domains emerged--&quot;strategies acquired before the incident, management during the incident,&quot; and &quot;postincident regulation&quot; of a review of the resources that had been available. In &quot;strategies acquired before the incident,&quot; participants described that program preparation, previous personal/clinical experiences, and self-regulating mental health strategies all assisted in managing emotions after the incident. During the incident and the time immediately after, participants discussed managing emotions with formal debriefing, sound policies and procedures, and communication with athletic training faculty and peers. After the incident, some noted that they had an abundance of resources available to them during the critical incident, whereas others noted feeling isolated. Conclusions: ATSs have resources available to them and an awareness of the impact critical incidents have on mental well-being. They relied on convenient coping mechanisms (eg, peers, preceptors, or faculty debrief) rather than formal counseling. Athletic training programs should provide students with resources and support before and after incidents."
    mergedFrom:
      - source: eric
        ericId: EJ1405506
    finalDecision: exclude
  - title: "Clinical Reasoning Development: Exploring the Role of Instructor Feedback during Health Professions Clinical Education Experiences"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About instructor feedback in clinical education, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: ED657010
    authors: Ryan T. Orrison
    year: "2024"
    abstract: "The growing complexity of the healthcare environment and the need to equitably address the health and well-being of a diverse patient population has increased the requirement for practical instruction and assessment of clinical reasoning in the training of health professionals. Formative assessment, in the form of feedback, is recognized as an essential instructional component in teaching clinical reasoning. This study aimed to investigate the role of feedback in developing clinical reasoning during clinical education. This investigation consisted of three unique studies completed in phases. The scoping review methodology in phase one revealed six themes from the included articles (n = 28) regarding the use of feedback in teaching and learning clinical reasoning. Phase two consisted of an electronic survey administered to 91 student physical therapists completing one of their final clinical experiences before graduation. The survey collected data about self-efficacy for clinical reasoning, student feedback orientation, and their perceptions of the quality and delivery of feedback. Relationships were identified between student feedback orientation ([beta] = 0.663, t(85) = 4.21, p &lt; 0.000), self-efficacy for using feedback ([beta] = 0.355, t(90) = 4.21, p &lt; 0.001) and their self-efficacy for clinical reasoning. In phase three, the researcher conducted a retrospective analysis using dyads (n = 1544) created using survey responses from student physical therapists (n = 395) and characteristics of clinical instructors. A small positive effect was seen in students' self-assessed clinical reasoning skills when paired with a Credentialed Clinical Instructor or board-certified specialist during clinical experiences (t(1538) = 2.38, p = 0.017). In sum, the findings from all three phases of this investigation provide insight into the role of feedback in clinical reasoning development, which can be used to inform future research. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED657010
    finalDecision: exclude
  - title: The Effects of Curricular Change on Student Learning and Well-Being in Biomedical and Clinical Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About curricular change effects on student learning, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: ED645565
    authors: Rachel Jalaire Tomco Novak
    year: "2022"
    abstract: "The implementation of curriculum change, and innovative pedagogical theory, can help educators and administrators in higher education further the learning gains of students in the sciences. But the introduction of new methods of teaching, or curricular restructuring, can be interpreted by students differently, potentially affecting students' emotional states as well as their relationships with peers. To support not only the learning of students, but also their emotional and social well-being, pedagogical and curricular theory should be enacted in ways that take into consideration the full scope of the student experience. In this dissertation, the implementation of curriculum and pedagogical theory, and the effects of the usage of active learning methods, are examined through student learning gains as well as through any reported social and emotional affects. In the first chapter we examine the history and realization of the integration of clinical and biomedical sciences in the field of predoctoral dental education. In the second chapter, the effects of early experiential learning opportunities are examined via students' progression to clinical competence and students' self-reported confidence in a predoctoral dental program. In the third chapter, we consider and review how curricular change may affect students' emotional states, their relationships with peers, and if the concept of stereotype threat played any role in the complication observed. Lastly, in chapter four we explore how active learning may benefit, or hinder, the learning of students in an undergraduate anatomy course with social anxiety in a virtual learning environment. Throughout this dissertation, we seek to promote student learning through the use of educational best practices and consider how curriculum and pedagogical changes might also affect the feelings and emotional states of students, for the purpose of building a considerate and effective educational environment. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED645565
    finalDecision: exclude
  - title: "Athletic Training Clinical Education Assessments: An Exploration of the at Milestones"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: ED643247
    authors: Chelsey Bruns
    year: "2022"
    abstract: "Athletic training is an evolving and growing allied health profession. With recent changes in educational degree requirements and standards, many athletic training programs are reviewing current assessments. The AT Milestones project was developed to provide programs with a guide for student assessment within athletic training education programs (ATEPs). While these Milestones have slowly been implemented into select ATEPs, there is currently no published empirical research on the application of the AT Milestones in athletic training education. The purpose of this study was to examine athletic training educators' perceptions of current clinical evaluations, and program evaluations' alignment to the principles of the AT Milestones. The survey had a response rate of 13.2%. Overall, faculty including program directors and clinical coordinators were satisfied or very satisfied with their current clinical evaluations. The majority of respondents reported that they had a general understanding of the AT Milestones (50%). Programs seeking accreditation or in good standing were more likely to have made or are planning to make revisions to clinical evaluations and were more likely to implement the AT Milestones than those programs voluntarily withdrawing. The average AT Milestones Alignment Criteria Score (ATMACS) among the participating institutions was 55%, indicating an average alignment with the AT Milestones. This research seeks to contribute to the limited literature base on clinical assessments in athletic training and serve as a guide for administrators in developing clinical evaluations that best fit the needs of their respective programs. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED643247
    finalDecision: exclude
  - title: "&quot;Pancreata: The Keto Struggle&quot;: An Innovative Educational Tale-Based Game for Diabetic Ketoacidosis Revitalizes Collaborative Learning, Learner&apos;s Engagement among Undergraduate Medical Students"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Educational game for medical students, not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    ericId: EJ1471694
    authors: Krishna Mohan Surapaneni
    year: "2025"
    abstract: "Traditional didactic teaching methods in medical education, while foundational, often lead to passive learning and insufficient engagement. &quot;Pancreata--The Keto Struggle,&quot; an educational tale-based game for diabetic ketoacidosis (DKA), was developed to address these challenges by promoting collaborative learning, enhancing student engagement, and improving knowledge retention through an interactive and narrative-driven approach. This study involved 150 first-year medical students divided into 25 small groups of 6 each. Participants were assessed before and after engaging with the game through a structured formative assessment, a validated questionnaire measuring engagement and learning effectiveness, and a confidence level questionnaire. In-depth small-group interviews were also conducted for qualitative feedback and thematic analysis was performed. Statistical analyses were performed using SPSS version 17. The introduction of &quot;Pancreata--The Keto Struggle&quot; resulted in significant improvements in students' formative assessment scores, from a mean of 19.2 ± 1.9 before the game to 39.3 ± 2.2 out of 50 after the game (p &lt; 0.0001). Notably, students demonstrated the highest confidence gains in managing DKA and interpreting laboratory results. Qualitative analysis identified seven common themes reflecting the game's impact on learning: collaboration, retention of concepts, internal drive, self and peer assessment, joyful learning, beyond books, and aesthetic content. Over 95% of students reported increased engagement and learning effectiveness due to the game's intrinsic motivation, narrative, and group learning mechanics. &quot;Pancreata--The Keto Struggle&quot; effectively revitalizes collaborative learning in medical education by integrating game-based learning with traditional teaching methods. The game not only facilitates a deeper understanding of complex clinical conditions like DKA but also broadly improves students' clinical management skills and confidence. These findings underscore the potential of educational tale based games to enrich medical education and advocate for their broader application across curricula."
    mergedFrom:
      - source: eric
        ericId: EJ1471694
    finalDecision: exclude
  - title: "Student Nurses&apos; Experience of Moral Distress in Clinical Practice Experience: A Hermeneutic Phenomenological Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about nursing students' moral distress"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: ED635890
    authors: Stutte L
    year: "2023"
    abstract: "This study aimed to explore the experience of moral distress in pre-licensure student nurse's during their clinical placement experience. Nurses and student nurses are susceptible to moral distress when faced with ethical dilemmas in nursing practice. For nurses, this may impact their personal and professional relationships, and in some instances results in leaving the profession. The lived experience of moral distress in the pre-licensure student nurse population is under-researched and thus a knowledge-gap related to this phenomenon. The general research question that guided this study was: &quot;What is the lived experience of student nurses who experienced moral distress during their clinical placement experience in their program of study?&quot; The sub-questions that guided the research were: (1) &quot;What patient care experiences evoke feelings of moral distress?&quot;; (2) &quot;How does a student nurse describe the experience of moral distress in a patient care experience?&quot;; (3) &quot;How does the student nurse perceive moral distress in a patient care experience?&quot;; and (4) &quot;What is the impact of the experience of moral distress on the student nurse? Hermeneutic phenomenological research design was used to gather and interpret a student nurse's experience of moral distress. Noddings' ethic of care and Rushton et al. Moral Distress Framework were the theoretical frameworks that guided this study. Twenty-one pre-licensure student nurses from twelve states were interviewed using semi-structured interview protocol. Thematic analysis of the interviews was completed. Study findings suggest that student nurses: 1) identified a triggering event, 2) identified the ethical value violated, 3) experienced emotional and physical symptoms, and 5) did not take corrective action. Themes related to the impact of moral distress on attitude towards the nursing profession were: 1) not supported by clinical faculty, 2) opinion of the profession, and 3) influence on the student nurse practice. The significance of these findings suggest that administrators and other nursing leaders need to develop interventions that can be used to and taught to manage the experience of moral distress. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED635890
    finalDecision: exclude
  - title: Online Case-Based Learning for Medical Students as a Teaching Method for Biochemistry at King Abdulaziz University amid COVID-19 Pandemic (a Study Conducted in 2021)
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Case-based learning for biochemistry (study conducted 2021), not about generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    ericId: EJ1419177
    authors: Eman A. Kubbara, Nasreldin Marhoum Ahmed, Turki Alamri, Marwan A. Bakarman, Husam Malibary, Rasha Eid, Abdulrahman Omer Alzahrani
    year: "2024"
    abstract: "Online case-based learning (CBL) is a method used by King Abdulaziz University to teach medical students in their preclinical years. The use of CBL in basic sciences is important for enabling medical students to correlate basic sciences with future clinical practice. This study implemented online CBL for biochemistry teaching as part of the medical genetic module for 3rd-year preclinical students. Teaching during the study was completely online because of restricted precautions caused by the COVID-19 pandemic, except for practical sessions that were held on campus. The case was presented to the students involved in prenatal screening and diagnosis. Students were guided to learn and discuss the biochemical tests used for prenatal screening and diagnosis and their clinical importance. They were divided into two groups: the control group was given an online lecture and the intervention group was given an online CBL session before the lecture. The online lecture was given to both groups simultaneously by the same instructor, and then 10 MCQs tests were distributed to both groups after the lecture, and their test scores were compared. A 10-question survey was administered to the intervention group to assess their perception of the online CBL session after the test. A significant difference was found between the intervention and control groups regarding test scores (p &lt; 0.001), and most students found the online CBL session enjoyable and motivating."
    mergedFrom:
      - source: eric
        ericId: EJ1419177
    finalDecision: exclude
  - title: "Assessment for Learning: The University of Toronto Temerty Faculty of Medicine M.D. Program Experience"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about assessment program at a medical school"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: EJ1353920
    authors: Tait G, Kulasegaram K
    year: "2022"
    abstract: "(1) Background: Programmatic assessment optimizes the coaching, learning, and decision-making functions of assessment. It utilizes multiple data points, fit for purpose, which on their own guide learning, but taken together form the basis of holistic decision making. While they are agreed on principles, implementation varies according to context. (2) Context: The University of Toronto MD program implemented programmatic assessment as part of a major curriculum renewal. (3) Design and implementation: This paper, structured around best practices in programmatic assessment, describes the implementation of the University of Toronto MD program, one of Canada's largest. The case study illustrates the components of the programmatic assessment framework, tracking and making sense of data, how academic decisions are made, and how data guide coaching and tailored support and learning plans for learners. (4) Lessons learned: Key implementation lessons are discussed, including the role of context, resources, alignment with curriculum renewal, and the role of faculty development and program evaluation. (5) Conclusions: Large-scale programmatic assessment implementation is resource intensive and requires commitment both initially and on a sustained basis, requiring ongoing improvement and steadfast championing of the cause of optimally leveraging the learning function of assessment."
    mergedFrom:
      - source: eric
        ericId: EJ1353920
    finalDecision: exclude
  - title: "Examining the Professional Identity Development of Professional Master&apos;s Athletic Training Students: A Cohort Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on professional identity development"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: EJ1366221
    authors: Gardiner-Shires A, Kloepfer M
    year: "2023"
    abstract: "Context: Professional identity development in professional master's (PM) athletic training students occurs over time and is influenced by numerous socializing factors. Although socialization processes of athletic training students have been examined, professional identity development related to the Weidman et al graduate and professional student socialization framework [Weidman JC, Twale DJ, Stein EL, et al. &quot;Socialization of Graduate and Professional Students in Higher Education: A Perilous Passage?&quot; ASHE-ERIC higher education report. Vol 28, No. 3. Jossey-Bass Higher and Adult Education Series; 2001] has never been examined in athletic training. This model values the multiple networks, individuals, and experiences that influence graduate students throughout their program. Objective: To assess the lived experiences of 1 cohort of students enrolled in a PM athletic training program and determine what curricular and clinical education factors influence the development of professional identity. Design: Qualitative study. Setting: Focus group interview. Patients or Other Participants: Ten of 12 (83%) students (5 females and 5 males) from a single PM athletic training cohort program participated. Data Collection and Analysis: We conducted a focus group interview, which we transcribed verbatim, with participants during the last week of their last semester in the PM athletic training program. Data analysis was deductive and then inductive throughout the coding process, and we reached data saturation at the individual participant and cohort levels. We ensured trustworthiness through multiple analyst triangulation, peer expert review, and multiple data sources. Results: Three factors influenced the development of the PM athletic training student's professional identity: (1) their clinical education experiences, (2) the cohort experience, and (3) their school/life balance. Within the clinical education experience, the immersive experience, clinical setting, and patients and preceptors were also influential. Conclusions: This study used the Weidman et al socialization model. The experiences of PM athletic training students and their professional identity development are inclusive of the relationship with others throughout the educational experience, as well as their self-reflective practices within the field. It is important for stakeholders to understand the many factors that influence professional identity development."
    mergedFrom:
      - source: eric
        ericId: EJ1366221
    finalDecision: exclude
  - title: A Phenomenological Reading of Integration in the Longitudinal Integrated Clerkship
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on clerkship integration phenomenology"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    ericId: EJ1488208
    authors: Nicola Parkin
    year: "2025"
    abstract: "This paper reports on a phenomenological reading of integration in education programs, through three layers of specificity: (1) broadly, on integration as an educational idea; (2) more particularly, on integration as expressed in the program model of the longitudinal integrated clerkship (LIC); and (3) with the highest degree of specificity, on integration 'at work' in an actual LIC program. Each layer of penetration found different aspects of the phenomenological nature of integration. A further integrative leap across these aspects phenomenologically revealed four essential qualities of integration, being: integration transforms parts into wholes; integration's animating locus is mutable, mobile and multiple; integration is already at work and is sustained through possibility; and integration holds open for 'more' integration. By appreciating integration for its essential qualities, this research adds foundational value to the literature on educational integration, and opens new questions about how we, as educators, might deliberately work with both its presence and its promise in our educational pedagogies, practices and programs."
    mergedFrom:
      - source: eric
        ericId: EJ1488208
    finalDecision: exclude
  - title: "Procedural Simulation in Venipuncture for Medical Undergraduates and Its Transfer to the Bedside: A Cluster Randomized Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about procedural simulation for venipuncture"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: EJ1428986
    authors: Kaumudee Kodikara, Thilanka Seneviratne, Ranjan Premaratna
    year: "2024"
    abstract: "Simulation is accepted as an effective method of learning procedural skills. However, the translational outcomes of skills acquired through simulation still warrants investigation. We designed this study to assess if skills laboratory training in addition to bedside learning (intervention group [IG]) would provide better learning results than bedside learning alone (control group [CG]) in the context of venipuncture training. This prospective, cluster-randomized, single-blind study took place at the Faculty of Medicine, University of Kelaniya, Sri Lanka. Seventeen clusters of second-year medical students were randomly assigned to either IG or CG. The IG trained on venipuncture in the skills laboratory, receiving instruction after modified Payton's Four Step Method. Following the training, students of both IG and CG underwent bedside learning for one month. Afterward, students of both groups performed venipuncture on actual patients in a clinical setting. An independent, blinded assessor scored students' performance using the Integrated Procedural Protocol Instrument (IPPI) and a checklist. Patients assessed students' performance with the Communication Assessment Tool (CAT). Eight and nine clusters were randomized to the intervention and control groups, respectively. IG completed significantly more single steps of the procedure correctly (IG: 19.36 ± 3.87 for checklist items; CG: 15.57 ± 4.95; p &lt; 0.001). IG also scored significantly better on IPPI ratings (median: IG: 27 (12) vs. CG: 21 (8); p &lt; 0.001). Rated by patients, students' communication skills did not significantly differ between the two groups. Simulation-based venipuncture training enabled students to perform the procedure on actual patients with a higher technical accuracy than students who learned venipuncture at the bedside. Students were able to transfer the skills acquired through venipuncture training at the skill laboratory to the bedside."
    mergedFrom:
      - source: eric
        ericId: EJ1428986
    finalDecision: exclude
  - title: An Investigation of Preclinical Medical Students&apos; Preference for Summative or Formative Assessment for Physiology Learning
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about student assessment preferences in physiology"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus in the title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    ericId: EJ1402470
    authors: Rae M, Abdulla M
    year: "2023"
    abstract: Both summative and formative assessments are known to facilitate student learning and understanding and help students to identify areas of weakness. However, few studies have investigated students' preference for either summative or formative evaluations, particularly in the area of preclinical medicine. The current study addresses this deficit by surveying 137 first-year graduate entry to medicine (GEM) preclinical medical students from 2 consecutive years (2018-2019 and 2019-2020), for their thoughts on the 6 summative (i.e., for a small percentage of marks), proctored and the 5 informal, formative (i.e., no marks available) continuous assessments in physiology that they encountered in semesters 1 and 2, respectively. Our survey revealed that between 75 and 90% of students felt that both evaluation formats were roughly equally useful (i.e., selecting options, &quot;agree&quot; or &quot;strongly agree&quot;) both for providing feedback about their understanding of physiology and for identifying deficits in their physiology knowledge. However, although a significantly larger number of students felt that summative evaluations motivated them to study more than the formative evaluations (P = 0.006), overall, more students favored formative over summative assessments. Notably, however, GEM students from nonbiomedical backgrounds were significantly more in favor of summative assessments than those from either biomedical backgrounds (P = 0.003) or the whole GEM survey cohort (P = 0.01). The implications of these findings will be discussed, with suggestions as to how the student views outlined here might be facilitated within an academic program to maximize both student learning as well as their motivation to study and keep up with taught material.
    mergedFrom:
      - source: eric
        ericId: EJ1402470
    finalDecision: exclude
  - title: "Therapeutic Presence in Music Therapy Education: An Arts-Based Phenomenological Inquiry"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about music therapy education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: ED634424
    authors: Eggerding E
    year: "2023"
    abstract: "Little music therapy research has been published on therapeutic presence (TP). This study explored the inclusion of TP in music therapy education. Participants were eight credentialed music therapists who identified as music therapy educators and/or clinical supervisors. They attended one of two focus groups and were asked to submit a 1-minute excerpt of themselves improvising a reflection of their focus group's discussion. Focus group data revealed four themes: (1) Defining TP; (2) Education and Supervision Responsibilities; (3) TP is Taught Through Experiential Learning; and (4) Learning TP is a Developmental Process. Participants defined TP as simultaneously incorporating both states of being and action. For them, TP was primarily taught in clinical training, including supervision, though is also taught in academic courses through experiential teaching methods. Improvisation data resulted in an original composition, &quot;Therapeutic Presence: Improvisation Collage,&quot; which includes excerpts from all six improvisation recordings. Its elements of tempo, key, dynamics, and instrumentation also reflect the commonalities in the elements among the six improvisations. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED634424
    finalDecision: exclude
  - title: "Professional Master&apos;s Athletic Training Students&apos; Career Influences Part I: Perceptions of Athletic Training"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; career perceptions survey"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: EJ1329044
    authors: Nokes R, Pitney W, Bowman T, Nottingham S
    year: "2022"
    abstract: "Context: As athletic training education transitions to a professional master's degree, understanding the perceptions professional master's athletic training students have of athletic training is important. Objective: To examine second-year professional master's students' perceptions of the athletic training profession, a career in the profession, and identify the factors that influence their perceptions. Design: Convergent mixed methods. Setting: Online surveys and individual phone interviews. Patients or Other Participants: A total of 80 second-year professional master's students (63 female, 13 male, 4 no response, age = 24.63 ± 2.29 years), who were enrolled in the final semester of their program in the spring of 2019, completed the online survey. Ten survey respondents completed the follow-up phone interviews. Data Collection and Analysis: A survey instrument and a semistructured interview guide were developed to answer the research questions. Both were validated externally by 3 independent researchers using a content-validity indexing tool. The data from the survey and interviews were woven together and merged to provide 1 comprehensive results section. Trustworthiness was established using triangulation, member checks, memos, and peer debriefing. Results: Three higher-order themes emerged from the data: (1) perceptions of the athletic training profession: lack of appreciation and awareness for the profession from others, rewarding profession, and dynamic profession; (2) perceptions of a career in athletic training: low pay, long hours and inconsistent schedules, and inability to have work-life balance; (3) factors influencing perceptions: clinical experiences and interactions with athletic trainers. Conclusions: Second-year professional master's athletic training students developed positive and negative perceptions about the profession and a career in the profession during their professional education experiences. The factors identified were professional socializing agents and should be considered when designing clinical education experiences."
    mergedFrom:
      - source: eric
        ericId: EJ1329044
    finalDecision: exclude
  - title: "Teaching by Concordance: Individual versus Team-Based Performance"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About teaching by concordance methodology, no mention of generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: EJ1391386
    authors: Abouzeid E, Sallam M
    year: "2023"
    abstract: "Clinical competence is multi-dimensional and must be acquired by all medical students. Accordingly, a prospective quasi-experimental study was conducted to evaluate the merging of script concordance testing (SCT) and team-based learning (TBL) as a teaching/learning approach for medical students in clinical settings. The study comprised three phases: Phase 1 (preparatory phase) involved students' preparation and preparation of the SCT. In Phase 2 (implementation phase), the individual and team SCT (iSCT and tSCT, respectively) were applied. In Phase 3 (evaluation phase), the score results were compared, and students' feedback was obtained. The SCT/TBL approach improved the clinical reasoning skills of the students in some vignettes, and the tSCT helped those achieving lower marks improve their performance. The approach supported discussions and helped students correct their mistakes and improve their problem-solving and reasoning skills. The SCT/TBL approach improved students' performance, especially that of those achieving lower marks."
    mergedFrom:
      - source: eric
        ericId: EJ1391386
    finalDecision: exclude
  - title: "Body Painting, Ultrasound, Clinical Examination, and Peer-Teaching: A Student-Centered Approach to Enhance Musculoskeletal Anatomy Learning"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About anatomy teaching methods (body painting, ultrasound, peer-teaching), not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus is indicated"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    ericId: EJ1405693
    authors: Alessandro Bilella, Elisabeth Eppler, Karl Link, Luis Filgueira
    year: "2024"
    abstract: The presented course, established 2016 as a compulsory elective for 22nd-year bachelor medical students, aimed to enhance deep learning of upper and lower limb anatomy from a clinical perspective by a maximum of student-centered activities combining hands-on skills training with team-learning. Three cohorts (in total 60 students) participated in this study. Students rotated through body painting, ultrasound, and clinical investigation supervised by faculty or an experienced clinician. Teams of 3-4 students prepared presentations on clinical anatomy and pathological conditions, which by teacher- and peer assessments on average achieved &gt;85% (mean 17.8/20 points ± 1.06). After each activity session, the students reported their learning experience through a reflective diary. Fifty students (83%) evaluated the course by a voluntary anonymous questionnaire combining Likert-type scale and free-text questions to assess, predominantly, perception of course activities and their perceived influence on learning anatomy. Journal reports and questionnaires revealed that the students highly valued the course, and 92% (29 females, 17 males) rated group work satisfying or well-perceived. The highest appreciation achieved ultrasound followed by clinical examination and body painting, which one third proposed to integrate into the regular dissection course. All students recommended the course to their younger peers. This course was feasible to integrate in the pre-existing curriculum. Limiting factors to offer this elective course to more students are availability of clinical teachers, technical equipment, and education rooms. Being student-directed tasks, body painting and reflective diary-writing would be feasible to implement without additional faculty, which we recommend to educators for student engagement activation.
    mergedFrom:
      - source: eric
        ericId: EJ1405693
    finalDecision: exclude
  - title: Teaching Note - Adapting to Online Live Streamed OSCEs
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Teaching note, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    ericId: EJ1430615
    authors: Karen M. Sewell, Katherine Occhiuto, Sarah Tarshis, Alicia Kalmanovich, Sarah Todd
    year: "2024"
    abstract: This teaching note describes how a school of social work adapted Objective Structured Clinical Examinations (OSCE) during the COVID-19 pandemic to an online live-streamed format, providing students the opportunity to engage in this experiential learning activity despite pandemic-related limitations. A step-by-step process for developing online OSCEs is described, highlighting three key areas - planning, training, and implementation - and the significance of developing a community of practice. Reflections from our team on promising practices for conducting a live-stream OSCE, and insights from the perspectives of simulated clients (n=4), and experienced OSCE raters (n=6) are shared. Implications are described to support future teaching as educators are compelled to continuously prepare social work students for the online realities of practice beyond COVID-19.
    mergedFrom:
      - source: eric
        ericId: EJ1430615
    finalDecision: exclude
  - title: Psychometric Properties and Clinical Use of Language Sample Analysis Measures
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about language sample analysis in speech-language pathology"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: ED649039
    authors: Amy Wilder
    year: "2023"
    abstract: "Language sample analysis (LSA) represents a venerated and ecologically valid method for diagnosing, identifying goals, and measuring progress in children with developmental language disorder (DLD). With many LSA measures available, previous research offers limited guidance on which measures should be prioritized based on their robust reliability, validity, and clinical utility. This dissertation examined psychometric properties and clinical use of selected LSA measures within four dominant LSA frameworks: Computerized Language Analysis (CLAN), Systematic Analysis of Language Transcription (SALT), Sampling Utterances and Grammatical Analysis Revised (SUGAR), and finite verb morphology composite (FVMC), using three related individual studies. Study 1 examined the reliability of LSA measures in shorter, 25- and 50-utterance sample cuts from children aged 5-7 years with and without DLD. Study 1 results showed acceptable relative reliability ([rho] [greater than or equal to] 0.85) in 50-utterance samples compared to 100-utterance samples for 22 of 30 LSA measures. Using the same study sample, Study 2 examined the diagnostic accuracy of those 22 LSA measures for identifying cases of DLD using four DLD reference standards, including a standardized omnibus language test, clinical markers, a parent checklist, and clinical identification. Study 2 results showed acceptable diagnostic accuracy (LR+ [greater than or equal to]4 and LR- [less than or equal to]0.4 with at least two of four DLD reference standards) for Developmental sentence scoring (DSS) sentence points for 5-year-olds, the SALT subordination index, omissions and errors, and percent grammatical utterances (PGU) for 6-year-olds, and MLUCLAN and MLUSUGAR for 7-year-olds. Study 3 examined the clinical use of LSA measures using survey data from practicing SLPs in the United States (n = 337). Results indicated 60% of SLPs used LSA in the past year. LSA skill level, training, and serving preschool or elementary school children predicted LSA use, while workplace, caseload, and years of experience were not significant predictors. Most SLPs reported using self-designed LSA protocols (62%), followed by SALT (23%), and SUGAR (12%) protocols. SLPs who did not use LSA reported limited time (74%), limited resources (59%), and limited expertise (41%) as barriers and identified additional training on LSA computer programs (52%) and access to automatic speech recognition programs (49%) as facilitators to their adoption of LSA. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED649039
    finalDecision: exclude
  - title: "The Impact of Healthcare Digitalization on the Medical Education Curricula and Programs: Points of Convergence and Divergence"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About healthcare digitalization impact on curricula broadly, not specifically about generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: EJ1407002
    authors: Aygul A. Khafizova, Almaz M. Galimov, Saida R. Kharisova, Ludmila Y. Grebenshchikova, Roza I. Yagudina, Lyudmila M. Smirnova
    year: "2023"
    abstract: This study analyzes how medical educational programs are evolving to address the digital transformation in healthcare. Surveys, curriculum analysis, and faculty interviews were utilized to examine technology integration. Findings indicate electronic medical records are considered the most significant digital achievement, though only 4% of beginner faculty emphasized telemedicine. Curriculum analysis revealed increasing modifications related to technology, informatics, and data analytics, especially in certain specialties like biochemistry and biophysics. Interviews provided insights into digital integration opportunities and challenges. A key theme was re-evaluating pedagogical strategies as students rely more on technology. Interviewees also envisioned emerging innovations enabling personalized, immersive learning, but cautioned against over-dependence on technology impeding development of clinical skills and humanism. Overall, while core medical fundamentals remain unchanged, curriculum content is adapting to incorporate new competencies like digital literacy. However, strategic integration remains crucial to balance technology's benefits and drawbacks. Challenges like technical difficulties, costs, and over-reliance must be addressed. The outlook is increased technology utilization, with solutions like AI-enabled adaptive learning on the horizon. However, investing in infrastructure and faculty development will be vital. Medical schools must leverage technology to elevate learning while retaining humanistic values. This study provides timely insights into medical education's digital transformation, laying groundwork for further research on optimizing technology integration while upholding the human core of medicine.
    mergedFrom:
      - source: eric
        ericId: EJ1407002
    finalDecision: exclude
  - title: "Suicide Prevention Training for Social Work Students in Rural Practicum Placements: Current Opportunities and Areas for Growth"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About suicide prevention training for social work students, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are not medical or health-professional learners"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: EJ1434564
    authors: Mary Christensen
    year: "2024"
    abstract: Students and practicum instructors in a primarily rural state were surveyed about suicide prevention practice and training opportunities provided to social work trainees during their practicum placements. A cross-sectional, electronic survey was administered to N = 86 students and N = 64 practicum instructors and descriptive statistics were analyzed. Most students (87.1%) and practicum instructors (86.6%) felt that students should have the opportunity to engage with suicidal clients while in practicum. Many students reported having had contact with a suicidal client while in practicum (70.6%) and had engaged in some sort of practice related to suicide prevention (82.6%). The most frequently reported practice behaviors were suicidal inquiry (62.8%), risk assessment (60.7%), disposition planning (51.0%), and safety planning (49.0%). Over half (52.9%) had not used any evidence-based tools for screening and follow-up and only one quarter (25.5%) had provided postvention services. Many students (72.9%) were concerned about iatrogenic risk. Although most practicum instructors felt comfortable addressing suicide prevention in supervision (74.4%), many (64.1%) also reported the desire for additional training. Although students appear to have opportunities to engage in suicide prevention in practicum, there are specific knowledge and practice gaps regarding evidence-based tools, postvention, and the &quot;myth&quot; of iatrogenic risk.
    mergedFrom:
      - source: eric
        ericId: EJ1434564
    finalDecision: exclude
  - title: Simulation Influenced Self-Efficacy in New Graduate Nurses and Their Clinical Practice
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about simulation and self-efficacy in nursing"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus in the title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    ericId: ED624840
    authors: Bierhup A
    year: "2022"
    abstract: "New graduate nurses are faced with complicated and arduous patient circumstances found within the clinical practice setting, which requires competent nursing actions for the best outcomes. Nursing education uses simulation to facilitate clinical experiences, assisting in the development of nursing students' clinical competency and growth in confidence and self-efficacy in their clinical abilities. The purpose of this descriptive, correlational study was to determine the influence that prelicensure nursing simulation experiences had on new graduate nurses' self-efficacy and their self-efficacy in clinical practice. A convenience sample of 222 new registered nurses who had experienced at least one prelicensure nursing simulation experience and graduated within the past year from a prelicensure nursing program volunteered to participate. The instruments to collect data included the General Self-Efficacy Scale and the adapted Self-Efficacy in Clinical Performance scale (Cheraghi et al., 2009; Schwarzer &amp; Jerusalem, 1995). A 4-point Likert scale was used for the General Self-Efficacy Scale, while a 5-point Likert scale was used for the Self-Efficacy in Clinical Performance scale. The greater the agreement with the statement, the higher the score on the scales. The results from this study shows that prelicensure nursing simulation experiences positively impacted both the self-efficacy of new graduate nurses and the self-efficacy in their clinical practice. A Pearson correlation identified a positive, significant relationship (r=0.669, p &lt; 0.01) between the self-efficacy of new graduate nurses and their self-efficacy in clinical practice influenced by their experiences in their prelicensure nursing simulation. The findings of this study found that new graduate nurses who perceive their self-efficacy as higher also have greater self-efficacy in their abilities to provide patient care in part because of their prelicensure nursing simulation experience. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED624840
    finalDecision: exclude
  - title: Exploring the Use of Problem-Based Learning in Clinical Embryology Training
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; PBL in embryology"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: EJ1451504
    authors: Michael Carroll, Ana-Maria Tomova, Helen R. Hunter
    year: "2024"
    abstract: Problem-Based Learning (PBL) is an effective teaching method in many fields, in particular the medical disciplines. Clinical embryology deals with all aspects of assisted conception including insemination and embryo transfer. Clinical embryologists deal with daily issues that require troubleshooting and problem solving. The aim of this study was to explore and share the use of PBL teaching in a clinical embryology training programme. Students were given real-case scenarios and tasked with formulating a solution. A survey of questions to evaluate the PBL session was developed using a 5-point Likert scale. The scores obtained from these tests were assessed and analysed using Mann-Whitney U-tests (p = 0.05). The PBL teaching offered a format for students to develop critical problem-solving skills in a safe environment, which encourages learning through problem solving by creating a usable body of knowledge and clinical skill, which are imperative for clinical practice.
    mergedFrom:
      - source: eric
        ericId: EJ1451504
    finalDecision: exclude
  - title: Does Following an &quot;Excellent&quot; Candidate in the Objective Structured Clinical Examination Affect Your Checklist Score?
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about OSCE scoring bias"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus in the title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    ericId: EJ1424023
    authors: Craig Brown, Mintu Nath, Wendy Watson, Mary Joan Macleod
    year: "2024"
    abstract: "Purpose: The OSCE is regarded as the gold standard of competence assessment in many healthcare programs, however, there are numerous internal and external sources of variation contributing to checklist marks. There is concern amongst organisers that candidates may be unfairly disadvantaged if they follow an &quot;excellent&quot; preceding candidate. This study assessed if average checklist scores differed depending on who a candidate follows accounted for different sources of variation. Design/methodology/approach: This study examined assessment data from final year MBChB OSCEs at the University of Aberdeen and categorised candidates into three levels dependent on examiner awarded global scores of preceding candidates for each station. Data were modelled using a linear mixed model incorporating fixed and random effects. Findings: A total of 349 candidates sat the OSCEs. The predicted mean (95% CI) score for students following an &quot;excellent&quot; candidate was 21.6 (20.6, 22.6), followed &quot;others&quot; was 21.5 (20.5, 22.4) and followed an &quot;unsatisfactory&quot; student was 22.2 (21.1, 23.3). When accounted for individual, examiner and station levels variabilities, students following an &quot;excellent&quot; candidate did not have different mean scores compared to those who followed &quot;other&quot; (p = 0.829) or &quot;unsatisfactory&quot; candidates (p = 0.162), however, students who followed an &quot;unsatisfactory&quot; student scored slightly higher on average compared to those who followed &quot;other&quot; (p = 0.038). Originality/value: There was weak evidence that candidate's checklist variations could be attributed to who they followed, particularly those following unsatisfactory students; the difference in predicted mean scores may be of little practical relevance. Further studies with multiple centres may be warranted assuring perceived fairness of the OSCE to candidates and educators."
    mergedFrom:
      - source: eric
        ericId: EJ1424023
    finalDecision: exclude
  - title: "An Exploratory Case Study: Examining the Design of Clinical Immersion from the Preceptors&apos; Perspective"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; clinical immersion design"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: EJ1409301
    authors: Chyrsten L. Gessel, Stephanie M. Singe, Heidi M. Crocker
    year: "2024"
    abstract: "Context: The clinical immersion experience is a key part of the socialization of the professional athletic training student. Clinical immersion offers the student the chance to experience the totality of the role of the athletic trainer. Programmatic autonomy allows many athletic training educators the opportunity to be creative in the implementation of clinical immersion(s). Objective: Examine clinical immersion from the preceptors' perspective to understand their opinions regarding the structure of the experience and preferences they may have surrounding clinical immersion structure. Design: Qualitative exploratory case study design. Setting: Individual one-on-one video-conference interview. Patients or Other Participants: Eight athletic training preceptors (6 female, 2 male) completed one-on-one interviews and were 38 ± 6 years old with 10 ± 3 years of experience as a preceptor and 14 ± 7 years of experience as a certified athletic trainer. Data Collection and Analysis: Semistructured interviews were conducted with all preceptors. Interviews were recorded and transcribed using a video-conferencing software. An iterative approach with principles of phenomenological research was used to code the data. Data saturation guided recruitment and cessation of new interviews. Basic member checks, reflexivity, and multiple analyst triangulation were used to determine trustworthiness. Results: The following four major themes emerged: (1) preceptors prefer the clinical immersion experience to be longer than the minimum accreditation requirement, (2) preceptors prefer to supervise second-year students in the immersion experience, (3) immersion allows for meaningful relationship development between the preceptor and student, and (4) preceptors treat immersions as a transition to practice mechanism. Conclusions: Preceptors perceived the clinical immersion experience to be a facilitator of strong preceptor-student relationships and a facilitator of transition to practice. They believed that the clinical immersion should be longer than 4 weeks and occur late in the athletic training curriculum."
    mergedFrom:
      - source: eric
        ericId: EJ1409301
    finalDecision: exclude
  - title: Experience of Supervisors in the Field
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Generic title about supervisor experiences, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus is indicated"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    ericId: ED659235
    authors: Kathryn M. Williams
    year: "2024"
    abstract: "In this study, the researcher explored the lived experiences of clinical supervisors working in the field. The supervisors' experiences were collected via interviews as they reflected on their work supervising counselors seeking state licensure. They also responded to a demographic questionnaire. The participants were asked about their interest in supervision, training experience, supervisor development, and supervision competency. The goal of this study was to hear and share the voices and experiences of supervisors in the field, a large and underrepresented population of supervisors. The researcher used Interpretive Phenomenological Analysis to gather, analyze, and synthesize the stories of the six participants. The findings that emerged included four major themes: 1) teaching and learning, 2) relationship dynamics, 3) setting and populations, and 4) emotions. Two of the themes had subthemes. Within the first theme, learning and teaching, there were four subthemes: 1) experience as a supervisee, 2) learning and knowledge, 3) teaching, and 4) growth and development. Within the theme of relationship dynamics, there were three subthemes: 1) emotional support, 2) communication, and 3) collaboration. The limitations of this study included a lack of diversity in both race and gender. A sampling limitation is that the participants were self-selected, which is known to be influenced by interest or knowledge of a topic (Patton, 2014). Social desirability could also be a risk in this study if the participants shared their experiences in a way that they believed would please the researcher. Also, this study focused on the experience of the supervisor, which is only one part of the supervisory experience. The implications of this study have the potential to influence supervision practices, policies related to licensure supervision, supervision research, and the quality and competency of supervision. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED659235
    finalDecision: exclude
  - title: "&quot;Hold Your Breath and Bear Down&quot;: Using Valsalva&apos;s Maneuver to Explore Venous Variations in the Neck"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About clinical anatomy technique (Valsalva maneuver), not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus is indicated"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    ericId: EJ1377806
    authors: Krinke R, Yoachim S
    year: "2022"
    abstract: The remarkable variation in superficial veins of the neck challenges anatomy students during cadaveric exploration. Despite frequently serving as cannulation sites, conduits for catheter placement, grafts following carotid endarterectomy, and a visual representation of central venous pressure, the jugular venous network and its variations are not well elucidated. A lack of health care provider awareness of venous variation may increase risk of clinical complications including ecchymosis, necrosis, and increased intraoperative bleeding. This study integrates an exploration of superficial venous variation in the necks of cadaveric specimens with a simple living anatomy exercise for anatomy students, aimed to shed light on anatomical variation and the typical venous patterns students may encounter in lab and clinically. Cadaveric dissections were completed on twenty four specimens to identify common variations from the textbook venous pattern of the neck and 3D renderings were created to help students better visualize anomalies. Next, students were instructed to perform the Valsalva maneuver to distend venous structures in the neck for visualization; photographs were taken to demonstrate the range of visible structures and patterns in living subjects. Four patterns were identified via cadaveric exploration and illustrated via three-dimensional (3D) renderings, and subject photographs were used to enhance learning by engaging students in the prediction of underlying structures. This activity deepened students' appreciation for anatomical variation and promoted awareness needed for pre-interventional planning.
    mergedFrom:
      - source: eric
        ericId: EJ1377806
    finalDecision: exclude
  - title: "Getting More out of Clinical Documentation: Can Clinical Dashboards Yield Clinically Useful Information?"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About clinical dashboards for documentation, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: EJ1434372
    authors: Charmaine K. Higa-McMillan, Alayna L. Park, Eric L. Daleiden, Kimberly D. Becker, Adam Bernstein, Bruce F. Chorpita
    year: "2024"
    abstract: "This study investigated coded data retrieved from clinical dashboards, which are decision-support tools that include a graphical display of clinical progress and clinical activities. Data were extracted from clinical dashboards representing 256 youth (M age = 11.9) from 128 practitioners who were trained in the Managing and Adapting Practice (MAP) system (Chorpita &amp; Daleiden in BF Chorpita EL Daleiden 2014 Structuring the collaboration of science and service in pursuit of a shared vision. 43(2):323 338. 2014, Chorpita &amp; Daleiden in BF Chorpita EL Daleiden 2018 Coordinated strategic action: Aspiring to wisdom in mental health service systems. 25(4):e12264. 2018) in 55 agencies across 5 regional mental health systems. Practitioners labeled up to 35 fields (i.e., descriptions of clinical activities), with the options of drawing from a controlled vocabulary or writing in a client-specific activity. Practitioners then noted when certain activities occurred during the episode of care. Fields from the extracted data were coded and reliability was assessed for Field Type, Practice Element Type, Target Area, and Audience (e.g., &quot;Caregiver Psychoeducation: Anxiety&quot; would be coded as Field Type = Practice Element; Practice Element Type = Psychoeducation; Target Area = Anxiety; Audience = Caregiver). Coders demonstrated moderate to almost perfect interrater reliability. On average, practitioners recorded two activities per session, and clients had 10 unique activities across all their sessions. Results from multilevel models showed that clinical activity characteristics and sessions accounted for the most variance in the occurrence, recurrence, and co-occurrence of clinical activities, with relatively less variance accounted for by practitioners, clients, and regional systems. Findings are consistent with patterns of practice reported in other studies and suggest that clinical dashboards may be a useful source of clinical information. More generally, the use of a controlled vocabulary for clinical activities appears to increase the retrievability and actionability of healthcare information and thus sets the stage for advancing the utility of clinical documentation."
    mergedFrom:
      - source: eric
        ericId: EJ1434372
    finalDecision: exclude
  - title: Generalism in Clinical Practice and Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on generalism in clinical practice"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    ericId: ED663681
    authors: Sophie Park E, Kay Leedham-Green E
    year: "2024"
    abstract: Generalism is a key approach to healthcare organisation and delivery that enables person-centred, dynamic and cost-effective patient care. With its emphasis on adaptability, generalism requires expansive, nurturing and personalised approaches to clinical education in which a generalist attends to and explores patient priorities when problem-setting and co-creating management plans. &quot;Generalism in Clinical Practice and Education&quot; outlines a generalist philosophy of practice which is brought to life through interleaved examples. Written by a range of international clinicians, patients and academics, this book does not prescribe one 'right' way to do generalism. Rather, it seeks to inspire readers' future engagement with generalism in practice and learning through sharing underpinning concepts, values and principles. This 'big picture' attention to generalism across public health, social determinants of health and clinical care is at the heart of sustainable and efficient use of resources to prioritise those in need. The book explores four key principles which in practice aim to achieve creative, inclusive and agile approaches to clinical care. The goal is to support generalism in clinical practice and education, and to produce clinical practitioners and learners that enjoy, embrace and enhance future clinical care.
    mergedFrom:
      - source: eric
        ericId: ED663681
    finalDecision: exclude
  - title: "Experimental Learning Theory as a Means to Improve Communication during Clinical Experiences in Physical Therapy Education: A Qualitative Case Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About experiential learning theory for communication in PT education, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI/LLM use is indicated in this education theory case study title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    ericId: ED658883
    authors: Nicholas Rodio
    year: "2024"
    abstract: "Communication between Doctor of Physical Therapy (DPT) students and the university's academic faculty during clinical experiences throughout the DPT program can be varied. This may lead to a variety of issues going unnoticed or unreported. The qualitative case study design uses focus group and one-on-one interviews as means of data collection to understand the individual student experiences and how each feel that communication would impact their learning during clinical experiences. The data collected will be transcribed and analyzed using first and second cycle coding for themes. The paradigm associated with this research study will be social constructivism, which deals with seeking an understanding of the world in which we live and work. Gergen describes these individually specific experiences with the idea of multiple realities, which emerge from people's differing experiences. (Gergen et al.1996). This paradigm closely relates to the researcher's past experiences as a DPT student, clinical instructor, and currently a DPT faculty member. The key questions that guided this research are the following: (1) In what ways does the clinical experience enhance the student's education and promote learning through experiential learning theory? (2) What were some common issues or limitations that impacted learning while on clinical experience? (3) What role does communication while on clinical experience play in the&amp;#xa0;students critical thinking and conceptualization clinical concepts? (4) How could communication be improved while on clinical experiences? (5) What impact could technology-based communication (i.e., discussion boards or video chats) have during clinical experiences to improve communication and allow for increased conceptualization of knowledge? (6) In what areas do clinical instructors feel the student's learning experience would benefit from expanded academic faculty communication to promote critical thinking and clinical application of knowledge? [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED658883
    finalDecision: exclude
  - title: "Considerations for Paediatric Student-Led Telepractice in Speech-Language Therapy: A Pilot Observational Study from South Africa"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on telepractice in speech-language therapy"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    ericId: EJ1417734
    authors: Jennifer Watermeyer, Rhona Nattrass, Johanna Beukes, Sonto Madonsela, Megan Scott
    year: "2024"
    abstract: "Background: COVID-19 necessitated emergency telepractice for student-led speech-language therapy clinical practicals in training institutions, with limited preparation and evidence-based guidelines. Beyond the pandemic, practitioners and university training sites are likely to continue to offer telepractice necessitating thorough preparation for telepractice services underpinned by a comprehensive understanding of the complexities involved in online therapy. Aims: Adopting realist evaluation principles, our aim in this paper was to explore broadly what works and does not work in a set of student-led telepractice sessions in a diverse, resource-limited context. The broader goal of this project was to provide evidence-based support to enhance the efficiency and success of telepractice sessions in student clinical training contexts. Methods &amp; Procedures: We used qualitative observational methods with reflexive thematic analysis to analyse 28 video recordings and 61 observation notes of student-led paediatric telepractice sessions from a South African university clinic as part of a pilot study. Outcomes &amp; Results: We identified four overarching considerations for student-led telepractice: (1) additional, specific preparation is required, (2) with greater management of technology and adaptation of tasks, especially during times of poor connectivity; (3) telepractice relies heavily on caregiver input and collaboration; and (4) promoting engagement online, holding a client's attention, building rapport and offering reinforcement are critical skills that are complicated by the lack of face-to-face contact. Conclusions &amp; Implications: Our findings indicate that telepractice pedagogy needs to be explicitly taught and students require practical assistance as they learn how to use this service delivery approach effectively. There are some aspects peculiar to telepractice that require unique consideration and planning, especially in contexts where service providers and users may be unfamiliar with this form of service provision. The findings of this pilot study can be used by clinical educators and student clinicians to enhance clinical training opportunities involving telepractice."
    mergedFrom:
      - source: eric
        ericId: EJ1417734
    finalDecision: exclude
  - title: Evaluation of Georgia State University&apos;s CREATE Teacher Residency Program -- Cohorts 3 through 5. A Final Report of a Quasi-Experiment in Georgia. Research Report
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 6: Teacher residency program evaluation, not medical/health-professional education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are not medical/health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    ericId: ED629891
    authors: Andrew P. Jaciw, Sze-Shun Lau, Jenna Zacamy, Li Lin
    year: "2023"
    abstract: "The Collaboration and Reflection to Enhance Atlanta Teacher Effectiveness (CREATE) program aims to increase teacher effectiveness and retention through fostering spaces that support the development of critically conscious, compassionate, and highly skilled educators. Specifically, CREATE's three-year teacher residency program seeks to recruit, support, and retain a diverse, highly effective, and culturally competent educator workforce that is committed to working in high-needs schools and to improving student outcomes. With funding from the Supporting Effective Educator Development (SEED) grant, this evaluation focused on the effectiveness of CREATE's teacher residency program for three staggered cohorts of pre-service and early career teachers from 2017--18 through 2021--2022. Using a quasi-experimental design, the study compares the experiences and outcomes of (1) CREATE residents and (2) comparison subjects participating in the traditional credentialing pathway, with both groups joining the study when they were enrolled in their final year at Georgia State University's College of Education and Human Development (GSU CEHD). The study followed participants for three years: their field experience/student-teaching year, and their first two years as official teachers of record. Researchers assessed primary impacts after participants completed two years as credentialed teachers. Program implementation and the experiences of early career teachers were measured across all study years. By the end of their second year of teaching, CREATE teachers reported more favorable levels of self-compassion, teacher burnout, and stress management and empathy related to teaching than teachers in the comparison group. The research team did not find a statistically significant impact of CREATE on student achievement in English Language Arts, as measured by the Georgia Milestones assessment in grades 3 through 8. The researchers hypothesize that this may have been impacted by a limited sample size of teachers teaching in the tested grade levels and subject areas. A prior study of CREATE [see ED611803] demonstrated a positive impact of CREATE on uninterrupted graduation and retention of teachers, especially for Black educators, with most of the benefit accrued early through a difference in graduation rates. In the current study, graduation rates were high for CREATE and comparison participants (&gt;90%) and the impact on teacher retention was not replicated. These findings pose questions about the differences in programming, recruitment, and hiring between the years during which each study was conducted, and about how COVID-19 may have affected the dynamics of teacher hiring and retention."
    mergedFrom:
      - source: eric
        ericId: ED629891
    finalDecision: exclude
  - title: "Academic Motivation and Study Habits of Minority Allied Health Students: A Descriptive Qualitative Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on academic motivation and study habits"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: ED645847
    authors: Mia Chrishawn Small
    year: "2024"
    abstract: "The purpose of this qualitative descriptive study was to describe how racial and ethnic minority allied health students at a 4-year college in the Midwestern United States described their general academic motivations and study habits to prepare for clinical examinations. The self-determination theory (SDT) was the conceptual framework for the study. Research questions used to determine how racial and ethnic minority allied health students described their general academic motivation and study habits to prepare for clinical examinations at a 4-year college in the Midwestern United States. Individual interviews and two separate focus groups were used to collect data from twenty racial and ethnic minority allied health students. The researcher thematically analyzed the qualitative data using Braun &amp; Clarke six stages of thematic analysis. Four themes emerged from analyzing the data: (1) social support and connectedness, (2) performance (3) effective study methods and (4) clinical preparation. The theoretical implications of the research were that participants were generally motivated by family, friends, faculty, and staff. Their study habits included the use of technology, hands-on clinical activities, and peer-to-peer learning when preparing for clinical examinations. Based on these findings the researcher recommended additional research exploring the general academic motivation and study habits of racial and minority allied health students preparing for clinical examinations and its linkage to academic performance. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED645847
    finalDecision: exclude
  - title: Student Perceptions of Twitter Poll-based Formative Assessment in a Rural Medical College in India
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about Twitter polls for assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus in the title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    ericId: EJ1408310
    authors: Himel Mondal, Shaikat Mondal, Nirupama Ray
    year: "2023"
    abstract: Formative assessment is vital for student learning and engagement. Social media platforms like Twitter have gained popularity in medical education, but little research has explored student perceptions of formative assessment through Twitter. This study aimed to observe participation rates in Twitter poll-based formative assessment and survey students' perceptions of it in a rural medical college in India. Fifteen formative assessment sessions were conducted using a Twitter poll after physiology lectures, with five multiple-choice questions per session. The number of participants in each poll was recorded. A questionnaire collected student perceptions after the sessions. Across 15 Twitter poll sessions, we received an average of 12.65 ± 5.53 (median 12) responses. Most students (63%) did not participate, with only 4% participating in over 10 sessions. A total of 107 students participated in the survey, resulting in an 85.6% response rate. Students found social media-based formative assessment interesting and effective for increasing attention in class. However, they preferred alternative platforms like Telegram and Instagram for formative assessment. In conclusion, most students in a rural medical college in India do not participate in Twitter poll-based formative assessment. Nevertheless, students found it interesting and effective, highlighting the importance of considering students' platform preferences for implementing social media-based formative assessment.
    mergedFrom:
      - source: eric
        ericId: EJ1408310
    finalDecision: exclude
  - title: Entrustable Professional Activities as a Framework for Assessment in Pharmacy Experiential Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about EPA framework in pharmacy education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: ED659138
    authors: Jennie Jarrett
    year: "2024"
    abstract: "Entrustable professional activities (EPAs) and their corresponding entrustment supervision (ES) scales are an innovative education assessment model to provide authentic assessment and growth of health professions education learners. The overall goal of this work is to gain insight into the utility of an EPA framework for assessment within pharmacy experiential education. The purpose of the research program is to: 1.) determine the growth of the pharmacy students across their experiential curriculum; 2.) evaluation the pharmacy student-perceived growth across their experiential curriculum; 3.) determine differences in growth from the faculty and student perspectives; 4.) describe facilitators and barriers to assessing students utilizing entrustment-supervision scales within an EPA framework. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED659138
    finalDecision: exclude
  - title: How Graduating BSW Students Manage Uncertainty in an Objective Structured Clinical Examination
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about BSW students in OSCE"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are BSW students, not medical/health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    ericId: EJ1367882
    authors: Tufford L, Thieu V, Zhao R
    year: "2022"
    abstract: "Professional uncertainty is a feature of generalist social work practice, particularly for students completing their social work education or those new to the profession. This study saw graduating Bachelor of Social Work students (N = 34) engage in a 15-minute interview with a Standardized Client (SC) in an Objective Structured Clinical Examination on a case vignette with the mother of a transgender person. Following the interview, participants responded to structured reflection questions. Responses revealed that almost half (n = 16) the sample disclosed feelings of uncertainty and lack of knowledge regarding the clinical issue. Thematic analysis of the responses revealed the following themes: 1. Manifestations of Uncertainty; 2. Strategies to Manage Uncertainty; and 3. Navigating Uncertainty in Future Practice. This paper offers suggestions for social work educators to address students' feelings of uncertainty and lack of knowledge in generalist social work practice."
    mergedFrom:
      - source: eric
        ericId: EJ1367882
    finalDecision: exclude
  - title: "360 VR PBL: A New Format of Digital Cases in Clinical Medicine"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: VR-based PBL, not generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses on VR/PBL with no indication of generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: EJ1361498
    authors: Davidsen J, Larsen D, Paulsen L, Rasmussen S
    year: "2022"
    abstract: In this paper, we present and discuss an explorative study on the use of a social 360° virtual reality (360VR) for supporting case-based Problem Based Learning (case-PBL) in clinical medical education. In the context of case-PBL, we argue that our social 360VR learning space extends the design and application of cases in medical education by including elements from project-PBL. Three groups tested the learning design as a part of the clinical exercises in their 5. Semester bachelor course. After the social 360VR activity, the students performed a physical examination of the collateral and cruciate ligaments of the knee like the one in the training material. Our preliminary findings indicate that the students immersed in social 360VR collaboratively establish a mutual understanding of how to perform the examination through identifying problems related to the examination and by taking responsibility for their own and the other group members learning.
    mergedFrom:
      - source: eric
        ericId: EJ1361498
    finalDecision: exclude
  - title: "HandsIn: A Hand Therapy Educational Tool for Occupational Therapy Students"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about a hand therapy educational tool for OT students"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: ED660470
    authors: Nora Kinslow
    year: "2024"
    abstract: "Fieldwork education is a critical component in occupational therapy education, aiding in the transition from classroom learning to practical application. It has been cited recently that therapists have expressed concern regarding student preparedness for fieldwork in a hand therapy setting. Hand therapy is a specialty practice area where historically students with a strong foundation in anatomy and prior exposure to hand therapy practice may seek this type of fieldwork. However, hand therapists have reported a lack of student preparedness as a potential barrier for supervising in that setting (Short et al., 2018). To account for these concerns, &quot;HandsIn&quot; is a program created to assist students in increasing their knowledge, confidence, and level of preparedness prior to a hand therapy fieldwork placement. &quot;HandsIn&quot; is a free, digital, asynchronous supplemental educational program to allow students of any geographic or financial situation to participate. The program itself will consist of five learning modules: 1. Anatomy, 2. Diagnosis, 3. Treatments, 4. Orthotics and 5. Case Studies. The planning and selection of information is assessed via needs assessment to therapists and educators to identify what students are being taught and what is expected of them. &quot;HandsIn&quot; has been developed with the consideration of educators, fieldwork supervisors, and ACOTE standards. Ongoing program evaluation will continue to enhance content and usability to meet entry-level expectations and best clinical practice. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED660470
    finalDecision: exclude
  - title: "Professional Master&apos;s Athletic Training Students&apos; Career Influences Part II: Short-Term and Long-Term Career Intentions"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About athletic training students' career intentions, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: EJ1329046
    authors: Nokes R, Pitney W, Nottingham S, Bowman T
    year: "2022"
    abstract: "Context: As athletic training education continues to evolve to a professional master's degree, understanding the career intentions of students preparing to graduate from professional master's programs is essential. Objective: To examine the career intentions of second-year professional master's athletic training students and identify the factors that influence their intentions. Design: Convergent mixed methods. Setting: Online surveys and individual phone interviews. Patients or Other Participants: A total of 80 second-year professional master's students (63 women, 13 men, 4 no response, age = 24.63 ± 2.29 years) who were enrolled in the final semester of their program in spring 2019 participated in the online survey. Ten survey respondents participated in follow-up phone interviews. Data Collection and Analysis: An online survey was designed and administered to answer the research questions. Semistructured phone interviews were conducted as a follow-up to the survey to gain further insight into the research questions. The data from the survey and interviews were woven together and merged to provide a comprehensive display of results. Triangulation, member checks, memos, and peer debriefing were completed for trustworthiness. Results: Two higher-order themes emerged from the data: career progression and factors influencing career intentions. We identified 3 lower-order themes for career progression: short-term career plans, concerns over lack of experience, and long-term career plans. Two lower-order themes were identified for factors influencing career intentions: clinical experience and mentorship. Conclusions: Second-year professional master's athletic training students have short-term and long-term career plans. Their career intentions were influenced by past clinical experiences and the mentorship received from their clinical preceptors and faculty members during their professional education. Unfortunately, many students were having difficulty obtaining positions after graduation due to a lack of professional experience. As a result, some turned to internship or residency positions to gain experience. [For Part I, see EJ1329044.]"
    mergedFrom:
      - source: eric
        ericId: EJ1329046
    finalDecision: exclude
  - title: "Perspectives of a Mental Health Emergency Standardized Patient Encounter: A Follow-Up Interview of Athletic Trainers"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on standardized patient encounters for athletic trainers"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    ericId: EJ1355741
    authors: Winkelmann Z, Neil E, Granger K, Eberman L
    year: "2022"
    abstract: "Context: Athletic trainers should be able to recognize, refer, and provide support to patients with mental health challenges such as anxiety, depression, and suicidal ideation. However, clinicians may not have the opportunity to identify the signs and symptoms in an authentic scenario until a patient presents in their facility. Therefore, supplemental clinical education via standardized patient (SP) encounters may allow for an opportunity to practice mental health care in a safe manner. However, data to support if a SP encounter has a lasting effect on one's clinical practice are lacking. Objective: To reflect on a SP encounter focused on a mental health emergency (suicidal ideation) and the influence, if any, the encounter had on one's clinical practice 1.5 to 3 years following the encounter. Design: Consensual qualitative research tradition. Setting: One-on-one interview. Patients or Other Participants: Twelve graduates from the same postprofessional athletic training program (men = 6, women = 6) participated in this study. Main Outcome Measure(s): Participants completed a one-on-one interview guided by a 7-question, semistructured interview protocol. A 3-person coding team identified domains and categories. Results: Two domains emerged from the study: (1) learning experience and (2) patient approach. The learning environment created an emotionally realistic space with reassurance and collaborative learning. Specifically, the participants were able to transform their practice from errors made during the SP encounter. The SP encounter had positive, long-term effects on their clinical care including improvements in empathy, active listening, and emergency planning. Conclusions: Like other research about the longitudinal effects of SP encounters, participants applied the learning outcomes from the encounter in their clinical practice 1.5 to 3 years later. We suggest mental health SP encounters be used to prepare athletic trainers, regardless of experience, for emergency mental health crises."
    mergedFrom:
      - source: eric
        ericId: EJ1355741
    finalDecision: exclude
  - title: "Development of an Online OSCE Midwifery Test Package: A Practical Response to Academic Challenges"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About online OSCE development, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus is indicated"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    ericId: ED652244
    authors: Piyanut Xuto, Azadeh Stark
    year: "2023"
    abstract: "To developed an educational tool for an online OSCE to evaluate the practice readiness of the fourth-year nursing students. Online professional proficiency evaluation of nursing students can be a valid alternative to traditional methods. We designed a one group pre-posttest study. The 51 nursing students were recruited. We implemented a 3-stage of online Objective Structured Clinical Examination Midwifery Test Package (OSCEMTP). The package covered seven segments which developed to enhance the cognitive capability, clinical capability, and professional capabilities. The Practice readiness questionnaire was developed to assess the nursing students' readiness. Content validity index was 1, and reliability with Cronbach's alpha was 0.79. OSCEMTP contains 7 segments: 1) contraceptive; 2) antenatal care; 3) admission interview; 4) first stage of labor; 5) second and third stage of labor; 6) fourth stage of labor and 7) breastfeeding. Each segment has a scenario, a competency evaluation, and tools box for demonstration of clinical skills. Practice readiness was improved in the post-test (p&lt;0.001). Performance OSCEMTP should be evaluated across diverse population of nursing students to ascertain its validity. An online OSCE Test package is an education tool to enhance the practice readiness before turning to be a professional nurse. [For the full proceedings, see ED652228.]"
    mergedFrom:
      - source: eric
        ericId: ED652244
    finalDecision: exclude
  - title: "Exploring Spatial Understanding and Cognitive Load Using Ultrasound in Learning Cardiac Anatomy: A Pilot Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about ultrasound use in learning cardiac anatomy"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: EJ1488426
    authors: Johanna Maria de Lange, Karin J. Baatjes, Wouter Willaert, Janine C. Correia
    year: "2025"
    abstract: Although ultrasound (US) appears to complement traditional anatomy teaching, limited objective data exist on its efficacy. Existing literature often relies on student perceptions rather than performance-based outcomes. Additionally, the role of spatial understanding (SU)--the ability to mentally manipulate and interpret 3D anatomical relationships--and cognitive load (CL)--the mental effort required to learn--remains underexplored in the context of US-based instruction. The study consisted of three parts, with assessments before and after the US session. Prior to the session, students completed two paper-based tests on SU and cardiovascular system (CVS) anatomy. During the session, cardiac anatomy was explored through an introduction to US physics, a practical demonstration, and hands-on practice. Post-session, SU and CVS knowledge were reassessed, and participants completed a CL Scale Questionnaire. Thirty-one students participated in the study. Pre- and post-testing of CVS anatomy knowledge showed a mean increase of 11.33% (p &lt; 0.05), while participants' mean SU scores improved from 65.71% to 81.04% (p &lt; 0.05). The highest student rating on the CL Scale was observed when measuring the germane load, specifically the item assessing perceived learning (8.55 ± 1.31), while the lowest rating was reported for measurement of extraneous load, particularly the item assessing distractions (1.23 ± 1.61). This study provided insightful reports on the efficacy of US on SU and CL in anatomy education, showing its potential to improve learning outcomes and prepare students for clinical practice.
    mergedFrom:
      - source: eric
        ericId: EJ1488426
    finalDecision: exclude
  - title: An Action Research Evaluation Using Films in a Workshop for Nursing Clinical Educators
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about using films in nursing education workshops"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: EJ1438849
    authors: Colleen Ryan, Margaret McAllister, Craig Batty, Robert Vanderburg, Jan Cattoni
    year: "2024"
    abstract: "Clinical nurse educators within Queensland, Australia, tend to be selected for the job based on their clinical expertise and consequently may lack knowledge of educational theories and practices. Within the Nursing School in which this project was based, there is interest in developing expertise in Transformative Learning theory. To share this knowledge with clinical educators, a workshop was designed using short films produced by the research team that focused on potentially transformative moments in clinical placement settings that the literature and the researchers' experience had shown to be important. As the project developed iteratively through various stages, action research was chosen as the most suitable methodology to research and document the evaluation of this training workshop, which brought together 20 nursing clinical nurse educators. Interpretive data analysis approaches were employed to understand participants' learning experiences resulting from the workshop, which led to four key themes: 1) first encounters with students need to be engaging and energising; 2) teach students how to bracket personal needs so they can provide professional empathic actions; 3) teach students to value both procedural- and person-centred care; 4) a transformative learning pedagogy is useful for clinical nurse educators to frame and enhance their teaching practice. Learning experiences produced using an action research methodology can stimulate clinical nurse educators to engage with and use relevant educational theory in ways that enable some of the more nuanced and affective dimensions of clinical training."
    mergedFrom:
      - source: eric
        ericId: EJ1438849
    finalDecision: exclude
  - title: Perceptions of Graduate Nurse Practitioner Faculty on the Use of Simulation in Lieu of Traditional Clinical Hours
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on simulation replacing clinical hours"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    ericId: ED633014
    authors: Strong V
    year: "2022"
    abstract: "Simulation in undergraduate nursing curriculum has been identified as an effective and safe substitution for the traditional clinical experience; however, graduate nurse education programs could not substitute clinical hours with simulation-based education. These clinical hours required NP students to provide direct face-to-face patient care. Simulation had wide variability in the number of hours utilized within each specialty. NONPF discussed the use of simulation, as it pertains to the curriculum of the NP student on more than one occasion and concluded its impact on NP education was a potential alternative to traditional clinical experiences. Studies indicated that graduate nursing students enjoy simulation, but few studies showed efficacy of simulation as an alternative to the clinical experience at the graduate level and often lacked rigor. The literature was also insufficient of the perceptions of nurse practitioner faculty on the use of simulation. Results found that graduate faculty viewed simulation favorably yet limitations of existing research included low response rates of faculty, pre-selected options for the questionnaires based on the authors' opinions, and potential contamination of the sample towards favoring simulation. The purpose of this study was to describe the perceptions of graduate nurse practitioner faculty towards the use of simulation in lieu of the traditional clinical experience. It also explored the faculties' beliefs as well as perceived barriers to the implementation of simulation as an alternative to the traditional clinical experience. Lastly, it discovered how the COVID-19 pandemic affected the perceptions of graduate NP faculty on the use of simulation. The sample consisted of ten NP faculty representing all regions of the U.S. and five NONPF specialties, including DNP and online programs. The interviews were conducted via a video platform. Results were manually and computer coded, using NVivo®, until data saturation was met. The results of this study revealed 7 themes: (1) NP faculty have a favorable impression of the use of simulation in NP curriculum, (2) simulation bridges theory to practice, (3) NP education can be taught with simulation, (4) simulation implementation faces a myriad of barriers, (5) NP faculty have a knowledge deficit of simulation pedagogy, (6) COVID-19 affected the perceptions about simulation, and (7) NP faculty do not support the use of simulation in lieu of traditional clinical hours. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED633014
    finalDecision: exclude
  - title: "Oncology Residents&apos; Experiences of Decision-Making in a Clinical Learning Environment: A Phenomenological Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About oncology residents' decision-making experiences, no mention of generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: EJ1403156
    authors: Sanatani M, Muir F
    year: "2023"
    abstract: "Oncology residents routinely engage in ethically complex decision-making discussions with patients, while observing and interacting with their teaching consultant. If clinical competency in oncology decision-making guidance is to be taught deliberately and effectively, it is necessary to understand resident experiences in this context to develop appropriate educational and faculty development initiatives. Four junior and two senior postgraduate oncology residents participated in semi-structured interviews during October and November 2021 which explored their experiences of real-world decision-making scenarios. Van Manen's phenomenology of practice was used in an interpretivist research paradigm. Transcripts were analysed to articulate essential experiential themes, and composite vocative narratives were created. Three essential themes were identified: (1) residents often endorsed different decision-making approaches than supervising consultants, (2) residents experienced inner conflict, and (3) residents struggled to find their own approach to decision-making. Residents experienced being torn between a perceived obligation to defer to consultant directives, and a desire for increasing ownership of decision-making while not feeling empowered to discuss their opinions with the consultants. Residents described their experiences around ethical position awareness during decision-making in a clinical teaching context as challenging, with experiences suggesting moral distress combined with inadequate psychological safety to address ethical conflicts and unresolved questions of decision ownership with supervisors. These results suggest the need for enhanced dialogue and more research to reduce resident distress during oncology decision-making. Future research should be aimed at discovering novel ways in which residents and consultants could interact in a unique clinical learning context including graduated autonomy, a hierarchical gradient, ethical positions, physician values, and sharing of responsibility."
    mergedFrom:
      - source: eric
        ericId: EJ1403156
    finalDecision: exclude
  - title: "Athletic Training Preceptors&apos; Levels of Cultural Competence: A Cross-Sectional Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About cultural competence of athletic training preceptors, no generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: EJ1355739
    authors: León Zaragoza E, Lough N, Samuel M, Girouard T
    year: "2022"
    abstract: "Context: The field of athletic training promotes cultural competence education to prevent negative consequences in health care that can result from a lack of cultural awareness. While prior studies have assessed cultural competence among faculty, students, and athletic trainers, no study has specifically focused on preceptors. Objective: The objective of this study was to assess cultural competence among a subsection of athletic training preceptors. Design and Setting: Cross-sectional online survey. Patients or Other Participants: Active preceptors from Commission on Accreditation of Athletic Training Education (CAATE) accredited programs in good standing from two districts of the National Athletic Trainer's Association. Measurements: The Cultural Competency Assessment (CCA) consists of 27 Likert-type response items split into the Culture Awareness and Sensitivity (CAS) subscale and the Cultural Competence Behaviors (CCB) subscale. Higher scores indicate higher cultural competence. Descriptive statistics were used to analyze the data. Results: Sixty-five preceptors of the estimated 260 were included in the analysis. The combined average score on the CCA was 89.72 ± 11.46 out of 135 (67.9%). A paired-samples t- test revealed that preceptors scored significantly better in the CAS compared to the CCB [t(64) = 12.47, P &lt; 0.001], with averages of 37.27 ± 3.47 out of 55 (67.77%) and 52.44 ± 10.08 out of 80 (65.5%), respectively. Conclusions: This study provides data on athletic training preceptor's awareness and behaviors relative to cultural competence. Results demonstrate a need to further develop cultural competence among preceptors. The role of a preceptor has been shown to have a great influence on student development and retention in the profession. Preceptors should be considered as another source of knowledge of cultural competence."
    mergedFrom:
      - source: eric
        ericId: EJ1355739
    finalDecision: exclude
  - title: Real-Life Scenario Blended Teaching Approach for Nurturing Inquisitive Learning of Central Nervous System in Medical Students
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; blended teaching approach"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: EJ1381559
    authors: Narayanan S, Merghani T
    year: "2023"
    abstract: Among the various systems taught in the preclinical phases, the nervous system is more challenging to learn than other systems. In this report, a novel teaching methodology, &quot;real-life scenario (RLS) blended teaching,&quot; is described and its effectiveness in facilitating inquisitive learning in undergraduate medical students is evaluated. This mixed-method study was conducted among three groups (&quot;group 1,&quot; n = 83; &quot;group 2,&quot; n = 85; and &quot;group 3;&quot; n = 79) of undergraduate medical students (18--20 yr) in the neurology and behavioral sciences module. RLS was presented to students in the form of demonstrations, role-plays, videos, and group activities. &quot;Group 1&quot; students underwent traditional teaching-learning sessions. &quot;Group 2&quot; students underwent RLS blended sessions and were provided with multiple miniassignments in a vignette format. &quot;Group 3&quot; students received RLS blended sessions, multiple miniassignments, peer discussions, multiple formative assessments, and facilitator feedback sessions. The student performances on different exams were compared in terms of their group, and their perceptions of RLS were documented. Students exposed to RLS sessions blended with multiple assignments, peer discussions, multiple formative assignments, and facilitator feedback sessions performed well in the final summative assessments (67.87%) compared with those exposed to RLS sessions and assignments (50.21%) or exposed to traditional teaching alone (50.34%). RLS sessions increased students' curiosity and motivated them to learn the subject well. RLS sessions stimulated student interest and facilitated their learning. RLS along with effective use of multiple assignments, formative assessments, and/or feedback sessions significantly improved student learning. This demonstrates the effectiveness of this active method in teaching various subjects with appropriate modifications.
    mergedFrom:
      - source: eric
        ericId: EJ1381559
    finalDecision: exclude
  - title: "Alignment of Learning Objectives, Assessments, and Active Learning to Promote Critical Thinking in a First-Year Medical Physiology Course: Lessons Learned"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about course design and alignment of learning objectives"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: EJ1429053
    authors: Lisa Carney Anderson, Carolina Fernandez-Branson
    year: "2024"
    abstract: Medical students must be adept at critical thinking to successfully meet the learning objectives of their preclinical coursework. To encourage student success on assessments, the course director of a first-year medical physiology course emphasized the use of learning objectives that were explicitly aligned with formative assessments in class. The course director introduced the physiology discipline, learning objectives, and evidence-based methods of studying to students on the first day of class. Thereafter, class sessions started with a review of the learning objectives for that session and included active learning opportunities such as retrieval practice. The instructor provided short answer formative assessments aligned with the learning objectives, intended to help the students apply and integrate the concepts. Midsemester, students received a link to an online survey with questions on studying habits, class attendance, and student engagement. After finals, students were invited to participate in focus groups about their class experience. A qualitative researcher moderated focus groups, recorded responses, and analyzed the narrative data. Of 175 students, 95 submitted anonymous online surveys. Student engagement was significantly correlated with in-person class attendance (r = 0.26, T = 2.5, P = 0.01) and the completion of open-ended formative assessments (r = 0.33, T = 3.3, P = 0.001). Focus groups were held via videoconference. From the class, 14 students participated in 4 focus groups; focus group participants were mostly women (11 of 14) and mostly in-class attendees (13 of 14). The students in this sample valued critical thinking but misunderstood expectations on exams and few students used learning objectives to study.
    mergedFrom:
      - source: eric
        ericId: EJ1429053
    finalDecision: exclude
  - title: Using an Accelerated Undergraduate Needs Finding Course to Build Skills, Inspire Confidence, and Promote Interest in Health Technology Innovation
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About health technology innovation course, no mention of generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: EJ1430493
    authors: Lyn Denend, Susie Spielman, Ross Venook, Ravinder D. Pamnani, David Camarillo, James Wall, Joseph Towles
    year: "2023"
    abstract: Many undergraduate educational experiences in biomedical design lack clinical immersion-based needs finding training for students. Convinced of the merits of this type of training for undergraduates, but unable to offer a quarter-long course due to faculty and administrative constraints, we developed an accelerated block-plan course, during which students were dedicated solely to our class for 3 weeks. The course focused on the earliest stages of the health technology innovation process--conducting effective clinical observations and performing comprehensive need research and screening. We grounded the course in experiential learning theory (with hands-on, collaborative, and immersive experiences) and constructivist learning theory (where students integrated prior knowledge with new material on need-driven innovation). This paper describes the design of this intensive block-plan course and the teaching methods intended to support the achievement of five learning objectives. We used pre- and post-course surveys to gather self-reported data about the effect of the course on student learning. Despite the accelerated format, we saw statistically significant gains for all but one sub-measure across the learning objectives. Our experience supports key benefits of the block-plan model, and the results indicate that specific course design choices were effective in achieving positive learning outcomes. These design decisions include (1) opportunities for students to practice observations before entering the clinical setting; (2) a framework for the curriculum that reinforced important concepts iteratively throughout the program; (3) balanced coverage of preparation, clinical immersion, and need research; (4) extensive faculty and peer coaching; and (5) providing hands-on prototyping opportunities while staying focused on need characterization rather than solution development. Based on our experience, we expect that this model is replicable across institutions with limited bandwidth to support clinical immersion opportunities.
    mergedFrom:
      - source: eric
        ericId: EJ1430493
    finalDecision: exclude
  - title: Virtual Gaming Simulation and the Development of Nursing Students&apos; Clinical Judgment Skills
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About virtual gaming simulation, not generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM is indicated"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    ericId: ED647027
    authors: Sherry Jean Wright
    year: "2022"
    abstract: "The purpose of this qualitative descriptive study is to explore how community college nursing faculty with experience of virtual gaming simulation (VGS) from a community college in north central Texas describe the challenges and benefits of utilizing VGS as a teaching tool to help nursing students progress in the development of clinical judgment skills. Benner's nursing theory and Tanner's clinical judgment model were used to guide the following research questions: How do nursing faculty describe the challenges of utilizing VGS as a teaching tool to help nursing students progress in the development of clinical judgment skills? How do nursing faculty describe the benefits of utilizing VGS as a teaching tool to help nursing students progress in the development of clinical judgment skills? Snowball sampling yielded 13 study participants. Via thematic analysis, seven themes emerged: (1) Nursing faculty experience frustration with technology when utilizing VGS to teach clinical judgment skills. (2) Nursing students experience frustration with technology when utilizing VGS to learn clinical judgment skills. (3) VGS does not offer the same level of experience for development of clinical judgment as the skills lab. (4) Novice nursing students are able to progress in expertise using VGS. (5) Nursing standards and guidelines are addressed and maintained in VGS programs. (6) Required clinical hours are made available through VGS despite the lack of clinical hour availability elsewhere. (7) One-on-one faculty hours are not required for VGS versus the skills lab and nursing facilities. In conclusion, nursing faculty agreed that VGS was a valuable teaching tool. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED647027
    finalDecision: exclude
  - title: Student Response to Varied Instructional Methods in Level I Fieldwork Experiences
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about instructional methods in fieldwork"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus in the title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    ericId: ED638837
    authors: Camille Turner
    year: "2023"
    abstract: "This applied dissertation was designed to investigate student responses to three different instructional strategies used to fulfill level I fieldwork requirements: traditional in-clinic rotation, a combination of virtual simulation and traditional in-clinic rotations, and virtual simulation. A multi-decade occupational therapy (OT) fieldwork shortage was exacerbated by the COVID-19 pandemic, in which OT education programs struggled to provide accreditation-required clinical education. Virtual simulation programs were utilized to fulfill some requirements, particularly more introductory-level fieldwork. Several cohorts of a Masters of Science in Occupational Therapy &quot;bridge&quot; program for licensed, practicing Certified Occupational Therapy Assistants and Physical Therapist Assistants were the subjects of this study. Very little research exists about this group of occupational therapy students. Students who graduated prior to the pandemic fulfilled level I fieldwork requirements with brief (4-day) traditional in-clinic fieldwork rotations. Students in the program in 2020 completed two brief traditional in-clinic rotations and one virtual simulation rotation. Students in the program from 2020 on completed only virtual simulation to fulfill the level I fieldwork requirement. This study utilized National Board for Certification in Occupational Therapy (NBCOT) first time pass rate exam data and Fieldwork Performance Evaluation scores for each cohort between 2016-2022 as outcome measures. Using a situated learning theoretical framework to design the study, the researcher hypothesized the null: there would be no statistically significant differences of the NBCOT exam score means or in the Fieldwork Performance Evaluation Form (FWPE) scores between groups that experienced the three instructional methods used to fulfill level I graduation requirements: Virtual Simulation, (very brief) traditional in-clinic fieldwork rotations, or a combination of both traditional in-clinic rotation and virtual simulation. This study also investigated potential associations between the amount of virtual simulation a student experienced and FWPE scores as well as NBCOT first-time pass rates. This study will provide more information about instructional methods for fulfilling level I requirements that may be especially supportive of &quot;bridge&quot; students' learning needs while taking into account their work experience and the additional life demands of this population of nontraditional adult learners. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED638837
    finalDecision: exclude
  - title: "Language Matters: LENA Technology in Research and Practice"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: LENA is language environment analysis technology, not generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus is indicated"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    ericId: ED648811
    authors: Sarah M. Avendano
    year: "2022"
    abstract: "Early Expressive language exposure is associated with child language acquisition and advantageous long-term developmental outcomes. The measurement of expressive language in a child's immediate environment is critical to the early identification of children who are at risk of low expressive language exposure, such as children with language delays or disabilities including Autism Spectrum Disorder (ASD). A team of researchers, educators, and engineers developed Language ENvironmental Analysis™ (LENA), a computerized language processing software, to ease the language sampling process. Recently, language acquisition researchers have proposed that LENA technology has greater application in clinical practice to collect naturalistic language acquisition data to inform educational services. Therefore, the present dissertation investigated the use of LENA technology in applied research, uses LENA technology to take naturalistic repeated measures in a clinical setting, and makes suggestions for clinical research (see Chapter 2, 3, and 4). Chapter 2 was a systematic literature review of the use of LENA technology in applied research to identify information on study methodology, LENA technology implementation, and the use of novel variables derived from LENA technology in applied research. The results showed that researchers are increasingly using LENA technology with more diverse populations and using more diverse LENA measures to measure the language environment. Specifically, the number of different languages, ethnicities, and abilities have increased in research using LENA technology to evaluate language environments. However future research needs to include better descriptions of procedural information, which is critical for an emerging body of research.Chapter 3 depicted the expressive language trajectories of preschool-aged children with ASD who are receiving Early Intensive Behavioral Intervention (EIBI). Overall, the present investigation demonstrates that minimally verbal pre-school aged children with ASD demonstrate growth in expressive language trajectory in EIBI and that EIBI may be equally beneficial, in terms of expressive language growth, for children of all expressive language levels. The authors observed a waxing and waning of child language growth over the course of a year, consistent with prior research that highlights the importance of measuring individual child expressive language trajectories in research.Finally, Chapter 4 introduced LENA technology as a behavioral observation measure to behavior analytic practitioners to improve the language environment of young children with ASD. Discussions of what LENA programs are available for behavior analytic use with young children with ASD, how practitioners can use LENA technology in behavior analytic services, and considerations for practical use of LENA in behavior analytic services. The authors provided examples throughout the paper to illustrate the use of LENA technology in behavior analytic practice to improve the language environments of young children with ASD. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED648811
    finalDecision: exclude
  - title: Athletic Trainers&apos; Perceptions of the Structure and Barriers to Online Continuing Education Clinical Documentation Modules
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about online continuing education modules for athletic trainers"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: EJ1437787
    authors: Sara L. Nottingham, Tricia M. Kasamatsu, Julie M. Cavallario, Cailee E. Welch Bacon
    year: "2024"
    abstract: "Context: Athletic trainers (ATs) appreciate the accessibility of web-based continuing education (CE) opportunities. ATs describe needing more CE opportunities related to clinical documentation, but the effectiveness of CE in this content area has not been studied. Objective: Obtain ATs' perceptions of their experiences accessing web-based CE specific to clinical documentation. Design: Qualitative study. Setting: Web-based audio interviews. Patients or Other Participants: Twenty-nine ATs, averaging 36.2 ± 9.0 years of age, including 16 women and 13 men who represented 8 different clinical practice settings. Data Collection and Analysis: Participants previously completed 1 of 2 web-based CE modules specific to clinical documentation, a personalized learning pathway (PLP) or passive reading list (PAS). After completing the modules, 1 investigator interviewed each participant regarding their experiences accessing and completing the modules. Following the Consensual Qualitative Research approach, 4 investigators (3 primary coders and 1 internal auditor) inductively analyzed the interview transcripts. We completed 5 rounds of consensus coding and finalized themes and supporting categories. Results: We identified 2 themes--(1) structure and (2) barriers to completing the modules. Although both groups were satisfied with the content of the modules, PLP participants were more satisfied with the structure and experience of accessing the resources than the PAS participants. PLP participants enjoyed the engagement of the variety of resources included, whereas PAS participants inconsistently accessed reading materials based on personal preference. Both groups described time as a barrier to completing the modules, and some participants experienced technology barriers. Conclusions: ATs value web-based CE opportunities that compile information related to a topic. Learners find a variety of learning formats, including videos, case studies, knowledge checks, and reflection prompts, to be more engaging than only reading materials. Although time continues to be a barrier to completing CE, offering accessible and engaging learning opportunities improves satisfaction with completing CE."
    mergedFrom:
      - source: eric
        ericId: EJ1437787
    finalDecision: exclude
  - title: "Developing an Ethical Framework for Informed Consent Using Human Fetal and Embryological Collections: An Australian Perspective"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About ethical frameworks for informed consent in anatomical collections, no generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: EJ1459930
    authors: Joyce El-Haddad, Nalini Pather
    year: "2025"
    abstract: "The term &quot;human fetal and embryological collections&quot; refers to repositories or archives that house remains of human fetuses and embryos. Most of these remains have been obtained without informed consent from the next of kin, thus reflecting a time in history where this may have been acceptable. Previous quantitative studies seeking stakeholder perceptions toward these collections suggest that there is misalignment with the values of society today, and the current guiding frameworks pertaining to these collections. The aim of this study was to qualitatively explore and analyze the perceptions of key stakeholders regarding human fetal and embryological collections. Twenty-seven individuals signed up and completed the demographic survey. However, 25 attended the focus group interviews. The study sought to provide an in-depth exploration of how stakeholders perceive the value of fetal and embryological collections, and the importance of informed consent. The mean age of participants was 29.1 years of age with a gender distribution of 40.7% (n = 11 out of 27) men and 50.3% (n = 16 out of 27) of women. Thematic analysis identified four themes: (1) consent; (2) preparation for clinical practice; (3) equity and fairness; and (4) educational value, with several subthemes identified at macro, meso, and micro ethical levels. Macro subthemes included importance of informed consent, equity and fairness, and genetic composition. Meso subthemes included respect and privacy, and legal and institutional considerations. Micro subthemes included emotional considerations, preparation for clinical practice, and educational value. The results of this study indicate that ethical issues surrounding human fetal and embryological collections need careful consideration based on macro, meso, and micro ethical frameworks."
    mergedFrom:
      - source: eric
        ericId: EJ1459930
    finalDecision: exclude
  - title: Replacing the Pediatric Traditional Clinical Experience with High-Fidelity Simulation in an Associate Degree Nursing Program
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about high-fidelity simulation in nursing"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus in the title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    ericId: ED620862
    authors: Roop J
    year: "2022"
    abstract: "The lack of access for the traditional clinical experience for nursing students, especially those in an Associate Degree nursing program, and the projected shortage of nursing faculty to educate nursing students in the clinical setting, fosters the utilization of different educational methodologies. The purpose of this action research study was to compare and assess the traditional clinical experience with high-fidelity simulation and evaluate the effectiveness of high-fidelity simulation as a replacement for the traditional clinical experience. The National League for Nursing (NLN) recognizes simulation as a teaching methodology in nursing education. Each State Board of Nursing (BON) contends that the use of simulation in replacement of the traditional clinical experience promotes enhanced learning and alleviates the limitation to traditional clinical placement. Jeffries (2005) simulation framework is the most notable nursing theory developed in simulation-based education. Simulation has transcended to the virtual world. Virtual Simulation (vSim) for Nursing offers online patient simulation scenarios (Jeffries, 2016). The Clinical Learning Environment Comparison Survey 2.0© (CLECS 2.0) was utilized to compare the traditional clinical experience with high-fidelity simulation and virtual simulation, evaluating how effectively the three environments meet the nursing students learning needs (Leighton et al., 2020). [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED620862
    finalDecision: exclude
  - title: Formative Assessments Predict Performance in Anatomy Course and Can Be Used to Identify At-Risk Students Preemptively
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About formative assessments in anatomy course predicting performance, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: EJ1475984
    authors: Gurvinder Kaur, Stephanie Stroever, Megh Gore, Bridget Vories, Vaughan H. Lee, Keith N. Bishop, Brandt L. Schneider
    year: "2025"
    abstract: "Background: Formative assessments build a positive learning environment and provide feedback to enhance learning. This study examined the impact of online formative and low-stake summative assessments on medical students' learning outcomes in the Clinically Oriented Anatomy course from 2016 to 2020. We aimed to demonstrate that formative assessments clarify exam expectations, decrease test anxiety, and facilitate the early identification of academically at-risk students, enabling timely academic counseling and guidance. Methods: Students completed seven formative and nine low-stake summative assessments. Results: Strong correlations were observed between formative and summative exams performance (Spearman's rho (r[subscript s]) = 0.60-0.72) and between summative exams and NBME performance (r[subscript s] = 0.70-0.77). A multivariable model identified students scoring below 75% on the unit 1 summative exam as academically at-risk. While this model effectively identified students who were not at risk, at-risk students consistently underperformed on early assessments compared to those who achieved high pass or honors grades. Over 90% of students reported that formative assessments clarified exam expectations, reduced test anxiety, and improved exam performance. Conclusions: Over 4 years, formative assessments enhanced learning outcomes, clarified examination expectations, and reduced student stress. Online formative assessments are valuable for the early identification of at-risk students, allowing timely interventions, such as academic counselling and remediation, to improve student learning experience."
    mergedFrom:
      - source: eric
        ericId: EJ1475984
    finalDecision: exclude
  - title: "The Development and Content Validity of an Assessment Instrument in Simulation for Advanced Practice Provider Fellowship Programs: An Exploratory Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Simulation assessment instrument development, no mention of generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: ED660177
    authors: Olivia Nicastro
    year: "2024"
    abstract: "Advanced practice provider (APP) fellowship programs are comprehensive programs designed for physician assistants (PAs) and nurse practitioners (NPs) to acquire the necessary knowledge and skills to deliver safe, high-quality care in their specialized areas of practice (Advanced Practice Provider Fellowship Accreditation TM, 2023). An APP fellowship program's mission is to guide the APP fellow to an expected level of confidence in meeting core competencies within a specific specialty (Klimpl et al., 2019; Reville &amp; Foxwell, 2021). The assessment of competence is often a complex process. Simulation has become a learning modality across the globe in medical, nursing, and pharmacy in undergraduate, graduate, and post-graduate training to manage the most complex clinical situations (Padilha et al., 2018; Seybert et al., 2019; Weersink et al., 2019). As APP fellowship programs germinate, there is a growing need to provide a valid instrument to measure competence within simulation. Advanced practice provider fellowship programs utilize structured simulation as an assessment methodology of clinical competence. Faculty of these programs must have access to a valid instrument to provide both formative and summative assessments to APP fellows. The International Nursing Association for Clinical and Simulation Learning (2021) provided standards of best practice for patient simulation training including evaluation of learning and performance with the utility of a valid, reliable instrument. The purpose of this exploratory study was to design, develop, and assess the content validity index of a modified Advanced Practice Provider Queen's Simulation Assessment Tool (APP-QSAT) based on best evidence and resources available for the assessment of a simulation activity for APPs within a fellowship program. This research study had the opportunity to review data from nine content experts within the field of simulation. The exploratory nature was appropriate due to no published instrument that specifically addressed the population of APPs within an acute care setting could be discovered. The APP-QSAT consists of the following domains: primary assessment, diagnostic actions, therapeutic actions, communication, and overall performance. Thirty items were analyzed by nine content experts on a 4-point Likert scale from 1--&quot;Not at all relevant/clear&quot; to 4--&quot;Very relevant/clear&quot; on the content validity index tool. After two rounds of content validation with the nine content experts, the APP-QSAT demonstrated having excellent content validity with an item-level content validity index of 1.00. A modified kappa of 1.00 was also analyzed to exclude chance agreement among the content experts. The APP-QSAT is a modifiable template designed to measure competence for APPs in acute care settings, particularly within APP fellowships. The APP-QSAT aims to capture the full spectrum of clinical competence including not only technical skills but also critical thinking, decision-making, communication, and interprofessional collaboration. Limitations of the study reflected the lack of reliability studies performed on the APP-QSAT and was a small sample population of content experts. Despite the limitations, this study provided a valid instrument to provide formative and summative assessments for learners. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED660177
    finalDecision: exclude
  - title: "Exploring the Relationships between First Impressions and MMI Ratings: A Pilot Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About MMI ratings and first impressions, no mention of generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: EJ1376631
    authors: Klusmann D, Knorr M, Hampe W
    year: "2023"
    abstract: The phenomenon of first impression is well researched in social psychology, but less so in the study of OSCEs and the multiple mini interview (MMI). To explore its bearing on the MMI method we included a rating of first impression in the MMI for student selection executed 2012 at the University Medical Center Hamburg-Eppendorf, Germany (196 applicants, 26 pairs of raters) and analyzed how it was related to MMI performance ratings made by (a) the same rater, and (b) a different rater. First impression was assessed immediately after an applicant entered the test room. Each MMI-task took 5 min and was rated subsequently. Internal consistency was [alpha] = 0.71 for first impression and [alpha] = 0.69 for MMI performance. First impression and MMI performance correlated by r = 0.49. Both measures weakly predicted performance in two OSCEs for communication skills, assessed 18 months later. MMI performance did not increment prediction above the contribution of first impression and vice versa. Prediction was independent of whether or not the rater who rated first impression also rated MMI performance. The correlation between first impression and MMI-performance is in line with the results of corresponding social psychological studies, showing that judgements based on minimal information moderately predict behavioral measures. It is also in accordance with the notion that raters often blend their specific assessment task outlined in MMI-instructions with the self-imposed question of whether a candidate would fit the role of a medical doctor.
    mergedFrom:
      - source: eric
        ericId: EJ1376631
    finalDecision: exclude
  - title: An Experiential Learning Course for Cardiovascular and Sleep Technology
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on experiential learning course design"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    ericId: EJ1369562
    authors: Durocher J, Toorongian C, Thivierge G
    year: "2022"
    abstract: "Undergraduate programs related to allied health are often pursued by students to prepare for entrance into professional programs, but many students also ask about what types of employment options they may have upon completion of the bachelor's degree. This experiential learning course in cardiovascular and sleep technology was designed to provide opportunities for students to gain hands-on clinical skills that could help them to enter professional programs or to find employment in an allied health field such as cardiopulmonary rehabilitation, cardiovascular technology, or sleep technology. The prerequisites for this course were the completion of two semesters of anatomy and physiology. This course was based on experiential learning and was structured into three modules: clinical exercise physiology, autonomic physiology, and sleep physiology. As part of the course students completed a 4-page scientific report for one designated topic within each of the modules. The two exams in the course were essay based. The design of this course required students to review key topics from anatomy and physiology, to comprehend peer-reviewed manuscripts, to gain hands-on experiences with sophisticated physiological equipment, to work as individuals and in groups, and to become better oral and written communicators. The sleep physiology module included an introduction to electroencephalography (EEG) and a student-led nap study, which may be an effective way to introduce students to sleep medicine. We are hopeful that the summary of this course will be useful to physiology educators as they work to provide the most meaningful experiences to their undergraduates in the health sciences."
    mergedFrom:
      - source: eric
        ericId: EJ1369562
    finalDecision: exclude
  - title: "Factors Influencing Speech Pathology Practice in Dysphagia after Stroke: A Qualitative Focus Group Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About speech pathology clinical practice, not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus is indicated"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    ericId: EJ1431160
    authors: Jacinda Choy, Fereshteh Pourkazemi, Hans Bogaardt, Caitlin Anderson, Shing Yee Chai, Roxanna N. Pebdani
    year: "2024"
    abstract: "Background: Dysphagia affects over half of adults after stroke. Dysphagia rehabilitation aims to improve swallowing and reduce negative outcomes for these adults. However, significant variability exists in dysphagia rehabilitation. Research is needed to explore the underlying clinician-specific and workplace factors that contribute to variability in dysphagia rehabilitation after stroke. Aim: To explore factors influencing speech pathology practice in dysphagia rehabilitation after stroke. Methods &amp; Procedures: We used a phenomenological approach with an interpretivist perspective. Twenty speech pathologists working in dysphagia rehabilitation participated from different workplace settings around Australia. Five semi-structured focus groups were conducted online. Data were inductively analysed using thematic analysis with a coding reliability method. Outcomes &amp; Results: Four themes were discussed within focus groups: (1) relationship between experienced and less experienced clinicians: 'Following what other people have done', (2) need for collaborative learning: 'A safe space to share and train', (3) variation between settings impacts on continuity of care: 'There's a difference between community and acute', and (4) working effectively with multidisciplinary teams (MDT): 'An MDT which can listen to the voice of speech pathology'. Conclusions &amp; Implications: Relationships between senior and junior speech pathologists, within speech pathology and MDT, and across inpatient and community settings influenced speech pathology practice. Flattened hierarchies in speech pathology, collaborative learning in workplaces, mutual respect within teams and connection across inpatient and community settings could improve the quality and consistency of dysphagia rehabilitation after stroke."
    mergedFrom:
      - source: eric
        ericId: EJ1431160
    finalDecision: exclude
  - title: "Simulation-Based Education in Classroom and Clinical Settings in Sub-Saharan Africa: A Systematic Review"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About simulation-based education, not generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    ericId: EJ1399989
    authors: Msosa A, Msiska M, Mapulanga P, Mtambo J, Mwalabu G
    year: "2023"
    abstract: "Purpose: The purpose of this systematic review was to explore the benefits and challenges in the implementation of simulation-based education (SBE) in the classroom and clinical settings in sub-Saharan Africa. The objectives of this systematic review were to identify the benefits of utilising SBE in the classroom and clinical practice in sub-Saharan Africa and to assess the challenges in the implementation of SBE in the classroom and clinical practice in sub-Saharan Africa. Design/methodology/approach: Five databases were searched for existing English literature (Medline, CINAHL and Science Direct), including grey literature on the subject. Out of 26 eligible studies conducted in sub-Saharan Africa between 2014 and 2021, six studies that used mixed-methods design were included. Hawker &quot;et al.&quot;'s framework was used to assess the quality of the studies. Quantitative data were presented using descriptive and inferential statistics in the form of means and standard deviations while qualitative data were analysed and presented thematically. Findings: Quantitative findings showed that participants rated SBE highly in terms of teaching (93.2%), learning (91.4%) and skill acquisition (88.6%). SBE improved the clinical skill competency from 30% at baseline to 75% at the end. On the other hand, qualitative findings yielded themes namely: improved confidence and competence; knowledge acquisition and critical thinking; motivation and supervision; independent, self-paced learning; simulation equipment and work schedules; and planning and delivery of simulation activity. Pedagogical skills, competence and confidence are some of the elements that determine the feasibility of implementing SBE in the classroom and clinical settings. Practical implications: SBE could help to bridge the gap between theory and practice and improve the quality of care provided by nurses. Simulation-based training is effective in improving the clinical skills of midwives and increasing their confidence in providing care. However, SBE trainees require motivation and close supervision in classroom settings if simulation is to be successfully implemented in sub-Saharan Africa. Furthermore, careful planning of scenarios, students briefing and reading of content prior to implementation facilitate effective simulation. Originality/value: While there may be a lack of literature on the use of SBE for training nurses and midwives in the developing world, there is growing evidence that it can be an effective way to improve clinical skills and quality of care. However, there are also significant challenges to implementing simulation-based training in resource-limited settings, and more research is needed to understand how best to address these challenges. This study fills this gap in the literature."
    mergedFrom:
      - source: eric
        ericId: EJ1399989
    finalDecision: exclude
  - title: "Assessing Constructs Underpinning Clinical Educator Engagement in Physiotherapy (Part A): A Scoping Review of Available Instruments"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About clinical educator engagement instruments, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are not medical or health-professional learners"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: EJ1492153
    authors: Joanna Hargreaves, Clare Pekin, Deanne Gannaway, Shaun O’Leary
    year: "2025"
    abstract: "Educator identity, motivation, and self-efficacy have been shown to positively impact clinical educator performance and perseverance in work-integrated learning contexts. Little is known about the impact of these constructs in the physiotherapy profession. A scoping review was conducted to identify instrument(s) to assess these constructs in physiotherapy clinical educators. Two reviewers independently screened literature published between 2002-2022. The quality of instrument measurement properties was appraised using ten COSMIN Risk of Bias criteria. The search yielded 4,976 records, of which 58 met inclusion criteria. Sixteen instruments were appraised as 'sufficient' on [greater than or equal to]8 COSMIN Criteria for good measurement quality. No instrument was identified that assessed all three constructs in healthcare educators, nor that could be directly applied to physiotherapy. The need for a purpose-built instrument to evaluate educator identity, motivation, self-efficacy, as well as operational factors in physiotherapists was indicated. [Note: The page range (639-664) shown on the PDF footers is incorrect. The correct page range is 639-665.]"
    mergedFrom:
      - source: eric
        ericId: EJ1492153
    finalDecision: exclude
  - title: "Teaching Reasoning, Reflective Practice, and Evidence-Based Practice: Educator Consensus Strategies"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about educator teaching strategies"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus in the title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    ericId: EJ1447719
    authors: Hannah K. Burke, Pat L. Sample, Anita C. Bundy, Virginia Spielmann, Shelly J. Lane
    year: "2024"
    abstract: Although reasoning, reflective practice, and evidence-based practice are essential professional skills for occupational therapy students, there is not a clear understanding of how these skills are taught. We used Delphi methodology to explore how occupational therapy educators conceptualize best-practices for facilitating the development of clinical reasoning, professional reasoning, reflective practice, and evidence-based practice. Participants were required to be educators in a master's-level or higher occupational therapy program for at least three years, based in the United States, and available across multiple survey rounds. The nine participants nominated a total of 94 teaching strategies, of which 90.4% (n = 85) reached consensus after Round 3. Consensus strategies were largely experiential, active, and explicit teaching approaches. Consideration of how these teaching strategies used by occupational therapy educators support students in integrating skills with each other and into practice can facilitate increased professional competence within occupational therapy.
    mergedFrom:
      - source: eric
        ericId: EJ1447719
    finalDecision: exclude
  - title: Feasibility and Reliability of the Pandemic-Adapted Online-Onsite Hybrid Graduation OSCE in Japan
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: OSCE feasibility study, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: EJ1429003
    authors: Satoshi Hara, Kunio Ohta, Daisuke Aono, Toshikatsu Tamai, Makoto Kurachi, Kimikazu Sugimori, Hiroshi Mihara, Hiroshi Ichimura, Yasuhiko Yamamoto, Hideki Nomura
    year: "2024"
    abstract: Objective structured clinical examination (OSCE) is widely used to assess medical students' clinical skills. Virtual OSCEs were used in place of in-person OSCEs during the COVID-19 pandemic; however, their reliability is yet to be robustly analyzed. By applying generalizability (G) theory, this study aimed to evaluate the reliability of a hybrid OSCE, which admixed in-person and online methods, and gain insights into improving OSCEs' reliability. During the 2020-2021 hybrid OSCEs, one examinee, one rater, and a vinyl mannequin for physical examination participated onsite, and a standardized simulated patient (SP) for medical interviewing and another rater joined online in one virtual breakout room on an audiovisual conferencing system. G-coefficients and 95% confidence intervals of the borderline score, namely border zone (BZ), under the standard 6-station, 2-rater, and 6-item setting were calculated. G-coefficients of in-person (2017-2019) and hybrid OSCEs (2020-2021) under the standard setting were estimated to be 0.624, 0.770, 0.782, 0.759, and 0.823, respectively. The BZ scores were estimated to be 2.43-3.57, 2.55-3.45, 2.59-3.41, 2.59-3.41, and 2.51-3.49, respectively, in the score range from 1 to 6. Although hybrid OSCEs showed reliability comparable to in-person OSCEs, they need further improvement as a very high-stakes examination. In addition to increasing clinical vignettes, having more proficient online/on-demand raters and/or online SPs for medical interviews could improve the reliability of OSCEs. Reliability can also be ensured through supplementary examination and by increasing the number of online raters for a small number of students within the BZs.
    mergedFrom:
      - source: eric
        ericId: EJ1429003
    finalDecision: exclude
  - title: "Predicting PTCPI Scores: The Role of Student Personality"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on predicting scores from personality traits"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    ericId: ED621711
    authors: McDermott H
    year: "2022"
    abstract: "Background: Physical therapist student clinical performance is an essential indicator of professional competence later as a practicing clinician. The Physical Therapist Clinical Performance Instrument (PTCPI) is the most common instrument used by physical therapy programs in the United States. Currently, however, researchers have found little to no association between PTCPI scores and indicators of academic success (e.g., GPA, GRE) or program success (i.e., NPTE pass rate). The absence of these associations raises critical concerns about the construct validity of the PTCPI and suggests that other non-cognitive student attributes may explain variation in the instrument. Therefore, the purpose of the present dissertation was to examine the associations between CI-rated PTCPI scores and PT student personality attributes. Methodology: After IRB approval, data were drawn from both archival and primary data sources from second and third-year PT students (N = 123) at a physical therapy program located in the southern United States. Data included scores from the 60-item HEXACO-PI-R personality instrument, CI-rated PTCPI scores from their first full-time clinical education experience, first-year cumulative professional GPA, undergraduate GPA, and prerequisite GRE scores. The HEXACO-PI-R instrument consists of 6 broad domains (Honesty-Humility, Emotionality, Extraversion, Agreeableness, Conscientiousness, Openness to Experience) and four facets subsumed in each domain. Results: Bivariate correlations and multivariate linear regressions found a consistent relationship between the HEXACO-PI-R and PTCPI outcomes. Specifically, the Honesty-Humility domain o significantly negatively correlated to CI-rated PTCPI scores from both the midterm and final evaluations. However, there was no association with any personality domain and PTCPI midterm to final evaluation change scores. When controlling for academic factors, Honesty-Humility explained unique variation in PTCPI scores. Exploratory analyses further revealed that the HEXACO-PI-R facets might predict PTCPI scores. Discussion: By examining personality characteristics as potential predictors at both the bivariate and multivariate levels, the present dissertation study found correlations and predictors of PTCPI scores. Although the effect sizes were small in magnitude, they were statistically significant and largely consistent with personality research in theory. In particular, the Honesty-Humility domain emerged as a significant variable to consider with CI-rated PTCPI scores. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED621711
    finalDecision: exclude
  - title: A System-Based Research Ethics Architecture for Meeting New Thresholds for Human Subject Protections in Research in the Latest ICH-GCP E6 E6 (R3)
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About research ethics architecture, not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: EJ1485566
    authors: Nabila Khwaja, Manju Varghese, Amal Robay
    year: "2025"
    abstract: ICH-GCP guidelines have provided international ethical, scientific, quality standards for designing, conducting, recording, and reporting of clinical trials, and are hugely popular in the region. Recent revisions to its guidance; ICH-GCP; E6 (R3) have brought significant changes in both structure and content from its predecessor, affecting every aspect of trial conduct, oversight, and planning through reporting. The latest version of ICH-GCP E6 (R3) is ensuring the rights, safety welfare and the well-being of human rights subjects. It aims to adapt and stay relevant in the ever-evolving world of clinical trials, embracing diverse types and settings, and keeping pace with technological and methodological advances. There are many themes and structural changes in E6 (R3). In this article, we provide a detailed account of how the human subject system-based research ethics architecture (HS-SBREA) at Weil Cornell Medicine-Qatar (WCM-Q) addresses the many themes and structural changes in E6 (R3). Particularly the requirement to foster a culture of ethical conduct &amp; build quality management systems to provide protection to human participants in clinical research. We believe that this can be achieved through a primacy of ethical principle-based approach. The focus on primacy of principles has long been established at WCM-Q as a goal for promoting the universal applicability of the four bioethical principles. This has been important in addressing the unique challenges posed by ethical relativism, which is an inevitable outcome of a diverse, rich heterogenous population in Qatar and within the professional biomedical workforce at WCM-Q. We address some of the themes in this discourse to contextualize our responses as a research institution and medical college.
    mergedFrom:
      - source: eric
        ericId: EJ1485566
    finalDecision: exclude
  - title: "Assessing Constructs Underpinning Clinical Educator Engagement in Physiotherapy (Part B): Instrument Construction Using a Modified eDelphi"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About instrument construction for clinical educator engagement using Delphi method, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Does not indicate generative AI/LLM use"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
    ericId: EJ1492161
    authors: Joanna Hargreaves, Deanne Gannaway, Shaun O’Leary
    year: "2025"
    abstract: "Assessing the constructs that impact active engagement of physiotherapists in clinical educator roles could guide initiatives toward sustainability of this workforce. Part B of this series describes the construction of a single purpose-built instrument from items identified in a scoping review (Part A). A multi-professional panel of clinical education experts undertook a modified e-Delphi across three rounds of review: commenting on the relevance of proposed synthesized items (Round one), voting on revised items and demographic questions (Round two), and authentically completing the instrument (Round three). The response rates from 31 participants were 100% (Round one), 97% (Round two) and 87% (Round 3). This study constructed a new instrument consisting of 40 items designed to assess identified factors that underly participation in clinical education by physiotherapists in Australia. Future study is required to apply the instrument to its target population and to evaluate the instrument's psychometric properties."
    mergedFrom:
      - source: eric
        ericId: EJ1492161
    finalDecision: exclude
  - title: "The Willingness of Elderly Care Program Students to Care for Older Adults and the Associated Factors: A Multi-Centered Research"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about elderly care students' willingness to care"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: EJ1384323
    authors: Okur E, Akbal Y, Yagci Sentürk A, Dastan B, Kuralay Ç
    year: "2023"
    abstract: Prospective technicians for elderly care are indispensable elements that should be included in the relevant system to meet the ever-increasing care needs of the aging world population and increase the quality of care. This research was conducted to evaluate the attitudes of students in the elderly care program at the associate degree toward the older adults and their willingness to provide care. The sample of this descriptive and cross-sectional study comprised 525 volunteer students in elderly care programs in five vocational schools of higher education located in northeastern Turkey between June 2020 and September 2020. Mean, standard deviation, number, percentage, independent &quot;t&quot;-test, and correlation analysis were used to evaluate the data. The study showed that the mean age of the students involved in the study was 20.3 ± 1.6. 77.7% of them were female, and 61.9% were in the first grade. Students' willingness to care for older adults was found to be at a moderate level (36.3 ± 6.0). The students had a negative attitude (137.4 ± 26.7) about aging and elderliness. The willingness of male students to care for older adults was significantly higher (p &lt; 0.05). Those who did not live with an older adult in the same house before had a more negative attitude toward them (p &lt; 0.05). To get rid of personal and social stereotypes and have a positive attitude toward older adults, students should be provided to work in clinical environments that will allow them to spend more time with them.
    mergedFrom:
      - source: eric
        ericId: EJ1384323
    finalDecision: exclude
  - title: Preservice Teacher Perceptions of Instructional Rounds
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about preservice teacher education, not medical/health education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are preservice teachers, not medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: EJ1447169
    authors: Madelon McCall, Kenley Ritter, Abigail Gardner
    year: "2024"
    abstract: "Purpose: The purpose of this qualitative instrumental case study was to determine the perceptions of preservice teachers (PSTs) on the effectiveness of instructional rounds as a clinical experience in promoting awareness of student diversity and supporting the acquisition of professional knowledge (Essential 2). Design/methodology/approach: The instructional rounds were implemented in a junior-level general pedagogy course prior to formal clinical experiences. Professional development school (PDS) personnel supported the course instructors by scheduling the classroom observations, supervising groups of PSTs and debriefing the PSTs after each observation (Essentials 4 and 8). The data were collected through an end-of-course survey of 18 secondary PSTs. Findings: There were several themes that emerged from the analysis of data. First, the study revealed that PSTs credited the variety of campuses visited as supporting their awareness of student diversity and varied instructional strategies. Second, PSTs acknowledged that the instructional rounds supported their connection of theory to practice. Finally, over 70% of the participants noted that they most enjoyed in-person experiences in different classrooms to observe students and teachers in action. Research limitations/implications: The findings for this study were specific to the teacher preparation program (TPP) utilized for the research. Each TPP requires different coursework and clinical experiences; therefore, the inclusion of instructional rounds may not be possible in all programs. Yet, the implementation of the rounds as a PST experience prior to clinical experiences is a strategy to consider to support the preparation of PSTs for their clinical experiences. Originality/value: This study supports the continuation of instructional rounds at the teacher preparation program where the research was conducted. This research also informs other TPPs that strive to provide early clinical experiences that support PSTs' emerging perceptions of student diversity and applications of instructional knowledge."
    mergedFrom:
      - source: eric
        ericId: EJ1447169
    finalDecision: exclude
  - title: "Developing an Asynchronous LGBTQ+ Affirmative Counseling Training: A Mixed-Methods Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about counseling training development"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: EJ1451764
    authors: Amber L. Pope, Noelle St. Germain-Sehr, Bianca R. Augustine, Amanda St. Germain-Sehr, Tai Lexumé, Jeff Moe, Senttra Snowden-Gregg, Tamika N. Jackson
    year: "2024"
    abstract: Master's level counseling students completed a 5-week online asynchronous LGBTQ+ affirmative counseling training. Using a mixed-methods and quasi-experimental design, results indicated that participants' LGBTQ+ knowledge, clinical skills, and advocacy increased posttraining. Content analysis revealed four themes of how students experienced the training. Implications, limitations, and future directions are discussed.
    mergedFrom:
      - source: eric
        ericId: EJ1451764
    finalDecision: exclude
  - title: The Impact of an Educational Human Trafficking Panel on Occupational Therapy Students&apos; Knowledge and Self-Efficacy
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: EJ1441160
    authors: Sarbinaz Bekmuratova, Andrea Thinnes, Yongyue Qi, Arduizur Carli Richie Zavaleta, Ashlynn York
    year: "2023"
    abstract: A rise in attention to and assistance for human trafficking (HT) victims and survivors has resulted in a call to action for occupational therapists and other healthcare professionals. Victims and survivors often seek healthcare services in a variety of settings, yet ill-equipped healthcare professionals lacking training and self-efficacy with this population have left many unidentified needs unaddressed. Occupational therapists possess the skills necessary to support and assist survivors of HT in their reintegration and healing processes. However, little to no specific training for practitioners in this field has been developed. This study explored how an educational panel of OTs and HT experts who have worked with occupational therapists impacted occupational therapists' knowledge and perceived self-efficacy regarding HT and its intersection with occupational therapy. A 1.5-hour interactive panel was assembled and prepared for a synchronous Zoom meeting by the researchers. Eighty students completed both pre- and post-surveys. Post-panel surveys revealed that students' knowledge of the intersection between HT and occupational therapy improved, their perceived self-efficacy in assisting victims and survivors of HT increased, and their perspective on the panel format and content was favorable. The survey findings also indicated students' desire for continued professional education and occupational therapy practice skills focused on the topic of HT. One way to address the gaps in the knowledge and self-efficacy of healthcare providers is to equip them with knowledge and skills on treating HT victims through training during their didactic curriculum and in clinical practice.
    mergedFrom:
      - source: eric
        ericId: EJ1441160
    finalDecision: exclude
  - title: "Addressing a Crisis in Nursing Practice: Exploring the Impact of Multimedia Case-Based Learning on the Clinical Judgment of Undergraduate Nursing Students"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about multimedia case-based learning in nursing"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: ED659247
    authors: Macy Jenette Bennett
    year: "2024"
    abstract: "Clinical judgment is a priority nursing skill to ensure the delivery of safe and effective nursing care. New graduate nurses lack the clinical judgment skills that are necessary for entry-level nursing practice. Nurse educators must therefore implement teaching strategies that foster clinical judgment. This study evaluated the clinical judgment of undergraduate nursing students after the implementation of multimedia case studies related to the concept of perfusion. Two research questions guided this study: (a) in what ways do undergraduate nursing students demonstrate clinical judgment after completing a series of perfusion-related multimedia case studies and (b) to what extent do undergraduate nursing students' perceptions of clinical judgment change after the completion of perfusion-related multimedia case studies? This study utilized a convergent mixed methods approach with 34 undergraduate nursing students who engaged in 4 weeks of multimedia case-based learning related to the concept of perfusion. Quantitative data was collected during two nursing simulations and analyzed with the Lasater Clinical Judgment Rubric (LCJR) using descriptive statistics and a Wilcoxon signed-rank test. A significant increase was noted in the subscale of interpreting from the pre-intervention score to the post-intervention score. The other subscales of the LCJR did not demonstrate a significant difference. Qualitative data obtained during a pre-intervention and post-intervention clinical care reflection was analyzed against predetermined keys through a process of deductive analysis to evaluate clinical judgment. Four participants were noted to have improved clinical judgment, while two participants' clinical judgment stayed the same and one participant's clinical judgment decreased. Qualitative data obtained from a semi-structured interview was analyzed using inductive analysis to reveal three themes: (1) noticing patient data, (2) interpreting patient data, and (3) responding to patient care situations. During the interview, participants displayed sound clinical judgment in each of the five aspects of the LCJR. Participants found that the case studies promoted clinical judgment by increasing familiarity with course content, stimulating deeper connections, and providing opportunities for extra practice. These findings support the use of multimedia CBL as a strategy to facilitate clinical judgment within nursing education. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED659247
    finalDecision: exclude
  - title: Athletic Training Students&apos; Perceptions of Translating Knowledge from Classroom to Clinical Practice
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: EJ1405507
    authors: Cailee E. Welch Bacon, Ryan Kroskie, Alicia M. Pike Lacy, Julie M. Cavallario
    year: "2024"
    abstract: "Context: As the health care landscape evolves, athletic training students (ATSs) must gain the skills necessary to effectively translate new evidence into practice to optimize patient outcomes. Knowledge translation is an iterative process that promotes the application of new knowledge by users, but little is known about how students perceive this translation process during athletic training education. Objective: To explore ATSs' perceptions of knowledge translation. Design: Cross-sectional. Setting: Web-based survey with open-ended questions. Patients or Other Participants: A total of 255 professional ATSs accessed the survey; 168 students (118 women, 48 men, 2 missing; age = 22 ± 2.3 years) completed the entire survey (69.5% completion rate). Data Collection and Analysis: A Web-based survey composed of 6 open-ended questions was distributed to ATSs via a recruitment e-mail forwarded by the Commission on Accreditation of Athletic Training Education-accredited athletic training program director on behalf of the research team. Due to the textual nature of the data, we used a multiphased, multianalyst data analysis approach guided by the consensual qualitative research tradition to analyze responses to the open-ended items. Self-reported topics that ATSs translated into clinical practice were categorized into the preestablished practice analysis domains identified by the Board of Certification. Results: Topics within the examination, assessment, and diagnosis domain were most frequently reported by respondents. In addition to the topics translated, 3 themes emerged during data analysis. Students discussed their perceptions regarding the (1) application of knowledge gained in clinical practice, (2) challenges prohibiting successful knowledge translation, and (3) strategies to promote the knowledge translation process. Conclusions: Findings from this study suggest a need to promote synergy between faculty and preceptors to ensure that didactically taught knowledge is aligned with what students are encountering during clinical experiences. Incorporating established knowledge translation strategies may help ATSs bridge the knowledge-to-practice gaps they perceive as current challenges inhibiting successful translation of new knowledge into the clinical environment."
    mergedFrom:
      - source: eric
        ericId: EJ1405507
    finalDecision: exclude
  - title: "Identifying Opportunity Gaps in Clinical Practice: A Concept Map Analysis"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on concept map analysis of clinical practice gaps"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    ericId: EJ1451246
    authors: Megan E. Lynch, Sara E. Harms, Tamara Shattuck, Jennifer Phaiah, Katrina Hall, Wendy Baker, Rene Roselle
    year: "2024"
    abstract: This study seeks to understand how clinical practice perpetuates inequities in initial teacher education programs and to identify opportunity gaps within clinical practice. Nine clinical faculty created concept maps as a reflective activity to name the opportunity gaps they see in their clinical practice context. Authors engaged in a thematic analysis of eight participants' concept maps, revealing systemic inequities in access, outcomes, and opportunities within clinical practice. Notably, there was a focus on the experiences of teacher candidates, school-based teacher educators, and university-based teacher educators, with less explicit attention paid to those in clinical practice from marginalized and minoritized groups. Opportunity gaps were identified in the distribution of funding and resources for clinical practice; the (lack of) shared values and vision for collaboration and professional learning between institutions and school districts, and societal and systemic perspectives on clinical practice. The study's findings underscore the importance of addressing these disparities to enhance the effectiveness of initial teacher preparation and to ensure equitable experiences for all clinical practice partners. This study contributes to the literature on educational equity and highlights the need for systemic changes within teacher education programs to address opportunity gaps in clinical practice.
    mergedFrom:
      - source: eric
        ericId: EJ1451246
    finalDecision: exclude
  - title: Scaffolded Simulation in Psychiatric Mental Health Nursing Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Scaffolded simulation in nursing education, not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    ericId: EJ1414772
    authors: Lori Bobo, Stacy Mikel, Yolanda Chandler, Hungwei Tseng
    year: "2023"
    abstract: "Objectives: The purpose of this study was to investigate the impacts of using scaffolded clinical simulations on nursing students' confidence in clinical reasoning, clinical judgment, and critical thinking skills. Next, we also attempted to gain insights into students' perceptions of the benefits of scaffolded clinical simulations. Method: We used a mixed-methods research design to investigate the impacts of using scaffolded clinical simulations on 133 second-semester baccalaureate nursing students' confidence in clinical reasoning, clinical judgment, and critical thinking skills. Results: Findings from this study indicate that students who perceived the benefits of scaffolded simulation activities (i.e., peer observations, debriefings, and self-reflections) were more likely to engage in this deep learning process, which in turn led to their higher confidence in clinical reasoning, clinical judgment, and critical thinking skills. Conclusions: This study demonstrates that scaffolded simulations in psychiatric clinical settings, coupled with the novice to expert model in nursing education, are a valuable tool for preparing nursing students for the rigors of clinical practice, despite decreased in-person client experiences in nursing programs. Implications: This study has implications for designing and implementing scaffolded clinical simulations that foster clinical judgment and help students perform tasks with which they are already familiar, while new responsibilities are introduced throughout the semester."
    mergedFrom:
      - source: eric
        ericId: EJ1414772
    finalDecision: exclude
  - title: Bringing Back the Core Concepts of Physiology in an Advanced Medical School Senior Elective
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about physiology course design"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: EJ1452624
    authors: Feroza Daroowalla, David M. Harris, Jonathan Kibble
    year: "2024"
    abstract: Integrating physiology core concepts into the clinical years of medical education has been challenging despite efforts. This article describes a fourth-year medical school elective, Advanced Physiology in Critical Care Medicine, that focused on integrating physiology core concepts in a case-based learning approach. The elective used interdisciplinary faculty in a virtual forum. Senior students were asked to generate mechanism of disease (MOD) maps, highlight the physiology core concepts associated with paper cases of critically ill patients, and discuss with faculty the relevance of the underlying basic science. The weekly footprint consisted of a student-led session presenting MOD maps for three cases, which examined aspects of core physiology concepts, and, later in the same week, student presentation of order sets for the management of the cases. Students ended the 4-wk elective with a mini-grand rounds presentation on a topic of their choice incorporating the core concept paradigm. Student perception data and faculty reflections of the elective course are included. Student data and faculty observations suggest that students appreciate and can apply physiological core concepts to patient care. Faculty experience in the course suggests that this senior elective helped them better approach all preclinical teaching with the Core Concepts framework in mind.
    mergedFrom:
      - source: eric
        ericId: EJ1452624
    finalDecision: exclude
  - title: "Belongingness in the Clinical Learning Environment: A Novel Descriptive Study Focusing on Diagnostic Medical Sonography Students"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on belongingness in clinical learning"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: ED640780
    authors: Kimberly Michael
    year: "2023"
    abstract: "The purpose of this quantitative study was to explore diagnostic medical sonography (DMS) students' feeling of belongingness in the clinical learning environment. The Belongingness Scale Clinical Placement Experience (BES-CPE) questionnaire and demographic questions were used to test five research questions. The online survey was administered to DMS students who were members of the Society of Diagnostic Medical Sonography (SDMS) and to seven DMS programs not requiring SDMS membership. The results indicated DMS students (N = 42) experience some felt belongingness in the clinical setting based on the BES-CPE mean score of 3.71. Five of the 34 BES-CPE items found significance between student age and the feeling of belongingness with the difference seen between those &quot;24-26 years of age&quot; and those &quot;27 years of age and up.&quot; No relationship was indicated between belongingness and student gender. Significance was revealed between clinical placement time and belongingness in 16 of the 34 items. Those who felt belonging early (&quot;Week 1 and Week 2&quot;) were more likely to respond positively to a BES-CPE item than those who reported belongingness later in their clinical experience (&quot;Week 8 or not at all&quot;). Finally, DMS students learning in a small (5 rooms or fewer) ultrasound department had significantly higher mean scores on 16 of the 34 BES-CPE items when compared to those learning in a &quot;large&quot; (&quot;6 or more rooms&quot;) ultrasound department. These findings are important as the clinical learning environment is a required and significant component in health professions programs. It is recommended that programs evaluate their clinical setting for belongingness and future studies focus on better understanding belongingness in this learning environment. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED640780
    finalDecision: exclude
  - title: A Tutorial on Discourse Analysis in Healthy and Pathological Ageing
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; tutorial on discourse analysis in ageing"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    ericId: EJ1407633
    authors: Ana Varela Suárez
    year: "2024"
    abstract: "Background: Age is a key factor when dealing with language and speech disorders, as it entails a progressive loss of neuroplasticity even in healthy individuals. Apart from this, ageing also affects our word-retrieval abilities, and thus, our discursive skills, particularly in people suffering from neurodegenerative diseases. Therefore, descriptions and/or measures of communicative performance always need to be interpreted through the lens of variation across the lifespan. Aim: This paper's main objective is to create a general tutorial for researchers willing to start delving into discourse analysis, both in healthy and pathological ageing. Methods: An eight-step tutorial on discourse analysis in the elderly is presented. Each of these steps starts with general recommendations and progresses to more specific topics that may be relevant when conducting this type of research. All of the steps have been extrapolated from an extensive literature review on discourse analysis. Main Contributions: This work presents an easy-to-follow, step-by-step tutorial on discourse analysis in the elderly. It is aimed at clinical researchers who are taking their first steps in discourse analysis. It may also be useful for those who are already familiar with the methodology but may be interested in reading a general overview on the topic. Moreover, it offers new insights into the following topics: types of research questions, advantages and disadvantages of the different research methodologies and ethical considerations for data production in clinical linguistics. Conclusions: Discourse analysis in the elderly is a highly complex issue that may require researching from different approaches and disciplines. This implies following a well-planned and thorough process, which we have detailed through the following eight steps: (i) reviewing literature; (ii) formulating the research question; (iii) designing the study; (iv) producing data; (v) selecting technological tools for data treatment; (vi) transcribing the corpus; (vii) annotating the corpus and (viii) analysing and interpreting the results."
    mergedFrom:
      - source: eric
        ericId: EJ1407633
    finalDecision: exclude
  - title: A Case Study of Pre-Service Teachers&apos; Emerging Perceptions of Social-Emotional Learning (SEL)
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 6: About pre-service teachers, not medical/health-professional learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are pre-service teachers, not medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: ED653812
    authors: Lance Perry
    year: "2024"
    abstract: "In 2017, the National Commission on Social, Emotional, and Academic Development examined the relevant evidence regarding social-emotional learning (SEL) across various disciplines. The commission concluded that social and emotional competencies are essential to developmental growth, and overall success in school and life. This qualitative case study captures pre-service teachers' emerging perceptions of SEL throughout the teacher training process. This study seeks to inform the practice of developing pre-service teachers during their time in university to prepare them to meet the SEL expectations of school systems in their K-12 educational careers. This investigation included faculty and pre-service teacher focus groups and individual interviews with the participating pre-service teachers. Various documents were also collected and analyzed. The findings of this study revealed pre-service teacher concerns regarding their own SEL development while in university, the absence of focused SEL training, and missed opportunities for SEL understanding in their clinical experiences. The findings can be used to understand pre-service teacher perceptions of SEL-related training better and inform courses and objectives to improve instructional practices and new teacher experiences. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED653812
    finalDecision: exclude
  - title: "The Benefits and Challenges of the Use of Digital Technology on Clinical Learning of Undergraduate Nursing Students during the COVID-19 Pandemic: An Integrative Literature Review"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About digital technology broadly during COVID-19, not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Digital technology focus does not indicate generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    ericId: EJ1415267
    authors: Kholofelo Lorraine Matlhaba, Sisinyana Hannah Khunou
    year: "2023"
    abstract: "The purpose of this study was to compile research on the advantages and challenges of using digital technology in clinical education for undergraduate nursing students during the COVID-19 pandemic. The study employed an integrative literature review with the use of the five steps as per Cooper's framework. Data were collected from four electronic databases to access research articles published from 2019 to 2023. Out of 440 potential candidates, eighteen satisfied the inclusion criteria. Three themes emerged from the analysis namely: 1) the benefits, 2) the challenges of digital technology, and 3) attitudes towards the use of technology in clinical learning. The use of digital technology helps nursing students become more competent and confident in their ability to practice in a friendly environment. It is imperative that there should be opportunities for workshops to create awareness regarding the importance of digital technology. The practical implications of the study are that nursing education institutions should invest in the purchase of digital technology equipment as a means of advancing the implementation of new educational advancements. The stakeholders in clinical teaching and learning should be better equipped by health facilities and nursing education institutions to make better use of digital technologies."
    mergedFrom:
      - source: eric
        ericId: EJ1415267
    finalDecision: exclude
  - title: Clinical Reasoning Development Following a Simulation-Based Learning Experience in Doctor of Physical Therapy Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About simulation-based clinical reasoning in PT education, no generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: ED657367
    authors: Benjamin Lee Wolden
    year: "2024"
    abstract: "Clinical Reasoning (CR) integrates thinking and decision-making in clinical practice (Huhn et al., 2019). CR is an established area of research in Doctor of Physical Therapy (DPT) education (Musolino &amp; Jensen, 2019; Jensen &amp; Mostrom, 2012) and has been acknowledged as a core competency of physical therapist residency education (APTA Residency Competency Work Group, 2020). Despite the established research, the assessment of CR has emerged as a topic of importance due to the need for standardized assessment of CR in DPT education that can lead to high-quality research and implementation of evidence-informed teaching and learning (Reilly et al. 2021; Furze et al., 2022; Christensen et al., 2017). Simulation-Based Learning (SBL) is an area of recent focus amongst DPT educators that has demonstrated the ability to improve student clinical decision-making, CR, and critical thinking in the health professions (Macauley et al., 2017). SBL is &quot;a technique that creates a situation or environment to allow persons to experience a representation of a real event for the purpose of practice, learning, evaluation, testing, or to gain understanding of systems of human actions&quot; (Lioce et al., 2020, p. 44). SBL promotes active learning and emotional engagement of the learner and has been integrated into DPT education (LeBlanc &amp; Posner, 2022). Need for the Current Research Despite the recognized benefits of SBL, the effects of Simulation-Based Learning Experiences (SBLE) on DPT student CR have yet to be investigated. Additionally, limited tools are available to assess student CR in DPT education. One recently developed assessment tool is the Clinical Reasoning Assessment Tool (CRAT), which can be used to evaluate CR in both didactic and clinical settings. The CRAT assesses three domains of CR by rating a student's foundational knowledge, psychomotor skills, and conceptual reasoning on a 0 to 16 Likert scale (McDevitt et al., 2019). The CRAT is designed to track student CR assessment and development longitudinally over time (Furze et al., 2015; McDevitt et al., 2019), but the reliability psychometrics of the CRAT have not been established. This three-article dissertation investigated CR development in DPT students following a SBLE. The results of this dissertation will inform subsequent research. The combination of the three articles provides needed insight into the development of CR in DPT students following a SBLE in a DPT education program. The three articles include the following: 1. Assess the reliability of the CRAT. 2. Perform a systematic literature review on the effects of high-fidelity SBL on DPT student learning and performance. 3. Assess the effect of a high-fidelity SBLE on DPT student CR as measured by the CRAT. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED657367
    finalDecision: exclude
  - title: Nature Based Interventions for Human Health and Wellbeing
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; nature-based health interventions"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Not focused on medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: ED638673
    authors: Samantha I. Moyers
    year: "2023"
    abstract: "Background: Nature exposure is beneficial to human health and wellbeing, but the evidence base for nature-based interventions (NBI) is still developing. Heterogeneous naming conventions, study designs, and intervention contexts create difficulty in accessing and assessing the evidence for such programming. While NBI have been utilized in a wide array of human populations, no interventions have been reported among clinical health professions students. Aims: The objectives of this dissertation project were to review the existing body of NBI literature exploring health and wellbeing outcomes, and to conduct a feasibility trial of forest bathing, a type of NBI, among clinical students. Specifically, this study aimed to: (&quot;Aim 1&quot;) conduct a scoping review to (1) identify the different nomenclature used to define NBI, (2) describe the NBI utilized and the contexts in which they occurred, and (3) describe the methodologies used in studies of NBI; (&quot;Aim 2&quot;) assess the feasibility of implementing a pilot forest bathing intervention for stress management among clinical health professions students; and (&quot;Aim 3&quot;) assess the preliminary effectiveness of a pilot forest bathing intervention for stress management among clinical health professions students. Methods: (&quot;Aim 1&quot;) A scoping review was conducted following a prospectively published protocol and adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews. We searched PubMed, Web of Science, Scopus, and ProQuest Dissertations and Theses Global for studies on NBI in humans. Dual independent screening and single-author data abstraction were conducted. (&quot;Aims 2 and 3&quot;) A single-arm feasibility trial of a six-week forest bathing intervention was conducted. Proctored, self-guided sessions were held on weekday afternoons in a local arboretum. The protocol was prospectively registered and CONSORT reporting guidelines were followed. Participants were clinical students enrolled in a public university in the United States. Aim 2 used qualitative (surveys, focus group, and researcher observation) and quantitative (surveys) methods to assess the feasibility of implementing the program. Aim 3 used quantitative survey data to explore preliminary effectiveness measures: perceived stress, attitudes toward spending time in nature (TSN), self-efficacy TSN, and intentions TSN. Results: (&quot;Aim 1&quot;) A total of 406 papers comprising 416 reported studies were included. Multi-day programs, repeated short-session programs, and single short-session programs comprised the sample. NBI were reported in 36 countries and published as dissertations and peer-reviewed papers in 160 journals. 155 unique NBI names were reported. (&quot;Aim 2&quot;) 413 students were invited to participate in the program, and 13 students enrolled. Seven individuals participated in at least one session; of those attending at least once, five (71.4%) attended at least five of the six sessions. Measures and data collection methods were well-received and reliably reported. Interpersonal commitment was a primary facilitator to participation, and time constraints were a key barrier. (Aim 3) Participants reported positive affective impacts during 63.6% (n=21) of sessions, and present stress declined significantly over a single 40-minute session (p=0.0007). Over the course of the study and into follow-up, stress decreased by half of a point on the Perceived Stress-10 scale per week (p=0.0308). Participating enrollees and non-participating enrollees may have differed on baseline perceived stress, baseline positive attitudes TSN, and time spent in nature in the week prior to enrollment. Conclusion: (&quot;Aim 1&quot;) Taken on the whole, NBI are highly diverse in design and reporting. The field of research would be strengthened by the development of (1) consistent naming conventions and (2) NBI-specific reporting guidelines (i.e., a checklist). (&quot;Aims 2 and 3&quot;) While recruitment proved challenging in this population, preliminary evidence suggests that forest bathing or other NBI may be acceptable and beneficial for clinical students experiencing stress. Further feasibility work should explore weekend-based NBI programming offering guided activities. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED638673
    finalDecision: exclude
  - title: "The Role of Education, Concept Knowledge, Work Setting and Clinical Experience in Communication Partner Training: A Survey of Flemish Speech and Language Therapists"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Communication partner training survey, not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    ericId: EJ1400099
    authors: Barberis M, Vandermosten M
    year: "2023"
    abstract: "Background: Aphasia can affect the communication between the person with aphasia (PWA) and the communication partner (CP). It is therefore necessary to support both the PWA and their CPs. Communication partner training (CPT) focuses on training communication between dyads of whom one person has aphasia. Although there is increasing evidence supporting CPT as an effective intervention to improve communication and reduce the psychosocial consequences of stroke, implementation in clinical practice remains limited. Aim: To understand the mechanisms behind the practice--evidence gap currently hindering CPT implementation, this study investigated the role of (1) education, (2) concept knowledge, (3) work setting and (4) clinical experience in CPT. Methods &amp; Procedures: Flemish speech and language therapists (SLTs) clinically involved in aphasia rehabilitation were surveyed online regarding CPT. Statistical analyses include descriptive statistics to report survey results and non-parametric group comparisons to investigate the role of the four variables on CPT. Outcomes &amp; Results: In this study 72 SLTs were included, of whom 73.61% indicated they deliver CPT but of whom only 43.10% indicated CP presence during therapy. The most frequently identified barriers to CPT delivery were lack of time and CPT-specific knowledge. Other barriers were lack of resources, work setting dependent factors, PWA or CP dependent factors, individual therapy to the PWA being of higher priority, existing CPT methods and interventions being perceived as unclear and feeling uncertain about CPT delivery. Concerning the role of the four variables on CPT delivery, neither education nor concept knowledge had a significant effect on CPT delivery. Work setting and clinical experience did, however, influence CPT delivery. More specifically, CPT delivery and CP presence were higher in the private practice (chronic phase) compared to the other three settings and experienced SLTs deliver CPT more often compared with less experienced SLTs. Conclusions &amp; Implications: To reduce the practice-evidence gap, we suggest prioritising the two most frequently identified barriers, that is, lack of time and CPT-specific knowledge. To overcome the time barrier in CPT, we propose implementing automated natural speech analysis to reduce the workload. To enhance CPT-specific knowledge, speech and language therapy curricula should provide more in-depth theory and hands-on practice for CPT. In addition, increased awareness about CPT-specific methods is needed to further support clinical practice."
    mergedFrom:
      - source: eric
        ericId: EJ1400099
    finalDecision: exclude
  - title: Integration of Co-Curricular Experiential Learning in BME Programs to Increase Student Success
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Biomedical engineering experiential learning, not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are not medical/health-professional learners (biomedical engineering program)."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    ericId: EJ1482571
    authors: Isgard S. Hueck, Alexandre Guével, Rob S. MacLeod, Kristen Billiar
    year: "2025"
    abstract: There is strong evidence that co-curricular experiential learning activities positively contribute to the development of well-rounded and successful engineers. However, discussions and surveys conducted during the 5th BME Educational Summit in May 2024 revealed that co-curricular activities--such as optional engineering internships, co-ops, independent research, and clinical immersion experiences--are often neither integrated into BME curricula nor guided by evidence-based pedagogy. In this perspective article, we propose that framing these co-curricular experiences within a cohesive pedagogical model, such as Kolb's Experiential Learning Theory (ELT), can enhance and standardize student learning outcomes. We review existing literature on the benefits of co-curricular experiential learning, present survey data on BME student participation across various institutions, address barriers to participation, and discuss the level of integration within academic curricula. Additionally, we describe Kolb's ELT, explore how BME programs are incorporating experiential activities, and provide specific examples of programs that emphasize student preparation, outcome setting, and reflective learning for co-curricular activities. We conclude that further research on the application of ELT to co-curricular experiential learning experiences is essential for establishing best practices to maximize student learning from these activities.
    mergedFrom:
      - source: eric
        ericId: EJ1482571
    finalDecision: exclude
  - title: Examining the Impact of Clinical Simulation on Undergraduate Nursing Students&apos; Success at a Historically Black College and University (HBCU)
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About clinical simulation impact on nursing students, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: ED652886
    authors: Alva Renee Millican
    year: "2023"
    abstract: "In the discipline of nursing education, it is very important that nursing students complete their didactic and clinical education skills in order to practice as a registered nurse. It is also very important that they prepare and study to successfully pass the National Council Licensure Examination (NCLEX) and become licensed as a registered nurse to practice in the profession of nursing. The field of simulation technology is quickly expanding, and institutions of higher nursing education are making investments in this technology. Simulation technology has become increasingly integrated into the nursing curriculum, and it possesses a great deal of potential for undergraduate nursing education. Unfortunately, this potential is both underappreciated and not fully utilized. In previous studies, simulation technology was utilized amongst undergraduate students and was shown to be very effective, and students were able to improve their clinical abilities in an environment that was safe, non-threatening, and immersive. The clinical culmination of the baccalaureate nurses' education should produce an advanced beginner (Benner, 2020). Nursing programs work to transition individuals to meet or exceed the advanced beginner level of competency upon graduating. In nursing schools across the country, the use of simulation technology has increased. Research is lacking in simulation technology on expanding the knowledge of diverse students and improving the synthesis of didactic and clinical learning related to progression and transition from the senior level to graduation and licensure. The senior level is the pinnacle of the academic nursing curriculum for students in a wide range of nursing schools. Clinicals in nursing schools are a crucial component of a student's training for a career in nursing. The following skillsets--confidence, the capacity to make connections, the capacity for critical thought, leadership abilities, and the capacity to think like a beginning nurse--are expected of nursing students at the senior undergraduate level (Benner, 2020). At this level, simulation can be essential for preparing students for providing real-world patient care. Various studies on simulation technology and academic success were evaluated, but they did not address its impact or use among students at historically black institutions and universities. In 2003, the National League of Nursing (NLN) recognized the need to reform clinical education to help meet the urgent demands in the nursing workforce. Innovative strategies that offer the nursing student a controlled and safe setting where they can apply didactic content safely are at the forefront of faculty. Simulation has been a welcomed addition and teaching strategy. Of benefit is the ability to individualize each clinical simulation scenario to meet the course objectives. This gives the nursing instructor the opportunity to hone in on areas that may be more difficult for the student to comprehend. Clinical simulation is described as &quot;an activity or event replicating clinical practice using scenarios, high-fidelity manikins, medium-fidelity manikins, standardized patients, role playing, skills stations, and computer-based critical thinking simulations (NCSBN, 2014).&quot; This study seeks to examine the impact of using computer simulation technology as a supplemental pedagogy to didactic instruction to improve performance predictor exam scores in baccalaureate senior nursing students at a historically black college or university (HBCU). [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED652886
    finalDecision: exclude
  - title: Perceptions of Occupational Therapy Student Fieldwork Readiness in the Acute Care Setting
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About OT student fieldwork readiness perceptions, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI/LLM component is indicated."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    ericId: EJ1488993
    authors: Jackie Dusing, Lisa Borrero, Elizabeth Horrall Stith, Kristina Watkins
    year: "2025"
    abstract: "Occupational therapy student readiness for Level II fieldwork is necessary for preparing novice clinicians and is challenging to assess, as it requires applying didactic knowledge in real-world clinical settings. There is currently a gap in the literature regarding student perceptions of their readiness for Level II fieldwork, especially in the acute care setting. The aim of this qualitative study is to better understand Doctor of Occupational Therapy (OTD) students' perceptions of readiness for Level II fieldwork in an acute care setting. This basic interpretive qualitative research study utilized a purposeful sampling with students completing their Level II fieldwork in various acute care hospitals. Semi-structured interviews were conducted virtually and audio recorded. Transcripts were de-identified, transcribed, coded, re-coded, and themes categorized. Information power was achieved after 11 participants were interviewed for this study. Themes that emerged from the analysis were: (1) didactic knowledge translation, (2) complexity of acute care occupational therapy, and (3) collaboration and therapeutic use of self in fieldwork. There is an opportunity to have more focused acute care education and practical hands-on skills training needed to perform occupational therapy services in this setting. Additionally, collaboration with acute care Level II fieldwork settings will help facilitate the application of this specialized education and skills."
    mergedFrom:
      - source: eric
        ericId: EJ1488993
    finalDecision: exclude
  - title: The Impact of a Psychosocial-Focused Experiential Learning Activity on Occupational Therapy Students&apos; Perceived Clinical Reasoning
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus in the title."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    ericId: EJ1467341
    authors: Molly Bathje, Kristi Escobar, Meghan Crisp, Catherine Killian, Charlotte Royeen
    year: "2022"
    abstract: A primary objective of occupational therapy education is to facilitate development of clinical reasoning skills. These skills are complex and difficult to cultivate in classroom settings, therefore educators often use experiential learning activities to support clinical reasoning development. Most of the literature about experiential learning activities aimed at developing clinical reasoning focuses on activities occurring inperson, with individuals in physical disabilities settings. This research addresses the gap in the literature by evaluating the impact of a group based, psychosocial focused experiential learning activity that occurred virtually and in-person on entry-level occupational therapy students' perceived clinical reasoning. Students (n=36) completed the Self-Assessment of Clinical Reasoning and Reflection (SACRR) before and after engaging in a six-week experiential learning activity. The mean total score for the SACRR increased after the learning activity (Z=-4.92, p=0.00). Mean scores on 25 of 26 subtests increased and the change on 19 of the 26 items was statistically significant. Items about applying theory to practice increased the most, indicating that students' perceived abilities increased related to applying theory. Additionally, the learning activity occurred on virtual platforms, demonstrating the potential role of virtual platforms in experiential learning for clinical reasoning development. Overall, this study found that an experiential learning activity designed to address psychosocial needs of groups, using in-person and virtual delivery, increased occupational therapy students' perceived clinical reasoning. Results add to the literature about clinical reasoning development in students by providing evidence for the use of group based, psychosocial focused learning activities delivered virtually and in person.
    mergedFrom:
      - source: eric
        ericId: EJ1467341
    finalDecision: exclude
  - title: "Transitioning to Online Clerkship during Unprecedented Times: An Innovative Online Flipped In-Patient Clerkship"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on transitioning clerkship to online format"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    ericId: EJ1370779
    authors: Annamalai N, Tangiisuran B, Athirah Daud N
    year: "2022"
    abstract: In the context of the COVID-19 pandemic, innovative instructional design plays a crucial role to reach physically distanced students. This present study examined students' experience in an online flipped in-patient clerkship which was designed based on the 5E framework emphasising engagement, exploration, explanation, elaboration and evaluation phases. A survey and open-ended questions were employed to understand students' experience of the online flipped in-patient clerkship. The study found that in the pandemic situation, online flipped in-patient clerkship is an effective solution and their positive themes revolved around four main themes (i) effective and well-designed online content (ii) actual role of the pharmacist in in-patient clerkship (iii) communication (iv) visualising the actual situation. Based on the data, one model that would be able to facilitate and enhance the use of online flipped in-patient clerkship is suggested.
    mergedFrom:
      - source: eric
        ericId: EJ1370779
    finalDecision: exclude
  - title: "Athletic Training Student Core Competency Professional Behavior Implementation between Immersive and Non-Immersive Clinical Experiences: A Report from the Association for Athletic Training Education (AATE) Research Network"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about athletic training clinical experiences"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: EJ1397433
    authors: Jones B, Cavallario J, Van Lunen B, Walker S, Bay C, Welch Bacon C
    year: "2023"
    abstract: "Context: The Commission on Accreditation of Athletic Training Education requires athletic training programs to emphasize the use of professional behaviors that are associated with 6 core competencies, 5 of which were measured in this study: patient-centered care, interprofessional education and collaborative practice (IPECP), evidence-based practice (EBP), health information technology (HIT), and quality improvement (QI). The purpose of this study was to examine the association between clinical experience type and student implementation of behaviors associated with the core competencies. Design: Multisite, panel design. Setting: Twelve professional athletic training programs (7 graduate, 5 undergraduate). Patients or Other Participants: A total of 338 athletic training students logged patient encounters for 1 academic year in the E*Value system. Main Outcome Measure(s): Students reported clinical experience type (immersive versus nonimmersive) and implementation of behaviors associated with core competencies. Counts of professional behaviors were calculated, and differences in behavior implementation between immersive (ICEs) and non-immersive (N-ICEs) clinical experiences were assessed using a generalized estimating-equations approach for patient-centered care, IPECP, EBP, HIT, and QI behaviors (P &lt; 0.05). Results: Students implemented more behaviors associated with IPECP (P = 0.002), EBP (P = 0.002), and HIT (P = 0.042) during ICEs than N-ICEs. Students implemented the QI behavior more often during N-ICEs than during ICEs (P = 0.001). Patient-centered care behavior did not differ between clinical experience types. Conclusions: Immersive clinical experiences facilitate increased implementation of behaviors associated with EBP, IPECP, and HIT, while N-ICEs offered increased opportunities for QI behaviors. Program administrators should consider placement of ICEs and N-ICEs in the curriculum that align with students' capability to perform core competency behaviors. Preceptors of both ICEs and N-ICEs should be encouraged to provide students with opportunities to implement all core competencies during their clinical experience."
    mergedFrom:
      - source: eric
        ericId: EJ1397433
    finalDecision: exclude
  - title: "Teacher Identity Development: A Formative Approach to Pre-Service Teachers&apos; Dispositional Development"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 6: About pre-service teachers, not medical/health-professional learners; also no mention of generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are pre-service teachers, not medical/health-professional learners"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
    ericId: EJ1381644
    authors: Young T, Pettit T, Kalbach L, Diercks R, Vargason L
    year: "2022"
    abstract: As teacher shortages increase across the United States, educator preparation programs are tasked with preparing a new generation of teachers equipped to succeed in an increasingly challenging landscape. Along with ensuring that candidates develop deep content knowledge and effective pedagogical skills, national accreditation bodies also require educator preparation programs to monitor and support pre-service teachers' dispositional development. The purpose of this qualitative study was to understand teachers, administrators, clinical experience supervisors, and education faculty members' perspectives of the dispositional qualities that pre-service teachers need to develop to be successful educators. Our goal was first to gain an understanding and then to use it to develop a tool to teach and evaluate the dispositional characteristics pre-service teachers need to successfully transition into their teaching careers. We collected data from 30 participants via an open-ended survey and a subsequent focus group of five participants. Using open and focused coding followed by code mapping, three themes emerged including a commitment to (1) professionalism, (2) relationships, and (3) learning. We used these three themes and their 12 subthemes, along with feedback from our participants, to create a &quot;Teacher Identity Development&quot; tool. With specific indicators of each disposition, this tool can be used by educator preparation programs not only to monitor and assess pre-service teachers' discrete dispositional skill development but, more importantly, to facilitate the development of a teacher identity as candidates journey toward becoming effective educators in our current educational landscape.
    mergedFrom:
      - source: eric
        ericId: EJ1381644
    finalDecision: exclude
  - title: "Self-Directed Learning and the Student Learning Experience in Undergraduate Clinical Science Programs: A Scoping Review"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About self-directed learning, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus is indicated"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    ericId: EJ1484733
    authors: Ashleigh Finn, Caitlin Fitzgibbon, Natalie Fonda, Cameron M. Gosling
    year: "2025"
    abstract: "Health professional organisations are increasingly promoting the use of self-directed learning. Furthermore, the rapidly evolving field of healthcare has meant that there is greater emphasis within tertiary education for students to become self-directed learners and possess the skills to engage in life-long learning. The aim of this scoping review was to explore the drivers that improve the student learning experience, in undergraduate clinical science programs that utilise self-directed learning. The Joanna Briggs Institute Scoping Review Methodology guided this study. The electronic databases MEDLINE, Embase, Emcare, Scopus and ERIC were comprehensively searched in April 2022 and re-run August 2023, for peer-reviewed research articles published in English. The original search was developed in MEDLINE and then adapted to each database. Following the Joanna Briggs Scoping Review methodology, articles were screened first by title and abstract and then by full text. Included articles were assessment for methodological quality. The search strategy yielded 2209 articles for screening. 19 met the inclusion criteria. Five key factors were identified which improve the student learning experience in self-directed learning: (i) curricular elements; (ii) educator influence; (iii) impact of peers; (iv) environment; and (v) clinical placement experiences. There are many curricular, environmental, and external factors which can improve the student learning experience in programs that utilise self-directed learning. Greater understanding of these factors will allow educators within clinical science programs to implement self-directed learning strategies more effectively within curriculum."
    mergedFrom:
      - source: eric
        ericId: EJ1484733
    finalDecision: exclude
  - title: "Cultural Competence with Humility Using Interprofessional Multicultural Learning Activities: Student Perceptions"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about cultural competence and interprofessional learning"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: EJ1441239
    authors: Michele Tilstra, Cara Berg-Carramusa, Tiffany J. Peets, Karen M. Keptner
    year: "2023"
    abstract: "This study examined students' perceptions of interprofessional multicultural learning activities used to develop cultural competence with humility (CCH). Limited research exists on student perceptions of learning activities for CCH in entry-level occupational therapy educational programs. This exploratory, mixed methods study used an anonymous online survey, the &quot;Learning Activities Survey (LAS),&quot; to collect student quantitative ratings and qualitative feedback about CCH learning activities and their experience within the &quot;Counselors and Occupational Therapists Professionally Engaged in the Community (COPE)&quot; program. A deductive thematic approach was used by two investigators for qualitative analyses of &quot;COPE&quot; learning activities by alignment in four a priori CCH constructs: cultural awareness, cultural knowledge, cultural skills, and cultural desire. Consensus was gained through discussion. Nineteen of 29 (65.5%) students completed the survey. The mean score on the LAS for 6 of 12 activities (50%) was at least &quot;Moderately important&quot; (mean [greater than or equal to] 3.0) to student learning. Twelve of 19 students (63%) rated 11 of 12 learning activities as Moderately important (mean [greater than or equal to] 3.0). Qualitatively, cognitive knowledge was the strongest reported CCH learning construct within the &quot;COPE&quot; program with a frequency of 32. The combined quantitative and qualitative responses indicated the learning activities positively influenced students' learning of CCH. This study may inform occupational therapy curricular activities that satisfy accreditation requirements and expectations of the profession to meet the cultural needs of society. It also provides support for revisions to occupational therapy educational program standards to better align with recent literature."
    mergedFrom:
      - source: eric
        ericId: EJ1441239
    finalDecision: exclude
  - title: Long-Term Effect of Large-Scale Simulation Curation and Participation
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about simulation curation and participation"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: EJ1366227
    authors: Eberman L, Neil E, Granger K, Winkelmann Z
    year: "2023"
    abstract: "Context: Previous research has found simulation with debriefing to be helpful in developing self-confidence, improving clinical competence, identifying knowledge deficits, and implementing knowledge into practice in the short term. However, the long-term implications of simulation curation and participation are unknown. Objective: The purpose of this study was to evaluate the long-term effect of large-scale simulation curation and participation as part of an advanced-practice athletic training course. Design: Qualitative phenomenological approach. Setting: Video-conferencing software. Patients or Other Participants: From among 60 potential participants, 11 individuals participated in a long-term, follow-up interview to explore their recollections, perceptions, and subsequent experiences from curating and participation in largescale simulation. Main Outcome Measures: Deidentified transcripts were checked for accuracy and sent for member checking. Subsequently, a 3-person data analysis team used several sequenced rounds of review, aligned with consensual qualitative research, to the analyze data. Trustworthiness was established with member checking, multianalyst triangulation, and auditing. Results: Three domains emerged from the data: emotional reaction, improvements to practice, and the value of debriefing. When the &quot;emotional reaction&quot; domain was described, learners focused on the &quot;reality,&quot; &quot;overload,' and need to &quot;maintain composure&quot; of the encounter. Within their clinical practice, &quot;improvements&quot; were made primarily regarding &quot;mindset,&quot; &quot;teaching,&quot; &quot;collaboration,&quot; &quot;emergency planning,&quot; and &quot;triage.&quot; Finally, learners noted the &quot;value of debriefing&quot; as &quot;humbling,&quot; a time for &quot;realized deficiency,&quot; and a time of &quot;emotional decompression.&quot; Conclusions: Simulation-based learning in advanced-practice clinicians leads to perceived increase in skills such as intra-and interpersonal skills and emotional readiness. Long-term effects of simulation demonstrated that learners could translate these skills into clinical practice even 2 to 3 years post experience. Finally, the use of debriefing is a critical component to both the learner's skill acquisition and translation of knowledge in all simulation-based experiences."
    mergedFrom:
      - source: eric
        ericId: EJ1366227
    finalDecision: exclude
  - title: Meaning Making among Professional Master&apos;s Athletic Training Students
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About meaning making in athletic training students, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI/LLM component is indicated."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    ericId: EJ1352091
    authors: Myers S, Taylor K, Singe S, Barrett J
    year: "2022"
    abstract: "Context: New approaches to education and pedagogy are needed in athletic training education to better support developmental benchmarks that cultivate skills for lifelong learning. Objective: Explore and understand students' perceptions of their education and how those perceptions support their development toward self-authorship through the complexity of their meaning making. Design: Qualitative, grounded theory. Setting: Division I Commission on Accreditation of Athletic Training Education athletic training programs. Patients or Other Participants: Twelve participants (9 female, 3 male; age = 24 ± 2 years) in their second year of athletic training education were recruited for the study. Of the 12 participants, 11 were engaged in an immersive clinical experience. Data Collection and Analysis: Participants engaged in a semistructured video conference interview. Data were analyzed using grounded theory approach to understand the complexity of students' ways of making meaning cognitively, intrapersonally, and interpersonally. Data saturation, peer review, member checks, and theoretical triangulation were used to establish credibility. Results: Participants exhibited meaning making in 2 distinct ways: (1) external guidance and (2) movement toward internal guidance. The categories were further broken down by theme. Placing responsibility for knowledge on authorities, need for step-by-step guidance, and seeking approval marked the themes of external guidance. Building confidence and role identity development marked themes of movement toward internal guidance. Conclusions: Participants who were reliant on external guidance required external authorities for knowledge acquisition and learning. Some participants demonstrated deeper meaning making when discussing their experiences with clinical education."
    mergedFrom:
      - source: eric
        ericId: EJ1352091
    finalDecision: exclude
  - title: Establishing Content Validity of the Clinical Action Cognitive Processing and Critical Thinking Instrument (Clinical Action CPCT) Using a Qualitative Content Analysis Design
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About validating a clinical instrument using qualitative methods, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI/LLM component is indicated."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    ericId: ED665124
    authors: Jennifer Lynn Havenstein
    year: "2024"
    abstract: "Background: Current evaluation methods for nurse anesthesia students primarily assess didactic knowledge while lacking measures to evaluate students' critical thinking and application of clinical knowledge in decision making (Council on Accreditation of Nurse Anesthetists [COA], 2022; Staun et al., 2020). Purpose: This study aimed to develop an assessment tool to capture critical thinking through clinical actions in nurse anesthesia simulations. The tool sought to support educators in assessing students' application of didactic knowledge during a real-time scenario by identifying critical decision points. The study used a high-fidelity simulation to examine how critical thinking integrates with clinical decision making to contribute to a nuanced evaluation resource for nurse anesthesia education. Theoretical Framework/Philosophical Underpinnings: The study's theoretical framework combines constructivist principles with the Paul-Elder critical thinking model and Bloom's Taxonomy that support higher-order cognitive processing. Methods: This study employed a 3-phase, qualitative content analysis approach. Phase 1 involved a non-medical participant reviewing a simulated rapid sequence induction (RSI) video to help capture clinical steps objectively and avoid assumptions tied to tacit knowledge. Phase 2 engaged an expert panel to evaluate the tool's content validity through the Content Validity Index and semi-structured interviews for refinement. In Phase 3, faculty used the tool in simulation debriefings to assess its practical reliability in capturing critical thinking. Results: The results indicated the tool's effectiveness in evaluating critical thinking and decision making under simulated conditions, particularly in RSI scenarios. Thematic analysis emphasized the tool's ability to bridge the gap between didactic knowledge and practical decision making. Statistical analysis showed moderate to perfect interrater reliability (Cohen's kappa = 0.5 to 1.0) and high content validity. Conclusions: Conclusions suggest that while promising, the tool requires further refinement to ensure consistent use across different raters. Recommendations include improving its capacity to capture clinical rationale, increasing the sample size to mitigate variability, and adding customizable sections to reflect diverse anesthesia practices. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED665124
    finalDecision: exclude
  - title: "Getting Real in Interprofessional Clinical Placements: Patient-Centeredness in Student Teams&apos; Collaborative Learning"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Interprofessional clinical placements and collaborative learning, not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    ericId: EJ1385109
    authors: Jensen C, Norbye B, Dahlgren M, Iversen A
    year: "2023"
    abstract: "Collaboration between healthcare providers helps tackle the increasing complexity of healthcare. When learning teamwork, interprofessional students are expected to work patient-centered; recognizing the patient's expertise and partnering with them. Research on interprofessional education (IPE) for undergraduates has illuminated learning outcomes, organization of learning activities, change in attitudes, etc. But, we know little about the interaction between patients and interprofessional student teams. This study aimed to explore how interprofessional student teams and patients interact in interprofessional clinical placements. With a focused ethnographic approach, participant observation and qualitative interviews were conducted in two contexts; a physical and an online arrangement. Central ideas in Goffman's dramaturgy constituted a theoretical lens. A reflexive thematic analysis generated three themes: (1) Preparing safe and comfortable encounters with patients, (2) Including and excluding the patient in the encounter, and (3) Adjusting to the patient's situation. We identified students' intentions of patient-centeredness when preparing encounters, but patients did not always feel included and listened to in encounters. After encountering patients, student teams adjusted their teamwork, by changing the team composition or the planned clinical interventions to better meet the patients' needs. Notably, team-based patient encounters led to a different view of the patient, their health issues, and how to collaborate. Our findings can inform educators of the importance of addressing patient-centered care in interprofessional learning arrangements. Today, clinical interprofessional placements may not exploit the potential for learning about patient-centeredness. A thematization of this, e.g., in supervision in future clinical placements can ensure an enhanced focus on this in interprofessional teamwork."
    mergedFrom:
      - source: eric
        ericId: EJ1385109
    finalDecision: exclude
  - title: "The Roles of the Academic in Health Sciences: Where to Start from a Faculty Development Perspective"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About faculty development roles in health sciences, no generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: EJ1383545
    authors: Van Wyk C, Van Zyl G
    year: "2023"
    abstract: "In an effort to offer a more focussed and competency-based approach to faculty development for newly appointed academics, this study prioritised the roles of the medical teacher, focussing on orientation initiatives. At a research-led university, we used 15 roles of a medical teacher. Health science academics were asked which professional development activities should be available to newly appointed educators with reference to teaching-learning and research development. Quantitative and descriptive research were followed, using a self-administered questionnaire which was completed by 129/256 (50%) health science academics. The top five roles were ranked as follows: Role Model for Students (83.6%); Information Provider in the Classroom (82.8%); Facilitator of Learning (78.1%); Information Provider in the Clinical Setting (75.0%); and Assessor of Student Learning (74.4%). Although research development was ranked important, respondents indicated that it should receive attention at a later stage and not necessarily during orientation. Academics (90.6%) indicated that professional career development should be individualised. Faculty development should encourage gradual development over time, addressing all the roles of the academic, including research. At different institutions, various roles are considered important; and institutional research should be done to guide faculty development."
    mergedFrom:
      - source: eric
        ericId: EJ1383545
    finalDecision: exclude
  - title: Undergraduate Nursing Students&apos; Perceptions of Clinical Readiness Increase with Integration of Simulation Using ATI-Supported Resources
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About simulation with ATI resources, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Simulation-based nursing resource title does not indicate generative AI/LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    ericId: ED634212
    authors: Trill K
    year: "2023"
    abstract: "The purpose of this study was to increase first-semester Historically Black College and University (HBCU) nursing students' simulation exposure by 50% using the ATI education platform and measure student perceptions of readiness for clinical rotations. This study addressed the research gap of simulation in beginning nursing courses and the impact on student perceptions of self-efficacy in future clinical rotations and contributes to the larger body of simulation research by answering the following questions: 1. What are student perceptions regarding simulation and clinical readiness? 2. How does exposure to simulation impact student clinical performance outcomes? 3. To what degree does student exposure to simulation, via ATI, increase student clinical pass rates? This non-equivalent, quasi-experimental, and quantitative two-year study examined first-year nursing students' perceptions of simulation and clinical readiness at a national HBCU. The survey employed referenced the International Nursing Association for Clinical Simulation and Learning (INACSL) and the National League for Nursing Jeffries Simulation Theory guidelines. Cronbach's Alpha was conducted to evaluate the reliability of the survey scale and was found to have a reliability coefficient of [alpha] = 0.88. HBCUs play a lead role in preparing skilled graduates to aid in the nursing shortage and meeting the National Academy of Medicine's (NAM) strategic plan for promoting a diverse healthcare workforce. The existing curriculum for a Nursing Fundamentals course was enhanced by increasing the integration of simulation with ATI-supported resources and implementing the survey tool. SPSS data analysis showed that 87% of students felt that simulation prepared them for clinical experiences and that students would benefit from additional simulation courses. The study suggested that simulation can increase student clinical pass rates, but further investigation on the impact on didactic learning and NCLEX pass rates is needed. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED634212
    finalDecision: exclude
  - title: "Will You Stay or Will You Go?: The Role of Incidental Learning within a Liminal Space in Shaping Student Nurses&apos; Perceptions of Nursing"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About incidental learning and nursing perceptions, not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus is indicated"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    ericId: EJ1460039
    authors: Louise Nagle, Juliet Mc Mahon, Mary Fitzpatrick
    year: "2025"
    abstract: Currently, in Ireland, there is a high rate of attrition among newly qualified student nurses. This paper specifically seeks to provide insights into this issue through an exploration of the experiences and perceptions of clinical placement from a sample of student nurses and preceptors in Ireland through the conceptual lenses of both incidental learning and liminality. We conclude that the power of incidental learning on students transversing a liminal space should not be underestimated and that in our study the surrounding context of support (particularly from preceptors) was critical. Incidental learning around aspects of organization culture and hierarchy that caused dissonance for students emerged as a consistent outcome. So too did time and resource pressure on preceptors due to staffing shortages which, in turn, impacted negatively on workplace learning opportunities beyond routine incidental learning and support available to students to constructively make sense of their experiences. We argue that results from this study can inform not just nurse education but may also give pause for thought to those responsible for professions which have substantial work placement aspects as part of their qualifying route such as apprenticeships.
    mergedFrom:
      - source: eric
        ericId: EJ1460039
    finalDecision: exclude
  - title: Establish Dewey&apos;s Concept of Democracy in TVET in Kenya towards Realizing Sustainable Development. A Pragmatic Approach
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About democracy in TVET in Kenya, no relation to generative AI or medical education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are not medical or health-professional learners"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: EJ1459148
    authors: Alfred Simiyu, Stanley Omuterema, Juma Injendi, Philip Mukonvi
    year: "2024"
    abstract: "UNESCO, World Bank and different countries including Kenya have placed education and training as a central tool towards youth employability and realization of the industrialization agenda. Despite the dying industries and increase in demand for employability skill in training, TVET graduates lack employability skills. The study was tasked to establish how Dewey's concepts of democracy can facilitate TVET in Kenya to realize Sustainable Development. The study sought to answer the following research question: how can Dewey's concepts of democracy facilitate TVET in Kenya towards realizing Sustainable Development. The study used rational research design that adopted philosophical analysis and synthesis methods Both primary and secondary sources were purposefully sampled. Content analysis was adopted to analyze data. The study found out that the change from 8.4.4 education system to Competency Based Education and Training (CBET) in technical education and training was motivated by Dewey's principles of democracy of education and thus full implementation of CBET can lead to Sustainable development in Kenya. However, there exists challenges of unqualified trainers, inadequate resources and poor industrial and training institution linkages to breed skills for the world of work. The study recommends adequate funding of TVET institutions to facilitate TVET in Kenya to have adequate resources for trainee empowerment, coordinate collaborations between TVET institutions and industry in skilling for the world of work and, finally, promote critical skills in problematized learning."
    mergedFrom:
      - source: eric
        ericId: EJ1459148
    finalDecision: exclude
  - title: "Preparing Speech-Language Pathology Graduate Students for Effective Telepractice: Recommended Knowledge and Skills"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About telepractice preparation for SLP students, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: EJ1354346
    authors: Lowman J, Walker J, Houston K
    year: "2022"
    abstract: "Telepractice has been recognized as an efficacious service delivery model for more than 15 years. Yet, most speech-language pathologists found themselves ill-prepared to transition in-person clinical services to a digital format in response to the pandemic. Confusion regarding telepractice regulatory, reimbursement, technology, and delivery requirements stemmed from a lack of formal training, as most practicing clinicians lacked graduate preparation in this arena. Graduate speech-language pathology programs were stymied as to specific telepractice knowledge and skills that should be incorporated into a preparatory program. This article outlines a set of telepractice competencies to guide graduate programs and to promote self-evaluation among practicing clinicians. The specified competencies are clustered into three domains: (1) regulatory, reimbursement, and ethics; (2) telecommunications technology; and (3) clinical telepractice. Examples of knowledge and skills specific to each competency are provided. Infusion of the telepractice competencies within the curriculum of three accredited graduate programs is illustrated."
    mergedFrom:
      - source: eric
        ericId: EJ1354346
    finalDecision: exclude
  - title: "Implementation of a Clinical Decision Support System Using Virtual Reality for Psychiatric Technician: a Feasibility and Acceptability Project"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: VR-based clinical decision support system, not generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Focuses on virtual reality/CDSS, not generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: ED649860
    authors: Constance Kunkler
    year: "2022"
    abstract: "Background Significance: A shortage of clinical sites has made it difficult for the psychiatric technician (PT) to practice skills, such as therapeutic communication. Therapeutic communication is an important component of the PT curriculum. The development of virtual reality (VR) software has the potential to enhance communication skills, addressing students' self-efficacy when communicating with patients suffering from mental illness. Problem: A lack of available clinical sites and COVID-19 has made it challenging for students to obtain hands-on performance skills, including their ability to develop the skill of therapeutic communication, and complete their training and certification. PT students are required to develop skills to assess the status of a patient's mental state and appropriately interact with their clients. Traditionally, this has been accomplished via on-site clinical experiences at health care sites. Methods: This was a feasibility and acceptability project using post-implementation surveys. The Simulation Evaluation Tool-Modified (SET-M) evaluates how well simulation met student learning outcomes, communication skills, and self-perception on ability to communicate. The System Usability Scale (SUS) evaluates the usability of the VR simulation technology. Interventions: A VR scenario featuring a patient with post-traumatic stress disorder (PTSD) was designed, and developed. The platform allowed the student to make critical decisions through the given scenario. Development of the scenario was guided by the &quot;INACSL Standards of Best Practice&quot; of the International Nursing Association for Clinical Simulation and Learning (INACSL) and the &quot;Clinical Practice Guidelines for the Management of PTSD and Acute Stress Disorder&quot; of the Department of Defense (DoD) and U.S. Veterans Affairs (VA). Content for the scenario was approved by content experts for validity. Training on the use of the VR platform was provided for faculty and PT students prior to completing the VR scenario. Evaluation: Data were analyzed to determine if the project aims were achieved. The aims of this project were to: (a) develop a 3D immersive clinical decision scenario using VR platform; (b) assess acceptance of the new technology; and (c) measure communication skills in a 3D VR environment. Results: The results of this project suggested that most students (84%) agreed that the VR software was easy to use. Ninety-six percent of students felt more confident in clinical decision and communication skills with the PTSD patient. The implementation of the VR was feasible and acceptable to students. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED649860
    finalDecision: exclude
  - title: "Clinical Reasoning in Pharmacy: What Do Eye Movements and Verbal Protocols Tell Us about the Processing of a Case Task?"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About eye movements and verbal protocols in clinical reasoning, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Does not indicate generative AI/LLM use"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
    ericId: EJ1416371
    authors: Ilona Södervik, Leena Hanski, Henny P. A. Boshuizen, Nina Katajavuori
    year: "2024"
    abstract: This study investigates pharmacy students' reasoning while solving a case task concerning an acute patient counselling situation in a pharmacy. Participants' (N = 34) reasoning processes were investigated with written tasks utilizing eye-tracking in combination with verbal protocols. The case was presented in three pages, each page being followed by written questions. Eye movements were recorded during case processing. Success in the task required differentiating the relevant information from the task redundant information, and initial activation of several scripts and verification of the most likely one, when additional information became available. 2nd (n = 16) and 3rd (n = 18)-year students' and better and worse succeeding students' processes were compared. The results showed that only a few 2nd-year students solved the case correctly, whereas almost all of the 3rd-year students were successful. Generally, the average total processing times of the case material did not differ between the groups. However, better-succeeding and 3rd-year students processed the very first task-relevant sentences longer, indicating that they were able to focus on relevant information. Differences in the written answers to the 2nd and 3rd question were significant, whereas differences regarding the first question were not. Thus, eye-tracking seems to be able to capture illness script activation during case processing, but other methods are needed to depict the script verification process. Based on the results, pedagogical suggestions for advancing pharmacy education are discussed.
    mergedFrom:
      - source: eric
        ericId: EJ1416371
    finalDecision: exclude
  - title: Insights from a Virtual Clinical Immersion Program
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about a virtual clinical immersion program"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: EJ1461039
    authors: Ellen P. Brennan-Pierce, Susan G. Stanton, Julie A. Dunn
    year: "2025"
    abstract: Clinical immersion programs provide opportunities for biomedical engineering (BME) students to observe the clinical environment and medical devices in use, often leading to the identification of unmet clinical needs. Due to hospital restrictions during the COVID-19 pandemic, in-person clinical immersion programs were generally not possible in summer 2020. Therefore, a 6-week virtual clinical immersion program ran that summer. The program included meetings with guest clinicians and medical device sales representatives twice per week and a group discussion held once per week. The meetings incorporated de-identified videos of medical procedures, clinician commentary of the videos, live video tours of hospital areas, clinician presentations, presentations and demonstrations by medical device sales representatives, and opportunities for discussions with these guests. The meetings were recorded and saved to create a Virtual Clinical Immersion Library. Pre- and post-program student self-assessment surveys showed significant increases in five ABET learning outcomes, two BME learning outcomes, and four program-specific learning outcomes. Post-graduation survey results of alumni from this program showed that all respondents had secured a job in the biomedical/engineering field or postgraduate education less than 3 months after graduation. These alumni are currently employed in the fields of biomedical products, healthcare, research and development, higher education, biotech, consulting, pharmaceutical, and other engineering. Overall, this virtual clinical immersion program filled a gap caused by COVID-19 pandemic closures and provided many benefits to the students that participated. The virtual program also provides an enduring library of video resources for current and future BME students.
    mergedFrom:
      - source: eric
        ericId: EJ1461039
    finalDecision: exclude
  - title: Teaching Resources for the European Open Platform for Prescribing Education (EurOP[superscript 2]E)--A Nominal Group Technique Study
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About prescribing education resources using nominal group technique, not generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: EJ1431715
    authors: Michiel J. Bakkum, Bryan J. Loobeek, Milan C. Richir, Paraskevi Papaioannidou, Robert Likic, Emilio J. Sanz, Thierry Christiaens, João N. Costa, Lorena Dima, Fabrizio de Ponti, Cornelis Kramers, Jeroen van Smeden, Michiel A. van Agtmael, Jelle Tichelaar, EurOP[superscript 2]E Consortium
    year: "2022"
    abstract: The European Open Platform for Prescribing Education (EurOP2E) seeks to improve and harmonize European clinical pharmacology and therapeutics (CPT) education by facilitating international collaboration and sharing problem-based, online, open educational resources. The COVID-19 pandemic forced teachers to switch to virtual modalities, highlighting the need for high-quality online teaching materials. The goal of this study was to establish the online problem-based teaching resources needed to sustain prescribing education during the pandemic and thereafter. A nominal group technique study was conducted with prescribing teachers from 15 European countries. Results were analyzed through thematic analysis. In four meetings, 20 teachers from 15 countries proposed and ranked 35 teaching materials. According to the participants, the most necessary problem-based-online teaching materials related to three overarching themes. Related to learning outcomes for CPT, participants proposed creating prescription scenarios, including materials focusing on background knowledge and resources on personalized medicine and topical/ethical issues such as the prescription's impact on planetary health. Second, related to teaching, they proposed online case discussions, gamification and decision support systems. Finally, in relation to faculty development, they recommend teacher courses, a repository of reusable exam questions and harmonized formularies. Future work will aim to collaboratively produce such materials.
    mergedFrom:
      - source: eric
        ericId: EJ1431715
    finalDecision: exclude
  - title: "Onboarding New Hires in the Clinical Informatics and Practice Support (CIPS) Department with the Tiered Skills Acquisition Model (TSAM): A Program Evaluation"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About onboarding program evaluation, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are not medical or health-professional learners"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: ED665059
    authors: Angela Hackney
    year: "2024"
    abstract: "Introduction: Over the past decade, informaticists expressed dissatisfaction with their training, feeling unprepared for the demands of their role. To address this, we developed the Tiered Skills Acquisition Model (TSAM) program in 2023 to improve the onboarding process of new hires. The goal in evaluating TSAM was to assess its effectiveness and refine training methodologies to build a stronger, more resilient team of informaticists. Methods: A comprehensive literature review of nursing and informatics training methodologies informed the development of the evaluation framework. Both qualitative and quantitative methods were used, including participant surveys and performance metrics as well as a human resources data report. Qualitative data were analyzed for recurring themes, while quantitative data was statistical assessed. Results: The evaluation highlighted TSAM's strong impact, with all 11 preceptees achieving EPIC certification (100% pass rate). No significant link was found between tenure and perceived competence, confidence, or job satisfaction, showing consistent support across all experience levels. TSAM also boosted job and training satisfaction, and those with just one month of tenure felt significantly more prepared than longer-tenured staff. The program was highly cost-effective when looking at staff retention and turnover, saving an estimated $1 to $4.8 million (Gallup, 2019). Conclusions: TSAM demonstrates strong early effectiveness, but future improvements should focus on long-term support and refining the orientation process to maintain its impact over time. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED665059
    finalDecision: exclude
  - title: Exploring Clinical Judgment in an Accelerated Undergraduate Nursing Curriculum
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about clinical judgment in nursing curriculum"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: ED636301
    authors: Farrar M
    year: "2023"
    abstract: "Nurses play a pivotal role in the healthcare world. They play a role in many settings, caring for patients with chronic issues to patients in life threatening situations. These settings call for nurses to be able to analyze patient situations and correctly intervene. This skill is clinical judgment. However, Kavanagh and Swzeda (2018) have shown that new graduate nurses do not demonstrate this skill, potentially causing patient harm. Findings such as this one have led to a reformation of the National Council Licensure Examination for Registered Nurses (NCLEX-RN). The revamped exam called the Next Generation NCLEX (NGN) will focus on exam takers' clinical judgment. However, nurse educators are concerned about students' ability to pass the NGN, particularly since NCLEX-RN pass rates have consistently dropped over the last five years (National Council of State Boards of Nursing, n.d.-c.). This assessment aims to explore the presence of clinical judgment in an accelerated bachelor of nursing (ABSN) program in a Mississippi school of nursing. Clinical judgment must be built in the nursing school curriculum to better patient outcomes and to assist students in passing the NGN. This project seeks to assess the written curriculum (e.g., syllabi, module outcomes, etc.) for evidence of clinical judgment. After this, program faculty will be interviewed to assess where clinical judgment is incorporated that may or may not be reflected in the written curriculum. These aspects of assessment will assist in giving a more complete picture of clinical judgment in the curriculum. Finally, my leadership in my role in this changing, challenging time in nursing education will be discussed. Many opportunities are now available to show strong leadership. Being a scholarly, positive, servant leader is my aim. Servant leadership allows for the encouragement and development of others as I humbly lead the way for opportunities. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED636301
    finalDecision: exclude
  - title: Implementation of Simulation for Critical Thinking Development
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About simulation for critical thinking, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: ED658847
    authors: Deborah Denise Floyd
    year: "2024"
    abstract: "Research has shown that new graduate registered nurses (NGRNs) do not possess the critical thinking skills needed to provide adequate patient care upon entrance into the workforce. Today's healthcare facilities demand that NGRNs possess the critical thinking skills that will allow them to apply their knowledge to patient care situations. Traditional nursing programs focus on time spent in the classroom, with hands-on experience limited to the time spent in clinical site rotations. Clinical experiences provide the environment for students to transfer their knowledge to patient care situations. These clinical hours are the only opportunities for students to apply their classroom knowledge to real-life situations. Clinical experiences are limited by facility staffing, census, and acuity. Implementing computer simulations in the nursing program curriculum provides opportunities for students to apply their knowledge to clinical scenarios, which assists in developing critical thinking skills. This project was designed to demonstrate the effect that added simulations have on the critical thinking development in students, and the objectives were to obtain quantitative and qualitative data to support this demonstration. Quantitative and qualitative data were obtained using 3-point Likert scale surveys and open-ended discussion questions. Participating faculty completed pre-project, post-simulation, and post-project surveys. Results: Faculty survey results indicated that simulation scenarios assist in developing critical thinking skills while providing hands-on experiences in a controlled environment without fear of negative impacts on patients. Student survey results included positive aspects such as immediate instructor feedback and the opportunity to apply classroom knowledge to patient situations. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED658847
    finalDecision: exclude
  - title: Investigation of Learning and Employment Status in the Undergraduate Education of Speech Rehabilitation in Mainland China
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; survey of learning/employment status"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: EJ1478066
    authors: Gao Xiao, Wang Yongli, Liang Zhuoer, Dai Yuan, Zuo Jing, Wan Qin
    year: "2025"
    abstract: "Background: Speech-language pathology (SLP) in China, referred to as 'speech rehabilitation', addresses developmental and acquired communication disorders, including language delays, aphasia, and dysphagia. Despite the population exceeding 85 million individuals with communication disabilities, China faces a critical shortage of certified professionals, with fewer than 2000 practitioners nationwide. Objective: To investigate the learning and employment status of undergraduate speech rehabilitation programs in mainland China, with the aim of identifying systemic gaps and proposing actionable reforms. Methods: An online questionnaire was administered to 67 undergraduate students majoring in hearing and speech rehabilitation, with a focus on theoretical learning, practice, and employment status. Additionally, six department chairmen were surveyed to analyse the training programs, teaching staff, curriculum, and practical experiences. Results: Regarding employment, 54.3% (25/46) of the graduates had monthly incomes between 5000 and 10 000 RMB. The top three fields of employment are hearing, speech, and language rehabilitation. Clinical internships averaged 10.17 months in various settings, including hospitals and schools. In terms of learning, the average number of full-time teaching staff was 8.17, with 28.33% being associate professors and 19.17% professors. There is no uniform standard for compulsory courses, but institutions offer a range of electives. Satisfaction with learning and practice was rated at 3.88 on a 5-point Likert scale (somewhat satisfied), while employment satisfaction was lower at 3.53 (between neutral and somewhat satisfied). Conclusion: Employment in speech rehabilitation is promising but requires an independent certification system. Internship arrangements are reasonable, with high student satisfaction. Educational institutions focus on foundational knowledge and skills but need to improve research skills and faculty quality. Suggestions: (1) The link between theoretical learning and clinical practice should be strengthened by incorporating more case studies and enhancing the quality and diversity of clinical practice. (2) Establish an independent certification system for speech rehabilitation to develop discipline and motivate career advancement. (3) Improve faculty quality through better selection, training, development, and fostering an academic climate and collaboration."
    mergedFrom:
      - source: eric
        ericId: EJ1478066
    finalDecision: exclude
  - title: "&quot;Putting on Our People Lens&quot;: Lived Experience as Pedagogy"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on lived experience as pedagogy"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    ericId: EJ1438144
    authors: Tessa Zoe Milman, Sarah Bream, Celso Delgado, Erin McIntyre, Tristan Scremin
    year: "2024"
    abstract: "In the professional education of mental health practitioners, including occupational therapists, there has been a lack of meaningful inclusion of people labeled with mental illness into curricula, beyond guest speaker panels and presentations. This study explored the experiences of students, faculty, and 'Experts by Experience' within a mental health occupational therapy course that incorporated Experts with lived experience as co-facilitators of weekly fieldwork debriefs. The study utilized focus groups and interviews to understand the experiences of students, mental health faculty, and 'Experts by Experience'. Key themes that emerged from the qualitative data analysis were organized under three broad categories: 1) Students experienced powerful insights, 2) Experts conveyed the complexity of the work, and 3) Faculty grew from co-creating learning experiences with the Experts. This research makes a significant contribution to occupational therapy education by shifting the Expert's role beyond traditional speaker panels or storytelling. This broader responsibility elevated experiential knowledge into the realm of practice in clinical reasoning by shifting the context of the knowledge from storytelling to support practice reasoning. While this created significant learning opportunities for the students, it also did appear to cause emotional risk for the 'Experts by Experience'. It is important that efforts to include 'Experts by Experience' in curriculum also include sources of support and financial remuneration."
    mergedFrom:
      - source: eric
        ericId: EJ1438144
    finalDecision: exclude
  - title: Exploring the Influence of Boundary Objects on the Relationships between Elementary Preservice Teachers and Cooperating Teachers
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 6: About elementary preservice teachers, not medical/health-professional learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are preservice teachers, not health-professional learners"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    ericId: ED654010
    authors: Erin O'Halloran
    year: "2024"
    abstract: "The present bounded case study explored preservice teachers' experiences using a boundary object in the form of a mathematics course assignment during a pre-student teaching clinical experience. The study focused on preservice teachers' perceptions of how utilizing a boundary object influenced the dynamics of their hierarchical relationships with their cooperating teachers, their approaches to teaching mathematics while at their placement, and their abilities to negotiate between their teacher education programs and clinical experiences. The research questions asked: How, if at all, can a mathematics course assignment from a teacher education program serve as an effective boundary object to (1) impact the power dynamics between preservice teachers and their cooperating teachers, (2) limit the default adoption of cooperating teachers' teaching styles in mathematics, and (3) support preservice teachers as they navigate complex negotiations between their teacher education courses and clinical experiences? The study included nine preservice teachers enrolled in an Elementary and Middle Education Program at a large, midwestern, urban university. Data sources included questionnaires, course assignments, interviews, and focus groups. Findings suggest that a boundary object in the form of a mathematics course assignment (a) provided preservice teachers with a greater sense of power in their relationships with their cooperating teachers, (b) facilitated an opportunity for preservice teachers to deviate from their cooperating teachers' teaching styles and implement teaching practices more closely aligned with their own ideal style, (c) provided preservice teachers with a crucial opportunity to practice ambitious mathematics instruction they intended to use in their future classrooms, and (d) helped to bridge preservice teachers coursework and clinical experiences. Given the study findings, it would be beneficial for teacher education programs to incorporate assignments designed to serve as boundary objects to help facilitate valuable and productive field experiences for preservice teachers. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED654010
    finalDecision: exclude
  - title: Impact of a Simulation-Based Skills Program on Nursing Students&apos; Proficiency, Confidence, Enjoyment, and Anxiety
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on simulation-based skills program"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment/training is simulation-based"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    ericId: ED634939
    authors: Dormeus C
    year: "2023"
    abstract: "The purpose of this study was to evaluate and describe how a realistic and evidence-based simulation training on clinical skills impacts nursing students' proficiency, self-assessment of proficiency, confidence, enjoyment, and anxiety. The study explored the impacts this simulation training program had on FSU's nursing students enrolled in 2nd and 3rd semesters. A total of 170 nursing students participated in this study. Different modalities of simulation equipment (task trainers and high fidelity mannikins) were utilized to achieve the learning objectives of each skill taught during this training. The design of this skills training follows the Simulation Design standard of best practice by INACSL (Watts et al., 2021). Questionnaires with Likert scale and open-text responses were used to collect data on the students' self-reported proficiency, confidence, enjoyment, and anxiety. Achievement of skill proficiency was measured by faculty using checklists for each skill taught. I served as both the researcher and the designer of the boot camp skills course. The results of this study indicated that most nursing students who participated in the simulation training achieved proficiency in the clinical skills taught. Results also revealed that self-reported proficiency, confidence, and enjoyment significantly increased for all skills taught in the training program. However, the findings did not reveal a statistically significant difference between the pre and post-simulation means for self-reported anxiety. Introducing a simulation boot camp for skills practice before students start clinical rotations seems like a good adjunct that supplements clinical practice. Students enjoyed the training, receiving feedback, and practicing in a simulated environment resembling reality. However, it remains to be seen whether the skills learned in this boot camp transfer into real-world practice. More research is needed to evaluate the transferability of skills acquired through simulation into clinical practice. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED634939
    finalDecision: exclude
  - title: Examining Admissions Interview Processes as Performance Predictors in Physician Assistant Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about admissions interview processes"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No generative AI or LLM focus."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    ericId: ED647877
    authors: Anne Wildermuth
    year: "2022"
    abstract: "Most physician assistant (PA) programs receive applications in excess of available seats, yet based on the 2019 PAEA Curriculum Report, the all-cause attrition rate is 7.4%. Given the large pool of applicants available to select from, it is valuable to identify students who will succeed in and graduate from the program. Admissions interviews are typically a component of the admissions process for physician assistant candidates. Outcomes of these interviews are high-stakes for the program and applicants, as they are often the final step prior to admissions decisions. Numerous different interview styles exist, yet there is no known research comparing programmatic performance outcomes based on the interview style utilized. The purpose of this study is to evaluate if panel interviews and/or multiple mini-interviews (MMI), two of the most commonly used interview modalities, are predictive of didactic performance, incidence of major professionalism events, and/or on-time program progression in PA education. Two groups of 118 and 125 students admitted using panel interviews and MMI were compared. Each group included students from two academic year cohorts, 2016-2017 for panel and 2018-2019 for MMI. Though sample size was limited by cohort size, sample size calculations based on correlation coefficient demonstrated adequate power. Descriptive statistics confirmed there was no statistically significant differences in the two groups on demographic and academic variables, including age, gender, military service, race/ethnicity, state residency, overall GPA, and science GPA. These similarities in the groups allowed for statistical comparison of the outcome variables. Linear regression, independent t-tests, Fisher's exact rests, and chi-square tests were used to evaluate the relationship between interview score and cumulative medicine exam scores, incidence of major professionalism events, and the ability to progress with the original cohort between the panel interview and MMI groups. There was no statistically significant relationship between interview score and score on a cumulative medicine examination for either group (panel r[superscript 2] = 0.01, MMI r[superscript 2] = 0.0002). Interview score percentage did not significantly predict incidence of major professionalism issues in the panel interview group (n = 6, p = 0.53) or the MMI group (p = 0.89). Only one student representing 0.1% of the group in the MMI cohort had a major professionalism issue compared to six (5%) in the panel interview group; this was a nearly statistically significant difference (p = 0.051). There was no statistically significant difference between the two groups on ability to progress with the original entering cohort; 11 students in the panel interview group (9%) and 7 students in the MMI group (5.6%) failed to progress. Interview score percentage did not predict ability to progress with the cohort in either group (panel p = 0.47, MMI p = 0.71). Elucidating if MMI and/or panel interviews are predictors of academic performance, major professionalism events, and ability to progress with the entering cohort is valuable, as interviews are influential in the admissions process. This research indicates that MMI may be a valuable tool in reducing the number of major professionalism events by screening out applicants who have latent professionalism problems. Additionally, though not statistically significant, there were objectively more students in the MMI group able to progress with their entering cohort than in the panel interview group. It is equally valuable to consider that admissions interviews do not appear to be predictors of isolated academic success, measured on a cumulative medicine examination, and there are other tools in place in many admissions processes to evaluate for academic ability. Considering the high-stakes nature of admissions interviews, the potentially limited predictive ability of admissions interviews on academic ability and possible predictive ability of professionalism event incidence and ability to withstand the heavy course load by progressing with the original entering cohort supports a holistic approach where a variety of experiences, attributes, and metrics are considered in admissions decisions. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED647877
    finalDecision: exclude
  - title: "Experimental Experiential Learning: A Qualitative Single Case Study Exploring the Pedagogical Experiences of High School Career and Technical Education Health Science Teachers during the Phenomenon of the 2019 Coronavirus Disease Pandemic"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; about high school teacher pedagogical experiences during COVID-19"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are high school teachers, not medical/health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    ericId: ED662301
    authors: TeKedra Pierre
    year: "2024"
    abstract: "The COVID-19 pandemic disrupted education systems across the world. High school Career and Technical Education (CTE) health science programs that rely on experiential learning activities for successful outcomes took a severe hit. Experiential learning opportunities such as internships, field experiences, and clinical rotations did not occur, and basic certifications were not earned. When high school students complete CTE health science programs that connect the theory and knowledge obtained in the classroom with practical application gained in the field, they are more likely to pursue an immediate career in healthcare or seek post-secondary education with a focus on medicine. Without a pipeline of students from high school to the healthcare industry or to post-secondary health science programs, the healthcare industry will see a significant decline of entry level medical professionals to support a fast-growing global population. This qualitative single case study explored the lived experience of four CTE health science teachers and their use of experiential learning strategies during the first three months of the COVID-19 pandemic. The study utilized Kolb's Experiential Learning Theory as the framework that guided the data collection and analysis. Semi-structured interviews and artifacts provided an in-depth account of the participants lived experience. As a requirement for the study, all participants had pre-pandemic teaching roles to make a fair comparison of their experience teaching and using experiential learning strategies during the COVID-19 pandemic. The study found that although CTE health science teachers are trained industry professionals, they still need resources and support when transitioning to a virtual classroom, with or without a pandemic. With the expansion of telehealth services, CTE health science teachers need industry support to provide experiential learning sites for both in-person and virtual training activities. An unexpected finding was that CTE health science teachers did not reflect and complete the experiential learning cycle which results in a meaningful experience. This study has implications and recommendations for future CTE programs; specifically, the need for well-trained teachers, strong industry partnerships, and solutions to an evolving virtual healthcare environment. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED662301
    finalDecision: exclude
  - title: How Does a Supervisor&apos;s Training and Experience Relate to a School Counselor&apos;s Development of Knowledge and Skills
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about school counselor supervision, not medical education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are school counselors, not medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: ED654015
    authors: Timothy Conway
    year: "2024"
    abstract: "This study and the literature demonstrate an overwhelming need for clinical supervision, but there is still a paucity of research on school counseling supervision. Clinical supervision in counseling requires at least three aspects: 1) clinical supervisors received formal training; 2) the belief that clinical supervision has strong effectiveness on supervisees' competence; and 3) a mental health professional provides supervision. School counselors continue to reference a need for supervision, but most notably, a gap for clinical supervision. School counselors are mostly supervised by principals, who reference their shortcomings in supervising the clinical skills of school counselors. Without effective supervision, school counselor's skills may erode and negatively impact students. The results of this study indicate that the general supervisors rated themselves significantly higher than school counseling supervisors in administrative, program, and clinical supervision. Findings in this study indicate a mismatch between the literature on clinical supervision, where school counselors highlight that they are not receiving clinical supervision. The results suggest general supervisors may lack the metacognitive skills and awareness to know that they do not have the skills to supervise school counselors effectively. A deceptive reassurance exists of supervisors' perceptions that they effectively supervise school counselors. The statistical significance found in this study based on the type of supervisor and years of experience and their connection with the delivery of specific supervision skills accentuates the results from the literature that further research is needed. Several suggestions for future research will be discussed. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED654015
    finalDecision: exclude
  - title: Deconstructing Curriculum and Assessment of Knowledge and Attitudes Regarding Pain in Pre-Licensure Occupational and Physical Therapy Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About curriculum and assessment of pain knowledge in OT/PT education, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: EJ1417687
    authors: Annie Burke-Doe, Kristen Johnson, Steve Laslovich, Kayla Smith, Christopher Ivey, Anna Edwards, Christopher Ingstad, Ellen Lowe, Jim Mathews, Terri Roberts, Susan MacDermott, Jon Warren
    year: "2024"
    abstract: "The purpose of this study was to: 1) examine existing interprofessional pain management curricula in a DPT and MOT program by mapping pain subject matter to the IASP interprofessional content to determine gaps, vertical and horizontal coherence and integration 2) evaluate pain knowledge and attitudes early and late curriculum within the current pre-licensure Doctor of Physical Therapy (DPT) and Master of Occupational Therapy (MOT) students, and 3) to define changes needed within the existing interprofessional pain management curricula to better prepare students for clinical experiences and professional practice. Pain content was mapped to the IASP core competencies to identify gaps in content and coherence. Students were evaluated on existing pain knowledge and attitudes using the City of Boston's Rehabilitation Professional's Knowledge and Attitudes Survey Regarding Pain (COBS). Two hundred and forty physical and occupational therapy students completed the study. The mean percentage of correct responses of 70.6% +/-25.6% did not significantly change across didactic terms. Curricular mapping can inform planning, design, implementation, and evaluation of the curriculum and alignment with IASP guidelines."
    mergedFrom:
      - source: eric
        ericId: EJ1417687
    finalDecision: exclude
  - title: The Role of Indoor Positioning Analytics in Assessment of Simulation-Based Learning
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About indoor positioning analytics in simulation, not generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: EJ1364060
    authors: Yan L, Martinez-Maldonado R, Zhao L, Dix S, Jaggard H, Wotherspoon R, Li X, Gaševic D
    year: "2023"
    abstract: Simulation-based learning provides students with unique opportunities to develop key procedural and teamwork skills in close-to-authentic physical learning and training environments. Yet, assessing students' performance in such situations can be challenging and mentally exhausting for teachers. Multimodal learning analytics can support the assessment of simulation-based learning by making salient aspects of students' activities visible for evaluation. Although descriptive analytics have been used to study students' motor behaviours in simulation-based learning, their validity and utility for assessing performance remain unclear. This study aims at addressing this knowledge gap by investigating how indoor positioning analytics can be used to generate meaningful insights about students' tasks and collaboration performance in simulation-based learning. We collected and analysed the positioning data of 304 healthcare students, organised in 76 teams, through correlation, predictive and epistemic network analyses. The primary findings were (1) large correlations between students' spatial-procedural behaviours and their group performances; (2) predictive learning analytics that achieved an acceptable level (0.74 AUC) in distinguishing between low-performing and high-performing teams regarding collaboration performance; and (3) epistemic networks that can be used for assessing the behavioural differences across multiple teams. We also present the teachers' qualitative evaluation of the utility of these analytics and implications for supporting formative assessment in simulation-based learning.
    mergedFrom:
      - source: eric
        ericId: EJ1364060
    finalDecision: exclude
  - title: Could Extended Reality Haptics Be Used in Health-Care Education? A Survey of Health-Care Students, Educators and Clinicians
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About extended reality haptics, not about generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: EJ1443219
    authors: Marc Gilbey, Shea Palmer, Louise Moody, Christopher Newton, Natasha Taylor, Ksenija Maravic da Silva
    year: "2024"
    abstract: "Purpose: This study, which is a cross-sectional survey, aims to investigate health-care academics, clinicians and students' perspectives of health-care simulation-based learning (SBL) and extended reality (XR) haptics use within health-care education. Participants' views regarding the application, barriers and facilitators of SBL and XR haptics were explored. Design/methodology/approach: The authors conducted an online international cross-sectional survey of 178 participants. Findings: The survey found high health-care SBL use (n = 97, 55.1%) but low awareness (n = 48, 27.3%) or prior use of XR haptics (n = 14, 7.9%). Participants expressed interest in XR haptic technology emphasising its potential in SBL, particularly for understanding anatomy and physiology, enhancing clinical reasoning and consultation and practical skills. Research limitations/implications: Whilst there was interest in XR haptics, few participants described previous experience of using this technology in SBL. A large percentage of the participants were UK-based. Most participants were from a nurse or physiotherapy professional background. Practical implications: XR haptics is a developing technology for SBL in health-care education. Whilst there was clear interest from survey participants, further research is now required to develop and evaluate the feasibility of using this technology in health-care education. Originality/value: Health-care students, educators and clinicians views on XR haptics have not previously been explored in the development and application of this technology. The findings of this survey will inform the development of XR learning scenarios that will be evaluated for feasibility in health-care SBL."
    mergedFrom:
      - source: eric
        ericId: EJ1443219
    finalDecision: exclude
  - title: Learning to Teach in the Primary School, Fifth Edition. Learning to Teach in the Primary School Series
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 6: About primary school teaching, not medical/health-professional learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Not focused on medical or health-professional learners"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    ericId: ED677164
    authors: Teresa Cremin E, Helen Hendry E, Anna Harrison E
    year: "2025"
    abstract: "How do you become an effective primary school teacher? What do you need to be able to do? What do you need to know? Flexible, effective and creative primary school teachers require subject knowledge, an understanding of their pupils and how they learn, a range of strategies for managing behaviour and organising environments for learning, and the ability to respond to dynamic classroom situations. The fifth edition of this bestselling textbook has been fully updated with the latest research and initiatives in the field, as well as the most recent curriculum and policy changes across the UK. It features two new co-editors and 13 new chapters and enhanced accessibility throughout. New or completely rewritten chapters have been included on: (1) Reading curriculum; (2) Writing curriculum; (3) Maths curriculum; (4) Science curriculum; (5) Arts-enriched curriculum; (6) Humanities curriculum; (7) Adaptive teaching; (8) Education and wellbeing; (9) Education for sustainability; and (10) Applying for jobs and preparing to be an ECT. A selection of extra tasks have been woven throughout, with an emphasis on innovative, reflective practice, and new 'vivid examples' bring each chapter's argument to life in a classroom context. Providing a comprehensive but accessible introduction to teaching and learning in the primary school, covering everything a trainee needs to know in order to gain QTS, this accessible and engaging textbook is essential reading for all students training to be primary school teachers. This textbook is supported by a free companion website with additional resources for instructors and students and an accompanying series of books on Teaching Creatively across the curriculum."
    mergedFrom:
      - source: eric
        ericId: ED677164
    finalDecision: exclude
  - title: Participants&apos; Perceptions of Video-Enhanced Dialogic Assessment as Reflective Practice in International Teacher Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 6: Not about medical/health-professional learners; focuses on international teacher education. Also criterion 1: no generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are not medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    ericId: EJ1428039
    authors: Elizabeth Hidson
    year: "2024"
    abstract: Initial teacher education in England is characterised by lesson observation and assessment of teaching evidence against criterion-referenced teachers' standards. In UK-based international teacher training, these situated practices are more challenging because of staff and trainees working in different time zones. In 2020, COVID-19 travel restrictions prevented assessors visiting trainees in international schools. Assessment therefore moved to a video-enhanced dialogic assessment process (VEDA), incorporating video-enhanced lesson observation and feedback, along with a dialogic online interview and evaluation of a digital portfolio of evidence. A first cycle of action research carried out in parallel to the change in provision gathered participants' perceptions from recordings. These are themed around the way that technology supported reflection during assessment, and the holistic nature of this approach. Findings suggest that video-enhanced dialogic assessment allows for a deep and authentic assessment experience, and is a better fit as a reflective pedagogy for assessing teachers than the pre-pandemic approach.
    mergedFrom:
      - source: eric
        ericId: EJ1428039
    finalDecision: exclude
  - title: "Student Psychosocial Well-Being and Burnout during Level II Fieldwork: An Explanatory Mixed Methods Analysis"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About student well-being and burnout during fieldwork, no generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: EJ1459970
    authors: Grace V. Perry, Katherine F. Varughese, Yongyue Qi, Cary Moore, Vanessa D. Jewell
    year: "2025"
    abstract: "The purpose of this study was to explore the influence fieldwork educator support has on stress, burnout, and self-efficacy, and examine how both students and fieldwork educators support student psychosocial well-being and protective factors during Level II fieldwork (FWII). An explanatory sequential mixed method design was used, including a student cross-sectional survey (n=129) followed by one student (n=5) and two fieldwork educator focus groups (n=12). Outcome measures utilized for the cross-sectional survey included the Perceived Stress Scale, self-developed Perceived Fieldwork Educator Support Questionnaire, Oldenburg Burnout Inventory, and New General Self-Efficacy Scale. Results showed that statistically significant correlations existed among perceived fieldwork educator support and the following: stress (r=-0.443, p&lt;0.01), self-efficacy (r=0.221, p&lt;0.05), and burnout (r=-0.468, p&lt;0.01). Findings of this study demonstrate that greater perceived fieldwork educator support was linked to lower stress, lower burnout, and greater self-efficacy. Themes identified included factors that influenced fieldwork student well-being during FWII; roles perceptions and expectations affected experiential education success; communication was key for promoting student well-being and collaborative relationships; and opportunities to improve experiential education. Protective factors preventing the development of stress and burnout for students included high self-efficacy and perceived fieldwork educator support. Results from this study can assist fieldwork coordinators and fieldwork sites with program development that promotes student psychosocial well-being and supportive relationship building between students and fieldwork educators during FWII."
    mergedFrom:
      - source: eric
        ericId: EJ1459970
    finalDecision: exclude
  - title: Critical, Project-Based Clinical Experiences as Sustainable &quot;Third Space&quot; Reforms
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on clinical education reforms"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    ericId: EJ1361410
    authors: Kristien Zenkov, Kristine E. Pytash
    year: "2022"
    abstract: "In this article, the authors--university faculty members working across college and school contexts in the United States--detail how teacher educators might address two challenges facing the teacher education field: (a) the long-standing critique of traditional teacher preparation in the United States as an isolated, questionably relevant ivory tower endeavor and (b) the community and political concerns and tensions that teachers and students are facing outside of, and increasingly within, school. They speculate that university-based teacher educators might attend to both issues by considering these as elements of the documented existential challenges facing the teacher education field and responding to these concerns by appealing to third space notions, Teacher Prep 3.0, and current clinical practice innovations with classroom-level interventions. In this conceptual article, the authors describe the theoretical bases for this reframing and detail the elements of critical, projectbased (CPB) clinical experience structures. Although, over the past decade, a range of scholars have explored CPB experiences, the authors propose that this clinical experience model might uniquely answer these teacher education critiques and community and political concerns. They illustrate their CPB efforts and discuss the issues that support and impede the implementation of such structures and roles, reflecting on how these mechanisms offer preservice teachers, veteran classroom teachers, school-based teacher educators, and university-based teacher educators a model for collaboratively, authentically engaging with some of the civic discourse concerns facing our democracy across school, university, and community contexts."
    mergedFrom:
      - source: eric
        ericId: EJ1361410
    finalDecision: exclude
  - title: "Trends in the Acquisition of Clinical Reasoning in the Assessment of Speech Sound Disorders: Using the Script Concordance Test"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on script concordance testing for clinical reasoning"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    ericId: EJ1484102
    authors: Ting-Fang Chan, Li-Li Yeh
    year: "2025"
    abstract: "Purpose: Clinical reasoning is essential for speech-language pathologists (SLPs) when addressing ill-defined questions in various clinical settings. This study focuses on the acquisition of clinical reasoning skills in SLP students, particularly their evolution with clinical experience. To achieve this, the study developed and validated the first cloud-based script concordance test (SCT) tailored for assessing clinical reasoning skills in SSD diagnosis. Methods: An expert panel of 15 SLPs (average experience of 20.3 years) helped develop and score the SCT, which was administered to 51 undergraduate students (22 sophomores, 18 juniors, and 11 seniors). Statistical analyses examined the predictive role and trend of clinical experience in three dimensions (utility, interpretation, and diagnosis). Results: A significant difference in mean SCT scores was found between the expert panel and student groups (p &lt; 0.05). Trend analysis showed a significant impact of clinical experience on SCT performance across all dimensions (all Fs &gt; 9.91, p &lt; 0.001), with greater experience linked to better reasoning skills. Low-scoring items highlighted challenges with stimulability testing, indicating a lack of clinical consensus. Conclusions: This study demonstrates that clinical reasoning skills in SSD assessment become more refined with accumulated experience. The SCT developed effectively differentiates reasoning abilities between experts and students, offering a valuable tool for advancing clinical decision-making in speech-language pathology. These findings have practical implications, empowering SLP educators to design effective training programs and preparing students for the challenges they may face in clinical practice."
    mergedFrom:
      - source: eric
        ericId: EJ1484102
    finalDecision: exclude
  - title: "Developing Cultural Competence: Exploring Students&apos; Understanding of Cultural Competence in an Entry-Level Physical Therapy Program"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; focuses on cultural competence in physical therapy education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: ED657150
    authors: John Jonghyun Lee
    year: "2024"
    abstract: "In healthcare, cultural competence is an essential skill identified as one of the most modifiable factors in resolving health disparities. Through the lens of multiple frameworks, including the Process of Cultural Competence in the Delivery of Healthcare Services, multicultural education, and reflective practitioner, this study seeks to explore how students understand the concept of cultural competence in an entry-level physical therapy program and where in the curriculum they locate their learning experiences of the idea. Third-year students from a single entry-level physical therapy program are recruited to participate in individual interviews, online questionnaires, and focus group discussions. The study used a qualitative case study approach with a constructivist grounded theory data collection and data analysis methodology. The cyclical data collection and analysis methods with an initial, focused, and theoretical coding scheme were adopted to generate themes to answer the research questions. Drawing on participants' understanding of cultural competence, this study illustrates that, although students exhibit analogous motivations and definitions of cultural competence, there exists a variation in their skills and knowledge pertinent to cultural competence. Moreover, the results advocate for continuous integration of classroom learning with practical clinical experiences, complemented by intentional learning experiences, to enhance students' cultural competence. The findings implicate the critical need for curricular revisions that emphasize cultural competence, ensuring students are adequately equipped to navigate society's rapidly diversifying demographic landscape. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED657150
    finalDecision: exclude
  - title: Nursing Students&apos; Perceptions of Using an Educational Modality of Online Clinical Learning
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About online clinical learning modality, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: ED648845
    authors: Esther Edgal
    year: "2022"
    abstract: "Many clinical nursing courses have changed to an online format due to the COVID-19 pandemic, causing challenges for both students and faculty in supporting students' academic needs. The purpose of this basic qualitative study was to investigate nursing students' perceptions of how the instruction in online modality clinical nursing courses supported their academic needs and faculty behaviors as well as gather the students' suggestions of what would improve online nursing education. The conceptual framework was adapted from Kearsley and Shneiderman's theory of engagement that offers instructional best practices for the online environment. The research questions related to nursing students' perceptions of the instruction from online clinical nursing courses and faculty behaviors as well as their suggestions of how to improve online clinical nursing education. A sample of 19 student nurses who took at least one semester of an online course and registered to continue the online clinical modality were interviewed face-to-face and using interactive video conferencing. Data collected from the interviews were analyzed using open and axial coding, resulting in the following themes: desire for professor engagement, peer interaction, barriers in online learning, and engagement with instructional materials. These emergent themes were used to develop a 3-day professional development training program that improves online clinical learning by increasing online clinical nursing educators' awareness of digital resources and understanding of how to create engaging online learning environments which improves their quality of instruction. The results of my study could lead to positive social change because increasing the quality of online clinical nursing instruction leads to nursing student retention and successful nursing student graduates who go on to provide care to their communities. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED648845
    finalDecision: exclude
  - title: Behavioral Assessment of Soft Skill Development in a Highly Structured Pre-Health Biology Course for Undergraduates
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About behavioral assessment of soft skills, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are not medical or health-professional learners"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    ericId: EJ1437942
    authors: Joanne Tran, Leo Meller, Vy Le, Jasmine Tam, Andrea Nicholas
    year: "2024"
    abstract: In this study, we assessed a highly structured, yearlong, case-based course designed for undergraduate pre-health students. We incorporated both content learning assessments and developed a novel method called Multiple Mini Exams for assessing course impact on the development of skills that professional schools often seek in pre-health students, focusing on students' abilities to collaborate with others, display bedside manners, synthesize patient case details, appropriately use scientific and medical language, and effectively attain patients' medical histories. This novel method utilized a rubric based on desired medical student skills to score videotaped behaviors and interactions of students role playing as doctors in a hypothetical patient case study scenario. Overall, our findings demonstrate that a highly structured course, incorporating weekly student performance and presentation of patient cases encompassing history taking, diagnosis, and treatment, can result in content learning, as well as improve desired skills specific for success in medical fields.
    mergedFrom:
      - source: eric
        ericId: EJ1437942
    finalDecision: exclude
  - title: "Pre-Service Music Educators&apos; Self-Efficacy towards Teaching in Secondary Instrumental Music Settings: An Action Research Study"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 6: About music educators, not medical or health-professional learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are music educators, not medical/health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    ericId: EJ1473434
    authors: Danni Gilbert
    year: "2025"
    abstract: The purpose of this exploratory, quantitative action research study was to determine whether methods coursework and practicum experiences influence pre-service music educators' self-efficacy towards teaching in secondary instrumental music settings. Junior music education students enrolled in an instrumental music methods course (n = 17) and in instrumental clinical experiences (n = 11) responded to the Preservice Music Teacher Self-Efficacy Survey (PMTSES) in January and again in May. Results indicated a statistically significant increase in beliefs about self-efficacy between the beginning of the semester (M = 2.93, SD = 0.33) and the conclusion of the semester (M = 3.35, SD = 0.37). However, the difference in means based upon practicum setting was nonsignificant. Therefore, it appears that completing methods coursework may be beneficial for pre-service music educators in improving self-efficacy, while the impact of practicum experiences is unclear in this setting.
    mergedFrom:
      - source: eric
        ericId: EJ1473434
    finalDecision: exclude
  - title: "Improving Nursing Education through an AI-Enhanced Mixed Reality Training Platform: Development and Pilot Evaluation"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Mixed reality training platform indicates simulation-based assessment context."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: AI-enhanced mixed reality platform for nursing students with simulated digital patients; assessment conducted entirely in a simulated environment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment/training occurs in a mixed-reality simulated environment without a workplace-based component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    ericId: EJ1477731
    authors: Kamelia Sepanloo, Daniel Shevelev, Md Tariqul Islam, Young-Jun Son, Shravan Aras, Janine E. Hinton
    year: "2025"
    abstract: Integrating Mixed Reality (MR) into nursing education and professional practice has recently captured significant interest as a transformative approach. This paper presents a comprehensive exploration and practical insights into designing and implementing an advanced MR training platform to provide nursing students with immersive experiences across various patient care scenarios. Further enhancing the platform's utility is the incorporation of a unique conversational artificial intelligence (AI) module. This innovation breathes life into digital patients, enabling dynamic and realistic interactions that challenge nursing students to develop clinical reasoning skills in a controlled yet flexible MR environment. The AI's capacity to understand and contextually react to the learner's' verbal and behavioral inputs simulates authentic patient interactions. A total of 7 nursing students and 3 nursing faculty engaged in the pilot study, which served as a proving ground for the MR training system's effectiveness. The study involved in-depth analysis, employing performance metrics, and evaluating situational awareness alongside cognitive workload using NASA Task Load Index (TLX) and learner's thought verbalizations. The primary objective was to create a system that enhances nursing students' competencies and readiness for clinical healthcare practice. This system can potentially elevate the preparedness of new graduate nurses by providing a rich, interactive learning environment that mirrors the complexity of real-life clinical settings.
    mergedFrom:
      - source: eric
        ericId: EJ1477731
    finalDecision: exclude
  - title: "Perceived Cultural Responsiveness in Clinical Practice: A Qualitative Study among Speech-Language Pathologist Graduates"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About cultural responsiveness in SLP practice, no generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    ericId: ED665193
    authors: Lizzette Roman-Marrero
    year: "2024"
    abstract: "The purpose of this phenomenological study was to investigate the perceived cultural responsiveness in clinical practice among SLP graduates in South Florida. A total of eight Speech-Language Pathologists (ages 28-42 years old) from South Florida who graduated between 2021 and 2023 were interviewed using a semi-structured interview protocol. The intent was to investigate the specific components and strategies SLP graduate programs provide to equip students with the skills needed to work effectively with culturally diverse populations (RQ1), academic training and hands-on clinical experiences that shape the SLPs' ability to serve culturally diverse populations (RQ2), and the perceived challenges newly employed SLPs faced regarding culturally responsive care (RQ3). Participants' responses to the interview questions were coded, generating themes and subthemes. The major theme identified for RQ1 was multicultural education and practical experience in SLP training, the major theme identified for RQ2 was the impact of multicultural education and practical experience in SLP training, and the major theme for RQ3 was cultural and linguistic limitations in SLP Practice. The results demonstrated that while participants reported benefiting from multicultural courses and clinical placements, they faced challenges applying this knowledge to diverse real-world situations. Language barriers, limited access to resources, and unfamiliar cultural contexts were common obstacles, underscoring the need for ongoing support and learning. The results of the study can inform future training of SLPS graduates including the integration of hands-on, culturally immersive experiences into their curricula. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]"
    mergedFrom:
      - source: eric
        ericId: ED665193
    finalDecision: exclude
  - title: "Applying Clinical Reasoning Theories to Kinesiology: Advancing the Education of Future Healthcare Professionals"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about clinical reasoning theories in kinesiology"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: EJ1364238
    authors: Ataman R, Ibey R
    year: "2022"
    abstract: "In Canada, kinesiology academic units are undergoing change. The growth of kinesiology as a health profession is reflected in increased clinical course offerings to train student clinicians. These courses require clinical educators, however, not all programs have clinical staff and faculty with the required knowledge or experience. In particular, prospective clinical educators may lack fundamental knowledge of clinical reasoning theory. Clinical reasoning is defined as problem solving and judgment in relation to patient or client assessment, diagnosis, treatment and management, and is considered to be essential for the development of autonomous health professionals. Clinical reasoning theory is applied in an educational context to streamline and enhance student clinician development. Unlike in the education of other health professionals, there has been no literature detailing the application of clinical reasoning theory to the kinesiology educational context. Thus, the objectives of this paper are to: (1) introduce the theories in clinical reasoning relevant to kinesiology, (2) link these theories to practical educational strategies and, (3) connect these strategies directly to the teaching of clinical kinesiology subjects."
    mergedFrom:
      - source: eric
        ericId: EJ1364238
    finalDecision: exclude
  - title: Cross-Cultural Validation of the Counseling Competencies Scale Revised (CCS-R)--Korean Version
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About cross-cultural validation of a counseling competencies scale, no mention of generative AI or LLMs"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    ericId: EJ1428520
    authors: Hang Jo, Nayoung Kim
    year: "2024"
    abstract: The purpose of the current study is to validate the Counseling Competencies Scale Revised (CCS-R). We used convenience sampling to recruit counselors and counselors-in-training (N = 156). Specifically, we had 130 female (83.3%) and 26 male (16.7%) participants, and the mean age was 33.16 (SD = 11.18), ranging from 20 to 62 years old. We used correlational analyses, ANOVA, and confirmatory factor analysis to understand the characteristics of the sample and assess the psychometrics of the CCS-R-K. Results showed the validity and reliability of the data, including the acceptable model fits and high alpha reliability coefficient values. We also found that the scores of the CCS-R-K were positively related to participants' years of clinical experience and educational levels. The CCS-R-K can be used as a measurement tool to objectively and comprehensively evaluate Korean counselors' competency in counselor education, contributing to cultivating counselors with the professional competencies required by our society.
    mergedFrom:
      - source: eric
        ericId: EJ1428520
    finalDecision: exclude
  - title: "Solidarity in Time of a Pandemic: a Qualitative Study of Student Experiences of Power-Sharing and Collaboration during COVID-19"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about student experiences during COVID-19"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    ericId: EJ1403351
    authors: Barros-Lane L, Smith D, McCarty D, Perez S, Sirrianni L
    year: "2023"
    abstract: "Using an anti-oppressive lens, this Bachelor of Social Work (BSW) program intentionally introduced power sharing between students and faculty during the first semester of the COVID-19 pandemic. This study qualitatively examined the students' response to this interpersonal intervention and found: (1) This experience helped all participants appreciate each person's unique contribution, which was helpful in building their confidence in themselves and each other as developing social work professionals, (2) Students experienced a greater sense of closeness with all members of the group, and (3) Working together as equals helped students feel supported as they faced difficulties in their personal lives. Anti-oppressive pedagogical strategies focusing on the nature of the relationship may provide high levels of support and empowerment for students."
    mergedFrom:
      - source: eric
        ericId: EJ1403351
    finalDecision: exclude
  - title: "Trainees&apos; Experienced Vocational Education and Training Quality: Formative Assessment of Scale Development and Validation"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 6: Vocational education and training, not medical/health-professional learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    ericId: EJ1476200
    authors: Garimidi Siva Sree, Porika Ramlal, Radha Yadav
    year: "2025"
    abstract: "Purpose: The study aims to propose and validate a multidimensional measure of trainees' experienced vocational education and training quality (TEVETQ) in an Indian context. Design/methodology/approach: This study explains a deductive approach in conceptualizing TEVETQ as a second-order, reflective-formative construct and its subsequent validation using SmartPLS version 3. The antecedent construct was assessed using an embedded two-stage method and finalized with five constructs, i.e. training facilities and equipment, trainer quality, placement counseling services, industrial exposure and extracurricular activities. The researcher analyzed the measurement invariance of composite models to ensure the power of statistical tests. The study findings conceptualize vocational education training as a formative construct in industries and can be generalized and extended to other sectors of the economy as well. Findings: The current study developed a conceptually congruent and empirically validated multidimensional measure of vocational education and training (VET) quality. Further, the study has found industrial exposure as a significant formative indicator for TEVETQ. With the novel TEVETQ construct, the study on the antecedents and consequences of VET quality has substantial scope to progress. Research limitations/implications: The current study is cross-sectional, and the data pertain to VET students in the Indian context. This limits the generalizability of the findings of this research. Practical implications: The TEVETQ construct, with its integral dimensions, imparts an appreciation of how the trainees define VET quality. The authors propose TEVETQ as an additional key metric in evaluating VET quality. Originality/value: Although extant literature on VET quality collectively concurs on its multidimensional nature, there is a limitation in validated measurements of VET quality. Hence, the current study addresses the research gaps by conceptualizing and validating a multidimensional measure of TEVETQ."
    mergedFrom:
      - source: eric
        ericId: EJ1476200
    finalDecision: exclude
  - title: Competency Experience-Based Training (CEBT) Model with Ubiquitous Community of Practice (U-CoP) to Enhance Transformation Digital Supervisor
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; digital training model"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: No indication of generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: EJ1362546
    authors: Prasongmanee C, Wannapiroon P, Nilsook P
    year: "2022"
    abstract: "Research subject Competency Experience-Based Training (CEBT) Model with Ubiquitous Community of Practice (U-CoP) to Enhance Transformation Digital Supervisor. This research aims to evaluate the digital supervisor competency trained with the Competency Experience-Based Training (CEBT) Model with Ubiquitous Community of Practice (U-CoP). The researcher has divided the research process into 3 steps as follows: Step 1: To develop the Competency Experience-Based Training (CEBT) Model with Ubiquitous Community of Practice (U-CoP) to enhance transformation digital supervisor. Step 2: To develop the Competency Experience-Based Training course with Ubiquitous Community of Practice (U-CoP) to enhance transformation digital supervisor. Step 3: Evaluate the digital supervisor competency trained with the Competency Experience-Based Training (CEBT) Model with Ubiquitous Community of Practice (U-CoP). The results of the research were as follows: 1) the Competency Experience-Based Training course with Ubiquitous Community of Practice (U-CoP) to enhance transformation digital supervisor, it consists of 3 main processes and 10 sub-steps. Ubiquitous community of practice consists of 2 parts. 1) Community of practice, and Ubiquitous technology 2) Competency Experience-Based Training course with Ubiquitous Community of Practice (U-CoP) to enhance transformation digital supervisor consisting of 6 components. The results of the evaluation of digital supervisor competency in training participants with a model developed using pre-training and post-training surveys showed that trainees scored higher than their pre-training digital supervisor competency at a statistically significant 0.01 level."
    mergedFrom:
      - source: eric
        ericId: EJ1362546
    finalDecision: exclude
  - title: "Physical Education in Early Childhood: Movement and Development from 3 to 6 Years"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 6: Not medical/health-professional learners; early childhood physical education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are children, not medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    ericId: ED677122
    authors: Markel Rico-González E
    year: "2025"
    abstract: This book provides a scientific foundation for the design of physical education and movement education in early childhood, focusing on the key development stage between 3 and 6 years of age. Drawing on cutting-edge contemporary research, the book examines physical education as a process for promoting holistic development and well-being of children from 3 to 6 years old and introduces developmental milestones across motor, cognitive, social, and emotional competencies. It examines the evidence base in core areas including teaching strategies for each of the main competencies, health, and inclusion, and explores advanced topics in early years education as applied to movement, including the use of technology and artificial intelligence in early childhood. This book is important for teacher educators, trainee teachers, and in-service practitioners with an interest in early years education, in general, and in physical education in early childhood, in particular.
    mergedFrom:
      - source: eric
        ericId: ED677122
    finalDecision: exclude
  - title: Exploring Perceptions of Alternative Assessment and Grading in Graduate Anatomy Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: No mention of generative AI or LLMs; about alternative assessment in anatomy education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    ericId: EJ1459928
    authors: Emily L. Dietrich, Sean C. McWatt
    year: "2025"
    abstract: "Alternative assessment approaches, such as pass/fail and feedback-based designs, aim to reduce academic stress and foster deeper learning. Few studies have examined feedback-based evaluation in formative settings in medical education, but none among graduate anatomy students. This exploratory study investigated the impact of feedback-based versus quiz-based assessments on graduate students' academic stress, motivation, and perceived learning quality in an anatomy course. Nine students were interviewed to discern perceptions of the impact of the different assessment types. Four instructors were interviewed to establish the philosophies behind their chosen assessment style, and their responses were compared to the perceptions of their students. Inductive thematic analyses of student interviews yielded multiple themes regarding considerations for the influence of assessment on academic-related anxiety and motivation: (1) communicating clear goals and expectations, (2) instructor approachability and individualized assessment, and (3) alignment between perceived effort and outcomes. Faculty interviewees demonstrated intentions to (1) promote non-technical skills, (2) provide authentic experiences, and (3) emphasize individualized assessment. However, there was some misalignment between instructors' goals and students' experiences, exacerbating students' stress and reducing their motivation. Feedback-based assessments were preferred for their perceived individualized nature and facilitation of deeper learning. Findings indicated that implementing feedback-based approaches, while ensuring constructive alignment, could reduce stress for graduate anatomy students. Furthermore, clear communication and instructor approachability can cultivate stronger teaching presence, which may enhance motivation, engagement, and the quality of learning outcomes. Implementing formative feedback-based assessments may, therefore, be an effective strategy to reduce stress and improve learning experiences for graduate-level anatomy students."
    mergedFrom:
      - source: eric
        ericId: EJ1459928
    finalDecision: exclude
  - title: "ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Technical AI benchmark for X-ray VQA, not about assessment of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Benchmark study is not focused on medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
    arxivId: "2506.04353"
    authors: Pal A, Lee J, Zhang X, Sankarasubbu M, Roh S, Kim W, Lee M, Rajpurkar P
    year: "2025"
    abstract: "We present ReXVQA, the largest and most comprehensive benchmark for visual question answering (VQA) in chest radiology, comprising approximately 696,000 questions paired with 160,000 chest X-rays studies across training, validation, and test sets. Unlike prior efforts that rely heavily on template based queries, ReXVQA introduces a diverse and clinically authentic task suite reflecting five core radiological reasoning skills: presence assessment, location analysis, negation detection, differential diagnosis, and geometric reasoning. We evaluate eight state-of-the-art multimodal large language models, including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge the gap between AI performance and clinical expertise, we conducted a comprehensive human reader study involving 3 radiology residents on 200 randomly sampled cases. Our evaluation demonstrates that MedGemma achieved superior performance (83.84% accuracy) compared to human readers (best radiology resident: 77.27%), representing a significant milestone where AI performance exceeds expert human evaluation on chest X-ray interpretation. The reader study reveals distinct performance patterns between AI models and human experts, with strong inter-reader agreement among radiologists while showing more variable agreement patterns between human readers and AI models. ReXVQA establishes a new standard for evaluating generalist radiological AI systems, offering public leaderboards, fine-grained evaluation splits, structured explanations, and category-level breakdowns. This benchmark lays the foundation for next-generation AI systems capable of mimicking expert-level clinical reasoning beyond narrow pathology classification. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA"
    mergedFrom:
      - source: arxiv
        arxivId: "2506.04353"
    finalDecision: exclude
  - title: "Universal Abstraction: Harnessing Frontier Models to Structure Real-World Data at Scale"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About structuring real-world data with frontier models, not about assessment/feedback in medical education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    arxivId: "2502.00943"
    authors: Wong C, Preston S, Liu Q, Gero Z, Bagga J, Zhang S, Jain S, Zhao T, Gu Y, Xu Y, Kiblawi S, Yegnasubramanian S, Botsis T, Borja M, Ahumada L, Murray J, Gan G, Weerasinghe R, Young K, Leidner R, Piening B, Bifulco C, Naumann T, Wei M, Poon H
    year: "2025"
    abstract: A significant fraction of real-world patient information resides in unstructured clinical text. Medical abstraction extracts and normalizes key structured attributes from free-text clinical notes, which is the prerequisite for a variety of important downstream applications, including registry curation, clinical trial operations, and real-world evidence generation. Prior medical abstraction methods typically resort to building attribute-specific models, each of which requires extensive manual effort such as rule creation or supervised label annotation for the individual attribute, thus limiting scalability. In this paper, we show that existing frontier models already possess the universal abstraction capability for scaling medical abstraction to a wide range of clinical attributes. We present UniMedAbstractor (UMA), a unifying framework for zero-shot medical abstraction with a modular, customizable prompt template and the selection of any frontier large language models. Given a new attribute for abstraction, users only need to conduct lightweight prompt adaptation in UMA to adjust the specification in natural languages. Compared to traditional methods, UMA eliminates the need for attribute-specific training labels or handcrafted rules, thus substantially reducing the development time and cost. We conducted a comprehensive evaluation of UMA in oncology using a wide range of marquee attributes representing the cancer patient journey. These include relatively simple attributes typically specified within a single clinical note (e.g. performance status), as well as complex attributes requiring sophisticated reasoning across multiple notes at various time points (e.g. tumor staging). Based on a single frontier model such as GPT-4o, UMA matched or even exceeded the performance of state-of-the-art attribute-specific methods, each of which was tailored to the individual attribute.
    mergedFrom:
      - source: arxiv
        arxivId: "2502.00943"
    finalDecision: exclude
  - title: "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Biomedical named entity recognition tool, not about assessment/feedback"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Conventional biomedical NER modeling, not generative AI for WBA assessment/feedback."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    arxivId: "2508.01630"
    authors: Panahi M
    year: "2025"
    abstract: "Named-entity recognition (NER) is fundamental to extracting structured information from the >80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act."
    mergedFrom:
      - source: arxiv
        arxivId: "2508.01630"
    finalDecision: exclude
  - title: "ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About creating a benchmark dataset for LLM clinical reasoning, not about assessment or feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    arxivId: "2505.22919"
    authors: Mehandru N, Golchini N, Bamman D, Zack T, Molina M, Alaa A
    year: "2025"
    abstract: "Large language models (LLMs) have been extensively evaluated on medical question answering tasks based on licensing exams. However, real-world evaluations often depend on costly human annotators, and existing benchmarks tend to focus on isolated tasks that rarely capture the clinical reasoning or full workflow underlying medical decisions. In this paper, we introduce ER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and decision-making in the emergency room (ER)--a high-stakes setting where clinicians make rapid, consequential decisions across diverse patient presentations and medical specialties under time pressure. ER-Reason includes data from 3,984 patients, encompassing 25,174 de-identified longitudinal clinical notes spanning discharge summaries, progress notes, history and physical exams, consults, echocardiography reports, imaging notes, and ER provider documentation. The benchmark includes evaluation tasks that span key stages of the ER workflow: triage intake, initial assessment, treatment selection, disposition planning, and final diagnosis--each structured to reflect core clinical reasoning processes such as differential diagnosis via rule-out reasoning. We also collected 72 full physician-authored rationales explaining reasoning processes that mimic the teaching process used in residency training, and are typically absent from ER documentation. Evaluations of state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and clinician-authored clinical reasoning for ER decisions, highlighting the need for future research to bridge this divide."
    mergedFrom:
      - source: arxiv
        arxivId: "2505.22919"
    finalDecision: exclude
  - title: Evidence-Based Guidelines for Podcast Production and Use in Competency-Based Health Professions Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About podcast production guidelines for health education, no generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.1016/j.pedhc.2025.12.009
    scopusId: SCOPUS_ID:105029518116
    authors: Carson R.A., Sobolewski B.
    year: "2026"
    abstract: "Background Podcasts are a popular educational tool in health professions that appeal to learners for their accessibility and flexibility. Skeptics criticize podcasts for issues related to quality and credibility, despite established quality criteria. Methods A podcast guideline was developed using a rapid review of the literature. Selected articles underwent synthesis and analysis. From these, key areas and recommendations were developed. Results Forty-eight articles informed the narrative synthesis that revealed 6 areas. Recommendations for areas included directives for 3 pertinent audiences: podcast producers, educators, and listeners and learners. Discussion High-quality podcasts are especially important to advocate for best practices in pediatric healthcare. The podcast guideline outlines evidence-based standards to guide podcast production and use for competency-based education for learner-centered outcomes. More research is needed to understand how podcast listening affects clinical outcomes. There is potential for artificial intelligence to advance podcasts, but it should be used cautiously and ethically."
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.pedhc.2025.12.009
        scopusId: SCOPUS_ID:105029518116
    finalDecision: exclude
  - title: "Evaluation of DeepSeek-R1 and contemporary large language models on the radiology board examination: A milestone achieved as open-source model matches performance with closed-source model"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Focuses on radiology board examination, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarking LLMs on radiology board exam questions; not about using AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Board-exam question answering study, not workplace-based assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.1016/j.ejrad.2025.112608
    scopusId: SCOPUS_ID:105025651010
    authors: Nakaura T., Kobayashi N., Masuda T., Nagayama Y., Uetani H., Kidoh M., Oda S., Funama Y., Hirai T.
    year: "2026"
    abstract: "Objectives Recent advances in large language models (LLMs), especially reasoning LLMs have demonstrated impressive reasoning capabilities in specialized domains. The purpose of this study is to evaluate the performance of new open-source reasoning LLM, DeepSeek-R1 and other contemporary LLMs on radiology board examination questions, comparing their accuracy to human radiologists. Materials and methods We assessed 10 LLMs, including both closed-source models (GPT-4o, o1, o3-mini, Claude 3.5 Sonnet, Gemini Flash 2.0) and open-source models (DeepSeek-R1, its distilled versions, and Llama 3.3 70B), on 105 non-image multiple-choice questions from the 2024 official board examination of the Japan Radiological Society. We evaluated accuracies of LLMs and compared those to accuracies of examinees (3rd-year radiology residents). Results DeepSeek-R1 and OpenAI o1 each answered 92/105 items correctly (87.6 %, 95 % CI: 80.1–92.5); McNemar’s test showed no significant head-to-head difference (p = 1.00), outperforming human radiologists’ mean accuracy of 67.6 ± 9.3 % (z-score = 2.15, equivalent to 98.4th percentile). DeepSeek-R1 demonstrated superior cost-efficiency with API costs approximately 1/27th of o1. DeepSeek-R1′s distilled 32B model (61.9 % accuracy) outperformed Llama 3.3 70B (57.1 %), despite having fewer parameters. Conclusion Open-source DeepSeek-R1 matches the performance of top closed-source models while offering superior cost-efficiency in radiology knowledge assessment."
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.ejrad.2025.112608
        scopusId: SCOPUS_ID:105025651010
    finalDecision: exclude
  - title: Using Consensus-Based Reasoning and Large Language Models to Extract Structured Data From Surgical Pathology Reports
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: LLMs extracting data from pathology reports, not about assessment or feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are not medical or health-professional learners"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    doi: 10.1016/j.labinv.2025.104272
    scopusId: SCOPUS_ID:105027679057
    authors: Tripathi A., Waqas A., Venkatesan K., Ullah E., Khan A., Khalil F., Chen W.S., Ozturk Z.G., Saeed-Vafa D., Bui M.M., Schabath M.B., Rasool G.
    year: "2026"
    abstract: Surgical pathology reports provide essential diagnostic information critical for cancer staging, treatment planning, and cancer registry documentation. However, their writing styles and formats vary widely, reflecting each pathologist's stylistic choices, institutional norms, and inherited practices from residency training. When performing large-scale data analysis, this unstructured nature and variability across tumor types and institutions pose significant hurdles for automated data extraction. To overcome these challenges, we present a consensus-driven, reasoning-based framework that adapts multiple locally deployed large language models (LLMs) to extract both standard diagnostic variables (such as site, laterality, histology, stage, grade, and behavior) and organ-specific biomarkers. Each LLM generates structured outputs, accompanied by justifications, which are subsequently evaluated for accuracy and coherence by 3 separate reasoning models (DeepSeek-R1-large, Qwen3-32B, and QWQ-32B). Final consensus values are determined through aggregation. Board-certified pathologists conducted expert validation. This framework was applied to >6100 pathology reports from The Cancer Genome Atlas (TCGA), spanning 10 organ systems, and 510 reports from Moffitt Cancer Center. For TCGA data set, automated evaluation demonstrated a mean accuracy of 84.9% ± 7.3%, with histology (88%), site (87%), stage (84%), and behavior (84%) showing the highest extraction accuracy averaged across all models. Expert review of randomly selected 138 reports confirmed high agreement for behavior (100.0%), histology (99%), grade (96%), and site (95%) in TCGA data set, with slightly lower performance for stage (89%) and laterality (88%). In Moffitt Cancer Center reports (brain, breast, and lung), accuracy remained high (88.2% ± 7.2%), with behavior (99%), histology (97%), laterality (96%), grade (94%), and site (93%) achieving strong agreement. Biomarker extraction achieved 70.6% ± 7.9% overall accuracy, with TP53 (84%) on brain tumor, Ki-67 (68%) on breast cancer, and ROS1 (82%) on lung cancer showing the highest accuracy. Interevaluator agreement analysis revealed high concordance (correlation values ≥0.93) across the 3 evaluation models. Statistical analyses revealed significant main effects of model type (F = 1716.82, P < .001), variable (F = 3236.68, P < .001), and organ system (F = 1946.43, P < .001), as well as model × variable × organ interactions (F = 24.74, P < .001), emphasizing the role of clinical context in model performance. These results highlight the potential of stratified, multiorgan evaluation frameworks with multievaluator consensus in LLM benchmarking for clinical applications. Overall, this consensus-based approach demonstrated that locally deployed LLMs can provide a transparent, accurate, and auditable solution for integration into real-world pathology workflows, such as synoptic reporting and cancer registry abstraction.
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.labinv.2025.104272
        scopusId: SCOPUS_ID:105027679057
    finalDecision: exclude
  - title: "Multimodal large language models in medical research and clinical practice: Development, applications, challenges and future"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Broad narrative review of multimodal LLMs, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Title is broad and not focused on assessment, feedback, or observation-record analysis."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.1016/j.neucom.2025.131817
    scopusId: SCOPUS_ID:105020261527
    authors: Xu P.J., Kan S.X., Jin J., Zhang Z.J., Gu Y.X., Zhang B., Zhou Y.L.
    year: "2026"
    abstract: In the highly intertwined field of healthcare, cross-modal medical data (including diagnostic imaging profiles, physiological time series signals, clinical voice archives, electronic medical records, surgical video streams, and genomics data) play a key role in clinical diagnostic and treatment decisions and basic medical research. Despite the breakthroughs in medical knowledge management and information processing made by generalized Large Language Models (LLMs), the existing systems are mainly limited to the processing capability of text modality, and this unimodal limitation seriously restricts the practical application value of intelligent medical systems. Multimodal Large Language Models (MLLMs) can realize multidimensional patient feature modeling through the deep semantic fusion of multimodal medical data, which can significantly improve the efficiency of disease identification and medical resource scheduling, and support the formulation of precise individual treatment strategies. In this paper, we systematically describe the development, applications, chanllenges and future of MLLMs in clinical practice and medical research, and provide a theoretical framework and practical guidelines for building a new generation of intelligent healthcare infrastructure. However, data silos, modality fusion strategies, computational resource requirements, and ethical compliance are still the main bottlenecks. Through this review, it is hoped that it can help more medical researchers to understand the progress of MLLMs technology, and help more medical researchers and MLLMs developers to work closely with each other through interdisciplinary cooperation, to create more products that are beneficial to the development of human health and put them into practical applications.
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.neucom.2025.131817
        scopusId: SCOPUS_ID:105020261527
    finalDecision: exclude
  - title: "SynDocDis: A Metadata-Driven Framework for Generating Synthetic Physician Discussions Using Large Language Models"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on synthetic text generation, not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Presents a framework for generating synthetic physician-to-physician dialogues using LLMs for privacy-preserving data generation; not about assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:17.284000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in workplace-based education."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:21.271000+00:00
    doi: 10.1007/978-3-032-05176-9_24
    scopusId: SCOPUS_ID:105016904368
    authors: Rubinstein B., Matos S.
    year: "2026"
    abstract: "Physician-physician discussions of patient cases represent a rich source of clinical knowledge and reasoning that could feed AI agents to enrich and even participate in subsequent interactions. However, privacy regulations and ethical considerations severely restrict access to such data. While synthetic data generation using Large Language Models offers a promising alternative, existing approaches primarily focus on patient-physician interactions or structured medical records, leaving a significant gap in physician-to-physician communication synthesis. We present SynDocDis, a novel framework that combines structured prompting techniques with privacy-preserving de-identified case metadata to generate clinically accurate physician-to-physician dialogues. Evaluation by five practicing physicians in nine oncology and hepatology scenarios demonstrated exceptional communication effectiveness (mean 4.4/5) and strong medical content quality (mean 4.1/5), with substantial inter-rater reliability (κ = 0.70, 95% CI: 0.67–0.73). The framework achieved 91% clinical relevance ratings, while maintaining doctors’ and patients’ privacy. These results place SynDocDis as a promising framework for advancing medical AI research ethically and responsibly through privacy-compliant synthetic physician dialogue generation with direct applications in medical education and clinical decision support."
    mergedFrom:
      - source: scopus
        doi: 10.1007/978-3-032-05176-9_24
        scopusId: SCOPUS_ID:105016904368
    finalDecision: exclude
  - title: Graph-empowered Text-to-SQL generation on Electronic Medical Records
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Technical NLP/database task on EMR, not about assessment or feedback in medical education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in education."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    doi: 10.1016/j.patcog.2025.111800
    scopusId: SCOPUS_ID:105007190991
    authors: Chen Q., Peng J., Song B., Zhou Y., Ji R.
    year: "2026"
    abstract: Electronic Medical Records (EMR) store comprehensive patient data, offering immense potential for improving medical decision-making, education, and research. However, extracting actionable insights from EMR remains challenging due to its complex, structured nature, which requires expertise in Structured Query Language (SQL). To address this, we propose leveraging Large Language Models (LLMs) combined with graph-empowered techniques to enhance Text-to-SQL capabilities in EMR systems. LLMs provide the ability to understand natural language queries, making data access more intuitive for non-technical users, while graph representations enable the modeling of intricate relationships between medical entities. This integration not only simplifies data querying but also captures the contextual and relational nuances inherent in EMR data. In this paper, we proposed a Text-to-SQL method based on graph structure injection, which aims to bridge the gap between medical professionals and complex data systems, promoting more efficient and accessible use of EMR for clinical and research applications. To demonstrate the effectiveness of our proposed method, we conduct extensive experiments on the MIMICSQL dataset. The experimental results show that our proposed method achieves 0.942 execution accuracy, reaching a new SOTA.
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.patcog.2025.111800
        scopusId: SCOPUS_ID:105007190991
    finalDecision: exclude
  - title: "The Growing Importance of Soft Skills in Medical Education in the AI Era: Balancing Humanistic Care and Artificial Intelligence"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Title suggests opinion/perspective piece about soft skills in the AI era, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article, systematic review, meta-analysis, or preprint"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    doi: 10.3390/ime4040050
    scopusId: SCOPUS_ID:105025796715
    authors: Simou E.
    year: "2025"
    abstract: The rapid integration of artificial intelligence (AI) into healthcare has reshaped medical education and clinical practice. While technological innovation is vital, soft skills are essential for preserving trust, ethical accountability, and humanistic care. This study explores the evolving role of soft skills in medical education in the AI era by examining definitional challenges, pedagogical strategies, and the integration of AI-related literacy. A narrative review methodology synthesized evidence across seven thematic domains, focusing on curricular integration, pedagogical strategies, and assessment approaches in medical education within AI-enabled learning environments. The findings demonstrated that soft skills improve patient adherence, satisfaction, safety, and trust; strengthen physicians’ professional identity, collaboration, and resilience; and enhance system-level outcomes, such as resilience, safety, and public trust. Experiential, reflective, and competency-based pedagogies remain the most effective instructional strategies, while AI-supported tools, including virtual patients, adaptive simulations, large language models (LLMs), and Retrieval-Augmented Generation systems (RAG), offer complementary benefits by enhancing doctor-patient communication, providing real-time personalized feedback, and strengthening clinical reasoning. Soft skills function as an interconnected and synergistic ecosystem that is reinforced by cognitive, affective, humanistic, and ethical mechanisms. Integrating these competencies with AI literacy promotes theoretical clarity, supports programmatic assessment, and fosters responsible innovation, ensuring that technological advancement enhances rather than diminishes the humanistic foundations of medicine.
    mergedFrom:
      - source: scopus
        doi: 10.3390/ime4040050
        scopusId: SCOPUS_ID:105025796715
    finalDecision: exclude
  - title: "Customized multimodal Diabot-GPT-4o enhances accuracy of image-based dietary assessments in dietetic trainees in Taiwan: validation against weighed food records"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are young adults providing food images for dietary assessment validation, not medical/health-professional learners; also criterion 2 (dietary assessment, not clinical observation assessment)."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:13.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on dietary intake estimation accuracy, not assessment/feedback of learner clinical performance or observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:18.261000+00:00
    doi: 10.1016/j.ajcnut.2025.10.013
    scopusId: SCOPUS_ID:105021510981
    authors: Chen Y.J., Chang C.C., Hoang Y.N., Lin A.W., Lin W.L., Lin C.Y., Patricia E., Tissadharma J.C., Kuanca J., Nobelta N., Theo K.A., Ho D.K.N., Wei P.H., Chang J.S.
    year: "2025"
    abstract: "Background: Automated image-based dietary assessments (IBDAs) using multimodal artificial intelligence (AI) chatbots show strong potential. However, sources of error at the human–AI interface in real-world use remain unclear. Objectives: In this study, we validated a GPT-4o-powered chatbot for automated IBDAs and identified key sources of error in free-living settings. Methods: In total, 714 food images were collected from 3-d weighed food records (WFRs) across 171 d from 57 young adults. Images were analyzed using 4 AI configurations: Diabot (DB), DBFN (customized GPT-4o), 4o, and 4oFN (noncustomized), where “FN” indicates inclusion of the food name input. Portion sizes and nutrient estimates were compared with WFRs using Bland–Altman plots with equivalence testing at ±10%, ±15%, and ±20% bounds. Results: Using images alone, DB recognized 74% of food items versus 59% for 4o. All AI configurations provided accurate estimates of portion sizes (±10%–15%, coefficient of variation [CV]: 13%), energy (±10%–20%, CV: 14%), and carbohydrates (CHOs; ±15%–20%, CV: 15%) but showed less consistency for fats (±10%–22%, CV: 24%) and proteins (±10%–>20.2%, CV: 18%). The custom DBFN outperformed 4oFN, achieving higher accuracy across more nutrients within the ±10% (weight, energy, fats, saturated fats, potassium, and magnesium), ±15% (proteins and sodium), and ±20% (CHOs and calcium) bounds and achieved the highest agreement with WFRs (Spearman's ρ = 0.863–0.662; Lin's concordance correlation coefficient = 0.874–0.540). Common errors at the human–AI interface included inaccurate portion-size estimates, obscured food visibility in images, poorly constructed prompts, omission or intrusion errors, and system-specific limitations, such as processing overload and configuration inconsistencies. Conclusions: Customized AI chatbots improved automated IBDAs, yet accuracy depends on clear images for food visibility and portion-size fidelity. Standardized AI-input procedures (FN, cooking state, prompt structure, and configuration) and expert oversight to detect and correct AI hallucinations (fabricated items, units, or quantities) remain essential for reliable, interpretable estimates."
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.ajcnut.2025.10.013
        scopusId: SCOPUS_ID:105021510981
    finalDecision: exclude
  - title: "Title: Assessing Quality of Scenario-Based Multiple-Choice Questions in Physiology: Faculty-Generated vs. ChatGPT-Generated Questions among Phase I Medical Students"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: ChatGPT-generated MCQs for Phase I (preclinical) physiology, classroom/exam-based, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:54.981000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:56.582000+00:00
    doi: 10.1007/s40593-025-00471-z
    scopusId: SCOPUS_ID:105002175514
    authors: Chauhan A., Khaliq F., Nayak K.R.
    year: "2025"
    abstract: "The integration of Artificial Intelligence (AI), particularly Chatbot Generative Pre-Trained Transformer (ChatGPT), in medical education has introduced new possibilities for generating various educational resources for assessments. However, ensuring the quality of ChatGPT-generated assessments poses challenges, with limited research in the literature addressing this issue. Recognizing this gap, our study aims to investigate the quality of ChatGPT-based assessment. In this study among first-year medical students, a crossover design was employed to compare scenario-based multiple-choice questions (SBMCQs) crafted by both faculty members and ChatGPT through item analysis to determine the quality of assessment. The study comprised three main phases: development, implementation, and evaluation of SBMCQs. During the development phase, both faculty members and ChatGPT generated 60 SBMCQs each, covering topics related to cardiovascular, respiratory, and endocrinology. These questions underwent assessment by independent reviewers, after which 80 SBMCQs were selected for the tests. Subsequently, in the implementation phase, one hundred and twenty students, divided into two batches, were assigned to receive either faculty-generated or ChatGPT-generated questions across four test sessions. The collected data underwent rigorous item analysis and thematic analysis to evaluate the effectiveness and quality of the questions generated by both parties. Only 9 of ChatGPT’s SBMCQs met ideal criteria MCQ on Difficulty Index, Discrimination Index and Distractor Effectiveness contrasting with 19 from faculty. Moreover, ChatGPT’s questions exhibited a higher rate of nonfunctional distractors (33.75% vs. faculty’s 13.75%). During focus group discussion, faculty highlighted importance of educators in reviewing, refining, and validating ChatGPT-generated SBMCQs to ensure their appropriateness within the educational context."
    mergedFrom:
      - source: scopus
        doi: 10.1007/s40593-025-00471-z
        scopusId: SCOPUS_ID:105002175514
    finalDecision: exclude
  - title: "Applying state-of-the-art artificial intelligence to grading in simulation-based education: assessment, feedback, and ROI"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Explicitly simulation-based education context only."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments; GPT-4 grades post-encounter notes from Clerkship OSCEs, which are simulation-based assessments."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 09:12:30.709000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment is explicitly simulation-based (OSCE/post-encounter notes) without a workplace-based component."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 09:12:32.041000+00:00
    doi: 10.1007/s44163-025-00417-3
    scopusId: SCOPUS_ID:105012869758
    authors: Campbell K.K., Holcomb M.J., Vedovato S., Young L., Danuser G., Dalton T.O., Jamieson A.R., Scott D.J.
    year: "2025"
    abstract: "Objective: Accreditation bodies are driving competency-based education in healthcare, prompting curriculum reform. Simulation-based education (SBE) addresses challenges curriculum reform has uncovered, like lack of standardization in bedside teaching. This study explores the impact of an AI-powered Automated System Protocol (ASP) for grading students’ post-encounter notes in Clerkship OSCEs, comparing it to the legacy human grader system. Methods: The ASP, utilizing GPT-4, mapped rubric items to prompts. Analyzing post-encounter notes from 684 medical students across four academic years, we compared ASP with legacy Standardized Patient Evaluator (SPE) grades. Time efficiency, cost savings, and ROI analyses assessed educational and financial implications. Results: Significant cost savings and efficiency gains were observed utilizing GPT-4 in comparison to SPEs. The Cost of Investment for ASP totaled $69,112 over 1,150 h. Comparing ASP to three SP graders yielded $13,112 in increased costs and initial time investment was required. However, beyond development time ASP execution-only, compared to legacy, showed an ROI of 589.44%, saving $47,877 with 87.5% time efficiency. ASP-execution versus three MD graders demonstrated an even stronger ROI of 797.09%. Conclusion: Implementing ASP in medical education provides substantial time and cost savings, enhancing ROI compared to legacy grading models. These findings highlight significant cost savings and efficiency improvements achievable through ASP implementation, positioning automated assessment as an innovative force shaping the future of medical education. By liberating human resources from manual grading and enhancing the immediacy of feedback, this approach contributes to a more efficient, effective, and engaging learning experience."
    mergedFrom:
      - source: scopus
        doi: 10.1007/s44163-025-00417-3
        scopusId: SCOPUS_ID:105012869758
    finalDecision: exclude
  - title: "TriMedPrompt: A unified prompting framework for realistic and layout-conformant clinical progress note synthesis"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Not focused on medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Framework for generating synthetic clinical progress notes for NLP research; not about assessment/feedback of learners. Also criterion 6: no learner participants."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:44.585000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on synthetic clinical note generation for NLP, not learner assessment/feedback or analysis of observation records in WBA."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:51.748000+00:00
    doi: 10.1016/j.jbi.2025.104927
    scopusId: SCOPUS_ID:105020582851
    authors: Keerthana G., Gupta M.
    year: "2025"
    abstract: "Clinical progress notes are critical artifacts for modeling patient trajectories, auditing clinical decision-making, and powering downstream applications in clinical natural language processing (NLP). However, public resources such as MIMIC-III provide limited progress notes, constraining the development of robust and generalizable machine learning models. This work proposes a novel hybrid prompting framework — TriMedPrompt — to generate high-quality, structurally and semantically coherent synthetic progress notes using large language models (LLMs). Our approach conditions the LLMs on a triad of complementary biomedical signals: (1) real-world progress notes from MIMIC-III, (2) clinically aligned case reports from the PMC Patients dataset, selected via embedding-based retrieval, and (3) structured disease-centric knowledge from PrimeKG. We design a multi-source, layout-aware prompting pipeline that dynamically integrates structured and unstructured information to produce notes across standard clinical formats (e.g., SOAP, BIRP, PIE, DAP). Through rigorous evaluations—including layout adherence, entity extraction comparisons, semantic similarity analysis, and controlled ablations, we demonstrate that our generated notes achieve a 98.6% semantic entity alignment score with real clinical notes, while maintaining high structural fidelity. Ablation studies further confirm the critical role of combining structured biomedical knowledge and unstructured narrative data in improving note quality. In addition, we illustrate the potential of our synthetic notes in privacy-preserving clinical NLP, offering a safe alternative for model development and benchmarking in sensitive healthcare settings. This work establishes a scalable, controllable paradigm for clinical text synthesis, significantly expanding access to realistic, diverse progress notes and laying the foundation for advancing trustworthy clinical NLP research."
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.jbi.2025.104927
        scopusId: SCOPUS_ID:105020582851
    finalDecision: exclude
  - title: "Generative Artificial Intelligence in Dermatology: A Primer"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: 'A Primer' indicates this is a review/overview, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article, systematic review, meta-analysis, or preprint (primer)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1016/j.det.2025.05.007
    scopusId: SCOPUS_ID:105009934778
    authors: Kantor J.
    year: "2025"
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.det.2025.05.007
        scopusId: SCOPUS_ID:105009934778
    finalDecision: exclude
  - title: Large Language Model (LLM)-Predicted and LLM-Assisted Calculation of the Spinal Instability Neoplastic Score (SINS) Improves Clinician Accuracy and Efficiency
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Focuses on clinicians, not medical or health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records; evaluates LLM for clinical decision support (SINS scoring), not learner assessment or feedback"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:47.998000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on LLM-assisted clinical scoring (SINS) as decision support rather than educational assessment/feedback or analysis of learner observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:55.729000+00:00
    doi: 10.3390/cancers17193198
    scopusId: SCOPUS_ID:105019226694
    authors: Chan M.D.Z., Tjio C.K.E., Chan T.L.Y., Tan Y.L., Chua A.X.Y., Loh S.K.Y., Leow G.Z.H., Gan M.Y., Lim X., Choo A.K., Liu Y., Tan J.W.P., Teo E.C., Yap Q.V., Yonghan T., Makmur A., Kumar N., Tan J.H., Hallinan J.T.P.D.
    year: "2025"
    abstract: "Background: The Spinal Instability Neoplastic Score (SINS) guides treatment for patients with spinal tumors, but issues arise with complexity, interobserver variability, and time demands. Large language models (LLMs) may help overcome these limitations. Objectives: This study evaluates the accuracy and efficiency of a privacy-preserving LLM (PP-LLM) for SINS calculation, with and without clinician involvement, to assess its feasibility as a clinical decision-support tool. Methods: This retrospective observational study was granted a Domain-Specific Review Board waiver owing to minimal risk. Patients from 2020 to 2022 were included. A PP-LLM was employed to maintain secure handling of patient data. A consensus SINS reference standard was established by musculoskeletal radiologists and an orthopedic surgeon. Eight orthopedic and oncology trainees were divided into two groups to calculate SINS, with and without PP-LLM assistance. LLM-predicted scores were also generated independently of any human input. Results: The main outcomes were agreement with the reference standard (measured by intraclass correlation coefficients [ICCs]) and time required for SINS calculation. The LLM-assisted method achieved excellent agreement (ICC = 0.993, 95%CI = 0.991–0.994), closely followed by the LLM-predicted approach (ICC = 0.990, 95%CI = 0.984–0.993). Clinicians working without LLM support showed a significantly lower ICC compared to both LLM methods (0.968, 95%CI = 0.960–0.975) (both p < 0.001). The LLM alone produced scores in approximately 5 s, while the median scoring time for LLM-assisted clinicians was 60.0 s (IQR = 46.0–80.0), notably shorter than the 83.0 s (IQR = 58.0–124.0) required without LLM assistance. Conclusions: An LLM-based approach, whether used autonomously or in conjunction with clinical expertise, enhances both accuracy and efficiency in SINS calculation. Adopting this technology may streamline oncologic workflows and facilitate more timely interventions for patients with spinal metastases."
    mergedFrom:
      - source: scopus
        doi: 10.3390/cancers17193198
        scopusId: SCOPUS_ID:105019226694
    finalDecision: exclude
  - title: "ECOSBot: a multicenter validation pilot study of a generative AI tool for OSCE-based nephrology training"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 3: OSCE-based assessment is a simulated environment, not workplace-based"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Assessment conducted entirely in simulated environments (OSCE-based)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1093/ckj/sfaf308
    scopusId: SCOPUS_ID:105019777044
    authors: Bentegeac R., Florens N., Maanaoui M., Maisons V., Lanot A., Bobot M., Brilland B., Glowacki F., Gérard E., Hazzan M., Amouyel P., Le Guellec B., Hamroun A.
    year: "2025"
    abstract: "Background Developing diagnostic reasoning in nephrology is particularly challenging due to its pathophysiological complexity and reliance on abstract clinical data. Objective Structured Clinical Examinations (OSCEs) are pivotal for nephrology training but remain resource-intensive and difficult to scale. Generative artificial intelligence (AI) offers a promising alternative, yet its capacity to emulate nephrology-specific OSCEs has not been formally assessed. Methods We developed ECOSBot, a web-based tool powered by GPT-4o, to simulate both standardized patients and examiners for nephrology-focused OSCEs. In this multicenter prospective study, undergraduate medical students from five French medical schools interacted with ECOSBot across four clinical stations. All interactions were double-rated by nephrology faculty members to establish a gold standard. ECOSBot's performance was evaluated against this standard using four criteria (script coverage, authenticity, correctness and relevance) for patient simulation, and via checklists and competency-based ratings for examiner scoring. Usability was assessed using the Chatbot Usability Questionnaire (CUQ), adapted to include six items on feedback quality. Results Ninety-one students generated 2939 prompts across 184 OSCE sessions. ECOSBot demonstrated high fidelity in patient simulation: authenticity 98.6% [95% confidence interval (CI) 98.2-99.0], correctness 98.3% (95% CI 97.9-98.7) and relevance 99.2% (95% CI 98.9-99.5), including during exchanges not explicitly covered by the pre-specified scenario. As an examiner, ECOSBot showed strong agreement with human raters on global scores [intraclass correlation coefficient (ICC) = 0.94, 95% CI 0.91-0.96], consistent across case formats, training levels and institutions. However, scoring of attitude and communication skills was less reliable (ICC = 0.44, 95% CI 0.28-0.58). Median CUQ score was 69.7/100, with 91.7% of students finding the tool highly useful for OSCE preparation in nephrology. Conclusions ECOSBot reliably simulated both roles in nephrology OSCEs with high fidelity and strong alignment with expert rating. While challenges remain for subjective skill assessment, this tool offers a scalable and autonomous solution to enhance nephrology education."
    mergedFrom:
      - source: scopus
        doi: 10.1093/ckj/sfaf308
        scopusId: SCOPUS_ID:105019777044
    finalDecision: exclude
  - title: "Implementing a competency based medical education curriculum in diagnostic radiology: Challenges and Pearls of Wisdom"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: About CBME curriculum implementation in radiology, no mention of generative AI"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Title does not indicate generative AI or LLM use."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.1067/j.cpradiol.2025.01.012
    scopusId: SCOPUS_ID:85216782551
    authors: Rogoza C., Fasih S., Kwan B.Y.M.
    year: "2025"
    abstract: In 2014, The Royal College of Physicians and Surgeons of Canada (RCPSC) began a national initiative to rollout Competency-Based Medical Education (CBME) for all postgraduate medical programs. This represents a paradigm shift in the approach to resident training and transformative changes on many levels. In 2017, the department of Diagnostic Radiology at Queen's University became an early adopter of the CBME training model. The department began curricular planning using program specific Entrustable Professional Activities (EPAs), milestones based on the CanMeds roles, and an assessment framework. Associated processes were created to support implementation, with formation of a new competence committee, structure and process for academic advisors, and faculty development. In July 2018, the model was implemented using an electronic portfolio system, Elentra. In July 2022, the RCPSC launched the national implementation of their CBME CBD model, which was implemented for the incoming cohorts in the department of Diagnostic Radiology. Drawing from CBME implementation in the department of Diagnostic Radiology at Queen's University, we highlight the challenges encountered at our institution, methods for addressing these challenges, and corresponding outcomes. From our experience, we aim to provide a roadmap for the reader that will aid in planning for CBME implementation at other institutions.
    mergedFrom:
      - source: scopus
        doi: 10.1067/j.cpradiol.2025.01.012
        scopusId: SCOPUS_ID:85216782551
    finalDecision: exclude
  - title: "Artificial Intelligence in Medical Education: A Narrative Review on Implementation, Evaluation, and Methodological Challenges"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Narrative review, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Narrative review (not an original research article, systematic review, meta-analysis, or preprint)."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    doi: 10.3390/ai6090227
    scopusId: SCOPUS_ID:105017141833
    authors: Roveta A., Castello L.M., Massarino C., Francese A., Ugo F., Maconi A.
    year: "2025"
    abstract: "Artificial Intelligence (AI) is rapidly transforming medical education by enabling adaptive tutoring, interactive simulation, diagnostic enhancement, and competency-based assessment. This narrative review explores how AI has influenced learning processes in undergraduate and postgraduate medical training, focusing on methodological rigor, educational impact, and implementation challenges. The literature reveals promising results: large language models can generate didactic content and foster academic writing; AI-driven simulations enhance decision-making, procedural skills, and interprofessional communication; and deep learning systems improve diagnostic accuracy in visually intensive tasks such as radiology and histology. Despite promising findings, the existing literature is methodologically heterogeneous. A minority of studies use controlled designs, while the majority focus on short-term effects or are confined to small, simulated cohorts. Critical limitations include algorithmic opacity, generalizability concerns, ethical risks (e.g., GDPR compliance, data bias), and infrastructural barriers, especially in low-resource contexts. Additionally, the unregulated use of AI may undermine critical thinking, foster cognitive outsourcing, and compromise pedagogical depth if not properly supervised. In conclusion, AI holds substantial potential to enhance medical education, but its integration requires methodological robustness, human oversight, and ethical safeguards. Future research should prioritize multicenter validation, longitudinal evaluation, and AI literacy for learners and educators to ensure responsible and sustainable adoption."
    mergedFrom:
      - source: scopus
        doi: 10.3390/ai6090227
        scopusId: SCOPUS_ID:105017141833
    finalDecision: exclude
  - title: "From significance to feedback: Designing assessment with purpose and rigor"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article, systematic review, meta-analysis, or preprint. This is an editorial/commentary discussing statistical methods (Cochran's Q, McNemar's tests) for evaluating a faculty supervision assessment instrument. Also criterion 1: generative AI (ChatGPT) was used only for manuscript language editing, not as part of assessment or feedback."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:50.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Editorial/commentary publication type, not original research, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:25:58.854000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Editorial/commentary, not original research/SR/MA."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1016/j.jclinane.2025.111980
    scopusId: SCOPUS_ID:105013750067
    authors: de Oliveira Filho G.R.
    year: "2025"
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.jclinane.2025.111980
        scopusId: SCOPUS_ID:105013750067
    finalDecision: exclude
  - title: "DeepSeek’s Readiness for Medical Research and Practice: Prospects, Bottlenecks, and Global Regulatory Constraints"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Title suggests a commentary/perspective piece, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    doi: 10.1007/s10439-025-03738-7
    scopusId: SCOPUS_ID:105003437989
    authors: MohanaSundaram A.S., Sathanantham S.T., Ivanov A., Mofatteh M.
    year: "2025"
    abstract: DeepSeek, an open-source multimodal Large Language Model (LLM), was launched by the Chinese startup (Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd.). Despite the lack of advanced artificial intelligence (AI) chips, the performance of its milestone version, “DeepSeek-V3,” has set an unprecedented benchmark among LLMs, surpassing existing models. Notably, the opportunity to deploy this model in the local system helps build better-performing “distilled versions” suitable for medical research (hypothesis generation, drafting patient consent forms and biostatistical analysis, etc.) and clinical practice (differential diagnosis from symptom clusters, current guideline-based treatment protocol design, interactive medical training, personalized patient education, etc.). However, privacy and security risks, ethical uncertainties, and diversified global AI regulations hinder its potential for sustainable integration into real-world applications.
    mergedFrom:
      - source: scopus
        doi: 10.1007/s10439-025-03738-7
        scopusId: SCOPUS_ID:105003437989
    finalDecision: exclude
  - title: Can GPTs Accelerate the Development of Intelligent Diagnosis and Treatment in Traditional Chinese Medicine? A Survey and Empirical Analysis
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Not focused on medical or health-professional learners in education assessment context"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records for learners. Focuses on adapting LLMs to TCM diagnosis and treatment, not educational assessment."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on LLM use for TCM diagnosis/treatment, not assessment, feedback, or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1111/jebm.70004
    scopusId: SCOPUS_ID:85219521019
    authors: Guo Y., Wang H., Ren X., Wang T., Chen W., Xu Z., Ge H.
    year: "2025"
    abstract: Intelligent traditional Chinese medicine (TCM) is a key pathway toward the modernization and globalization of TCM in the era of artificial intelligence. Due to its unique terminology and diagnostic framework, TCM's intelligentization process has long faced a range of challenges, from the digitization and formalization of knowledge bases to the differentiation of syndromes and personalized treatment. Recently, the advent of large language models (LLMs) like GPTs has marked a transformative milestone in semantic understanding tasks, attracting widespread attention from the medical, academic, and industrial communities. Nonetheless, LLMs often suffer from accuracy and logical reasoning limitations within specific fields and may manifest hallucinations in the generative outputs. Through a comprehensive review of existing literature and empirical analyses, this study delves into the potential and challenges of adapting LLMs to TCM. Promising perspectives on future developments at this innovative intersection are discussed.
    mergedFrom:
      - source: scopus
        doi: 10.1111/jebm.70004
        scopusId: SCOPUS_ID:85219521019
    finalDecision: exclude
  - title: Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT
    reviews:
      - decision: uncertain
        comment: ChatGPT for clinical supervision; directly relevant to WBA context
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:08.711000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:10.139000+00:00
      - decision: uncertain
        comment: ChatGPT-4 evaluated as clinical supervision tool in psychotherapy training with satisfaction data; unclear whether the clinical case scenario is workplace-based or simulated. Forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:23.599000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Examines AI as a supervision aid, not assessment/feedback analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:28.182000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are not medical/health-professional learners. The study involves gestalt psychotherapy trainees evaluating ChatGPT-4-generated clinical supervision feedback in a psychotherapy training school, not medical students, residents, or health-professional learners in a medical/clinical education context. Also criterion 4: setting is psychotherapy training, not workplace-based clinical education."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:50.890000+00:00
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:25:58.854000+00:00
    doi: 10.3390/informatics12010029
    scopusId: SCOPUS_ID:105001423766
    authors: Cioffi V., Ragozzino O., Mosca L.L., Moretto E., Tortora E., Acocella A., Montanari C., Ferrara A., Crispino S., Gigante E., Lommatzsch A., Pizzimenti M., Temporin E., Barlacchi V., Billi C., Salonia G., Sperandeo R.
    year: "2025"
    abstract: "Clinical supervision is essential for trainees, preventing burnout and ensuring the effectiveness of their interventions. AI technologies offer increasing possibilities for developing clinical practices, with supervision being particularly suited for automation. The aim of this study is to evaluate the feasibility of using ChatGPT-4 as a supervisory tool in psychotherapy training. To achieve this, a clinical case was presented to three distinct groups (untrained AI, pre-trained AI, and qualified human supervisor), and their feedback was evaluated by Gestalt psychotherapy trainees using a Likert scale rating of satisfaction. Statistical analysis, using the statistical package SPSS version 25 and applying principal component analysis (PCA) and one-way analysis of variance (ANOVA), demonstrated significant differences in favor of pre-trained AI feedback. PCA highlighted four components of the questionnaire: relational and emotional (C1), didactic and technical quality (C2), treatment support and development (C3), and professional orientation and adaptability (C4). The ratings of satisfaction obtained from the three kinds of supervisory feedback were compared using ANOVA. The feedback generated by the pre-trained AI (f2) was rated significantly higher than the other two (untrained AI feedback (f1) and human feedback (f3)) in C4; in C1, the superiority of f2 over f1 but not over f3 appears significant. These results suggest that AI, when appropriately calibrated, may be an appreciable tool for complementing the effectiveness of clinical supervision, offering an innovative blended supervision methodology, in particular in the area of career guidance."
    mergedFrom:
      - source: scopus
        doi: 10.3390/informatics12010029
        scopusId: SCOPUS_ID:105001423766
    finalDecision: exclude
  - title: "Application of large language models in clinical record correction: a comprehensive study on various retraining methods"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Title concerns clinical record correction methods, not medical/health-professional learners."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
      - decision: uncertain
        comment: LLMs used to assess and correct clinical records for medical education purposes. Potentially relevant to WBA (AI-driven feedback on clinical documentation), but the abstract does not clearly specify whether this occurs in a workplace-based or clinical education setting; forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:01.947000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Does not clearly study medical/health-professional learners as participants."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:08.201000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting. This study focuses on comparing LLM retraining methods for clinical record correction, evaluating technical approaches rather than assessing learners in workplace-based clinical education settings."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17T07:34:39.189Z
      - decision: exclude
        comment: "Exclusion criterion 6: The study evaluates model performance on record correction without medical/health-professional learner participants."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17T07:34:41.875Z
      - decision: exclude
        comment: "Exclusion criterion 6: Technical ML study evaluating model performance on record correction; no medical/health-professional learner participants."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1093/jamia/ocae302
    scopusId: SCOPUS_ID:85216606517
    authors: Maitin A.M., Nogales A., Fernández-Rincón S., Aranguren E., Cervera-Barba E., Denizon-Arranz S., Mateos-Rodríguez A., García-Tejedor Á.J.
    year: "2025"
    abstract: "Objectives: We evaluate the effectiveness of large language models (LLMs), specifically GPT-based (GPT-3.5 and GPT-4) and Llama-2 models (13B and 7B architectures), in autonomously assessing clinical records (CRs) to enhance medical education and diagnostic skills. Materials and Methods: Various techniques, including prompt engineering, fine-tuning (FT), and low-rank adaptation (LoRA), were implemented and compared on Llama-2 7B. These methods were assessed using prompts in both English and Spanish to determine their adaptability to different languages. Performance was benchmarked against GPT-3.5, GPT-4, and Llama-2 13B. Results: GPT-based models, particularly GPT-4, demonstrated promising performance closely aligned with specialist evaluations. Application of FT on Llama-2 7B improved text comprehension in Spanish, equating its performance to that of Llama-2 13B with English prompts. Low-rank adaptation significantly enhanced performance, surpassing GPT-3.5 results when combined with FT. This indicates LoRA’s effectiveness in adapting open-source models for specific tasks. Discussion. While GPT-4 showed superior performance, FT and LoRA on Llama-2 7B proved crucial in improving language comprehension and task-specific accuracy. Identified limitations highlight the need for further research. Conclusion: This study underscores the potential of LLMs in medical education, providing an innovative, effective approach to CR correction. Low-rank adaptation emerged as the most effective technique, enabling open-source models to perform on par with proprietary models. Future research should focus on overcoming current limitations to further improve model performance."
    mergedFrom:
      - source: scopus
        doi: 10.1093/jamia/ocae302
        scopusId: SCOPUS_ID:85216606517
    finalDecision: exclude
  - title: Keeping the Eye on the Needle…Reimaging Acupuncture Education
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs; focuses on acupuncture education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 1: Not about generative AI or LLMs."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    doi: 10.1089/acu.2024.0141
    scopusId: SCOPUS_ID:85205799125
    authors: Pock A.R.
    year: "2025"
    mergedFrom:
      - source: scopus
        doi: 10.1089/acu.2024.0141
        scopusId: SCOPUS_ID:85205799125
    finalDecision: exclude
  - title: Aspect-Based Medical Record Classification Using Large Language Model Guided Knowledge Graph
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Technical NLP paper about medical record classification, not about assessment or feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: No medical/health-professional learners are indicated"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    doi: 10.1109/IDAP68205.2025.11222391
    scopusId: SCOPUS_ID:105025034353
    authors: Işik E., Inan E.
    year: "2025"
    abstract: Traditional sentiment analysis approaches typically evaluate a text as a whole and assign it a single sentiment label, such as positive or negative. Although this method works well for many tasks, there are cases where it is more beneficial to understand sentiment related to specific aspects. To address this issue, Aspect-Based Sentiment Analysis (ABSA) focuses on analysing sentiment at the aspect level, treating it as a more detailed form of opinion mining. In this study, we proposed a method that initially identifies aspect terms as an extraction sub-task of anatomy terms by leveraging biomedical knowledge graphs. In the second subtask, we leverage well-known large language models to predict the sentiment polarities of these extracted aspect terms. The experimental results for each subtask demonstrate that the RaTE-NER-Deberta model yields the best performance in the anatomy aspect identification subtask, achieving precision, recall, and F1 scores of 65.385, 64.151, and 64.762, respectively. After identifying anatomical entities in the input texts using this model, we proceed with the classification task. The deberta-v3-base-absa-v1.1 model, a specialized version for aspect-based sentiment analysis, delivers the highest results, with a precision of 91.38, recall of 80.30, and an F1 score of 85.48.
    mergedFrom:
      - source: scopus
        doi: 10.1109/IDAP68205.2025.11222391
        scopusId: SCOPUS_ID:105025034353
    finalDecision: exclude
  - title: "Speech recognition tools for veterinary case learning: enhancing veterinary education with smartphone-based transcription and AI Summarization — a comparative study of workflow and usability"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are veterinary students and clinicians, not medical/health-professional learners"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:52.805000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Addresses transcription/summarization workflow usability, not assessment/feedback or analysis of clinical observation records for WBA."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:58.424000+00:00
    doi: 10.3389/fvets.2025.1690085
    scopusId: SCOPUS_ID:105026809761
    authors: Yogo T.
    year: "2025"
    abstract: "Background: Accurate documentation of clinical teaching sessions is critical, particularly in multilingual contexts. Recent advances in smartphone-based speech recognition and large language models (LLMs) may enhance transcription accuracy, streamline case summarization, and improve usability. However, their comparative performance in veterinary settings remains underexplored. Objectives: This study evaluated the quality, usability, and educational value of smartphone-native transcription compared with Whisper-based transcription and AI-assisted summarization in veterinary ophthalmology education. Methods: Clinical case discussions (n = 5) were recorded and transcribed using (1) iPhone-native speech recognition and (2) the Whisper automatic speech recognition system. Transcripts were further processed into SOAP-format summaries with and without LLM-based summarization. Final-year veterinary students (n = 4) and clinicians (n = 3) evaluated transcripts and summaries using a 5-point Likert scale across readability, accuracy, clinical clarity, and educational utility. Statistical comparisons were performed using Wilcoxon signed-rank tests. Results: iPhone-native transcription outperformed Whisper in readability, technical accuracy, and clinical flow (p < 0.05). AI-assisted SOAP-format summarization improved clarity and perceived learning value but occasionally introduced minor semantic distortions. Clinicians rated AI-enhanced summaries as more concise and educationally useful than raw transcripts. Both students and clinicians reported reduced cognitive load and usability with smartphone-based transcription workflows. Conclusion: Smartphone-native transcription combined with AI summarization provides a practical and effective workflow for veterinary education. While Whisper offers cross-device flexibility, its current accuracy in multilingual contexts is limited. Integration of smartphone transcription and LLM summarization may improve documentation, comprehension, and student engagement in clinical teaching."
    mergedFrom:
      - source: scopus
        doi: 10.3389/fvets.2025.1690085
        scopusId: SCOPUS_ID:105026809761
    finalDecision: exclude
  - title: Integration of Multi-Source Medical Data for Medical Diagnosis Question Answering
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Medical diagnosis QA system, not about assessment/feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on diagnosis question-answering systems, not assessment/feedback or analysis of clinical observation records in education."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    doi: 10.1109/TMI.2024.3496862
    scopusId: SCOPUS_ID:85209942277
    authors: Peng Q., Cai Y., Liu J., Zou Q., Chen X., Zhong Z., Wang Z., Xie J., Li Q.
    year: "2025"
    abstract: Medical question answering aims to enhance diagnostic support, improve patient education, and assist in clinical decision-making by automatically answering medical-related queries, which is an important foundation for realizing intelligent healthcare. Existing methods predominantly focus on extracting key information from a single data source, e.g., CT image, for answering. However, these methods are not enough to promote the development of intelligent healthcare, because they lack comprehensive medical diagnosis capabilities, which usually require the integration of multi-source data (e.g., laboratory tests, radiology images, pathology images, etc.) for processing. To address these limitations, our paper introduces the extended task of medical question answering, named medical diagnosis question answering MedDQA. MedDQA task aims to answer questions related to medical diagnosis based on multi-source data. Specifically, we introduce a corresponding dataset that incorporates multi-source diagnostic information from 250,917 patients in clinical data from hospital records, and utilize a large-scale model for constructing Q&A pairs. We propose a novel system based on large language models, named medical multi-agent (MMA) system, which includes a mechanism of multiple agents to handle different medical tasks. Each agent is specifically tailored to process various modalities of data and provide outputs in a uniform textual modality. Experimental results demonstrate that the MMA system's architecture significantly enhances the handling of multi-source data, thereby improving medical diagnosis, establishing a robust baseline for future research.
    mergedFrom:
      - source: scopus
        doi: 10.1109/TMI.2024.3496862
        scopusId: SCOPUS_ID:85209942277
    finalDecision: exclude
  - title: Reply
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: A reply/letter, not original research article"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Publication type is a reply"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    doi: 10.1016/j.jacr.2024.08.003
    scopusId: SCOPUS_ID:85210371600
    authors: Gulati V.
    year: "2024"
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.jacr.2024.08.003
        scopusId: SCOPUS_ID:85210371600
    finalDecision: exclude
  - title: Large language models can support generation of standardized discharge summaries – A retrospective study utilizing ChatGPT-4 and electronic health records
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About generating discharge summaries from EHRs, not about assessment or feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: No medical/health-professional learners are indicated"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    doi: 10.1016/j.ijmedinf.2024.105654
    scopusId: SCOPUS_ID:85206902315
    authors: Schwieger A., Angst K., de Bardeci M., Burrer A., Cathomas F., Ferrea S., Grätz F., Knorr M., Kronenberg G., Spiller T., Troi D., Seifritz E., Weber S., Olbrich S.
    year: "2024"
    abstract: "Objective: To evaluate whether psychiatric discharge summaries (DS) generated with ChatGPT-4 from electronic health records (EHR) can match the quality of DS written by psychiatric residents. Methods: At a psychiatric primary care hospital, we compared 20 inpatient DS, written by residents, to those written with ChatGPT-4 from pseudonymized residents’ notes of the patients’ EHRs and a standardized prompt. 8 blinded psychiatry specialists rated both versions on a custom Likert scale from 1 to 5 across 15 quality subcategories. The primary outcome was the overall rating difference between the two groups. The secondary outcomes were the rating differences at the level of individual question, case, and rater. Results: Human-written DS were rated significantly higher than AI (mean ratings: human 3.78, AI 3.12, p < 0.05). They surpassed AI significantly in 12/15 questions and 16/20 cases and were favored significantly by 7/8 raters. For “low expected correction effort”, human DS were rated as 67 % favorable, 19 % neutral, and 14 % unfavorable, whereas AI-DS were rated as 22 % favorable, 33 % neutral, and 45 % unfavorable. Hallucinations were present in 40 % of AI-DS, with 37.5 % deemed highly clinically relevant. Minor content mistakes were found in 30 % of AI and 10 % of human DS. Raters correctly identified AI-DS with 81 % sensitivity and 75 % specificity. Discussion: Overall, AI-DS did not match the quality of resident-written DS but performed similarly in 20% of cases and were rated as favorable for “low expected correction effort” in 22% of cases. AI-DS lacked most in content specificity, ability to distill key case information, and coherence but performed adequately in conciseness, adherence to formalities, relevance of included content, and form. Conclusion: LLM-written DS show potential as templates for physicians to finalize, potentially saving time in the future."
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.ijmedinf.2024.105654
        scopusId: SCOPUS_ID:85206902315
    finalDecision: exclude
  - title: KNOWLEDGE, ATTITUDES, AND PRACTICES REGARDING CHATBOTS AMONG HEALTHCARE PROFESSIONALS IN A TERTIARY CARE HOSPITAL
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About knowledge/attitudes/practices regarding chatbots, not about assessment or feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Title indicates attitudes/practices, not assessment or feedback processes."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.22159/ajpcr.2024v17i11.52355
    scopusId: SCOPUS_ID:85209554955
    authors: "Roy R., Aniket , Rahiman F., Vijayamathy "
    year: "2024"
    abstract: "Objective: (1) Primary: Determine healthcare professionals' knowledge, attitudes, and practices (KAP) related to AI Chatbots. (2) Secondary: Assess healthcare professionals' perspectives on using Chatbots as teaching tools and implementing them in the Competency-Based Medical Education curriculum. Methods: An online questionnaire was distributed to 132 health professionals, including faculty and CRMIs, through Google Forms. Data on artificial intelligence (AI)-related KAP and barriers were collected following IHEC approval. The KAP scores, along with the relationship between categorical variables - such as population type and the use of AI Chatbots - were analyzed using Statistical Package for the Social Sciences software. Results: The study revealed that participants had a moderate level of knowledge about AI Chatbots. Attitudes were mixed, with some skepticism about AI replacing human teachers but also recognition of its benefits. Most participants infrequently used AI Chatbots in their daily activities. Barriers to usage included lack of knowledge, limited access, time constraints, and curriculum gaps. Conclusion: This study underscored the need to enhance medical education with AI topics and address existing barriers. It is crucial to better prepare health professionals for AI integration to leverage AI's potential for improving patient care and training."
    mergedFrom:
      - source: scopus
        doi: 10.22159/ajpcr.2024v17i11.52355
        scopusId: SCOPUS_ID:85209554955
    finalDecision: exclude
  - title: Applications of generative artificial intelligence to augment clinician's capability for medical data analysis in RStudio
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About using generative AI for data analysis in RStudio, not about assessment or feedback"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    doi: 10.4103/ija.ija_264_24
    scopusId: SCOPUS_ID:85201720297
    authors: Mahajan V., Konar S., Ray A., Samra T.
    year: "2024"
    mergedFrom:
      - source: scopus
        doi: 10.4103/ija.ija_264_24
        scopusId: SCOPUS_ID:85201720297
    finalDecision: exclude
  - title: "Thank you artificial intelligence: Evidence-based just-in-time training via a large language model"
    reviews:
      - decision: uncertain
        comment: LLM for just-in-time training; may relate to clinical education with assessment/feedback components
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:31.333000+00:00
      - decision: uncertain
        comment: No abstract available; forwarded to full-text screening.
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:34.427000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: This is an invited commentary/editorial discussing the OARA (Operating and Anesthesia Reference Assistant) LLM developed by others. It presents no original data and is not an original research article, systematic review, meta-analysis, or preprint."
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17 04:25:53.576000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Invited commentary (not original research, systematic review, meta-analysis, or preprint)."
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17 04:26:01.509000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Invited commentary/editorial, not original research/SR/MA."
        reviewer: TK
        basis: fulltext
        timestamp: 2026-02-17T08:54:03.559Z
    doi: 10.1016/j.amjsurg.2024.04.007
    scopusId: SCOPUS_ID:85189971900
    authors: Boyer T.J., Mitchell S.A.
    year: "2024"
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.amjsurg.2024.04.007
        scopusId: SCOPUS_ID:85189971900
    finalDecision: exclude
  - title: Translating musculoskeletal radiology reports into patient-friendly summaries using ChatGPT-4
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Translating radiology reports for patients, not about assessment/feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on report simplification, not learner assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    doi: 10.1007/s00256-024-04599-2
    scopusId: SCOPUS_ID:85183667805
    authors: Kuckelman I.J., Wetley K., Yi P.H., Ross A.B.
    year: "2024"
    abstract: "Objective: To assess the feasibility of using large language models (LLMs), specifically ChatGPT-4, to generate concise and accurate layperson summaries of musculoskeletal radiology reports. Methods: Sixty radiology reports, comprising 20 MR shoulder, 20 MR knee, and 20 MR lumbar spine reports, were obtained via PACS. The reports were deidentified and then submitted to ChatGPT-4, with the prompt “Produce an organized and concise layperson summary of the findings of the following radiology report. Target a reading level of 8-9th grade and word count <300 words.” Three (two primary and one later added for validation) independent readers evaluated the summaries for completeness and accuracy compared to the original reports. Summaries were rated on a scale of 1 to 3: 1) summaries that were incorrect or incomplete, potentially providing harmful or confusing information; 2) summaries that were mostly correct and complete, unlikely to cause confusion or harm; and 3) summaries that were entirely correct and complete. Results: All 60 responses met the criteria for word count and readability. Mean ratings for accuracy were 2.58 for reader 1, 2.71 for reader 2, and 2.77 for reader 3. Mean ratings for completeness were 2.87 for reader 1 and 2.73 for reader 2 and 2.87 for reader 3. For accuracy, reader 1 identified three summaries as a 1, reader 2 identified one, and reader 3 identified none. For the two primary readers, inter-reader agreement was low for accuracy (kappa 0.33) and completeness (kappa 0.29). There were no statistically significant changes in inter-reader agreement when the third reader’s ratings were included in analysis. Conclusion: Overall ratings for accuracy and completeness of the AI-generated layperson report summaries were high with only a small minority likely to be confusing or inaccurate. This study illustrates the potential for leveraging generative AI, such as ChatGPT-4, to automate the production of patient-friendly summaries for musculoskeletal MR imaging."
    mergedFrom:
      - source: scopus
        doi: 10.1007/s00256-024-04599-2
        scopusId: SCOPUS_ID:85183667805
    finalDecision: exclude
  - title: Report of two cases of acute appendicitis prior to exams in medical students using OpenAI’s ChatGPT
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Case report, not original research article"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:05.210000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Case report on appendicitis is not about assessment/feedback in WBA"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:07.189000+00:00
    doi: 10.29333/ejgm/14571
    scopusId: SCOPUS_ID:85199027489
    authors: Moya-Salazar J., Salazar C.R., Goicochea-Palomino E.A.
    year: "2024"
    abstract: "Introduction: Acute appendicitis (AA) is a significant cause of emergency admissions among university students aged over 20 years. Students experience high demands during their undergraduate studies, often leading them to postpone mealtimes or rest breaks. Here, with the assistance of ChatGPT, we present two cases of medical students with AA who delayed their surgical treatment due to exams. Clinical cases: Two young patients (a 23-year-old female and a 20-year-old male) presented with severe abdominal pain, anorexia, and general discomfort. They had high C-reactive protein levels and white blood cell counts, as well as an ultrasound showing an enlarged appendix. Despite the confirmed diagnosis of appendicitis through abdominal ultrasound, both patients decided to postpone hospitalization and surgery due to university exams. They subsequently underwent uncomplicated emergency laparoscopic appendectomy. The clinical report of both cases was successfully addressed with ChatGPT, using the initial data from the medical record, and coherent discussions regarding patient management and follow-up were generated. Conclusions: The surgical intervention applied in both cases of AA was successful, and although both patients recovered without issues, delaying surgery can increase the risk of complications. It is necessary to educate the university population about the importance of immediate medical attention for the treatment of AA. ChatGPT appears to be a useful tool for organizing clinical information and improving discussions on therapeutic management."
    mergedFrom:
      - source: scopus
        doi: 10.29333/ejgm/14571
        scopusId: SCOPUS_ID:85199027489
    finalDecision: exclude
  - title: Evaluation of ChatGPT pathology knowledge using board-style questions
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 4: Board-style exam questions, not workplace-based assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Benchmarks ChatGPT knowledge on pathology board-style questions; does not use generative AI for assessment/feedback of learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:58.495000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Board-question performance study; not assessment/feedback or analysis of clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:04.178000+00:00
    doi: 10.1093/ajcp/aqad158
    scopusId: SCOPUS_ID:85183496849
    authors: Geetha S.D., Khan A., Khan A., Kannadath B.S., Vitkovski T.
    year: "2024"
    abstract: "Objectives: ChatGPT is an artificial intelligence chatbot developed by OpenAI. Its extensive knowledge and unique interactive capabilities enable its use in various innovative ways in the medical field, such as writing clinical notes and simplifying radiology reports. Through this study, we aimed to analyze the pathology knowledge of ChatGPT to advocate its role in transforming pathology education. Methods: The American Society for Clinical Pathology Resident Question Bank 2022 was used to test ChatGPT, version 4. Practice tests were created in each subcategory and answered based on the input that ChatGPT provided. Questions that required interpretation of images were excluded. We analyzed ChatGPT performance and compared it with average peer performance. Results: The overall performance of ChatGPT was 56.98%, lower than that of the average peer performance of 62.81%. ChatGPT performed better on clinical pathology (60.42%) than on anatomic pathology (54.94%). Furthermore, its performance was better on easy questions (68.47%) than on intermediate (52.88%) and difficult questions (37.21%). Conclusions: ChatGPT has the potential to be a valuable resource in pathology education if trained on a larger, specialized medical data set. Those relying on it (in its current form) solely for the purpose of pathology training should be cautious."
    mergedFrom:
      - source: scopus
        doi: 10.1093/ajcp/aqad158
        scopusId: SCOPUS_ID:85183496849
    finalDecision: exclude
  - title: "The Role of Artificial Intelligence in Surgery: What do General Surgery Residents Think?"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records. Cross-sectional survey of surgery residents' perceptions of AI in medicine."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:09.784000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Survey of resident perceptions of AI, not generative-AI assessment/feedback or analysis of observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:15.335000+00:00
    doi: 10.1177/00031348231209524
    scopusId: SCOPUS_ID:85174496369
    authors: St John A., Cooper L., Kavic S.M.
    year: "2024"
    abstract: "Background: Artificial intelligence (AI) holds significant potential in medical education and patient care, but its rapid emergence presents ethical and practical challenges. This study explored the perspectives of surgical residents on AI's role in medicine. Methods: We performed a cross-sectional study surveying general surgery residents at a university-affiliated teaching hospital about their views on AI in medicine and surgical training. The survey covered demographics, residents' understanding of AI, its integration into medical practice, and use of AI tools like ChatGPT. The survey design was inspired by a recent national survey and underwent pretesting before deployment. Results: Of the 31 participants surveyed, 24% identified diagnostics as AI's top application, 12% favored its use in identifying anatomical structures in surgeries, and 20% endorsed AI integration into EMRs for predictive models. Attitudes toward AI varied based on its intended application: 77.41% expressed concern about AI making life decisions and 70.97% felt excited about its application for repetitive tasks. A significant 67.74% believed AI could enhance the understanding of medical knowledge. Perception of AI integration varied with AI familiarity (P =.01), with more knowledgeable respondents expressing more positivity. Moreover, familiarity influenced the perceived academic use of ChatGPT (P =.039) and attitudes toward AI in operating rooms (P =.032). Conclusion: This study provides insights into surgery residents’ perceptions of AI in medical practice and training. These findings can inform future research, shape policy decisions, and guide AI development, promoting a harmonious collaboration between AI and surgeons to improve both training and patient care."
    mergedFrom:
      - source: scopus
        doi: 10.1177/00031348231209524
        scopusId: SCOPUS_ID:85174496369
    finalDecision: exclude
  - title: "Artificial intelligence: Revolutionizing cardiology with large language models"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Title suggests a narrative review or commentary about AI in cardiology, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:01.281000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of clinical observation records in education."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:03.278000+00:00
    doi: 10.1093/eurheartj/ehad838
    scopusId: SCOPUS_ID:85184151775
    authors: Boonstra M.J., Weissenbacher D., Moore J.H., Gonzalez-Hernandez G., Asselbergs F.W.
    year: "2024"
    abstract: Natural language processing techniques are having an increasing impact on clinical care from patient, clinician, administrator, and research perspective. Among others are automated generation of clinical notes and discharge letters, medical term coding for billing, medical chatbots both for patients and clinicians, data enrichment in the identification of disease symptoms or diagnosis, cohort selection for clinical trial, and auditing purposes. In the review, an overview of the history in natural language processing techniques developed with brief technical background is presented. Subsequently, the review will discuss implementation strategies of natural language processing tools, thereby specifically focusing on large language models, and conclude with future opportunities in the application of such techniques in the field of cardiology.
    mergedFrom:
      - source: scopus
        doi: 10.1093/eurheartj/ehad838
        scopusId: SCOPUS_ID:85184151775
    finalDecision: exclude
  - title: Research and Application of Dialogue Diagnosis and Treatment System Based on Large Language Model
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on an LLM diagnosis/treatment system, not learner assessment, feedback, or analysis of observation records"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment, feedback, or analysis of observation records for learners. Describes a conversational diagnosis/treatment system for doctor-patient interactions in a hospital, not about educational assessment or feedback for health-professional learners."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:20.609000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Study focuses on a clinical doctor-patient dialogue system and does not involve medical/health-professional learners."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:24.936000+00:00
    doi: 10.1109/ACAIT63902.2024.11021946
    scopusId: SCOPUS_ID:105009159401
    authors: Xu M.
    year: "2024"
    abstract: "The design, research and practical application of conversational diagnosis and treatment system based on Large Language Model (LLM) has potential value in improving outpatient efficiency and reducing medical error rate compared with traditional diagnosis and treatment scenarios. First, the design of the system is based on BERT's (Bidirectional Encoder Representations from Transformers) architectural idea. GPT-4 (Pre-Trained) model was introduced to generate and expand contexts, and a conversational diagnosis and treatment system based on a large language model was built. The system is pre-trained on a large-scale medical knowledge base, mimics the natural language communication between doctors and patients to carry out in-depth dialogue and interaction, accurately capture the symptoms and health information described by patients, and provide more effective diagnostic suggestions combined with the knowledge of diagnosis and treatment field. Secondly, the doctor-patient dialogue intention recognition algorithm is used to assist the model to capture and analyze the patient's emotional expression, medication history and pathological feedback to ensure the accurate transmission and processing of patient information. Finally, when tested in a Tier 3 hospital, the system showed significant advantages over the traditional single-mode dialogue system in three aspects: automatic generation of medical record reports, real-time response to patient inquiries, and simulated doctor-patient communication. After more than 1000 doctor-patient conversation tests, the system accurately identified the patient's intention ratio reached 95%, and the accuracy of diagnostic recommendations obtained a high evaluation of 93%. It is expected to become an important auxiliary tool for the medical industry in the future."
    mergedFrom:
      - source: scopus
        doi: 10.1109/ACAIT63902.2024.11021946
        scopusId: SCOPUS_ID:105009159401
    finalDecision: exclude
  - title: "Artificial intelligence and pain management: cautiously optimistic"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Title suggests commentary/opinion piece about AI in pain management, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/editorial publication type"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    doi: 10.1080/17581869.2024.2392483
    scopusId: SCOPUS_ID:85203507091
    authors: Srinivasan B., Venkataraman A., Raja S.N.
    year: "2024"
    mergedFrom:
      - source: scopus
        doi: 10.1080/17581869.2024.2392483
        scopusId: SCOPUS_ID:85203507091
    finalDecision: exclude
  - title: Large language models to evaluate racial discrepancies in performance status assignment
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: JCO supplement indicates conference abstract, not original research article"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:44.730000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on performance status assignment, not educational assessment/feedback of learners"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:47.301000+00:00
    doi: 10.1200/JCO.2024.42.16_suppl.1561
    scopusId: SCOPUS_ID:105023280875
    authors: Demirci A., Hong J.C., Zack T.
    year: "2024"
    abstract: "Background: Oncologic treatment eligibility relies heavily on “Performance Status” (PS), a subjective gauge of a patient’s overall health. Despite frameworks aimed at quantifying PS, these assessments remain prone to potential bias. A systematic evaluation of discrepancies in language between ECOG PS and race has yet to be conducted. Large language models (LLMs) have the ability to synthesize text and may enable an assessment of the relationship between race, note text, and physician-documented PS. We hypothesize that LLMs can quantify these relationships to understand potential inconsistencies in ECOG PS. Methods: In our single-institution cohort study, we examined patients from medical or radiation oncology clinics between January 2012 to December 2023 with documented ECOG PS. PS was extracted from clinical notes and redacted from downstream analyses. Notes were categorized by patient-reported race (Asian, Black, White, Other). 1,500 matched subjective assessments (from oncologist clinical notes) and PS were randomly sampled for each race, except for Black patients, where only 640 entries were available. We divided training and test sets using an 80-20 ratio across each race. Using the training cohort, we trained race-specific models to assign ECOG PS based on subjective assessment, using two LLMs: UCSFBERT (BERT pretrained on UCSF data) and RoBERTa. In the hold-out test sets, we applied each model across all races to assign ECOG PS based on subjective assessments. These were evaluated by micro-F1 (model accuracy) and Multiway ANOVA to compute model-specific p-values. Results: 13.6% of patients had a documented ECOG 2+, with variation across races (Asian: 16.1%, Black: 17.7%, White: 7.7%, Other: 15.3%). Models tended to assign Black patients with ECOG 0-1 with worse performance status (ECOG 2+). Other than RoBERTa models trained on Asian and White cohorts, all models trained on a specific race showed disparate classification results when applied to other races (out-of-domain), highlighting potential biases in LLM models trained where a specific race predominates (Table; ANOVA). Conclusions: This study demonstrates a novel way to use LLM to assess discordance within physician documentation and performance status assignment. It underscores the need for incorporating diverse demographic data when using LLM in medical contexts. This research has potential to dissect how bias may be propagated within physician practice and provide insight into known disparities in clinical trial enrollment and standard of care implementation. Research Sponsor: UCSF; Conquer Cancer, the ASCO Foundation."
    mergedFrom:
      - source: scopus
        doi: 10.1200/JCO.2024.42.16_suppl.1561
        scopusId: SCOPUS_ID:105023280875
    finalDecision: exclude
  - title: "ChatGPT as a life coach for professional identity formation in medical education: A self-regulated learning perspective"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About ChatGPT as a life coach for professional identity formation, not about assessment, feedback, or analysis of observation records"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 01:01:55.800000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Professional identity formation focus is not learner assessment/feedback or observation-record analysis."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 01:01:56.687000+00:00
    doi: 10.30191/ETS.202407_27(3).TP03
    scopusId: SCOPUS_ID:85199339485
    authors: Huang H., Lin H.C.
    year: "2024"
    abstract: Professional Identity Formation (PIF) is considered a crucial process in medical education. It involves how medical students identify their role as physicians, discover their professional positioning, and gradually develop their professional identity through social interactions. This qualitative descriptive study adopted the phenomenological method; it proposed the DSCOR (Diverse thinking, Seeking advice, Construction, Organizing and sharing, and Reflection) model based on the ChatGPT as a life coach (ChatGPT-LC) self-regulated learning (SRL) approach to design a PIF course for six medical students. Data collection included digital storytelling created using AI-generated techniques, learning sheets, direct observations, reflective feedback forms, and semi-structured interviews. The data transcription and analysis were conducted using Colaizzi’s method. The results revealed three benefits of Generative AI (i.e., ChatGPT), namely “increasing motivation for planning PIF, ” “strengthening the mastery of PIF, ” and “broadening the perspectives of PIF.” Moreover, the ChatGPT-LC SRL approach had a positive influence on students, helping them understand the significance of PIF in their personal development at the early stage. The artificial intelligence-generated content provided positive guidance and supportive learning, offering specific suggestions and assistance. This brought about benefits for learning, and provided initial evidence for the application of ChatGPT-LC in medical education.
    mergedFrom:
      - source: scopus
        doi: 10.30191/ETS.202407_27(3).TP03
        scopusId: SCOPUS_ID:85199339485
    finalDecision: exclude
  - title: Using ChatGPT to predict the future of personalized medicine
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About personalized medicine predictions, not about assessment or feedback of learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Title does not indicate assessment, feedback, or analysis of observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.1038/s41397-023-00316-9
    scopusId: SCOPUS_ID:85171478224
    authors: Patrinos G.P., Sarhangi N., Sarrami B., Khodayari N., Larijani B., Hasanzad M.
    year: "2023"
    abstract: Personalized medicine is a novel frontier in health care that is based on each person’s unique genetic makeup. It represents an exciting opportunity to improve the future of individualized health care for all individuals. Pharmacogenomics, as the main part of personalized medicine, aims to optimize and create a more targeted treatment approach based on genetic variations in drug response. It is predicted that future treatments will be algorithm-based instead of evidence-based that will consider a patient’s genetic, transcriptomic, proteomic, epigenetic, and lifestyle factors resulting in individualized medication. A generative pretrained transformer (GPT) is an artificial intelligence (AI) tool that generates language resembling human-like writing enabling users to engage in a manner that is practically identical to speaking with a human being. GPT’s predictive algorithms can respond to questions that have never been addressed. Chat Generative Pretrained Transformer (ChatGPT) is an AI chatbot’s advanced with conversational capabilities. In the present study, questions were asked from ChatGPT about the future of personalized medicine and pharmacogenomics. ChatGPT predicted both to be a promising approach with a bright future that holds great promises in improving patient outcomes and transforming the field of medicine. But it still has several limitations that need to be solved.
    mergedFrom:
      - source: scopus
        doi: 10.1038/s41397-023-00316-9
        scopusId: SCOPUS_ID:85171478224
    finalDecision: exclude
  - title: "ChatGPT for scientific community: Boon or bane?"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Title suggests a commentary/opinion piece, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/opinion-style article, not an eligible study design"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    doi: 10.1016/j.mjafi.2023.06.009
    scopusId: SCOPUS_ID:85167811728
    authors: Jain A.
    year: "2023"
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.mjafi.2023.06.009
        scopusId: SCOPUS_ID:85167811728
    finalDecision: exclude
  - title: "Performance and exploration of ChatGPT in medical examination, records and education in Chinese: Pave the way for medical AI"
    reviews:
      - decision: uncertain
        comment: ""
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting; primarily evaluates ChatGPT performance on medical licensing examinations with only small-scale exploration of discharge summaries"
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:30:50.333000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Primarily evaluates LLM exam performance and general education tasks, not workplace-based assessment/feedback on clinical observation records."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:31:55.586000+00:00
    doi: 10.1016/j.ijmedinf.2023.105173
    scopusId: SCOPUS_ID:85167842454
    authors: Wang H., Wu W.Z., Dou Z., He L., Yang L.
    year: "2023"
    abstract: "Background: Although chat generative pre-trained transformer (ChatGPT) has made several successful attempts in the medical field, most notably in answering medical questions in English, no studies have evaluated ChatGPT's performance in a Chinese context for a medical task. Objective: The aim of this study was to evaluate ChatGPT's ability to understand medical knowledge in Chinese, as well as its potential to serve as an electronic health infrastructure for medical development, by evaluating its performance in medical examinations, records, and education. Method: The Chinese (CNMLE) and English (ENMLE) datasets of the China National Medical Licensing Examination and the Chinese dataset (NEEPM) of the China National Entrance Examination for Postgraduate Clinical Medicine Comprehensive Ability were used to evaluate the performance of ChatGPT (GPT-3.5 and GPT-4). We assessed answer accuracy, verbal fluency, and the classification of incorrect responses owing to hallucinations on multiple occasions. In addition, we tested ChatGPT's performance on discharge summaries and group learning in a Chinese context on a small scale. Results: The accuracy of GPT-3.5 in CNMLE, ENMLE, and NEEPM was 56% (56/100), 76% (76/100), and 62% (62/100), respectively, compared to that of GPT-4, which was of 84% (84/100), 86% (86/100), and 82% (82/100). The verbal fluency of all the ChatGPT responses exceeded 95%. Among the GPT-3.5 incorrect responses, the proportions of open-domain hallucinations were 66 % (29/44), 54 % (14/24), and 63 % (24/38), whereas close-domain hallucinations accounted for 34 % (15/44), 46 % (14/24), and 37 % (14/38), respectively. By contrast, GPT-4 open-domain hallucinations accounted for 56% (9/16), 43% (6/14), and 83% (15/18), while close-domain hallucinations accounted for 44% (7/16), 57% (8/14), and 17% (3/18), respectively. In the discharge summary, ChatGPT demonstrated logical coherence, however GPT-3.5 could not fulfill the quality requirements, while GPT-4 met the qualification of 60% (6/10). In group learning, the verbal fluency and interaction satisfaction with ChatGPT were 100% (10/10). Conclusion: ChatGPT based on GPT-4 is at par with Chinese medical practitioners who passed the CNMLE and at the standard required for admission to clinical medical graduate programs in China. The GPT-4 shows promising potential for discharge summarization and group learning. Additionally, it shows high verbal fluency, resulting in a positive human–computer interaction experience. GPT-4 significantly improves multiple capabilities and reduces hallucinations compared to the previous GPT-3.5 model, with a particular leap forward in the Chinese comprehension capability of medical tasks. Artificial intelligence (AI) systems face the challenges of hallucinations, legal risks, and ethical issues. However, we discovered ChatGPT's potential to promote medical development as an electronic health infrastructure, paving the way for Medical AI to become necessary."
    mergedFrom:
      - source: scopus
        doi: 10.1016/j.ijmedinf.2023.105173
        scopusId: SCOPUS_ID:85167842454
    finalDecision: exclude
  - title: How large language models including generative pre-trained transformer (GPT) 3 and 4 will impact medicine and surgery
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Title suggests a commentary or perspective piece, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:39.072000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Not an original research article, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:41.772000+00:00
    doi: 10.1007/s10151-023-02837-8
    scopusId: SCOPUS_ID:85164826999
    authors: Atallah S.B., Banda N.R., Banda A., Roeck N.A.
    year: "2023"
    mergedFrom:
      - source: scopus
        doi: 10.1007/s10151-023-02837-8
        scopusId: SCOPUS_ID:85164826999
    finalDecision: exclude
  - title: "Evolution of Healthcare with ChatGPT: A Word of Caution"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/editorial about ChatGPT in healthcare, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:17.457000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Title indicates commentary/opinion format."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:18.815000+00:00
    doi: 10.1007/s10439-023-03225-x
    scopusId: SCOPUS_ID:85160276358
    authors: Sajjad M., Saleem R.
    year: "2023"
    mergedFrom:
      - source: scopus
        doi: 10.1007/s10439-023-03225-x
        scopusId: SCOPUS_ID:85160276358
    finalDecision: exclude
  - title: "Retrieve, Summarize, and Verify: How Will ChatGPT Affect Information Seeking from the Medical Literature?"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Not about assessment or feedback; about ChatGPT for literature searching/information seeking"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:14.385000+00:00
      - decision: exclude
        comment: "Exclusion criterion 2: Focuses on literature information seeking, not assessment/feedback/analysis of workplace-based observation records."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:16.044000+00:00
    doi: 10.1681/ASN.0000000000000166
    scopusId: SCOPUS_ID:85163968213
    authors: Jin Q., Leaman R., Lu Z.
    year: "2023"
    mergedFrom:
      - source: scopus
        doi: 10.1681/ASN.0000000000000166
        scopusId: SCOPUS_ID:85163968213
    finalDecision: exclude
  - title: "The Utility of Language Models in Cardiology: A Narrative Review of the Benefits and Concerns of ChatGPT-4"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Explicitly a narrative review, not original research or systematic review"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Narrative review publication type"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    doi: 10.3390/ijerph20156438
    scopusId: SCOPUS_ID:85167748409
    authors: Gala D., Makaryus A.N.
    year: "2023"
    abstract: Artificial intelligence (AI) and language models such as ChatGPT-4 (Generative Pretrained Transformer) have made tremendous advances recently and are rapidly transforming the landscape of medicine. Cardiology is among many of the specialties that utilize AI with the intention of improving patient care. Generative AI, with the use of its advanced machine learning algorithms, has the potential to diagnose heart disease and recommend management options suitable for the patient. This may lead to improved patient outcomes not only by recommending the best treatment plan but also by increasing physician efficiency. Language models could assist physicians with administrative tasks, allowing them to spend more time on patient care. However, there are several concerns with the use of AI and language models in the field of medicine. These technologies may not be the most up-to-date with the latest research and could provide outdated information, which may lead to an adverse event. Secondly, AI tools can be expensive, leading to increased healthcare costs and reduced accessibility to the general population. There is also concern about the loss of the human touch and empathy as AI becomes more mainstream. Healthcare professionals would need to be adequately trained to utilize these tools. While AI and language models have many beneficial traits, all healthcare providers need to be involved and aware of generative AI so as to assure its optimal use and mitigate any potential risks and challenges associated with its implementation. In this review, we discuss the various uses of language models in the field of cardiology.
    mergedFrom:
      - source: scopus
        doi: 10.3390/ijerph20156438
        scopusId: SCOPUS_ID:85167748409
    finalDecision: exclude
  - title: Chat GPT versus human touch in stereotactic radiosurgery
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: About ChatGPT in radiosurgery clinical practice, not about assessment or feedback in education"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:51.794000+00:00
      - decision: exclude
        comment: "Exclusion criterion 6: Participants are not medical or health-professional learners"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:53.464000+00:00
    doi: 10.1007/s11060-023-04353-z
    scopusId: SCOPUS_ID:85160415199
    authors: Dayawansa S., Mantziaris G., Sheehan J.
    year: "2023"
    mergedFrom:
      - source: scopus
        doi: 10.1007/s11060-023-04353-z
        scopusId: SCOPUS_ID:85160415199
    finalDecision: exclude
  - title: ChatGPT is not the solution to physicians’ documentation burden
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Title suggests commentary/opinion piece, not original research"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:58.216000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Commentary/opinion title, not an original research article, systematic review, meta-analysis, or preprint."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:59.773000+00:00
    doi: 10.1038/s41591-023-02341-4
    scopusId: SCOPUS_ID:85159107703
    authors: Preiksaitis C., Sinsky C.A., Rose C.
    year: "2023"
    mergedFrom:
      - source: scopus
        doi: 10.1038/s41591-023-02341-4
        scopusId: SCOPUS_ID:85159107703
    finalDecision: exclude
  - title: May Artificial Intelligence Influence Future Pediatric Research?—The Case of ChatGPT
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Title suggests a commentary/perspective piece about AI in research, not original research on assessment"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:06:11.469000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Title indicates a perspective/discussion piece, not an eligible study design"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:06:12.749000+00:00
    doi: 10.3390/children10040757
    scopusId: SCOPUS_ID:85153792452
    authors: Corsello A., Santangelo A.
    year: "2023"
    abstract: "Background: In recent months, there has been growing interest in the potential of artificial intelligence (AI) to revolutionize various aspects of medicine, including research, education, and clinical practice. ChatGPT represents a leading AI language model, with possible unpredictable effects on the quality of future medical research, including clinical decision-making, medical education, drug development, and better research outcomes. Aim and Methods: In this interview with ChatGPT, we explore the potential impact of AI on future pediatric research. Our discussion covers a range of topics, including the potential positive effects of AI, such as improved clinical decision-making, enhanced medical education, faster drug development, and better research outcomes. We also examine potential negative effects, such as bias and fairness concerns, safety and security issues, overreliance on technology, and ethical considerations. Conclusions: While AI continues to advance, it is crucial to remain vigilant about the possible risks and limitations of these technologies and to consider the implications of these technologies and their use in the medical field. The development of AI language models represents a significant advancement in the field of artificial intelligence and has the potential to revolutionize daily clinical practice in every branch of medicine, both surgical and clinical. Ethical and social implications must also be considered to ensure that these technologies are used in a responsible and beneficial manner."
    mergedFrom:
      - source: scopus
        doi: 10.3390/children10040757
        scopusId: SCOPUS_ID:85153792452
    finalDecision: exclude
  - title: "Large Language Models and the Retina: A Review of Current Applications and Future Directions"
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 7: Non-systematic review of LLMs and retina"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:33.890000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Review article not clearly a systematic review/meta-analysis/preprint/original research."
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:36.514000+00:00
    doi: 10.37845/ret.vit.2023.32.38
    scopusId: SCOPUS_ID:85185801602
    authors: Gilson A., Singer M., Xu H., Chen Q., Adelman R.A.
    year: "2023"
    abstract: Large Language Models (LLMs) have emerged as a potentially transformative force within retinal healthcare, promising substantial advancements that might be analogous to the impact of anti-VEGF injections. These models signify a shift in patient-provider dynamics and clinical documentation, offering avenues to expedite patient inquiries as well as automate documentation through integration with Electronic Health Records. LLMs may increase direct patient engagement and reduce physician burnout. Simultaneously, provider-centric models may aid in navigating intricate clinical scenarios and rare diseases by assisting in literature review. However, their integration poses unique challenges, including the integration of Protected Health Information, interpreting imaging and other information modalities besides text, and the persistent challenge of generating accurate and verifiable responses. These models mandate rigorous evaluation before integration into clinical workflow. As with all medical interventions, there will always be a possibility of negative outcomes, therefore the critical consideration revolves around the acceptable risk of LLMs vs the substantial benefits they may offer.
    mergedFrom:
      - source: scopus
        doi: 10.37845/ret.vit.2023.32.38
        scopusId: SCOPUS_ID:85185801602
    finalDecision: exclude
  - title: "ChatGPT and Large Language Models in Healthcare: Opportunities and Risks"
    reviews:
      - decision: uncertain
        comment: Broad overview of LLMs in healthcare; title is too general to determine if it covers WBA or assessment
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: uncertain
        comment: ""
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Narrative review/overview of ChatGPT opportunities and risks in healthcare; not original research, systematic review, or meta-analysis."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-16 23:31:38.683000+00:00
      - decision: exclude
        comment: "Exclusion criterion 7: Narrative review/opinion paper, not an eligible original research study, systematic review, or meta-analysis."
        reviewer: ai:codex
        basis: abstract
        timestamp: 2026-02-16 23:32:43.451000+00:00
    doi: 10.1109/AIBThings58340.2023.10291020
    scopusId: SCOPUS_ID:85178510215
    authors: Ali H., Qadir J., Alam T., Househ M., Shah Z.
    year: "2023"
    abstract: ChatGPT, a pre-Trained large language model (LLM), has the potential to transform healthcare by providing valid clinical insights and reducing doctors' workload. There are already signs that such tools can be useful for automating the generation of patient discharge reports, clinical vignettes, and radiology reports. Such tools can also capture the vast medical knowledge base as demonstrated by ChatGPT clearing the United States Medical Licensing Examination (USMLE). Such tools promise to make healthcare more accessible, scalable, and efficient, leading to better patient outcomes. However, such tools are far from perfect and well-known to be susceptible to error, misinformation, and bias. In this paper, we review the potential applications of ChatGPT in healthcare and also identify potentials risks that must be addressed before ChatGPT and other LLM tools can be safely adopted in healthcare. First, we offer case studies on using ChatGPT for passing USMLE, identifying prevention methods for cardiovascular disease, generating patient discharge reports, generating clinical vignettes, and generating radiology reports. Second, we present the opportunities that ChatGPT offers in healthcare. By leveraging its language generation and processing capabilities, ChatGPT can streamline and improve a range of healthcare tasks, from digitizing clinical notes and improving the accuracy of diagnosis to revolutionizing medical education and empowering patients with personalized healthcare information. Finally, we reflect on the associated risks and conclude that caution is advised in interpreting the results of ChatGPT as these studies are preliminary and not entirely error-free.
    mergedFrom:
      - source: scopus
        doi: 10.1109/AIBThings58340.2023.10291020
        scopusId: SCOPUS_ID:85178510215
    finalDecision: exclude
  - title: Evaluating the performance of ChatGPT in medical physiology university examination of phase I MBBS
    reviews:
      - decision: exclude
        comment: "Exclusion criterion 2: Tests ChatGPT's performance on a physiology exam, not about assessment or feedback of human learners"
        reviewer: ai:claude
        basis: title
        timestamp: 2026-02-16 09:05:48.884000+00:00
      - decision: exclude
        comment: "Exclusion criterion 4: Not in a workplace-based or clinical education setting (university examination)"
        reviewer: ai:codex
        basis: title
        timestamp: 2026-02-16 09:05:50.207000+00:00
    doi: 10.1152/ADVAN.00036.2023
    scopusId: SCOPUS_ID:85150966687
    authors: Subramani M., Jaleel I., Mohan S.K.
    year: "2023"
    abstract: San Francisco-based OpenAI Inc/LP developed and launched the revolutionary and enormous languagemodel powered by an artificial intelligence (AI) chatbot systemnamed ChatGPT on November 30, 2022, which generates humanoid responses to varied and diverse natural queries (1). Within a week of its release, ChatGPT has crossed the over 1million usermark (2). This ChatGPT (Chat Generative Pre-trained Transformer) is a freely accessible conversational AI tool that was developed on the concept of reinforcement learning fromhuman feedback (3). Ever since the integration of ChatGPT, the Microsoft Bing search engine has reached an impressive milestone ofmore than 100million daily active users and is soon to be made instantly available for millions of users online along with healthcare professionals and students (4). In recognition of its high impact, the efficiency of ChatGPT has also been tested in the United States Medical Licensing Examination (USMLE), and it has successfully cleared the test (5). Although ChatGPT cannot replace human thinking and reasoning, its usability in several fields including medical education and clinical decision making is worthy of more research that can assess its accuracy and utility in themedical field (6).
    mergedFrom:
      - source: scopus
        doi: 10.1152/ADVAN.00036.2023
        scopusId: SCOPUS_ID:85150966687
    finalDecision: exclude
  - title: "Rubrics to Prompts: Assessing Medical Student Post-Encounter Notes with AI"
    reviews:
      - decision: include
        comment: Identified via forward citation tracking (citing Burke 2024). AI-based assessment of medical student post-encounter notes; potentially WBA-relevant.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-17 00:00:00+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: Study conducted at UT Southwestern Simulation Center; assesses OSCE post-encounter notes (simulated environment, not workplace-based assessment)."
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-17 00:00:01+00:00
      - decision: exclude
        comment: "Exclusion criterion 3: OSCE-based assessment at a simulation center. Not workplace-based."
        reviewer: human:TK
        basis: abstract
        timestamp: 2026-02-17 00:00:02+00:00
    doi: 10.1056/aics2400631
    authors: Jamieson AR, Holcomb MJ, Dalton TO, Campbell KK, Vedovato S, Shakur AH, Kang S, Hein D, Lawson J, Danuser G, Scott DJ
    year: "2024"
    abstract: Case study from UT Southwestern Medical Center Simulation Center describing the first successful prospective deployment of a generative AI-based automated grading system for medical student post-encounter OSCE notes. Achieved up to 89.7% agreement with human expert graders (Cohen's kappa 0.79) and Spearman's correlation of 0.86 with total examination score. Analysis included 1124 students, 10175 post-encounter notes, and 156978 scored items.
    mergedFrom:
      - source: forward-citation
        doi: 10.1056/aics2400631
        note: Forward citation of Burke 2024 (10.2196/56342), identified via khan2025-forward-citations.yaml
    finalDecision: exclude
  - title: "Usefulness of Large Language Models (LLMs) for Student Feedback on H&P During Clerkship: Artificial Intelligence for Personalized Learning"
    reviews:
      - decision: uncertain
        comment: Identified via forward citation tracking (citing Burke 2024). LLMs (GPT-4) for H&P feedback during clerkship. Clerkship is WBA-relevant, but abstract describes 'case-based learning' which may be simulated rather than actual workplace assessment. Requires full-text review.
        reviewer: ai:claude
        basis: abstract
        timestamp: 2026-02-17 00:00:03+00:00
      - decision: uncertain
        comment: Forward citation. H&P feedback during clerkship could be WBA, but 'case-based learning' framing is ambiguous. Needs full-text to confirm setting.
        reviewer: human:TK
        basis: abstract
        timestamp: 2026-02-17 00:00:04+00:00
      - decision: include
        comment: Uses GPT-4/GPT-3.5 to provide personalized feedback on medical students' H&P notes during clinical clerkship rotations. Mixed-methods data from 100 M3 students on acceptability, personalization, critical thinking, and educational impact of LLM-generated feedback in workplace-based setting.
        reviewer: ai:claude
        basis: fulltext
        timestamp: 2026-02-17T07:34:39.189Z
      - decision: include
        comment: ""
        reviewer: ai:codex
        basis: fulltext
        timestamp: 2026-02-17T07:34:41.875Z
    doi: 10.1145/3712298
    authors: Bany Abdelnabi AA, Soykan B, Bhatti D, Rabadi G
    year: "2025"
    abstract: Large Language Models (LLMs) notably GPT-4, demonstrate exceptional language generation and comprehension abilities, and they have potential uses in clinical practice, learning, and medical research. In this study, we explore practical use of Large Language Models (LLMs) in enhancing case-based learning in medical education. The study employes a designed mixed-methods approach, combining quantitative metrics with qualitative feedback from 100 medical students, providing comprehensive insights into both the technical performance and educational value of LLM-based feedback systems. Our results indicate that LLMs can enhance medical students' History and Physical (H&P) skills by providing personalized insights, fostering critical thinking, and improving their abilities to analyze, diagnose, and present clinical cases. This study has surfaced significant insights into the potential benefits and limitations of integrating LLMs into medical education. Our findings show the positive impact of LLMs on enhancing personalized learning experiences, critical thinking, and the effectiveness of case-based learning aids and highlighting its limitations.
    mergedFrom:
      - source: forward-citation
        doi: 10.1145/3712298
        note: Forward citation of Burke 2024 (10.2196/56342), identified via khan2025-forward-citations.yaml
    finalDecision: include
reviewers:
  - name: ai:claude
    basis: title
  - name: ai:codex
    basis: title
  - name: ai:claude
    basis: abstract
  - name: ai:codex
    basis: abstract
  - name: ai:claude
    basis: fulltext
  - name: ai:codex
    basis: fulltext
  - name: TK
    basis: fulltext
