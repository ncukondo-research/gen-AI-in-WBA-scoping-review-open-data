# Supplementary Table S2. Mapping of Validity Evidence to Downing's Framework

| Study | Content | Response Process | Internal Structure | Relationship to Other Variables | Consequences |
|-------|---------|-----------------|--------------------|---------------------------------|-------------|
| @Gin2024-ss | Universal Sentence Encoder embeddings with principal component analysis retained 17 components capturing 33% of thematic variance for theme identification.; At least two medical education coders independently analyzed percentile narrative subsets and iterated to consensus theme labels for all components. | All AI and statistical analyses ran locally with de-identification, no cloud exposure, no participant-data training, and institutional review board approval.; Quality assurance combined iterative human-coder consensus with pronoun neutralization in training and analysis before final sentiment modeling. | Pre-mitigation sentiment showed approximately 5% female and 10% gender-neutral penalties; retraining mitigated classifier bias, while entrustment ratings lacked subgroup differences. | β = 0.47, OR = 1.60; Outputs differentiated writer role, gender, and underrepresented in medicine status, with group-specific theme-entrustment associations identified through interaction modeling. | N/A |
| @Jarry-Trujillo2024-kg | Prompt required error identification and concise feedback grounded in clinical experience and Operative Performance Rating System criteria for cholecystectomy.; Twenty scenarios were purposefully sampled to cover relevant, common, risky, and desirable technical movements in laparoscopic cholecystectomy.; Clinical and education experts reviewed usefulness, quality, and error naming, providing specialized appraisal of output appropriateness. | AI and surgeons correctly classified all 6 non-error scenarios as "no error detected," and identified all 14 predefined error scenarios; this functions as indirect grounding/error analysis; Scenario selection calibration, independent validation of text transformation, and identical instructions to surgeons and ChatGPT supported comparison fidelity. | N/A | usefulness 96.43%; Error-containing and non-error scenarios were perfectly separated, with 14 of 14 errors detected and 6 of 6 non-errors rejected. | Residents rated ChatGPT feedback useful in 96.43% of assessments, and blinded raters frequently misidentified ChatGPT outputs as human-written. |
| @ahmad-2025 | ChatGPT used faculty-matched instructions and ACGME competency definitions adapted from NEJM language to align coding criteria.; Analyses mapped feedback to all six ACGME competencies and reported cross-modality distributions across SIMPL-OR, OSATS, and EOR.; Two blinded faculty set the reference, and ChatGPT reached 90% concordance with Cohen''s kappa 0.94 on 60 entries. | Outputs included rationale linked to feedback text, but reasoning quality was not independently and systematically evaluated.; Entries were anonymized by removing resident and faculty identifiers before analysis, indicating explicit privacy-protective handling.; Quality checks included pretest concordance, session resets, memory clearing, and supplementary blinded human recoding during scaled analysis. | N/A | κ = 0.94; concordance 90% | N/A |
| @atsukawa-2025 | Prompt tuning on 40 reports aligned evaluations to six predefined revision criteria spanning C1 through C6.; Assessment content covered six revision domains, including findings, expression, interpretation/diagnosis, and additional tests/treatments.; Two board-certified radiologists independently reviewed 100 reports using the same six criteria for large language model comparison. | Brief rationales were provided per criterion, but no independent evaluation of reasoning quality was reported.; Yes (indirect: false-positive analysis as proxy) via disagreement/error patterns against radiologist judgments (kappa/accuracy by criterion), though not explicitly labeled hallucination; Inputs excluded patient identifiers and used only findings and diagnosis/impression report text for processing.; Model selection used three repeated runs with majority answers plus independent dual-radiologist review for comparison. | N/A | κ = 0.70–0.72; accuracy 0.86; First-term versus last-term comparisons showed significant differences for C1 through C3, with varying improvement patterns across residents. | N/A |
| @bala-2025 | Iterative one- and few-shot prompt tuning aligned outputs to discrepancy detection, not to a formal competency scoring rubric.; Prompts targeted clinically significant missed diagnoses and differentials, with qualitative checks to reduce chronic or clinically inconsequential findings.; Fourteen resident reviewers judged prediction correctness, while three senior residents adjudicated user-suggested misses by majority vote. | 24 model-predicted missed diagnoses were generated, 19 accepted as correct (implying 5 incorrect predictions); table examples include incorrect model outputs; Manual and regex de-identification removed HIPAA identifiers and critical-findings communications before IRB-approved processing via API.; Prompts and outputs underwent qualitative checks, validation-subset review, a predefined >=50% consensus rule, and senior-resident adjudication of suggested misses. | N/A | κ = 0.43; accuracy 79.2% | Mean satisfaction was 3.50/5 and perceived accuracy 3.64/5, with 71.43% preferring combined traditional and LLM feedback.; Approximately 20% prediction error was observed, including incorrect flagging of clinically insignificant or chronic findings as missed diagnoses. |
| @bany_abdelnabi-2025 | Section-by-section H&P prompts targeted HPI, histories, physical examination, reasoning, and management plan domains to align feedback with documentation criteria. | CoT prompting techniques used but reasoning quality not independently examined; 38% of students reported occasionally encountering hallucinations (false or logically incorrect information); IRB exemption, consent, and anonymous 5-digit codes were used, but specific de-identification safeguards for pasted H&P data were not detailed. | N/A | N/A | Students reported improved critical thinking and case-based learning support, but objective performance outcomes were not measured.; Acceptability was mixed-positive: 44% very satisfied, 37% neutral, 19% dissatisfied; 70%-88% reported helpfulness, relevance, efficiency, and empathetic interaction.; Reported challenges included hallucinations (38%), response variability from small prompt changes (51%), and difficulty prompting for chain-of-thought reasoning (47%). |
| @Kondo2025-jx | Prompt and extraction template were aligned with Model Core Curriculum symptom, examination, and procedure goals for clerkship.; Extraction was evaluated across Model Core Curriculum domains, with category-level sensitivity and specificity reported for symptoms, examinations, and procedures. | Yes (indirect: false-positive analysis as proxy) via mismatch analysis and high specificity reporting against student-corrected lists; Ethics approval, opt-out consent, and full anonymization were implemented to reduce participant-identification risk during data handling.; Pre-use validity checks assessed output format, expected-item matching, and reproducibility, then extracted lists were verified against student-corrected lists. | N/A | Jaccard = 0.59; sensitivity 62.39% | N/A |
| @kwan-2025 | Prompts used original QuAL definitions, then modified Evidence wording to improve classification-task alignment across dimensions.; All three QuAL domains were evaluated, with domain-level performance compared across prompting techniques and fine-tuning strategies.; Two independent raters set QuAL ground truth, and an experienced QuAL rater co-developed modified Evidence definitions. | Chain-of-Thought prompting techniques used but reasoning quality not independently examined; Deidentified comments were processed via OpenAI API privacy settings, with no retention for training and stricter confidentiality handling.; Preprocessing removed nulls, duplicates, and encoding errors, retained rater-agreed labels, and used stratified holdout test evaluation. | N/A | F1 = 0.827/0.949/0.933 | N/A |
| @lyo-2025 | Prompts used explicit severity and type categories modeled after institutional attending discrepancy attestation macros.; Discrepancies were classified across predefined severity and type domains to evaluate coverage of intended discrepancy constructs.; Three neuroradiologists reviewed outputs, and teaching-point relevance was rated relevant in 84.5% of cases. | CoT prompting techniques used but reasoning quality not independently examined; false positives (mean 3.3) and false negatives (mean 12) were quantified against expert ratings; Public model privacy constraints prevented deidentified institutional-report use, so external and synthetic datasets were analyzed under IRB exemption.; Three expert radiologists independently graded outputs in randomized order, with additional five-times repeated inference reproducibility checks. | Five repeated runs showed discrepancy-count ICC(2,1)=0.690, coefficient of variation=0.35, and maximal-severity weighted kappa=0.94. | F1 = 0.66; r = 0.778; κ = 0.35 | Expert radiologists judged teaching points relevant in 84.5% of cases, with higher relevance at greater maximal discrepancy severity. |
| @Partin2025-nj | Standardized prompts embedded verbatim ACGME milestone language and required all behaviors at each level before assigning milestone scores.; Scope was intentionally restricted to 11 of 19 subcompetencies targeting common feedback and remediation domains, excluding systems-based practices and practice-based learning. | Manual de-identification removed names, pronouns, and discoverable protected health information, then used random identifiers and password-protected storage.; Output generation used a fixed sequential workflow with identical prompts and a fresh chat per resident to limit carryover effects. | N/A | r = 0.6-1.0; Postgraduate year analyses showed weakest correlations in PGY-1, strongest in PGY-2, and moderate-high in PGY-3 with training-level mean-difference shifts. | N/A |
| @preiksaitis-2025 | Resident documentation was mapped to 895 MCPEM subcategories using intermediate SNOMED CT mapping to preserve granular clinical detail.; Coverage increased from 376.7 topics in postgraduate year 1 to 565.9 by postgraduate year 4, reaching 63.2%.; Four board-certified emergency physicians manually reviewed 500 random encounters and compared model classifications against expert consensus. | 43/420 (10.24%) model classifications disagreed with expert consensus in the validation sample; Data were deidentified and processed in a secure compliant environment, with no identifiable information leaving the repository and IRB approval.; Manual validation on 500 random encounters reported agreement with expert consensus and substantial physician interrater reliability. | N/A | agreement 89.76%; Exposure and acuity metrics increased across postgraduate years, but discriminant validity of a formal assessment score was not explicitly tested. | N/A |
| @Zhou2025-fj | N/A | In source-classification, GPT-3.5 had 50.0% accuracy, 18.2% recall for AI-generated comments, and 81.8% specificity for human-written comments, indicating frequent source-misattribution errors; Manual de-identification and Presidio PII removal were performed before AI processing, and ethics board approval was reported. | N/A | κ = 0.502; Fleiss' κ = -0.237; accuracy 80.5% | N/A |
| @furey-2025 | Prompt used direct growth mindset rewriting instructions, and classification relied on a pre-developed growth-versus-fixed language codebook.; Blinded coding evaluated growth-versus-fixed construct coverage, and 10 outputs with altered context or content were excluded.; Two blinded surgical resident reviewers trained in growth mindset language reviewed statements, with original coder adjudication for disagreements. | authors reported AI context/content fabrication; 10 of 42 AI-generated outputs were excluded because content/context changed and could add untrue information; Deidentified feedback statements were used, and the institutional review board classified the project as nonhuman-subject research.; Manual edits removed problematic presenter terminology, and substantially altered AI outputs were excluded before final dataset assembly. | N/A | sensitivity 75% | N/A |
| Total (N = 13) | 12/13 | 13/13 | 2/13 | 12/13 | 4/13 |

*Notes.* N/A = no evidence reported. Column headers correspond to the five sources of validity evidence in Downing's framework.
κ = Cohen's or Fleiss' kappa; ICC = intraclass correlation coefficient; F1 = F1 score; r = Pearson correlation coefficient; OR = odds ratio; β = standardized regression coefficient.
