# Supplementary Table S3. Validity Evidence Sub-item Distribution (with Individual Study Citations)

| Validity Source | Evidence Type | Description | Studies with Evidence | n |
|----------------|--------------|------------|---------------------|---|
| Content | Prompt-rubric alignment | How the AI prompt was designed in relation to the assessment rubric or competency framework, and whether alignment was checked. | @Jarry-Trujillo2024-kg; @ahmad-2025; @atsukawa-2025; @bala-2025; @bany_abdelnabi-2025; @Kondo2025-jx; @kwan-2025; @lyo-2025; @Partin2025-nj; @preiksaitis-2025; @furey-2025 | 11 |
| Content | Content coverage | Whether the study evaluated whether the AI adequately covered the intended assessment domains or competencies. | @Gin2024-ss; @Jarry-Trujillo2024-kg; @ahmad-2025; @atsukawa-2025; @bala-2025; @Kondo2025-jx; @kwan-2025; @lyo-2025; @Partin2025-nj; @preiksaitis-2025; @furey-2025 | 11 |
| Content | Expert review of content | Whether expert review was conducted on the AI's output for content appropriateness. | @Gin2024-ss; @Jarry-Trujillo2024-kg; @ahmad-2025; @atsukawa-2025; @bala-2025; @kwan-2025; @lyo-2025; @preiksaitis-2025; @furey-2025 | 9 |
| Response Process | AI reasoning transparency | Whether the study examined or reported on the quality of the AI's reasoning process (e.g., chain-of-thought, rationale for scores). Using chain-of-thought prompting alone is not sufficient; the study must evaluate the reasoning output. | none | 0 |
| Response Process | Hallucination assessment | Whether the study assessed or reported instances of AI-generated content not grounded in the source data (hallucinations or fabricated information), including false-positive analysis as an indirect proxy. | @Jarry-Trujillo2024-kg; @ahmad-2025; @atsukawa-2025; @bala-2025; @bany_abdelnabi-2025; @Kondo2025-jx; @lyo-2025; @preiksaitis-2025; @Zhou2025-fj; @furey-2025 | 10 |
| Response Process | Data security and privacy | Whether data security, patient privacy, or de-identification procedures related to AI processing were addressed. | @Gin2024-ss; @ahmad-2025; @atsukawa-2025; @bala-2025; @bany_abdelnabi-2025; @Kondo2025-jx; @kwan-2025; @lyo-2025; @Partin2025-nj; @preiksaitis-2025; @Zhou2025-fj; @furey-2025 | 12 |
| Response Process | Quality assurance procedures | Whether procedures for checking or ensuring the quality of AI outputs before use were described. | @Gin2024-ss; @Jarry-Trujillo2024-kg; @ahmad-2025; @atsukawa-2025; @bala-2025; @Kondo2025-jx; @kwan-2025; @lyo-2025; @Partin2025-nj; @preiksaitis-2025; @furey-2025 | 11 |
| Internal Structure | Reproducibility (test-retest) | Whether the same input was processed multiple times to assess output consistency, with quantitative metrics such as ICC, kappa, or percentage agreement. This is particularly relevant for LLMs, whose probabilistic generation means identical prompts can yield different outputs across runs. | @lyo-2025 | 1 |
| Internal Structure | Inter-model agreement | Whether different AI models were compared on the same data with agreement metrics reported. | none | 0 |
| Internal Structure | Internal consistency | Whether internal consistency of AI scoring across multiple assessment dimensions was assessed, such as whether items measuring the same construct yielded correlated scores. | none | 0 |
| Internal Structure | Bias and fairness | Whether the study examined whether AI assessment outputs differed systematically across demographic groups (gender, race/ethnicity, language background, training level) in a way that constitutes statistical bias, including DIF-equivalent analyses or subgroup comparisons. | @Gin2024-ss | 1 |
| Relationship to Other Variables | AI-human agreement | Whether agreement between AI and human assessments was quantified using metrics such as kappa, ICC, correlation, percentage agreement, or sensitivity/specificity. | @Gin2024-ss; @Jarry-Trujillo2024-kg; @ahmad-2025; @atsukawa-2025; @bala-2025; @Kondo2025-jx; @kwan-2025; @lyo-2025; @Partin2025-nj; @preiksaitis-2025; @Zhou2025-fj; @furey-2025 | 12 |
| Relationship to Other Variables | Discriminant ability | Whether the study assessed whether AI could distinguish between different learner performance levels or known groups (e.g., high vs. low performers, different PGY levels). | @Gin2024-ss; @Jarry-Trujillo2024-kg; @atsukawa-2025; @Partin2025-nj; @preiksaitis-2025 | 5 |
| Consequences | Impact on learner performance | Whether the study measured whether AI-based assessment improved learner performance, learning, or feedback uptake. | @bany_abdelnabi-2025 | 1 |
| Consequences | Stakeholder acceptability | Whether learner or faculty perceptions of AI assessment were reported, including satisfaction or trust measures. | @Jarry-Trujillo2024-kg; @bala-2025; @bany_abdelnabi-2025; @lyo-2025 | 4 |
| Consequences | Unintended consequences | Whether unintended consequences, risks, or ethical concerns of AI use in assessment were identified based on empirical data. | @bala-2025; @bany_abdelnabi-2025 | 2 |
