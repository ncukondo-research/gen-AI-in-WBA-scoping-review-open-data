# Supplementary Table S1. Characteristics of Included Studies

| Study | Country | Specialty | Design | Participants | Data Analyzed | AI Models | AI Role | WBA Tool | Customization | Key Findings |
|-------|---------|-----------|--------|-------------|---------------|-----------|---------|----------|---------------|--------------|
| @Gin2024-ss | US | General | Retrospective, quantitative | 552 trainees; 4,926 supervisors | 24,187 feedback dialogs/narratives | USE; BERT-PubMed | Feedback analysis; Sentiment and bias analysis | Narrative feedback; EPA (O-SCORE) | Custom-trained† | LLM-based NLP analyzed 24,187 feedback narratives for themes and sentiment, finding trainee-written narratives had 5.3% lower positive sentiment than supervisor-written ones, yet bias affected emotive language more than entrustment ratings. |
| @Jarry-Trujillo2024-kg | Chile | Surgery | Cross-sectional, quantitative | — | 20 surgical scenarios | ChatGPT-4 | Feedback generation; Error identification | Scenario-based assessment | No | ChatGPT 4.0 identified surgical errors and generated feedback from written laparoscopic cholecystectomy scenarios at rates comparable to experienced surgeons, with 96.43% resident-rated usefulness and median feedback quality of 8/10. |
| @ahmad-2025 | US | Otolaryngology-Head and Neck Surgery | Retrospective, quantitative | Residents | 836 feedback entries | GPT-4o | Feedback analysis; Competency mapping | SIMPL-OR; OSATS; EOR | No | ChatGPT-4o coded 836 narrative feedback entries from three assessment modalities in OHNS, achieving 90% concordance (kappa 0.94) with faculty and revealing significant cross-modality differences in feedback quality and competency coverage. |
| @atsukawa-2025 | Japan | Radiology | Retrospective, quantitative | Radiology residents (n = 9) | 7,376 reports | GPT-4o; Claude 3.5 Sonnet; Claude 3 Opus | Scoring; Report comparison; Longitudinal tracking | Radiology report review | No | GPT-4o scored radiology resident reports on six revision criteria with substantial radiologist agreement (kappa 0.67-0.74 for C1, C2, C6) and detected significant skill improvement over time for C1-C3 (P < 0.001, P = 0.023, P = 0.004). |
| @bala-2025 | US | Diagnostic Radiology | Retrospective, mixed methods | Radiology residents | 18,749 report pairs | GPT-4 | Report comparison; Feedback generation | Radiology report review | No | GPT-4 identified missed diagnoses in radiology resident preliminary reports, achieving 79.2% prediction accuracy and 79.2% sensitivity on 10 RPR4 test reports, with residents rating mean satisfaction 3.50/5. |
| @bany_abdelnabi-2025 | US | General | Prospective, mixed methods | Students (n = 100) | Document-level | GPT-4; GPT-3.5 | Feedback generation | H&P note review | No | GPT-3.5/GPT-4 provided section-by-section feedback on M3 students' H&P notes; 44% were very satisfied and 76% found feedback relevant, though 38% encountered hallucinations and 51% noted prompt-sensitivity issues. |
| @Kondo2025-jx | Japan | General | Retrospective, quantitative | Students (n = 20) | 40 datasets | GPT-4 Turbo; Gemini (unspecified version); Claude (unspecified version) | Experience tracking | Learning logs | No | GPT-4 Turbo extracted clinical clerkship experiences from learning logs against MCC goals, achieving moderate agreement with student-corrected lists (Jaccard 0.59, Cohen kappa 0.65) with high specificity (99.34%) but moderate sensitivity (62.39%). |
| @kwan-2025 | Canada | Surgery | Retrospective, quantitative | Residents | 2,229 narrative feedback comments | GPT-3.5; GPT-4; GPT-3.5 (fine-tuned) | Scoring; Feedback analysis | EPA; EPA narrative feedback; QuAL scoring | Fine-tuned | GPT-3.5 and GPT-4 automated QuAL scoring of EPA narrative feedback; fine-tuned GPT-3.5 achieved the highest F1 scores (Evidence 0.827, Suggestion 0.949, Connection 0.933) on the holdout test set. |
| @lyo-2025 | Not reported | Radiology; Neuroradiology | Retrospective, quantitative | — | 100 paired reports | GPT-4 Turbo | Report comparison; Scoring; Feedback generation | Radiology report review | No | GPT-4 Turbo compared paired preliminary and finalized radiology reports to detect and classify discrepancies, achieving moderate classification performance (weighted F1 0.64-0.66) and teaching-point relevance rated at 84.5%. |
| @Partin2025-nj | US | Family Medicine | Retrospective, quantitative | Family medicine residents (n = 24) | 24 faculty feedback datasets | GPT-4o-mini | Scoring; Competency mapping | Milestone assessment; Narrative feedback; Competency committee | No | ChatGPT 4o-mini assigned ACGME milestone levels from de-identified faculty narrative comments for 24 family medicine residents, showing strong correlations with CCC ratings in 15 of 16 domains (mean difference 0.58). |
| @preiksaitis-2025 | US | Emergency Medicine | Retrospective, quantitative | EM residents (n = 62) | 244,255 emergency department encounters | Gemini 1.5 Flash | Experience tracking; Feedback analysis | Learning logs; EHR documentation | RAG | Retrieval-augmented NLP pipeline classified 244,255 ED encounters into 895 MCPEM topics, tracking resident clinical exposure from PGY1 (mean 376.7 topics) to PGY4 (565.9 topics), with 89.76% agreement with expert consensus. |
| @Zhou2025-fj | Canada | Radiology | Retrospective, quantitative | Radiology residents and faculty (n = 10) | 220 sentences | GPT-3.5 | Feedback generation; Feedback analysis | EPA; EPA narrative feedback | No | GPT-3.5 generated synthetic narrative feedback for radiology residents and attempted source differentiation; human raters achieved 80.5% accuracy versus 50.0% for GPT-3.5, with low AI-human agreement (Fleiss' kappa -0.237). |
| @furey-2025 | US (inferred) | Surgery | Retrospective, qualitative | Students; residents | 83 feedback statements | Google Help Me Write | Mindset transformation | Narrative feedback; SIMPL | No | Google Chrome's generative AI tool rewrote fixed mindset feedback into growth mindset language, with blinded reviewers classifying 94.4% (17/18) of AI-modified statements as growth mindset language; AI authorship detection sensitivity was 75% and specificity 65%. |

*Notes.* ACGME = Accreditation Council for Graduate Medical Education; BERT = Bidirectional Encoder Representations from Transformers; EHR = electronic health record; EOR = end-of-rotation; EPA = entrustable professional activity; F1 = F1 score; GPT = Generative Pre-trained Transformer; H&P = history and physical; O-SCORE = Ottawa Surgical Competency Operating Room Evaluation; OR = odds ratio; OSATS = Objective Structured Assessment of Technical Skills; QuAL = Quality of Assessment for Learning; RAG = retrieval-augmented generation; SIMPL = System for Improving and Measuring Procedural Learning; SIMPL-OR = System for Improving and Measuring Procedural Learning in the Operating Room; US = United States; USE = Universal Sentence Encoder; US (inferred) indicates country inferred from institutional affiliations but not explicitly stated in the article.
†Custom-trained denotes a purpose-built model trained on task-specific data, distinct from fine-tuning a pre-trained large language model.
